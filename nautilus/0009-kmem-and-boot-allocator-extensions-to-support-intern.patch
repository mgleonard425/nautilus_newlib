From 81f73dc0b97bb5c48eb94268ae27d0069413a5d2 Mon Sep 17 00:00:00 2001
From: Peter Dinda <pdinda@northwestern.edu>
Date: Fri, 21 Jul 2017 18:34:47 -0500
Subject: [PATCH 09/10] kmem and boot allocator extensions to support internal
 garbage collection

- boot allocator tracks range of its own allocations for use by kmem
- kmem reports both the boot allocator range and its own range
  of private pointers to a garbage collector
- kmem supports block find/flag set/clear/map/etc to support
  a garbage collector or other tools
- kmem_malloc now optionally can zero allocated memory
  (particularly useful for a garbage collector to avoid false positives)
---
 include/nautilus/mm.h     |  35 +++++++++
 src/nautilus/mm/boot_mm.c |  38 +++++++++-
 src/nautilus/mm/kmem.c    | 179 ++++++++++++++++++++++++++++++++++++++++++++--
 3 files changed, 245 insertions(+), 7 deletions(-)

diff --git a/include/nautilus/mm.h b/include/nautilus/mm.h
index e98a0e4..6f1af9a 100644
--- a/include/nautilus/mm.h
+++ b/include/nautilus/mm.h
@@ -87,12 +87,37 @@ struct mem_region * kmem_get_base_zone(void);
 struct mem_region * kmem_get_region_by_addr(ulong_t addr);
 void kmem_add_memory(struct mem_region * mem, ulong_t base_addr, size_t size);
 
+// this the range of heap addresses used by the boot allocator [low,high)
+void kmem_inform_boot_allocation(void *low, void *high);
+
 // These functions operate the core memory allocator directly
 // You want to use the malloc()/free() wrappers defined below 
 // unless you know  what you are doing
 void * kmem_malloc(size_t size);
+void * kmem_mallocz(size_t size);
 void   kmem_free(void * addr);
 
+// Support functions for garbage collection
+// We currently assume these are done with the world stopped,
+// hence no locking
+
+// find the matching block that contains addr and its flags
+// returns nonzero if the addr is invalid or within no allocated block
+// user flags are allocate from low bit up, while kmem's flags are allocated
+// high bit down
+int  kmem_find_block(void *any_addr, void **block_addr, uint64_t *block_size, uint64_t *flags);
+// set the flags of an allocated block
+int  kmem_set_block_flags(void *block_addr, uint64_t flags);
+// apply an mask to all the blocks (and mask unless or=1)
+int  kmem_mask_all_blocks_flags(uint64_t mask, int or);
+
+// range of addresses used for internal kmem state that should be
+// ignored when pointer-chasing the heap, for example in a GC
+void kmem_get_internal_pointer_range(void **start, void **end);
+
+// check to see if the masked flags match the given flags
+int  kmem_apply_to_matching_blocks(uint64_t mask, uint64_t flags, int (*func)(void *block, void *state), void *state);
+
 int  kmem_sanity_check();
 
 #ifdef NAUT_CONFIG_ENABLE_BDWGC
@@ -105,9 +130,19 @@ void * GC_malloc(size_t);
 #endif
 #define free(a) 
 #else
+#ifdef NAUT_CONFIG_ENABLE_PDSGC
+void *nk_gc_pdsgc_malloc(uint64_t);
+#define malloc(s) nk_gc_pdsgc_malloc(s)
+#ifdef NAUT_CONFIG_EXPLICIT_ONLY_PDSGC
+#define free(s) kmem_free(s)
+#else
+#define free(s) 
+#endif
+#else
 #define malloc(s) kmem_malloc(s)
 #define free(a) kmem_free(a)
 #endif
+#endif
 
 
 /* arch specific */
diff --git a/src/nautilus/mm/boot_mm.c b/src/nautilus/mm/boot_mm.c
index 512c464..f484f6f 100644
--- a/src/nautilus/mm/boot_mm.c
+++ b/src/nautilus/mm/boot_mm.c
@@ -263,6 +263,30 @@ mm_boot_free_vmem (addr_t start, ulong_t size)
 }
 
 
+#define ALIGN_CEIL(x,a) ( (x)%(a) ? ((x)/(a) + 1) * (a) : (x) )
+
+static addr_t addr_low=-1;
+static addr_t addr_high=0;
+
+static void update_boot_range(addr_t start, addr_t end)
+{
+    if (start<addr_low) { 
+	addr_low = start;
+    }
+    if (end>addr_high) {
+	addr_high = end;
+    }
+    
+    BMM_DEBUG("Boot range is now %p-%p\n",addr_low,addr_high);
+}
+
+
+// Get the highest allocated address - for kmem.c
+void *boot_mm_get_cur_top()
+{
+    return (void*) addr_high;
+}
+
 /*
  * this is our main boot memory allocator, based on a simple 
  * bitmap scan. 
@@ -376,7 +400,12 @@ found:
     BMM_DEBUG("Allocated %d bytes, alignment %d (%d pages) at %p\n", size, align, areasize, ret);
 
     /* NOTE: we do NOT zero the memory! */
-    return (void*)pa_to_va((ulong_t)ret);
+    addr_t addr = pa_to_va((ulong_t)ret);
+
+
+    update_boot_range(addr,ALIGN_CEIL(addr+size,align));
+
+    return (void*)addr;
 }
 
 
@@ -476,6 +505,7 @@ add_free_pages (struct mem_region * region)
     return count*PAGE_SIZE;
 }
 
+
 /*
  * this makes the transfer to the kmem allocator, 
  * we won't be using the boot bitmap allocator anymore
@@ -514,6 +544,7 @@ mm_boot_kmem_init (void)
 
     BMM_PRINT("    =======\n");
     BMM_PRINT("    [TOTAL] (%lu.%lu MB)\n", count/1000000, count%1000000);
+
 }
 
 void 
@@ -549,4 +580,9 @@ mm_boot_kmem_cleanup (void)
 
     BMM_PRINT("    =======\n");
     BMM_PRINT("    [TOTAL] (%lu.%lu MB)\n", count/1000000, count%1000000);
+
+    BMM_PRINT("    Boot allocation range: %p-%p\n",(void*)addr_low, (void*)addr_high);
+
+    kmem_inform_boot_allocation((void*)addr_low,(void*)addr_high);
+   
 }
diff --git a/src/nautilus/mm/kmem.c b/src/nautilus/mm/kmem.c
index a5f30ff..12e4f5d 100644
--- a/src/nautilus/mm/kmem.c
+++ b/src/nautilus/mm/kmem.c
@@ -98,6 +98,7 @@ struct kmem_block_hdr {
     void *   addr;   /* address of block */
     uint64_t order;  /* order of the block allocated from buddy system */
     struct buddy_mempool * zone; /* zone to which this block belongs */
+    uint64_t flags;  /* flags for this allocated block */
 } __packed;
 
 
@@ -211,6 +212,7 @@ static inline void block_hash_free_entry(struct kmem_block_hdr *b)
 {
   b->addr = 0;
   b->zone = 0;
+  b->flags = 0;
   __sync_fetch_and_and (&b->order,0);
 }
 
@@ -337,6 +339,10 @@ kmem_add_memory (struct mem_region * mem,
     kmem_bytes_managed += chunk_size*num_chunks;
 }
 
+void *boot_mm_get_cur_top();
+
+static void *kmem_private_start;
+static void *kmem_private_end;
 
 /* 
  * initializes the kernel memory pools based on previously 
@@ -352,6 +358,8 @@ nk_kmem_init (void)
     uint64_t total_mem=0;
     uint64_t total_phys_mem=0;
     
+    kmem_private_start = boot_mm_get_cur_top();
+
     /* initialize the global zone list */
     INIT_LIST_HEAD(&glob_zone_list);
 
@@ -386,7 +394,7 @@ nk_kmem_init (void)
         list_for_each_entry(mem, &loc_dom->regions, entry) {
             struct mem_reg_entry * newent = mm_boot_alloc(sizeof(struct mem_reg_entry));
             if (!newent) {
-                ERROR_PRINT("Could not allocate mem region entry\n");
+                KMEM_ERROR("Could not allocate mem region entry\n");
                 return -1;
             }
             newent->mem = mem;
@@ -436,23 +444,46 @@ nk_kmem_init (void)
       return -1;
     }
 
+
+    // the assumption here is that no further boot_mm allocations will
+    // be made by kmem from this point on
+    kmem_private_end = boot_mm_get_cur_top();
+
     return 0;
 }
 
 
+// A fake header representing the boot allocations
+static void     *boot_start;
+static void     *boot_end;
+static uint64_t  boot_flags;
+
+void kmem_inform_boot_allocation(void *low, void *high)
+{
+    KMEM_DEBUG("Handling boot range %p-%p\n", low, high);
+    boot_start = low;
+    boot_end = high;
+    boot_flags = 0;
+    KMEM_PRINT("   boot range: %p-%p   kmem private: %p-%p\n",
+ 	       low, high, kmem_private_start, kmem_private_end);
+}
+
+
 /**
  * Allocates memory from the kernel memory pool. This will return a memory
- * region that is at least 16-byte aligned. The memory returned is zeroed.
+ * region that is at least 16-byte aligned. The memory returned is 
+ * optionally zeroed.
  *
  * Arguments:
  *       [IN] size: Amount of memory to allocate in bytes.
+ *       [IN] zero: Whether to zero the whole allocated block
  *
  * Returns:
  *       Success: Pointer to the start of the allocated memory.
  *       Failure: NULL
  */
-void *
-kmem_malloc (size_t size)
+static void *
+_kmem_malloc (size_t size, int zero)
 {
     void *block = 0;
     struct kmem_block_hdr *hdr = NULL;
@@ -461,7 +492,7 @@ kmem_malloc (size_t size)
     cpu_id_t my_id = my_cpu_id();
     struct kmem_data * my_kmem = &(nk_get_nautilus_info()->sys.cpus[my_id]->kmem);
 
-    KMEM_DEBUG("malloc of %lu bytes from:\n",size);
+    KMEM_DEBUG("malloc of %lu bytes (zero=%d) from:\n",size,zero);
     KMEM_DEBUG_BACKTRACE();
 
 #if SANITY_CHECK_PER_OP
@@ -515,7 +546,11 @@ kmem_malloc (size_t size)
     }
 
     KMEM_DEBUG("malloc succeeded: size %lu order %lu -> 0x%lx\n",size, order, block);
-      
+ 
+    if (zero) { 
+	memset(block,0,1ULL << hdr->order);
+    }
+     
 #if SANITY_CHECK_PER_OP
     if (kmem_sanity_check()) { 
 	panic("KMEM HAS GONE INSANE AFTER MALLOC\n");
@@ -528,6 +563,17 @@ kmem_malloc (size_t size)
 }
 
 
+void *kmem_malloc(size_t size)
+{
+    return _kmem_malloc(size,0);
+}
+
+void *kmem_mallocz(size_t size)
+{
+    return _kmem_malloc(size,1);
+}
+
+
 /**
  * Frees memory previously allocated with kmem_alloc().
  *
@@ -656,4 +702,125 @@ int kmem_sanity_check()
 
     return rc;
 }
+
+
+void  kmem_get_internal_pointer_range(void **start, void **end)
+{
+    *start = kmem_private_start;
+    *end = kmem_private_end;
+}
+
+int  kmem_find_block(void *any_addr, void **block_addr, uint64_t *block_size, uint64_t *flags)
+{
+    uint64_t i;
+    uint64_t order;
+    addr_t   zone_base;
+    uint64_t zone_min_order;
+    uint64_t zone_max_order;
+    addr_t   any_offset;
+    struct mem_region *reg;
+
+    if (!(reg = kmem_get_region_by_addr((addr_t)any_addr))) {
+	// not in any region we manage
+	return -1;
+    }
+
+    if (any_addr>=boot_start && any_addr<boot_end) { 
+	// in some boot_mm allocation that we treat as a single block
+	*block_addr = boot_start;
+	*block_size = boot_end-boot_start;
+	*flags = boot_flags;
+	KMEM_DEBUG("Search of %p found boot block (%p-%p)\n", any_addr, boot_start, boot_end);
+	return 0;
+    }
+
+    zone_base = reg->mm_state->base_addr;
+    zone_min_order = reg->mm_state->min_order;
+    zone_max_order = reg->mm_state->pool_order;
+
+    any_offset = (addr_t)any_addr - (addr_t)zone_base;
+    
+    for (order=zone_min_order;order<=zone_max_order;order++) {
+	addr_t mask = ~((1ULL << order)-1);
+	void *search_addr = (void*)(zone_base + (any_offset & mask));
+	struct kmem_block_hdr *hdr = block_hash_find_entry(search_addr);
+	// must exist and must be allocated
+	if (hdr && hdr->order) { 
+	    *block_addr = search_addr;
+	    *block_size = 0x1ULL<<hdr->order;
+	    *flags = hdr->flags;
+	    return 0;
+	    
+	}
+    }
+    return -1;
+}
+
+
+// set the flags of an allocated block
+int  kmem_set_block_flags(void *block_addr, uint64_t flags)
+{
+    if (block_addr>=boot_start && block_addr<boot_end) { 
+	boot_flags = flags;
+	return 0;
+
+    } else {
+
+	struct kmem_block_hdr *h =  block_hash_find_entry(block_addr);
+	
+	if (!h) { 
+	    return -1;
+	} else {
+	    h->flags = flags;
+	    return 0;
+	}
+    }
+}
+
+// applies only to allocated blocks
+int  kmem_mask_all_blocks_flags(uint64_t mask, int or)
+{
+    uint64_t i;
+
+    if (!or) { 
+	boot_flags &= mask;
+	for (i=0;i<block_hash_num_entries;i++) { 
+	    if (block_hash_entries[i].order) { 
+		block_hash_entries[i].flags &= mask;
+	    }
+	}
+    } else {
+	boot_flags |= mask;
+	for (i=0;i<block_hash_num_entries;i++) { 
+	    if (block_hash_entries[i].order) { 
+		block_hash_entries[i].flags |= mask;
+	    }
+	}
+    }
+
+    return 0;
+}
+    
+int  kmem_apply_to_matching_blocks(uint64_t mask, uint64_t flags, int (*func)(void *block, void *state), void *state)
+{
+    uint64_t i;
+    
+    if (((boot_flags & mask) == flags)) {
+	if (func(boot_start,state)) { 
+	    return -1;
+	}
+    }
+
+    for (i=0;i<block_hash_num_entries;i++) { 
+	if (block_hash_entries[i].order) { 
+	    if ((block_hash_entries[i].flags & mask) == flags) {
+		if (func(block_hash_entries[i].addr,state)) { 
+		    return -1;
+		}
+	    }
+	}
+    } 
+    
+    return 0;
+}
     
-- 
1.9.1

