From 3529c4e6b3d4b6e83cb2c11c4c3609b6ef865431 Mon Sep 17 00:00:00 2001
From: Matt George <11georgem@gmail.com>
Date: Fri, 7 Jul 2017 14:46:12 -0500
Subject: [PATCH 01/10] Port of Boehm-Demers-Weiser conservative garbage
 collector to Nautilus kernel and initial integration work.  This is an
 optionally configurable feature.

This brings BDWG into the kernel, and does sufficient integration to make it
possible to run the basic test suite on a single core.

Minor, compile-time configurable additions are made to the core codebase:

- add GC init to kernel startup
- add symbols to demarcate kernel globals as potential roots for GC
- add GC state initialization to threads as they are created
- add GC test call to shell
---
 Kconfig                                            |   53 +-
 include/gc/bdwgc/bdwgc.h                           |   40 +
 include/nautilus/naut_string.h                     |    3 +-
 include/nautilus/naut_types.h                      |    3 +
 include/nautilus/scheduler.h                       |    5 +
 include/nautilus/thread.h                          |    4 +
 link/nautilus.ld                                   |    6 +-
 link/nautilus.ld.hrt                               |    6 +
 link/nautilus.ld.palacios                          |    4 +
 link/nautilus.ld.xeon_phi                          |    5 +
 src/Makefile                                       |    1 +
 src/arch/x64/init.c                                |   11 +
 src/gc/Makefile                                    |    1 +
 src/gc/bdwgc/Makefile                              |   43 +
 src/gc/bdwgc/allchblk.c                            |  898 ++
 src/gc/bdwgc/alloc.c                               | 1363 +++
 src/gc/bdwgc/alpha_mach_dep.S                      |   86 +
 src/gc/bdwgc/atomic_ops_malloc.h                   |   44 +
 src/gc/bdwgc/atomic_ops_stack.h                    |  188 +
 src/gc/bdwgc/backgraph.c                           |  483 +
 src/gc/bdwgc/bdwgc.c                               |  145 +
 src/gc/bdwgc/blacklst.c                            |  289 +
 src/gc/bdwgc/checksums.c                           |  228 +
 src/gc/bdwgc/darwin_stop_world.c                   |  667 ++
 src/gc/bdwgc/dbg_mlc.c                             | 1214 +++
 src/gc/bdwgc/extra/AmigaOS.c                       |  623 ++
 src/gc/bdwgc/extra/MacOS.c                         |  156 +
 src/gc/bdwgc/extra/add_gc_prefix.c                 |   24 +
 src/gc/bdwgc/extra/gc.c                            |   82 +
 src/gc/bdwgc/extra/gcname.c                        |   17 +
 src/gc/bdwgc/extra/if_mach.c                       |   25 +
 src/gc/bdwgc/extra/if_not_there.c                  |   38 +
 src/gc/bdwgc/extra/msvc_dbg.c                      |  381 +
 src/gc/bdwgc/extra/setjmp_t.c                      |  145 +
 src/gc/bdwgc/extra/threadlibs.c                    |   83 +
 src/gc/bdwgc/finalize.c                            |  948 ++
 src/gc/bdwgc/gc_dlopen.c                           |  103 +
 src/gc/bdwgc/gcj_mlc.c                             |  278 +
 src/gc/bdwgc/headers.c                             |  403 +
 src/gc/bdwgc/include/atomic_ops.h                  |  389 +
 src/gc/bdwgc/include/bdwgc_internal.h              |  223 +
 src/gc/bdwgc/include/cord.h                        |  326 +
 src/gc/bdwgc/include/ec.h                          |   70 +
 src/gc/bdwgc/include/extra/gc.h                    |    2 +
 src/gc/bdwgc/include/extra/gc_cpp.h                |    2 +
 src/gc/bdwgc/include/gc.h                          | 1513 +++
 src/gc/bdwgc/include/gc_allocator.h                |  325 +
 src/gc/bdwgc/include/gc_amiga_redirects.h          |   30 +
 src/gc/bdwgc/include/gc_backptr.h                  |   98 +
 src/gc/bdwgc/include/gc_config_macros.h            |  349 +
 src/gc/bdwgc/include/gc_cpp.h                      |  406 +
 src/gc/bdwgc/include/gc_gcj.h                      |  109 +
 src/gc/bdwgc/include/gc_inline.h                   |  146 +
 src/gc/bdwgc/include/gc_mark.h                     |  231 +
 src/gc/bdwgc/include/gc_pthread_redirects.h        |   94 +
 src/gc/bdwgc/include/gc_tiny_fl.h                  |   90 +
 src/gc/bdwgc/include/gc_typed.h                    |  114 +
 src/gc/bdwgc/include/gc_version.h                  |   45 +
 src/gc/bdwgc/include/include.am                    |   59 +
 src/gc/bdwgc/include/javaxfc.h                     |   45 +
 src/gc/bdwgc/include/leak_detector.h               |   53 +
 src/gc/bdwgc/include/new_gc_alloc.h                |  483 +
 src/gc/bdwgc/include/private/config.h.in           |  237 +
 src/gc/bdwgc/include/private/cord_pos.h            |  118 +
 src/gc/bdwgc/include/private/darwin_semaphore.h    |   85 +
 src/gc/bdwgc/include/private/darwin_stop_world.h   |   46 +
 src/gc/bdwgc/include/private/dbg_mlc.h             |  166 +
 src/gc/bdwgc/include/private/gc_hdrs.h             |  204 +
 src/gc/bdwgc/include/private/gc_locks.h            |  247 +
 src/gc/bdwgc/include/private/gc_pmark.h            |  468 +
 src/gc/bdwgc/include/private/gc_priv.h             | 2415 +++++
 src/gc/bdwgc/include/private/gcconfig.h            | 2892 ++++++
 src/gc/bdwgc/include/private/msvc_dbg.h            |   69 +
 src/gc/bdwgc/include/private/pthread_stop_world.h  |   44 +
 src/gc/bdwgc/include/private/pthread_support.h     |  148 +
 src/gc/bdwgc/include/private/specific.h            |   98 +
 src/gc/bdwgc/include/private/thread_local_alloc.h  |  178 +
 src/gc/bdwgc/include/test.h                        |   15 +
 src/gc/bdwgc/include/weakpointer.h                 |  221 +
 src/gc/bdwgc/libatomic_ops/Makefile                |   21 +
 src/gc/bdwgc/libatomic_ops/atomic_ops.c            |  270 +
 src/gc/bdwgc/libatomic_ops/atomic_ops.h            |  389 +
 src/gc/bdwgc/libatomic_ops/atomic_ops/Makefile     |    4 +
 .../libatomic_ops/atomic_ops/generalize-small.h    | 1797 ++++
 .../atomic_ops/generalize-small.template           |  599 ++
 src/gc/bdwgc/libatomic_ops/atomic_ops/generalize.h | 1254 +++
 .../bdwgc/libatomic_ops/atomic_ops/sysdeps/README  |    7 +
 .../atomic_ops/sysdeps/acquire_release_volatile.h  |   62 +
 .../atomic_ops/sysdeps/aligned_atomic_load_store.h |   42 +
 .../sysdeps/all_acquire_release_volatile.h         |   31 +
 .../sysdeps/all_aligned_atomic_load_store.h        |   31 +
 .../atomic_ops/sysdeps/all_atomic_load_store.h     |   31 +
 .../libatomic_ops/atomic_ops/sysdeps/ao_t_is_int.h |  126 +
 .../atomic_ops/sysdeps/armcc/arm_v6.h              |  234 +
 .../atomic_ops/sysdeps/atomic_load_store.h         |   40 +
 .../sysdeps/char_acquire_release_volatile.h        |   53 +
 .../atomic_ops/sysdeps/char_atomic_load_store.h    |   40 +
 .../libatomic_ops/atomic_ops/sysdeps/emul_cas.h    |   76 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/alpha.h   |   63 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/arm.h     |  358 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/avr32.h   |   65 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/cris.h    |   69 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/hexagon.h |   98 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/hppa.h    |   95 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/ia64.h    |  285 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/m68k.h    |   67 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/mips.h    |  102 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/powerpc.h |  287 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/s390.h    |   63 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/sh.h      |   32 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/sparc.h   |   70 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/x86.h     |  229 +
 .../libatomic_ops/atomic_ops/sysdeps/gcc/x86_64.h  |  175 +
 .../atomic_ops/sysdeps/generic_pthread.h           |  269 +
 .../libatomic_ops/atomic_ops/sysdeps/hpc/hppa.h    |  100 +
 .../libatomic_ops/atomic_ops/sysdeps/hpc/ia64.h    |  166 +
 .../atomic_ops/sysdeps/ibmc/powerpc.h              |  124 +
 .../libatomic_ops/atomic_ops/sysdeps/icc/ia64.h    |  224 +
 .../sysdeps/int_acquire_release_volatile.h         |   53 +
 .../sysdeps/int_aligned_atomic_load_store.h        |   42 +
 .../atomic_ops/sysdeps/int_atomic_load_store.h     |   40 +
 .../libatomic_ops/atomic_ops/sysdeps/msftc/arm.h   |   90 +
 .../atomic_ops/sysdeps/msftc/common32_defs.h       |  118 +
 .../libatomic_ops/atomic_ops/sysdeps/msftc/x86.h   |  120 +
 .../atomic_ops/sysdeps/msftc/x86_64.h              |  158 +
 .../libatomic_ops/atomic_ops/sysdeps/ordered.h     |   33 +
 .../atomic_ops/sysdeps/ordered_except_wr.h         |   91 +
 .../atomic_ops/sysdeps/read_ordered.h              |   91 +
 .../sysdeps/short_acquire_release_volatile.h       |   53 +
 .../sysdeps/short_aligned_atomic_load_store.h      |   44 +
 .../atomic_ops/sysdeps/short_atomic_load_store.h   |   42 +
 .../atomic_ops/sysdeps/standard_ao_double_t.h      |   27 +
 .../libatomic_ops/atomic_ops/sysdeps/sunc/sparc.S  |    5 +
 .../libatomic_ops/atomic_ops/sysdeps/sunc/sparc.h  |   37 +
 .../libatomic_ops/atomic_ops/sysdeps/sunc/x86.h    |  164 +
 .../libatomic_ops/atomic_ops/sysdeps/sunc/x86_64.h |  171 +
 .../atomic_ops/sysdeps/test_and_set_t_is_ao_t.h    |   36 +
 .../atomic_ops/sysdeps/test_and_set_t_is_char.h    |   35 +
 src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.c     |  321 +
 src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.h     |   44 +
 src/gc/bdwgc/libatomic_ops/atomic_ops_stack.c      |  310 +
 src/gc/bdwgc/libatomic_ops/atomic_ops_stack.h      |  188 +
 src/gc/bdwgc/libatomic_ops/atomic_ops_sysdeps.S    |    9 +
 src/gc/bdwgc/libatomic_ops/config.h.in             |   70 +
 src/gc/bdwgc/ltmain.sh                             | 9661 ++++++++++++++++++++
 src/gc/bdwgc/mach_dep.c                            |  308 +
 src/gc/bdwgc/malloc.c                              |  579 ++
 src/gc/bdwgc/mallocx.c                             |  612 ++
 src/gc/bdwgc/mark.c                                | 1914 ++++
 src/gc/bdwgc/mark_rts.c                            |  786 ++
 src/gc/bdwgc/misc.c                                | 1943 ++++
 src/gc/bdwgc/naut_stop_world.c                     |  116 +
 src/gc/bdwgc/naut_threads.c                        |  361 +
 src/gc/bdwgc/new_hblk.c                            |  193 +
 src/gc/bdwgc/obj_map.c                             |   90 +
 src/gc/bdwgc/os_dep.c                              | 4773 ++++++++++
 src/gc/bdwgc/pcr_interface.c                       |  179 +
 src/gc/bdwgc/ptr_chck.c                            |  278 +
 src/gc/bdwgc/reclaim.c                             |  631 ++
 src/gc/bdwgc/sparc_mach_dep.S                      |   61 +
 src/gc/bdwgc/specific.c                            |  162 +
 src/gc/bdwgc/stubborn.c                            |   59 +
 src/gc/bdwgc/tests/CMakeLists.txt                  |   19 +
 src/gc/bdwgc/tests/Makefile                        |   16 +
 src/gc/bdwgc/tests/huge_test.c                     |   54 +
 src/gc/bdwgc/tests/initsecondarythread.c           |  100 +
 src/gc/bdwgc/tests/leak_test.c                     |   44 +
 src/gc/bdwgc/tests/middle.c                        |   25 +
 src/gc/bdwgc/tests/realloc_test.c                  |   38 +
 src/gc/bdwgc/tests/setjmp_t.c                      |  147 +
 src/gc/bdwgc/tests/smash_test.c                    |   28 +
 src/gc/bdwgc/tests/staticrootslib.c                |   33 +
 src/gc/bdwgc/tests/staticrootstest.c               |   57 +
 src/gc/bdwgc/tests/test.c                          | 1677 ++++
 src/gc/bdwgc/tests/test_cpp.cc                     |  303 +
 src/gc/bdwgc/tests/thread_leak_test.c              |   86 +
 src/gc/bdwgc/tests/threadkey_test.c                |   99 +
 src/gc/bdwgc/tests/trace_test.c                    |   41 +
 src/gc/bdwgc/thread_local_alloc.c                  |  217 +
 src/gc/bdwgc/typd_mlc.c                            |  726 ++
 src/gc/bdwgc/unused/README.QUICK                   |   88 +
 src/gc/bdwgc/unused/aclocal.m4                     | 1400 +++
 src/gc/bdwgc/unused/bdw-gc.pc.in                   |   10 +
 src/gc/bdwgc/unused/configure.ac                   |  817 ++
 src/gc/bdwgc/unused/configure.host                 |   61 +
 src/gc/bdwgc/unused/digimars.mak                   |   90 +
 src/gc/bdwgc/unused/dyn_load.c                     | 1482 +++
 src/gc/bdwgc/unused/gc.mak                         | 2219 +++++
 src/gc/bdwgc/unused/gc_cpp.cc                      |   79 +
 src/gc/bdwgc/unused/gc_cpp.cpp                     |    2 +
 src/gc/bdwgc/unused/real_malloc.c                  |   44 +
 src/gc/bdwgc/unused/win32_threads.c                | 2774 ++++++
 src/nautilus/naut_string.c                         |    9 +-
 src/nautilus/scheduler.c                           |   25 +-
 src/nautilus/shell.c                               |   13 +-
 src/nautilus/thread.c                              |   15 +
 196 files changed, 67919 insertions(+), 14 deletions(-)
 create mode 100644 include/gc/bdwgc/bdwgc.h
 create mode 100644 src/gc/Makefile
 create mode 100644 src/gc/bdwgc/Makefile
 create mode 100644 src/gc/bdwgc/allchblk.c
 create mode 100644 src/gc/bdwgc/alloc.c
 create mode 100644 src/gc/bdwgc/alpha_mach_dep.S
 create mode 100644 src/gc/bdwgc/atomic_ops_malloc.h
 create mode 100644 src/gc/bdwgc/atomic_ops_stack.h
 create mode 100644 src/gc/bdwgc/backgraph.c
 create mode 100644 src/gc/bdwgc/bdwgc.c
 create mode 100644 src/gc/bdwgc/blacklst.c
 create mode 100644 src/gc/bdwgc/checksums.c
 create mode 100644 src/gc/bdwgc/darwin_stop_world.c
 create mode 100644 src/gc/bdwgc/dbg_mlc.c
 create mode 100644 src/gc/bdwgc/extra/AmigaOS.c
 create mode 100644 src/gc/bdwgc/extra/MacOS.c
 create mode 100644 src/gc/bdwgc/extra/add_gc_prefix.c
 create mode 100644 src/gc/bdwgc/extra/gc.c
 create mode 100644 src/gc/bdwgc/extra/gcname.c
 create mode 100644 src/gc/bdwgc/extra/if_mach.c
 create mode 100644 src/gc/bdwgc/extra/if_not_there.c
 create mode 100644 src/gc/bdwgc/extra/msvc_dbg.c
 create mode 100644 src/gc/bdwgc/extra/setjmp_t.c
 create mode 100644 src/gc/bdwgc/extra/threadlibs.c
 create mode 100644 src/gc/bdwgc/finalize.c
 create mode 100644 src/gc/bdwgc/gc_dlopen.c
 create mode 100644 src/gc/bdwgc/gcj_mlc.c
 create mode 100644 src/gc/bdwgc/headers.c
 create mode 100644 src/gc/bdwgc/include/atomic_ops.h
 create mode 100644 src/gc/bdwgc/include/bdwgc_internal.h
 create mode 100644 src/gc/bdwgc/include/cord.h
 create mode 100644 src/gc/bdwgc/include/ec.h
 create mode 100644 src/gc/bdwgc/include/extra/gc.h
 create mode 100644 src/gc/bdwgc/include/extra/gc_cpp.h
 create mode 100644 src/gc/bdwgc/include/gc.h
 create mode 100644 src/gc/bdwgc/include/gc_allocator.h
 create mode 100644 src/gc/bdwgc/include/gc_amiga_redirects.h
 create mode 100644 src/gc/bdwgc/include/gc_backptr.h
 create mode 100644 src/gc/bdwgc/include/gc_config_macros.h
 create mode 100644 src/gc/bdwgc/include/gc_cpp.h
 create mode 100644 src/gc/bdwgc/include/gc_gcj.h
 create mode 100644 src/gc/bdwgc/include/gc_inline.h
 create mode 100644 src/gc/bdwgc/include/gc_mark.h
 create mode 100644 src/gc/bdwgc/include/gc_pthread_redirects.h
 create mode 100644 src/gc/bdwgc/include/gc_tiny_fl.h
 create mode 100644 src/gc/bdwgc/include/gc_typed.h
 create mode 100644 src/gc/bdwgc/include/gc_version.h
 create mode 100644 src/gc/bdwgc/include/include.am
 create mode 100644 src/gc/bdwgc/include/javaxfc.h
 create mode 100644 src/gc/bdwgc/include/leak_detector.h
 create mode 100644 src/gc/bdwgc/include/new_gc_alloc.h
 create mode 100644 src/gc/bdwgc/include/private/config.h.in
 create mode 100644 src/gc/bdwgc/include/private/cord_pos.h
 create mode 100644 src/gc/bdwgc/include/private/darwin_semaphore.h
 create mode 100644 src/gc/bdwgc/include/private/darwin_stop_world.h
 create mode 100644 src/gc/bdwgc/include/private/dbg_mlc.h
 create mode 100644 src/gc/bdwgc/include/private/gc_hdrs.h
 create mode 100644 src/gc/bdwgc/include/private/gc_locks.h
 create mode 100644 src/gc/bdwgc/include/private/gc_pmark.h
 create mode 100644 src/gc/bdwgc/include/private/gc_priv.h
 create mode 100644 src/gc/bdwgc/include/private/gcconfig.h
 create mode 100644 src/gc/bdwgc/include/private/msvc_dbg.h
 create mode 100644 src/gc/bdwgc/include/private/pthread_stop_world.h
 create mode 100644 src/gc/bdwgc/include/private/pthread_support.h
 create mode 100644 src/gc/bdwgc/include/private/specific.h
 create mode 100644 src/gc/bdwgc/include/private/thread_local_alloc.h
 create mode 100644 src/gc/bdwgc/include/test.h
 create mode 100644 src/gc/bdwgc/include/weakpointer.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/Makefile
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops.c
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/Makefile
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.template
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/generalize.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/README
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/acquire_release_volatile.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/aligned_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_acquire_release_volatile.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_aligned_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ao_t_is_int.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/armcc/arm_v6.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_acquire_release_volatile.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/emul_cas.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/alpha.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/arm.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/avr32.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/cris.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hexagon.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hppa.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/ia64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/m68k.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/mips.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/powerpc.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/s390.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sh.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sparc.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86_64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/generic_pthread.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/hppa.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/ia64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ibmc/powerpc.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/icc/ia64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_acquire_release_volatile.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_aligned_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/arm.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/common32_defs.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86_64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered_except_wr.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/read_ordered.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_acquire_release_volatile.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_aligned_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_atomic_load_store.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/standard_ao_double_t.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.S
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86_64.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_ao_t.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_char.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.c
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops_stack.c
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops_stack.h
 create mode 100644 src/gc/bdwgc/libatomic_ops/atomic_ops_sysdeps.S
 create mode 100644 src/gc/bdwgc/libatomic_ops/config.h.in
 create mode 100644 src/gc/bdwgc/ltmain.sh
 create mode 100644 src/gc/bdwgc/mach_dep.c
 create mode 100644 src/gc/bdwgc/malloc.c
 create mode 100644 src/gc/bdwgc/mallocx.c
 create mode 100644 src/gc/bdwgc/mark.c
 create mode 100644 src/gc/bdwgc/mark_rts.c
 create mode 100644 src/gc/bdwgc/misc.c
 create mode 100644 src/gc/bdwgc/naut_stop_world.c
 create mode 100644 src/gc/bdwgc/naut_threads.c
 create mode 100644 src/gc/bdwgc/new_hblk.c
 create mode 100644 src/gc/bdwgc/obj_map.c
 create mode 100644 src/gc/bdwgc/os_dep.c
 create mode 100644 src/gc/bdwgc/pcr_interface.c
 create mode 100644 src/gc/bdwgc/ptr_chck.c
 create mode 100644 src/gc/bdwgc/reclaim.c
 create mode 100644 src/gc/bdwgc/sparc_mach_dep.S
 create mode 100644 src/gc/bdwgc/specific.c
 create mode 100644 src/gc/bdwgc/stubborn.c
 create mode 100644 src/gc/bdwgc/tests/CMakeLists.txt
 create mode 100644 src/gc/bdwgc/tests/Makefile
 create mode 100644 src/gc/bdwgc/tests/huge_test.c
 create mode 100644 src/gc/bdwgc/tests/initsecondarythread.c
 create mode 100644 src/gc/bdwgc/tests/leak_test.c
 create mode 100644 src/gc/bdwgc/tests/middle.c
 create mode 100644 src/gc/bdwgc/tests/realloc_test.c
 create mode 100644 src/gc/bdwgc/tests/setjmp_t.c
 create mode 100644 src/gc/bdwgc/tests/smash_test.c
 create mode 100644 src/gc/bdwgc/tests/staticrootslib.c
 create mode 100644 src/gc/bdwgc/tests/staticrootstest.c
 create mode 100644 src/gc/bdwgc/tests/test.c
 create mode 100644 src/gc/bdwgc/tests/test_cpp.cc
 create mode 100644 src/gc/bdwgc/tests/thread_leak_test.c
 create mode 100644 src/gc/bdwgc/tests/threadkey_test.c
 create mode 100644 src/gc/bdwgc/tests/trace_test.c
 create mode 100644 src/gc/bdwgc/thread_local_alloc.c
 create mode 100644 src/gc/bdwgc/typd_mlc.c
 create mode 100644 src/gc/bdwgc/unused/README.QUICK
 create mode 100644 src/gc/bdwgc/unused/aclocal.m4
 create mode 100644 src/gc/bdwgc/unused/bdw-gc.pc.in
 create mode 100644 src/gc/bdwgc/unused/configure.ac
 create mode 100644 src/gc/bdwgc/unused/configure.host
 create mode 100644 src/gc/bdwgc/unused/digimars.mak
 create mode 100644 src/gc/bdwgc/unused/dyn_load.c
 create mode 100644 src/gc/bdwgc/unused/gc.mak
 create mode 100644 src/gc/bdwgc/unused/gc_cpp.cc
 create mode 100644 src/gc/bdwgc/unused/gc_cpp.cpp
 create mode 100644 src/gc/bdwgc/unused/real_malloc.c
 create mode 100644 src/gc/bdwgc/unused/win32_threads.c

diff --git a/Kconfig b/Kconfig
index 9f450fb..cb15a7a 100644
--- a/Kconfig
+++ b/Kconfig
@@ -86,8 +86,7 @@ menu "Platform/Arch Options"
           bool "4KB Pages"
 
     endchoice
-            
-        
+
     config PALACIOS
     	bool "Support for creating VMs using Palacios"
         depends on X86_64_HOST
@@ -204,6 +203,7 @@ config VIRTUAL_CONSOLE_SERIAL_MIRROR_ALL
         currently mirrored.
 
 
+
   menu "Scheduler Options"
 
     config UTILIZATION_LIMIT
@@ -403,7 +403,48 @@ config DEBUG_ISOCORE
    help
       Adds debugging output for isolated core execution
       The low-level code does not have debugging output
-   
+
+
+menu "Garbage Collection Options"
+
+   config GARBAGE_COLLECTION
+   bool "Enable Garbage Collection"
+   default n
+   help
+      If enabled, a conservative garbage collector is
+      used to garbage collect the kernel/HRT.  mallocs() pass to the
+      GC, frees() are ignored, and mallocs can trigger global GC
+
+   choice 
+    prompt "Select Garbage Collector"
+    depends on GARBAGE_COLLECTION
+    default ENABLE_BDWGC
+
+     config ENABLE_BDWGC
+      bool "BDWGC"
+      default n
+      depends on GARBAGE_COLLECTION
+      help
+        If enabled, the BDWGC garbage colector is used.
+
+   endchoice
+ 
+   config DEBUG_BDWGC
+       bool "Debug the BDWGC garbage collector"
+       default n
+       depends on ENABLE_BDWGC
+       help
+         If enabled, print debugging output from BDWGC
+
+  config TEST_BDWGC 
+   bool "Include the BDWGC garbage collection test suite"
+   default n
+   depends on ENABLE_BDWGC
+   help
+      Adddes test suite for BDWGC, which can be run from the shell
+
+ endmenu
+
 endmenu
 
 menu "AeroKernel Performance Optimizations"
@@ -711,15 +752,11 @@ menu "Parallel Runtime Integration"
           bool "No parallel runtime"
           
      endchoice
-        
+       
 
 endmenu
 
-
-
-
 source "src/dev/Kconfig"
 
 source "src/fs/Kconfig"
 
-
diff --git a/include/gc/bdwgc/bdwgc.h b/include/gc/bdwgc/bdwgc.h
new file mode 100644
index 0000000..0568cea
--- /dev/null
+++ b/include/gc/bdwgc/bdwgc.h
@@ -0,0 +1,40 @@
+/* 
+ * This file is part of the Nautilus AeroKernel developed
+ * by the Hobbes and V3VEE Projects with funding from the 
+ * United States National  Science Foundation and the Department of Energy.  
+ *
+ * The V3VEE Project is a joint project between Northwestern University
+ * and the University of New Mexico.  The Hobbes Project is a collaboration
+ * led by Sandia National Laboratories that includes several national 
+ * laboratories and universities. You can find out more at:
+ * http://www.v3vee.org  and
+ * http://xstack.sandia.gov/hobbes
+ *
+ * Copyright (c) 2017, Matt George <11georgem@gmail.com>
+ * Copyright (c) 2017, The V3VEE Project  <http://www.v3vee.org> 
+ *                     The Hobbes Project <http://xstack.sandia.gov/hobbes>
+ * All rights reserved.
+ *
+ * Authors:  Matt George <11georgem@gmail.com>
+ *
+ * This is free software.  You are permitted to use,
+ * redistribute, and modify it as specified in the file "LICENSE.txt".
+ */
+
+// This is a port of the Boehm garbage collector to
+// the Nautilus kernel
+
+#ifndef __BDWGC__
+#define __BDWGC__
+
+int  nk_gc_bdwgc_init();
+void nk_gc_bdwgc_deinit();
+
+void *nk_gc_bdwgc_thread_state_init(struct nk_thread *thread);
+void  nk_gc_bdwgc_thread_state_deinit(struct nk_thread *thread);
+
+#ifdef NAUT_CONFIG_TEST_BDWGC
+int  nk_gc_bdwgc_test();
+#endif
+
+#endif
diff --git a/include/nautilus/naut_string.h b/include/nautilus/naut_string.h
index 4b1c737..2f64346 100644
--- a/include/nautilus/naut_string.h
+++ b/include/nautilus/naut_string.h
@@ -8,7 +8,7 @@
  * led by Sandia National Laboratories that includes several national 
  * laboratories and universities. You can find out more at:
  * http://www.v3vee.org  and
- * http://xtack.sandia.gov/hobbes
+ * http://xstack.sandia.gov/hobbes
  *
  * Copyright (c) 2015, Kyle C. Hale <kh@u.northwestern.edu>
  * Copyright (c) 2015, The V3VEE Project  <http://www.v3vee.org> 
@@ -168,6 +168,7 @@ char * strstr (const char * haystack, const char * needle);
 int atoi (const char * buf);
 int strtoi (const char * nptr, char ** endptr);
 long strtol(const char * str, char ** endptr, int base);
+long atol(const char *nptr);
 uint64_t atox (const char * buf);
 uint64_t strtox (const char * nptr, char ** endptr);
 void str_toupper (char * s);
diff --git a/include/nautilus/naut_types.h b/include/nautilus/naut_types.h
index b77e5e6..de20e9b 100644
--- a/include/nautilus/naut_types.h
+++ b/include/nautilus/naut_types.h
@@ -64,6 +64,9 @@ typedef char          sint8_t;
 typedef ulong_t addr_t;
 typedef uchar_t bool_t;
 
+typedef unsigned long  uintptr_t;
+typedef long           intptr_t;
+
 //#define NULL ((void *)0)
 #ifndef NULL
 #define NULL 0
diff --git a/include/nautilus/scheduler.h b/include/nautilus/scheduler.h
index 106697e..1aeb914 100644
--- a/include/nautilus/scheduler.h
+++ b/include/nautilus/scheduler.h
@@ -173,6 +173,11 @@ void nk_sched_dump_cores(int cpu);
 // -1 => all CPUs
 void nk_sched_dump_time(int cpu);
 
+// map a functor over all threads on a cpu.
+// cpu==-means all cpus
+void nk_sched_map_threads(int cpu, void (func)(struct nk_thread *t, void *state), void *state);
+
+
 // Invoked by interrupt handler wrapper and other code
 // to cause thread context switches
 // Do not call this unless you know what you are doing
diff --git a/include/nautilus/thread.h b/include/nautilus/thread.h
index 81fc078..26434ba 100644
--- a/include/nautilus/thread.h
+++ b/include/nautilus/thread.h
@@ -199,6 +199,10 @@ struct nk_thread {
 
     struct nk_virtual_console *vc;
 
+#ifdef NAUT_CONFIG_GARBAGE_COLLECTION
+    void  *gc_state;
+#endif
+
     char name[MAX_THREAD_NAME];
 
     const void * tls[TLS_MAX_KEYS];
diff --git a/link/nautilus.ld b/link/nautilus.ld
index d9bfecb..2f7053c 100644
--- a/link/nautilus.ld
+++ b/link/nautilus.ld
@@ -56,7 +56,9 @@ SECTIONS
         *(.gcc_except_table*)
         *(.gnu.linkonce.gcc_except*)
     }
-    
+
+    _data_start = . ;    
+
     .data ALIGN(0x1000) : AT(ADDR(.gcc_except_table) + SIZEOF(.gcc_except_table))
     {
         *(.data*)
@@ -87,6 +89,8 @@ SECTIONS
     }
     
     _bssEnd = .; 
+
+    _data_end = .;
     
     /DISCARD/ :
     {
diff --git a/link/nautilus.ld.hrt b/link/nautilus.ld.hrt
index 7ed7d79..524cb72 100644
--- a/link/nautilus.ld.hrt
+++ b/link/nautilus.ld.hrt
@@ -45,6 +45,9 @@ SECTIONS
         *(.gcc_except_table*)
         *(.gnu.linkonce.gcc_except*)
     }
+
+    _data_start = .;
+
     .data ALIGN(0x1000) : AT(ADDR(.gcc_except_table) + SIZEOF(.gcc_except_table))
     {
         *(.data*)
@@ -68,6 +71,9 @@ SECTIONS
         *(.gnu.linkonce.b*)
     }
     _bssEnd = .;
+
+    _data_end = .;
+
     /DISCARD/ :
     {
         *(.comment)
diff --git a/link/nautilus.ld.palacios b/link/nautilus.ld.palacios
index cb9644d..5d38349 100644
--- a/link/nautilus.ld.palacios
+++ b/link/nautilus.ld.palacios
@@ -57,6 +57,8 @@ SECTIONS
         *(.gcc_except_table*)
         *(.gnu.linkonce.gcc_except*)
     }
+
+    _data_start = . ;    
     
     .data ALIGN(0x1000) : AT(ADDR(.gcc_except_table) + SIZEOF(.gcc_except_table))
     {
@@ -116,6 +118,8 @@ SECTIONS
     }
     
     _bssEnd = .; 
+
+    _data_end = .;
     
     /DISCARD/ :
     {
diff --git a/link/nautilus.ld.xeon_phi b/link/nautilus.ld.xeon_phi
index f7896bc..872e441 100644
--- a/link/nautilus.ld.xeon_phi
+++ b/link/nautilus.ld.xeon_phi
@@ -60,6 +60,9 @@ SECTIONS
         *(.gcc_except_table*)
         *(.gnu.linkonce.gcc_except*)
     }
+
+
+    _data_start = .;
     
     .data ALIGN(0x1000) : AT(ADDR(.gcc_except_table) + SIZEOF(.gcc_except_table))
     {
@@ -91,6 +94,8 @@ SECTIONS
     }
     
     _bssEnd = .; 
+
+    _data_end = .;
     
     /DISCARD/ :
     {
diff --git a/src/Makefile b/src/Makefile
index faef0c3..75285f9 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -4,6 +4,7 @@ obj-y += \
 	asm/ \
 	dev/ \
         fs/  \
+        gc/  \
 	acpi/ \
 	test/  
 
diff --git a/src/arch/x64/init.c b/src/arch/x64/init.c
index 9ac7445..bb7d124 100644
--- a/src/arch/x64/init.c
+++ b/src/arch/x64/init.c
@@ -93,6 +93,10 @@
 #include <dev/vesa.h>
 #endif
 
+#ifdef NAUT_CONFIG_ENABLE_BDWGC
+#include <gc/bdwgc/bdwgc.h>
+#endif
+
 extern spinlock_t printk_lock;
 
 
@@ -306,6 +310,11 @@ init (unsigned long mbd,
      * allocated in the boot mem allocator are kept reserved */
     mm_boot_kmem_init();
 
+#ifdef NAUT_CONFIG_ENABLE_BDWGC
+    // Bring up the garbage collector if enabled
+    nk_gc_bdwgc_init();
+#endif
+
     disable_8259pic();
 
     i8254_init(naut);
@@ -328,6 +337,7 @@ init (unsigned long mbd,
 
     pci_init(naut);
 
+
     nk_sched_init(&sched_cfg);
 
     /* we now switch away from the boot-time stack in low memory */
@@ -335,6 +345,7 @@ init (unsigned long mbd,
 
     mm_boot_kmem_cleanup();
 
+
     smp_setup_xcall_bsp(naut->sys.cpus[0]);
 
     nk_cpu_topo_discover(naut->sys.cpus[0]); 
diff --git a/src/gc/Makefile b/src/gc/Makefile
new file mode 100644
index 0000000..a2ba0f0
--- /dev/null
+++ b/src/gc/Makefile
@@ -0,0 +1 @@
+obj-$(NAUT_CONFIG_ENABLE_BDWGC) += bdwgc/
diff --git a/src/gc/bdwgc/Makefile b/src/gc/bdwgc/Makefile
new file mode 100644
index 0000000..2a18efd
--- /dev/null
+++ b/src/gc/bdwgc/Makefile
@@ -0,0 +1,43 @@
+CFLAGS += -Isrc/gc/bdwgc/include \
+	-DNAUT \
+	-Ulinux \
+	-U__linux__ \
+	-U__GNU__ \
+	-U__GLIBC__ \
+	-DNO_CLOCK \
+	-DSMALL_CONFIG \
+	-DGC_DISABLE_INCREMENTAL \
+	-DNO_GETCONTEXT \
+	-DNO_DEBUGGING
+
+obj-y += allchblk.o \
+	alloc.o \
+	backgraph.o \
+	blacklst.o \
+	checksums.o \
+	dbg_mlc.o \
+	finalize.o \
+	gc_dlopen.o \
+	gcj_mlc.o \
+	headers.o \
+	mach_dep.o \
+	malloc.o \
+	mallocx.o \
+	mark.o \
+	mark_rts.o \
+	misc.o \
+	new_hblk.o \
+	obj_map.o \
+	os_dep.o \
+	ptr_chck.o \
+	reclaim.o \
+	specific.o \
+	stubborn.o \
+	thread_local_alloc.o \
+	typd_mlc.o \
+	naut_threads.o \
+	naut_stop_world.o \
+	libatomic_ops/ \
+	bdwgc.o
+
+obj-$(NAUT_CONFIG_TEST_BDWGC) += tests/ 
diff --git a/src/gc/bdwgc/allchblk.c b/src/gc/bdwgc/allchblk.c
new file mode 100644
index 0000000..8669555
--- /dev/null
+++ b/src/gc/bdwgc/allchblk.c
@@ -0,0 +1,898 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1998-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+#ifdef GC_USE_ENTIRE_HEAP
+  int GC_use_entire_heap = TRUE;
+#else
+  int GC_use_entire_heap = FALSE;
+#endif
+
+/*
+ * Free heap blocks are kept on one of several free lists,
+ * depending on the size of the block.  Each free list is doubly linked.
+ * Adjacent free blocks are coalesced.
+ */
+
+
+# define MAX_BLACK_LIST_ALLOC (2*HBLKSIZE)
+                /* largest block we will allocate starting on a black   */
+                /* listed block.  Must be >= HBLKSIZE.                  */
+
+
+# define UNIQUE_THRESHOLD 32
+        /* Sizes up to this many HBLKs each have their own free list    */
+# define HUGE_THRESHOLD 256
+        /* Sizes of at least this many heap blocks are mapped to a      */
+        /* single free list.                                            */
+# define FL_COMPRESSION 8
+        /* In between sizes map this many distinct sizes to a single    */
+        /* bin.                                                         */
+
+# define N_HBLK_FLS (HUGE_THRESHOLD - UNIQUE_THRESHOLD)/FL_COMPRESSION \
+                                 + UNIQUE_THRESHOLD
+
+#ifndef GC_GCJ_SUPPORT
+  STATIC
+#endif
+  struct hblk * GC_hblkfreelist[N_HBLK_FLS+1] = { 0 };
+                                /* List of completely empty heap blocks */
+                                /* Linked through hb_next field of      */
+                                /* header structure associated with     */
+                                /* block.  Remains externally visible   */
+                                /* as used by GNU GCJ currently.        */
+
+#ifndef USE_MUNMAP
+
+#ifndef GC_GCJ_SUPPORT
+  STATIC
+#endif
+  word GC_free_bytes[N_HBLK_FLS+1] = { 0 };
+        /* Number of free bytes on each list.  Remains visible to GCJ.  */
+
+  /* Return the largest n such that                                     */
+  /* Is GC_large_allocd_bytes + the number of free bytes on lists       */
+  /* n .. N_HBLK_FLS > GC_max_large_allocd_bytes.                       */
+  /* If there is no such n, return 0.                                   */
+  GC_INLINE int GC_enough_large_bytes_left(void)
+  {
+    int n;
+    word bytes = GC_large_allocd_bytes;
+
+    GC_ASSERT(GC_max_large_allocd_bytes <= GC_heapsize);
+    for (n = N_HBLK_FLS; n >= 0; --n) {
+        bytes += GC_free_bytes[n];
+        if (bytes >= GC_max_large_allocd_bytes) return n;
+    }
+    return 0;
+  }
+
+# define INCR_FREE_BYTES(n, b) GC_free_bytes[n] += (b);
+
+# define FREE_ASSERT(e) GC_ASSERT(e)
+
+#else /* USE_MUNMAP */
+
+# define INCR_FREE_BYTES(n, b)
+# define FREE_ASSERT(e)
+
+#endif /* USE_MUNMAP */
+
+/* Map a number of blocks to the appropriate large block free list index. */
+STATIC int GC_hblk_fl_from_blocks(word blocks_needed)
+{
+    if (blocks_needed <= UNIQUE_THRESHOLD) return (int)blocks_needed;
+    if (blocks_needed >= HUGE_THRESHOLD) return N_HBLK_FLS;
+    return (int)(blocks_needed - UNIQUE_THRESHOLD)/FL_COMPRESSION
+                                        + UNIQUE_THRESHOLD;
+
+}
+
+# define PHDR(hhdr) HDR(hhdr -> hb_prev)
+# define NHDR(hhdr) HDR(hhdr -> hb_next)
+
+# ifdef USE_MUNMAP
+#   define IS_MAPPED(hhdr) (((hhdr) -> hb_flags & WAS_UNMAPPED) == 0)
+# else  /* !USE_MUNMAP */
+#   define IS_MAPPED(hhdr) 1
+# endif /* USE_MUNMAP */
+
+# if !defined(NO_DEBUGGING)
+void GC_print_hblkfreelist(void)
+{
+    struct hblk * h;
+    word total_free = 0;
+    hdr * hhdr;
+    word sz;
+    unsigned i;
+
+    for (i = 0; i <= N_HBLK_FLS; ++i) {
+      h = GC_hblkfreelist[i];
+#     ifdef USE_MUNMAP
+        if (0 != h) GC_printf("Free list %u:\n", i);
+#     else
+        if (0 != h) GC_printf("Free list %u (total size %lu):\n",
+                              i, (unsigned long)GC_free_bytes[i]);
+#     endif
+      while (h != 0) {
+        hhdr = HDR(h);
+        sz = hhdr -> hb_sz;
+        total_free += sz;
+        GC_printf("\t%p size %lu %s black listed\n", h, (unsigned long)sz,
+                GC_is_black_listed(h, HBLKSIZE) != 0 ? "start" :
+                GC_is_black_listed(h, hhdr -> hb_sz) != 0 ? "partially" :
+                                                        "not");
+        h = hhdr -> hb_next;
+      }
+    }
+#   ifndef USE_MUNMAP
+      if (total_free != GC_large_free_bytes) {
+        GC_printf("GC_large_free_bytes = %lu (INCONSISTENT!!)\n",
+                  (unsigned long) GC_large_free_bytes);
+      }
+#   endif
+    GC_printf("Total of %lu bytes on free list\n", (unsigned long)total_free);
+}
+
+/* Return the free list index on which the block described by the header */
+/* appears, or -1 if it appears nowhere.                                 */
+static int free_list_index_of(hdr *wanted)
+{
+    struct hblk * h;
+    hdr * hhdr;
+    int i;
+
+    for (i = 0; i <= N_HBLK_FLS; ++i) {
+      h = GC_hblkfreelist[i];
+      while (h != 0) {
+        hhdr = HDR(h);
+        if (hhdr == wanted) return i;
+        h = hhdr -> hb_next;
+      }
+    }
+    return -1;
+}
+
+void GC_dump_regions(void)
+{
+    unsigned i;
+    ptr_t start, end;
+    ptr_t p;
+    size_t bytes;
+    hdr *hhdr;
+    for (i = 0; i < GC_n_heap_sects; ++i) {
+        start = GC_heap_sects[i].hs_start;
+        bytes = GC_heap_sects[i].hs_bytes;
+        end = start + bytes;
+        /* Merge in contiguous sections.        */
+          while (i+1 < GC_n_heap_sects && GC_heap_sects[i+1].hs_start == end) {
+            ++i;
+            end = GC_heap_sects[i].hs_start + GC_heap_sects[i].hs_bytes;
+          }
+        GC_printf("***Section from %p to %p\n", start, end);
+        for (p = start; p < end;) {
+            hhdr = HDR(p);
+            if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+                GC_printf("\t%p Missing header!!(%p)\n", p, hhdr);
+                p += HBLKSIZE;
+                continue;
+            }
+            if (HBLK_IS_FREE(hhdr)) {
+                int correct_index = GC_hblk_fl_from_blocks(
+                                        divHBLKSZ(hhdr -> hb_sz));
+                int actual_index;
+
+                GC_printf("\t%p\tfree block of size 0x%lx bytes%s\n", p,
+                          (unsigned long)(hhdr -> hb_sz),
+                          IS_MAPPED(hhdr) ? "" : " (unmapped)");
+                actual_index = free_list_index_of(hhdr);
+                if (-1 == actual_index) {
+                    GC_printf("\t\tBlock not on free list %d!!\n",
+                              correct_index);
+                } else if (correct_index != actual_index) {
+                    GC_printf("\t\tBlock on list %d, should be on %d!!\n",
+                              actual_index, correct_index);
+                }
+                p += hhdr -> hb_sz;
+            } else {
+                GC_printf("\t%p\tused for blocks of size 0x%lx bytes\n", p,
+                          (unsigned long)(hhdr -> hb_sz));
+                p += HBLKSIZE * OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz);
+            }
+        }
+    }
+}
+
+# endif /* NO_DEBUGGING */
+
+/* Initialize hdr for a block containing the indicated size and         */
+/* kind of objects.                                                     */
+/* Return FALSE on failure.                                             */
+static GC_bool setup_header(hdr * hhdr, struct hblk *block, size_t byte_sz,
+                            int kind, unsigned flags)
+{
+    word descr;
+#   ifndef MARK_BIT_PER_OBJ
+      size_t granules;
+#   endif
+
+    /* Set size, kind and mark proc fields */
+      hhdr -> hb_sz = byte_sz;
+      hhdr -> hb_obj_kind = (unsigned char)kind;
+      hhdr -> hb_flags = (unsigned char)flags;
+      hhdr -> hb_block = block;
+      descr = GC_obj_kinds[kind].ok_descriptor;
+      if (GC_obj_kinds[kind].ok_relocate_descr) descr += byte_sz;
+      hhdr -> hb_descr = descr;
+
+#   ifdef MARK_BIT_PER_OBJ
+     /* Set hb_inv_sz as portably as possible.                          */
+     /* We set it to the smallest value such that sz * inv_sz > 2**32    */
+     /* This may be more precision than necessary.                      */
+      if (byte_sz > MAXOBJBYTES) {
+         hhdr -> hb_inv_sz = LARGE_INV_SZ;
+      } else {
+        word inv_sz;
+
+#       if CPP_WORDSZ == 64
+          inv_sz = ((word)1 << 32)/byte_sz;
+          if (((inv_sz*byte_sz) >> 32) == 0) ++inv_sz;
+#       else  /* 32 bit words */
+          GC_ASSERT(byte_sz >= 4);
+          inv_sz = ((unsigned)1 << 31)/byte_sz;
+          inv_sz *= 2;
+          while (inv_sz*byte_sz > byte_sz) ++inv_sz;
+#       endif
+        hhdr -> hb_inv_sz = inv_sz;
+      }
+#   else /* MARK_BIT_PER_GRANULE */
+      hhdr -> hb_large_block = (unsigned char)(byte_sz > MAXOBJBYTES);
+      granules = BYTES_TO_GRANULES(byte_sz);
+      if (EXPECT(!GC_add_map_entry(granules), FALSE)) {
+        /* Make it look like a valid block. */
+        hhdr -> hb_sz = HBLKSIZE;
+        hhdr -> hb_descr = 0;
+        hhdr -> hb_large_block = TRUE;
+        hhdr -> hb_map = 0;
+        return FALSE;
+      } else {
+        size_t index = (hhdr -> hb_large_block? 0 : granules);
+        hhdr -> hb_map = GC_obj_map[index];
+      }
+#   endif /* MARK_BIT_PER_GRANULE */
+
+    /* Clear mark bits */
+    GC_clear_hdr_marks(hhdr);
+
+    hhdr -> hb_last_reclaimed = (unsigned short)GC_gc_no;
+    return(TRUE);
+}
+
+#define FL_UNKNOWN -1
+/*
+ * Remove hhdr from the appropriate free list.
+ * We assume it is on the nth free list, or on the size
+ * appropriate free list if n is FL_UNKNOWN.
+ */
+STATIC void GC_remove_from_fl(hdr *hhdr, int n)
+{
+    int index;
+
+    GC_ASSERT(((hhdr -> hb_sz) & (HBLKSIZE-1)) == 0);
+#   ifndef USE_MUNMAP
+      /* We always need index to maintain free counts.  */
+      if (FL_UNKNOWN == n) {
+          index = GC_hblk_fl_from_blocks(divHBLKSZ(hhdr -> hb_sz));
+      } else {
+          index = n;
+      }
+#   endif
+    if (hhdr -> hb_prev == 0) {
+#       ifdef USE_MUNMAP
+          if (FL_UNKNOWN == n) {
+            index = GC_hblk_fl_from_blocks(divHBLKSZ(hhdr -> hb_sz));
+          } else {
+            index = n;
+          }
+#       endif
+        GC_ASSERT(HDR(GC_hblkfreelist[index]) == hhdr);
+        GC_hblkfreelist[index] = hhdr -> hb_next;
+    } else {
+        hdr *phdr;
+        GET_HDR(hhdr -> hb_prev, phdr);
+        phdr -> hb_next = hhdr -> hb_next;
+    }
+    FREE_ASSERT(GC_free_bytes[index] >= hhdr -> hb_sz);
+    INCR_FREE_BYTES(index, - (signed_word)(hhdr -> hb_sz));
+    if (0 != hhdr -> hb_next) {
+        hdr * nhdr;
+        GC_ASSERT(!IS_FORWARDING_ADDR_OR_NIL(NHDR(hhdr)));
+        GET_HDR(hhdr -> hb_next, nhdr);
+        nhdr -> hb_prev = hhdr -> hb_prev;
+    }
+}
+
+/*
+ * Return a pointer to the free block ending just before h, if any.
+ */
+STATIC struct hblk * GC_free_block_ending_at(struct hblk *h)
+{
+    struct hblk * p = h - 1;
+    hdr * phdr;
+
+    GET_HDR(p, phdr);
+    while (0 != phdr && IS_FORWARDING_ADDR_OR_NIL(phdr)) {
+        p = FORWARDED_ADDR(p,phdr);
+        phdr = HDR(p);
+    }
+    if (0 != phdr) {
+        if(HBLK_IS_FREE(phdr)) {
+            return p;
+        } else {
+            return 0;
+        }
+    }
+    p = GC_prev_block(h - 1);
+    if (0 != p) {
+      phdr = HDR(p);
+      if (HBLK_IS_FREE(phdr) && (ptr_t)p + phdr -> hb_sz == (ptr_t)h) {
+        return p;
+      }
+    }
+    return 0;
+}
+
+/*
+ * Add hhdr to the appropriate free list.
+ * We maintain individual free lists sorted by address.
+ */
+STATIC void GC_add_to_fl(struct hblk *h, hdr *hhdr)
+{
+    int index = GC_hblk_fl_from_blocks(divHBLKSZ(hhdr -> hb_sz));
+    struct hblk *second = GC_hblkfreelist[index];
+    hdr * second_hdr;
+#   if defined(GC_ASSERTIONS) && !defined(USE_MUNMAP)
+      struct hblk *next = (struct hblk *)((word)h + hhdr -> hb_sz);
+      hdr * nexthdr = HDR(next);
+      struct hblk *prev = GC_free_block_ending_at(h);
+      hdr * prevhdr = HDR(prev);
+      GC_ASSERT(nexthdr == 0 || !HBLK_IS_FREE(nexthdr)
+                || (signed_word)GC_heapsize < 0);
+                /* In the last case, blocks may be too large to merge. */
+      GC_ASSERT(prev == 0 || !HBLK_IS_FREE(prevhdr)
+                || (signed_word)GC_heapsize < 0);
+#   endif
+    GC_ASSERT(((hhdr -> hb_sz) & (HBLKSIZE-1)) == 0);
+    GC_hblkfreelist[index] = h;
+    INCR_FREE_BYTES(index, hhdr -> hb_sz);
+    FREE_ASSERT(GC_free_bytes[index] <= GC_large_free_bytes);
+    hhdr -> hb_next = second;
+    hhdr -> hb_prev = 0;
+    if (0 != second) {
+      GET_HDR(second, second_hdr);
+      second_hdr -> hb_prev = h;
+    }
+    hhdr -> hb_flags |= FREE_BLK;
+}
+
+#ifdef USE_MUNMAP
+
+#   ifndef MUNMAP_THRESHOLD
+#     define MUNMAP_THRESHOLD 6
+#   endif
+
+GC_INNER int GC_unmap_threshold = MUNMAP_THRESHOLD;
+
+/* Unmap blocks that haven't been recently touched.  This is the only way */
+/* way blocks are ever unmapped.                                          */
+GC_INNER void GC_unmap_old(void)
+{
+    struct hblk * h;
+    hdr * hhdr;
+    int i;
+
+    if (GC_unmap_threshold == 0)
+      return; /* unmapping disabled */
+
+    for (i = 0; i <= N_HBLK_FLS; ++i) {
+      for (h = GC_hblkfreelist[i]; 0 != h; h = hhdr -> hb_next) {
+        hhdr = HDR(h);
+        if (!IS_MAPPED(hhdr)) continue;
+
+        if ((unsigned short)GC_gc_no - hhdr -> hb_last_reclaimed >
+                (unsigned short)GC_unmap_threshold) {
+          GC_unmap((ptr_t)h, hhdr -> hb_sz);
+          hhdr -> hb_flags |= WAS_UNMAPPED;
+        }
+      }
+    }
+}
+
+/* Merge all unmapped blocks that are adjacent to other free            */
+/* blocks.  This may involve remapping, since all blocks are either     */
+/* fully mapped or fully unmapped.                                      */
+GC_INNER void GC_merge_unmapped(void)
+{
+    struct hblk * h, *next;
+    hdr * hhdr, *nexthdr;
+    word size, nextsize;
+    int i;
+
+    for (i = 0; i <= N_HBLK_FLS; ++i) {
+      h = GC_hblkfreelist[i];
+      while (h != 0) {
+        GET_HDR(h, hhdr);
+        size = hhdr->hb_sz;
+        next = (struct hblk *)((word)h + size);
+        GET_HDR(next, nexthdr);
+        /* Coalesce with successor, if possible */
+          if (0 != nexthdr && HBLK_IS_FREE(nexthdr)
+              && (signed_word) (size + (nextsize = nexthdr->hb_sz)) > 0
+                 /* no pot. overflow */) {
+            /* Note that we usually try to avoid adjacent free blocks   */
+            /* that are either both mapped or both unmapped.  But that  */
+            /* isn't guaranteed to hold since we remap blocks when we   */
+            /* split them, and don't merge at that point.  It may also  */
+            /* not hold if the merged block would be too big.           */
+            if (IS_MAPPED(hhdr) && !IS_MAPPED(nexthdr)) {
+              /* make both consistent, so that we can merge */
+                if (size > nextsize) {
+                  GC_remap((ptr_t)next, nextsize);
+                } else {
+                  GC_unmap((ptr_t)h, size);
+                  GC_unmap_gap((ptr_t)h, size, (ptr_t)next, nextsize);
+                  hhdr -> hb_flags |= WAS_UNMAPPED;
+                }
+            } else if (IS_MAPPED(nexthdr) && !IS_MAPPED(hhdr)) {
+              if (size > nextsize) {
+                GC_unmap((ptr_t)next, nextsize);
+                GC_unmap_gap((ptr_t)h, size, (ptr_t)next, nextsize);
+              } else {
+                GC_remap((ptr_t)h, size);
+                hhdr -> hb_flags &= ~WAS_UNMAPPED;
+                hhdr -> hb_last_reclaimed = nexthdr -> hb_last_reclaimed;
+              }
+            } else if (!IS_MAPPED(hhdr) && !IS_MAPPED(nexthdr)) {
+              /* Unmap any gap in the middle */
+                GC_unmap_gap((ptr_t)h, size, (ptr_t)next, nextsize);
+            }
+            /* If they are both unmapped, we merge, but leave unmapped. */
+            GC_remove_from_fl(hhdr, i);
+            GC_remove_from_fl(nexthdr, FL_UNKNOWN);
+            hhdr -> hb_sz += nexthdr -> hb_sz;
+            GC_remove_header(next);
+            GC_add_to_fl(h, hhdr);
+            /* Start over at beginning of list */
+            h = GC_hblkfreelist[i];
+          } else /* not mergable with successor */ {
+            h = hhdr -> hb_next;
+          }
+      } /* while (h != 0) ... */
+    } /* for ... */
+}
+
+#endif /* USE_MUNMAP */
+
+/*
+ * Return a pointer to a block starting at h of length bytes.
+ * Memory for the block is mapped.
+ * Remove the block from its free list, and return the remainder (if any)
+ * to its appropriate free list.
+ * May fail by returning 0.
+ * The header for the returned block must be set up by the caller.
+ * If the return value is not 0, then hhdr is the header for it.
+ */
+STATIC struct hblk * GC_get_first_part(struct hblk *h, hdr *hhdr,
+                                       size_t bytes, int index)
+{
+    word total_size = hhdr -> hb_sz;
+    struct hblk * rest;
+    hdr * rest_hdr;
+
+    GC_ASSERT((total_size & (HBLKSIZE-1)) == 0);
+    GC_remove_from_fl(hhdr, index);
+    if (total_size == bytes) return h;
+    rest = (struct hblk *)((word)h + bytes);
+    rest_hdr = GC_install_header(rest);
+    if (0 == rest_hdr) {
+        /* FIXME: This is likely to be very bad news ... */
+        WARN("Header allocation failed: Dropping block.\n", 0);
+        return(0);
+    }
+    rest_hdr -> hb_sz = total_size - bytes;
+    rest_hdr -> hb_flags = 0;
+#   ifdef GC_ASSERTIONS
+      /* Mark h not free, to avoid assertion about adjacent free blocks. */
+        hhdr -> hb_flags &= ~FREE_BLK;
+#   endif
+    GC_add_to_fl(rest, rest_hdr);
+    return h;
+}
+
+/*
+ * H is a free block.  N points at an address inside it.
+ * A new header for n has already been set up.  Fix up h's header
+ * to reflect the fact that it is being split, move it to the
+ * appropriate free list.
+ * N replaces h in the original free list.
+ *
+ * Nhdr is not completely filled in, since it is about to allocated.
+ * It may in fact end up on the wrong free list for its size.
+ * That's not a disaster, since n is about to be allocated
+ * by our caller.
+ * (Hence adding it to a free list is silly.  But this path is hopefully
+ * rare enough that it doesn't matter.  The code is cleaner this way.)
+ */
+STATIC void GC_split_block(struct hblk *h, hdr *hhdr, struct hblk *n,
+                           hdr *nhdr, int index /* Index of free list */)
+{
+    word total_size = hhdr -> hb_sz;
+    word h_size = (word)n - (word)h;
+    struct hblk *prev = hhdr -> hb_prev;
+    struct hblk *next = hhdr -> hb_next;
+
+    /* Replace h with n on its freelist */
+      nhdr -> hb_prev = prev;
+      nhdr -> hb_next = next;
+      nhdr -> hb_sz = total_size - h_size;
+      nhdr -> hb_flags = 0;
+      if (0 != prev) {
+        HDR(prev) -> hb_next = n;
+      } else {
+        GC_hblkfreelist[index] = n;
+      }
+      if (0 != next) {
+        HDR(next) -> hb_prev = n;
+      }
+      INCR_FREE_BYTES(index, -(signed_word)h_size);
+      FREE_ASSERT(GC_free_bytes[index] > 0);
+#   ifdef USE_MUNMAP
+      hhdr -> hb_last_reclaimed = (unsigned short)GC_gc_no;
+#   endif
+    hhdr -> hb_sz = h_size;
+    GC_add_to_fl(h, hhdr);
+    nhdr -> hb_flags |= FREE_BLK;
+}
+
+STATIC struct hblk *
+GC_allochblk_nth(size_t sz/* bytes */, int kind, unsigned flags, int n,
+                 GC_bool may_split);
+
+/*
+ * Allocate (and return pointer to) a heap block
+ *   for objects of size sz bytes, searching the nth free list.
+ *
+ * NOTE: We set obj_map field in header correctly.
+ *       Caller is responsible for building an object freelist in block.
+ *
+ * The client is responsible for clearing the block, if necessary.
+ */
+GC_INNER struct hblk *
+GC_allochblk(size_t sz, int kind, unsigned flags/* IGNORE_OFF_PAGE or 0 */)
+{
+    word blocks;
+    int start_list;
+    int i;
+    struct hblk *result;
+    int split_limit; /* Highest index of free list whose blocks we      */
+                     /* split.                                          */
+
+    GC_ASSERT((sz & (GRANULE_BYTES - 1)) == 0);
+    blocks = OBJ_SZ_TO_BLOCKS(sz);
+    if ((signed_word)(blocks * HBLKSIZE) < 0) {
+      return 0;
+    }
+    start_list = GC_hblk_fl_from_blocks(blocks);
+    /* Try for an exact match first. */
+    result = GC_allochblk_nth(sz, kind, flags, start_list, FALSE);
+    if (0 != result) return result;
+    if (GC_use_entire_heap || GC_dont_gc
+        || USED_HEAP_SIZE < GC_requested_heapsize
+        || GC_incremental || !GC_should_collect()) {
+        /* Should use more of the heap, even if it requires splitting. */
+        split_limit = N_HBLK_FLS;
+    } else {
+#     ifdef USE_MUNMAP
+        /* avoid splitting, since that might require remapping */
+        split_limit = 0;
+#     else
+        if (GC_finalizer_bytes_freed > (GC_heapsize >> 4)) {
+          /* If we are deallocating lots of memory from         */
+          /* finalizers, fail and collect sooner rather         */
+          /* than later.                                        */
+          split_limit = 0;
+        } else {
+          /* If we have enough large blocks left to cover any   */
+          /* previous request for large blocks, we go ahead     */
+          /* and split.  Assuming a steady state, that should   */
+          /* be safe.  It means that we can use the full        */
+          /* heap if we allocate only small objects.            */
+          split_limit = GC_enough_large_bytes_left();
+        }
+#     endif
+    }
+    if (start_list < UNIQUE_THRESHOLD) {
+      /* No reason to try start_list again, since all blocks are exact  */
+      /* matches.                                                       */
+      ++start_list;
+    }
+    for (i = start_list; i <= split_limit; ++i) {
+        struct hblk * result = GC_allochblk_nth(sz, kind, flags, i, TRUE);
+        if (0 != result) return result;
+    }
+    return 0;
+}
+
+STATIC long GC_large_alloc_warn_suppressed = 0;
+                        /* Number of warnings suppressed so far.        */
+
+/*
+ * The same, but with search restricted to nth free list.
+ * Flags is IGNORE_OFF_PAGE or zero.
+ * Unlike the above, sz is in bytes.
+ * The may_split flag indicates whether it's OK to split larger blocks.
+ */
+STATIC struct hblk *
+GC_allochblk_nth(size_t sz, int kind, unsigned flags, int n,
+                 GC_bool may_split)
+{
+    struct hblk *hbp;
+    hdr * hhdr;         /* Header corr. to hbp */
+                        /* Initialized after loop if hbp !=0    */
+                        /* Gcc uninitialized use warning is bogus.      */
+    struct hblk *thishbp;
+    hdr * thishdr;              /* Header corr. to hbp */
+    signed_word size_needed;    /* number of bytes in requested objects */
+    signed_word size_avail;     /* bytes available in this block        */
+
+    size_needed = HBLKSIZE * OBJ_SZ_TO_BLOCKS(sz);
+
+    /* search for a big enough block in free list */
+        hbp = GC_hblkfreelist[n];
+        for(; 0 != hbp; hbp = hhdr -> hb_next) {
+            GET_HDR(hbp, hhdr);
+            size_avail = hhdr->hb_sz;
+            if (size_avail < size_needed) continue;
+            if (size_avail != size_needed) {
+              signed_word next_size;
+
+              if (!may_split) continue;
+              /* If the next heap block is obviously better, go on.     */
+              /* This prevents us from disassembling a single large block */
+              /* to get tiny blocks.                                    */
+              thishbp = hhdr -> hb_next;
+              if (thishbp != 0) {
+                GET_HDR(thishbp, thishdr);
+                next_size = (signed_word)(thishdr -> hb_sz);
+                if (next_size < size_avail
+                    && next_size >= size_needed
+                    && !GC_is_black_listed(thishbp, (word)size_needed)) {
+                    continue;
+                }
+              }
+            }
+            if ( !IS_UNCOLLECTABLE(kind) && (kind != PTRFREE
+                        || size_needed > (signed_word)MAX_BLACK_LIST_ALLOC)) {
+              struct hblk * lasthbp = hbp;
+              ptr_t search_end = (ptr_t)hbp + size_avail - size_needed;
+              signed_word orig_avail = size_avail;
+              signed_word eff_size_needed = (flags & IGNORE_OFF_PAGE) != 0 ?
+                                                (signed_word)HBLKSIZE
+                                                : size_needed;
+
+
+              while ((ptr_t)lasthbp <= search_end
+                     && (thishbp = GC_is_black_listed(lasthbp,
+                                                      (word)eff_size_needed))
+                        != 0) {
+                lasthbp = thishbp;
+              }
+              size_avail -= (ptr_t)lasthbp - (ptr_t)hbp;
+              thishbp = lasthbp;
+              if (size_avail >= size_needed) {
+                if (thishbp != hbp &&
+                    0 != (thishdr = GC_install_header(thishbp))) {
+                  /* Make sure it's mapped before we mangle it. */
+#                   ifdef USE_MUNMAP
+                      if (!IS_MAPPED(hhdr)) {
+                        GC_remap((ptr_t)hbp, hhdr -> hb_sz);
+                        hhdr -> hb_flags &= ~WAS_UNMAPPED;
+                      }
+#                   endif
+                  /* Split the block at thishbp */
+                      GC_split_block(hbp, hhdr, thishbp, thishdr, n);
+                  /* Advance to thishbp */
+                      hbp = thishbp;
+                      hhdr = thishdr;
+                      /* We must now allocate thishbp, since it may     */
+                      /* be on the wrong free list.                     */
+                }
+              } else if (size_needed > (signed_word)BL_LIMIT
+                         && orig_avail - size_needed
+                            > (signed_word)BL_LIMIT) {
+                /* Punt, since anything else risks unreasonable heap growth. */
+                if (++GC_large_alloc_warn_suppressed
+                    >= GC_large_alloc_warn_interval) {
+                  WARN("Repeated allocation of very large block "
+                       "(appr. size %" GC_PRIdPTR "):\n"
+                       "\tMay lead to memory leak and poor performance.\n",
+                       size_needed);
+                  GC_large_alloc_warn_suppressed = 0;
+                }
+                size_avail = orig_avail;
+              } else if (size_avail == 0 && size_needed == HBLKSIZE
+                         && IS_MAPPED(hhdr)) {
+                if (!GC_find_leak) {
+                  static unsigned count = 0;
+
+                  /* The block is completely blacklisted.  We need      */
+                  /* to drop some such blocks, since otherwise we spend */
+                  /* all our time traversing them if pointer-free       */
+                  /* blocks are unpopular.                              */
+                  /* A dropped block will be reconsidered at next GC.   */
+                  if ((++count & 3) == 0) {
+                    /* Allocate and drop the block in small chunks, to  */
+                    /* maximize the chance that we will recover some    */
+                    /* later.                                           */
+                      word total_size = hhdr -> hb_sz;
+                      struct hblk * limit = hbp + divHBLKSZ(total_size);
+                      struct hblk * h;
+                      struct hblk * prev = hhdr -> hb_prev;
+
+                      GC_large_free_bytes -= total_size;
+                      GC_bytes_dropped += total_size;
+                      GC_remove_from_fl(hhdr, n);
+                      for (h = hbp; h < limit; h++) {
+                        if (h == hbp || 0 != (hhdr = GC_install_header(h))) {
+                          (void) setup_header(
+                                  hhdr, h,
+                                  HBLKSIZE,
+                                  PTRFREE, 0); /* Can't fail */
+                          if (GC_debugging_started) {
+                            BZERO(h, HBLKSIZE);
+                          }
+                        }
+                      }
+                    /* Restore hbp to point at free block */
+                      hbp = prev;
+                      if (0 == hbp) {
+                        return GC_allochblk_nth(sz, kind, flags, n, may_split);
+                      }
+                      hhdr = HDR(hbp);
+                  }
+                }
+              }
+            }
+            if( size_avail >= size_needed ) {
+#               ifdef USE_MUNMAP
+                  if (!IS_MAPPED(hhdr)) {
+                    GC_remap((ptr_t)hbp, hhdr -> hb_sz);
+                    hhdr -> hb_flags &= ~WAS_UNMAPPED;
+                    /* Note: This may leave adjacent, mapped free blocks. */
+                  }
+#               endif
+                /* hbp may be on the wrong freelist; the parameter n    */
+                /* is important.                                        */
+                hbp = GC_get_first_part(hbp, hhdr, size_needed, n);
+                break;
+            }
+        }
+
+    if (0 == hbp) return 0;
+
+    /* Add it to map of valid blocks */
+        if (!GC_install_counts(hbp, (word)size_needed)) return(0);
+        /* This leaks memory under very rare conditions. */
+
+    /* Set up header */
+        if (!setup_header(hhdr, hbp, sz, kind, flags)) {
+            GC_remove_counts(hbp, (word)size_needed);
+            return(0); /* ditto */
+        }
+#   ifndef GC_DISABLE_INCREMENTAL
+        /* Notify virtual dirty bit implementation that we are about to */
+        /* write.  Ensure that pointer-free objects are not protected   */
+        /* if it is avoidable.  This also ensures that newly allocated  */
+        /* blocks are treated as dirty.  Necessary since we don't       */
+        /* protect free blocks.                                         */
+        GC_ASSERT((size_needed & (HBLKSIZE-1)) == 0);
+        GC_remove_protection(hbp, divHBLKSZ(size_needed),
+                             (hhdr -> hb_descr == 0) /* pointer-free */);
+#   endif
+    /* We just successfully allocated a block.  Restart count of        */
+    /* consecutive failures.                                            */
+    GC_fail_count = 0;
+
+    GC_large_free_bytes -= size_needed;
+
+    GC_ASSERT(IS_MAPPED(hhdr));
+    return( hbp );
+}
+
+/*
+ * Free a heap block.
+ *
+ * Coalesce the block with its neighbors if possible.
+ *
+ * All mark words are assumed to be cleared.
+ */
+GC_INNER void GC_freehblk(struct hblk *hbp)
+{
+    struct hblk *next, *prev;
+    hdr *hhdr, *prevhdr, *nexthdr;
+    signed_word size;
+
+    GET_HDR(hbp, hhdr);
+    size = hhdr->hb_sz;
+    size = HBLKSIZE * OBJ_SZ_TO_BLOCKS(size);
+    if (size <= 0)
+      ABORT("Deallocating excessively large block.  Too large an allocation?");
+      /* Probably possible if we try to allocate more than half the address */
+      /* space at once.  If we don't catch it here, strange things happen   */
+      /* later.                                                             */
+    GC_remove_counts(hbp, (word)size);
+    hhdr->hb_sz = size;
+#   ifdef USE_MUNMAP
+      hhdr -> hb_last_reclaimed = (unsigned short)GC_gc_no;
+#   endif
+
+    /* Check for duplicate deallocation in the easy case */
+      if (HBLK_IS_FREE(hhdr)) {
+        if (GC_print_stats)
+          GC_log_printf("Duplicate large block deallocation of %p\n", hbp);
+        ABORT("Duplicate large block deallocation");
+      }
+
+    GC_ASSERT(IS_MAPPED(hhdr));
+    hhdr -> hb_flags |= FREE_BLK;
+    next = (struct hblk *)((word)hbp + size);
+    GET_HDR(next, nexthdr);
+    prev = GC_free_block_ending_at(hbp);
+    /* Coalesce with successor, if possible */
+      if(0 != nexthdr && HBLK_IS_FREE(nexthdr) && IS_MAPPED(nexthdr)
+         && (signed_word)(hhdr -> hb_sz + nexthdr -> hb_sz) > 0
+         /* no overflow */) {
+        GC_remove_from_fl(nexthdr, FL_UNKNOWN);
+        hhdr -> hb_sz += nexthdr -> hb_sz;
+        GC_remove_header(next);
+      }
+    /* Coalesce with predecessor, if possible. */
+      if (0 != prev) {
+        prevhdr = HDR(prev);
+        if (IS_MAPPED(prevhdr)
+            && (signed_word)(hhdr -> hb_sz + prevhdr -> hb_sz) > 0) {
+          GC_remove_from_fl(prevhdr, FL_UNKNOWN);
+          prevhdr -> hb_sz += hhdr -> hb_sz;
+#         ifdef USE_MUNMAP
+            prevhdr -> hb_last_reclaimed = (unsigned short)GC_gc_no;
+#         endif
+          GC_remove_header(hbp);
+          hbp = prev;
+          hhdr = prevhdr;
+        }
+      }
+    /* FIXME: It is not clear we really always want to do these merges  */
+    /* with USE_MUNMAP, since it updates ages and hence prevents        */
+    /* unmapping.                                                       */
+
+    GC_large_free_bytes += size;
+    GC_add_to_fl(hbp, hhdr);
+}
diff --git a/src/gc/bdwgc/alloc.c b/src/gc/bdwgc/alloc.c
new file mode 100644
index 0000000..6630850
--- /dev/null
+++ b/src/gc/bdwgc/alloc.c
@@ -0,0 +1,1363 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1996 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1998 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "private/gc_priv.h"
+
+#ifdef NAUT
+# include <nautilus/naut_types.h>
+# include "private/gc_locks.h"
+#else /** !NAUT */
+# include <stdio.h>
+# if !defined(MACOS) && !defined(MSWINCE)
+#   include <signal.h>
+#   if !defined(__CC_ARM) && !defined(NAUT)
+#     include <sys/types.h>
+#   endif
+# endif
+#endif
+
+/*
+ * Separate free lists are maintained for different sized objects
+ * up to MAXOBJBYTES.
+ * The call GC_allocobj(i,k) ensures that the freelist for
+ * kind k objects of size i points to a non-empty
+ * free list. It returns a pointer to the first entry on the free list.
+ * In a single-threaded world, GC_allocobj may be called to allocate
+ * an object of (small) size i as follows:
+ *
+ *            opp = &(GC_objfreelist[i]);
+ *            if (*opp == 0) GC_allocobj(i, NORMAL);
+ *            ptr = *opp;
+ *            *opp = obj_link(ptr);
+ *
+ * Note that this is very fast if the free list is non-empty; it should
+ * only involve the execution of 4 or 5 simple instructions.
+ * All composite objects on freelists are cleared, except for
+ * their first word.
+ */
+
+/*
+ * The allocator uses GC_allochblk to allocate large chunks of objects.
+ * These chunks all start on addresses which are multiples of
+ * HBLKSZ.   Each allocated chunk has an associated header,
+ * which can be located quickly based on the address of the chunk.
+ * (See headers.c for details.)
+ * This makes it possible to check quickly whether an
+ * arbitrary address corresponds to an object administered by the
+ * allocator.
+ */
+
+word GC_non_gc_bytes = 0;  /* Number of bytes not intended to be collected */
+
+word GC_gc_no = 0;
+
+#ifndef GC_DISABLE_INCREMENTAL
+  GC_INNER int GC_incremental = 0;      /* By default, stop the world.  */
+#endif
+
+#ifdef THREADS
+  int GC_parallel = FALSE;      /* By default, parallel GC is off.      */
+#endif
+
+#ifndef GC_FULL_FREQ
+# define GC_FULL_FREQ 19   /* Every 20th collection is a full   */
+                           /* collection, whether we need it    */
+                           /* or not.                           */
+#endif
+
+int GC_full_freq = GC_FULL_FREQ;
+
+STATIC GC_bool GC_need_full_gc = FALSE;
+                           /* Need full GC do to heap growth.   */
+
+#ifdef THREAD_LOCAL_ALLOC
+  GC_INNER GC_bool GC_world_stopped = FALSE;
+#endif
+
+STATIC word GC_used_heap_size_after_full = 0;
+
+/* GC_copyright symbol is externally visible. */
+char * const GC_copyright[] =
+{"Copyright 1988,1989 Hans-J. Boehm and Alan J. Demers ",
+"Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved. ",
+"Copyright (c) 1996-1998 by Silicon Graphics.  All rights reserved. ",
+"Copyright (c) 1999-2009 by Hewlett-Packard Company.  All rights reserved. ",
+"THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY",
+" EXPRESSED OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.",
+"See source code for details." };
+
+/* Version macros are now defined in gc_version.h, which is included by */
+/* gc.h, which is included by gc_priv.h.                                */
+#ifndef GC_NO_VERSION_VAR
+  const unsigned GC_version = ((GC_VERSION_MAJOR << 16) |
+                        (GC_VERSION_MINOR << 8) | GC_TMP_ALPHA_VERSION);
+#endif
+
+GC_API unsigned GC_CALL GC_get_version(void)
+{
+  return (GC_VERSION_MAJOR << 16) | (GC_VERSION_MINOR << 8) |
+          GC_TMP_ALPHA_VERSION;
+}
+
+/* some more variables */
+
+#ifdef GC_DONT_EXPAND
+  GC_bool GC_dont_expand = TRUE;
+#else
+  GC_bool GC_dont_expand = FALSE;
+#endif
+
+#ifndef GC_FREE_SPACE_DIVISOR
+# define GC_FREE_SPACE_DIVISOR 3 /* must be > 0 */
+#endif
+
+word GC_free_space_divisor = GC_FREE_SPACE_DIVISOR;
+
+GC_INNER int GC_CALLBACK GC_never_stop_func(void)
+{
+  return(0);
+}
+
+#ifndef GC_TIME_LIMIT
+# define GC_TIME_LIMIT 50  /* We try to keep pause times from exceeding  */
+                           /* this by much. In milliseconds.             */
+#endif
+
+unsigned long GC_time_limit = GC_TIME_LIMIT;
+
+#ifndef NO_CLOCK
+  STATIC CLOCK_TYPE GC_start_time = 0;
+                                /* Time at which we stopped world.      */
+                                /* used only in GC_timeout_stop_func.   */
+#endif
+
+STATIC int GC_n_attempts = 0;   /* Number of attempts at finishing      */
+                                /* collection within GC_time_limit.     */
+
+STATIC GC_stop_func GC_default_stop_func = GC_never_stop_func;
+                                /* accessed holding the lock.           */
+
+GC_API void GC_CALL GC_set_stop_func(GC_stop_func stop_func)
+{
+  DCL_LOCK_STATE;
+  GC_ASSERT(stop_func != 0);
+  LOCK();
+  GC_default_stop_func = stop_func;
+  UNLOCK();
+}
+
+GC_API GC_stop_func GC_CALL GC_get_stop_func(void)
+{
+  GC_stop_func stop_func;
+  DCL_LOCK_STATE;
+  LOCK();
+  stop_func = GC_default_stop_func;
+  UNLOCK();
+  return stop_func;
+}
+
+#if defined(GC_DISABLE_INCREMENTAL) || defined(NO_CLOCK)
+# define GC_timeout_stop_func GC_default_stop_func
+#else
+  STATIC int GC_CALLBACK GC_timeout_stop_func (void)
+  {
+    CLOCK_TYPE current_time;
+    static unsigned count = 0;
+    unsigned long time_diff;
+
+    if ((*GC_default_stop_func)())
+      return(1);
+
+    if ((count++ & 3) != 0) return(0);
+    GET_TIME(current_time);
+    time_diff = MS_TIME_DIFF(current_time,GC_start_time);
+    if (time_diff >= GC_time_limit) {
+        if (GC_print_stats) {
+          GC_log_printf(
+                "Abandoning stopped marking after %lu msecs (attempt %d)\n",
+                time_diff, GC_n_attempts);
+        }
+        return(1);
+    }
+    return(0);
+  }
+#endif /* !GC_DISABLE_INCREMENTAL */
+
+#ifdef THREADS
+  GC_INNER word GC_total_stacksize = 0; /* updated on every push_all_stacks */
+#endif
+
+/* Return the minimum number of bytes that must be allocated between    */
+/* collections to amortize the collection cost.  Should be non-zero.    */
+static word min_bytes_allocd(void)
+{
+    word result;
+#   ifdef STACK_GROWS_UP
+      word stack_size = GC_approx_sp() - GC_stackbottom;
+            /* GC_stackbottom is used only for a single-threaded case.  */
+#   else
+      word stack_size = GC_stackbottom - GC_approx_sp();
+#   endif
+
+    word total_root_size;       /* includes double stack size,  */
+                                /* since the stack is expensive */
+                                /* to scan.                     */
+    word scan_size;             /* Estimate of memory to be scanned     */
+                                /* during normal GC.                    */
+
+#   ifdef THREADS
+      if (GC_need_to_lock) {
+        /* We are multi-threaded... */
+        stack_size = GC_total_stacksize;
+        /* For now, we just use the value computed during the latest GC. */
+#       ifdef DEBUG_THREADS
+          GC_log_printf("Total stacks size: %lu\n",
+                        (unsigned long)stack_size);
+#       endif
+      }
+#   endif
+
+    total_root_size = 2 * stack_size + GC_root_size;
+    scan_size = 2 * GC_composite_in_use + GC_atomic_in_use / 4
+                + total_root_size;
+    result = scan_size / GC_free_space_divisor;
+    if (GC_incremental) {
+      result /= 2;
+    }
+    return result > 0 ? result : 1;
+}
+
+/* Return the number of bytes allocated, adjusted for explicit storage  */
+/* management, etc..  This number is used in deciding when to trigger   */
+/* collections.                                                         */
+STATIC word GC_adj_bytes_allocd(void)
+{
+    signed_word result;
+    signed_word expl_managed = (signed_word)GC_non_gc_bytes
+                                - (signed_word)GC_non_gc_bytes_at_gc;
+
+    /* Don't count what was explicitly freed, or newly allocated for    */
+    /* explicit management.  Note that deallocating an explicitly       */
+    /* managed object should not alter result, assuming the client      */
+    /* is playing by the rules.                                         */
+    result = (signed_word)GC_bytes_allocd
+             + (signed_word)GC_bytes_dropped
+             - (signed_word)GC_bytes_freed
+             + (signed_word)GC_finalizer_bytes_freed
+             - expl_managed;
+    if (result > (signed_word)GC_bytes_allocd) {
+        result = GC_bytes_allocd;
+        /* probably client bug or unfortunate scheduling */
+    }
+    result += GC_bytes_finalized;
+        /* We count objects enqueued for finalization as though they    */
+        /* had been reallocated this round. Finalization is user        */
+        /* visible progress.  And if we don't count this, we have       */
+        /* stability problems for programs that finalize all objects.   */
+    if (result < (signed_word)(GC_bytes_allocd >> 3)) {
+        /* Always count at least 1/8 of the allocations.  We don't want */
+        /* to collect too infrequently, since that would inhibit        */
+        /* coalescing of free storage blocks.                           */
+        /* This also makes us partially robust against client bugs.     */
+        return(GC_bytes_allocd >> 3);
+    } else {
+        return(result);
+    }
+}
+
+
+/* Clear up a few frames worth of garbage left at the top of the stack. */
+/* This is used to prevent us from accidentally treating garbage left   */
+/* on the stack by other parts of the collector as roots.  This         */
+/* differs from the code in misc.c, which actually tries to keep the    */
+/* stack clear of long-lived, client-generated garbage.                 */
+STATIC void GC_clear_a_few_frames(void)
+{
+#   ifndef CLEAR_NWORDS
+#     define CLEAR_NWORDS 64
+#   endif
+    volatile word frames[CLEAR_NWORDS];
+    BZERO((word *)frames, CLEAR_NWORDS * sizeof(word));
+}
+
+/* Heap size at which we need a collection to avoid expanding past      */
+/* limits used by blacklisting.                                         */
+STATIC word GC_collect_at_heapsize = (word)(-1);
+
+/* Have we allocated enough to amortize a collection? */
+GC_INNER GC_bool GC_should_collect(void)
+{
+    static word last_min_bytes_allocd;
+    static word last_gc_no;
+    if (last_gc_no != GC_gc_no) {
+      last_gc_no = GC_gc_no;
+      last_min_bytes_allocd = min_bytes_allocd();
+    }
+    return(GC_adj_bytes_allocd() >= last_min_bytes_allocd
+           || GC_heapsize >= GC_collect_at_heapsize);
+}
+
+/* STATIC */ GC_start_callback_proc GC_start_call_back = 0;
+                        /* Called at start of full collections.         */
+                        /* Not called if 0.  Called with the allocation */
+                        /* lock held.  Not used by GC itself.           */
+
+GC_API void GC_CALL GC_set_start_callback(GC_start_callback_proc fn)
+{
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_start_call_back = fn;
+    UNLOCK();
+}
+
+GC_API GC_start_callback_proc GC_CALL GC_get_start_callback(void)
+{
+    GC_start_callback_proc fn;
+    DCL_LOCK_STATE;
+    LOCK();
+    fn = GC_start_call_back;
+    UNLOCK();
+    return fn;
+}
+
+GC_INLINE void GC_notify_full_gc(void)
+{
+    if (GC_start_call_back != 0) {
+        (*GC_start_call_back)();
+    }
+}
+
+STATIC GC_bool GC_is_full_gc = FALSE;
+
+STATIC GC_bool GC_stopped_mark(GC_stop_func stop_func);
+STATIC void GC_finish_collection(void);
+
+/*
+ * Initiate a garbage collection if appropriate.
+ * Choose judiciously
+ * between partial, full, and stop-world collections.
+ */
+STATIC void GC_maybe_gc(void)
+{
+    static int n_partial_gcs = 0;
+
+    GC_ASSERT(I_HOLD_LOCK());
+    ASSERT_CANCEL_DISABLED();
+    if (GC_should_collect()) {
+        if (!GC_incremental) {
+            /* FIXME: If possible, GC_default_stop_func should be used here */
+            GC_try_to_collect_inner(GC_never_stop_func);
+            n_partial_gcs = 0;
+            return;
+        } else {
+#         ifdef PARALLEL_MARK
+            if (GC_parallel)
+              GC_wait_for_reclaim();
+#         endif
+          if (GC_need_full_gc || n_partial_gcs >= GC_full_freq) {
+            if (GC_print_stats) {
+              GC_log_printf(
+                  "***>Full mark for collection %lu after %ld allocd bytes\n",
+                  (unsigned long)GC_gc_no + 1, (long)GC_bytes_allocd);
+            }
+            GC_promote_black_lists();
+            (void)GC_reclaim_all((GC_stop_func)0, TRUE);
+            GC_notify_full_gc();
+            GC_clear_marks();
+            n_partial_gcs = 0;
+            GC_is_full_gc = TRUE;
+          } else {
+            n_partial_gcs++;
+          }
+        }
+        /* We try to mark with the world stopped.       */
+        /* If we run out of time, this turns into       */
+        /* incremental marking.                 */
+#       ifndef NO_CLOCK
+          if (GC_time_limit != GC_TIME_UNLIMITED) { GET_TIME(GC_start_time); }
+#       endif
+        /* FIXME: If possible, GC_default_stop_func should be   */
+        /* used instead of GC_never_stop_func here.             */
+        if (GC_stopped_mark(GC_time_limit == GC_TIME_UNLIMITED?
+                            GC_never_stop_func : GC_timeout_stop_func)) {
+#           ifdef SAVE_CALL_CHAIN
+                GC_save_callers(GC_last_stack);
+#           endif
+            GC_finish_collection();
+        } else {
+            if (!GC_is_full_gc) {
+                /* Count this as the first attempt */
+                GC_n_attempts++;
+            }
+        }
+    }
+}
+
+
+/*
+ * Stop the world garbage collection.  Assumes lock held. If stop_func is
+ * not GC_never_stop_func then abort if stop_func returns TRUE.
+ * Return TRUE if we successfully completed the collection.
+ */
+GC_INNER GC_bool GC_try_to_collect_inner(GC_stop_func stop_func)
+{
+  BDWGC_DEBUG("Running stop the world gc (GC_try_to_collect_inner)\n");
+  
+#   ifndef SMALL_CONFIG
+      CLOCK_TYPE start_time = 0; /* initialized to prevent warning. */
+      CLOCK_TYPE current_time;
+#   endif
+    ASSERT_CANCEL_DISABLED();
+    if (GC_dont_gc || (*stop_func)()) return FALSE;
+    if (GC_incremental && GC_collection_in_progress()) {
+      if (GC_print_stats) {
+        GC_log_printf(
+            "GC_try_to_collect_inner: finishing collection in progress\n");
+      }
+      /* Just finish collection already in progress.    */
+        while(GC_collection_in_progress()) {
+            if ((*stop_func)()) return(FALSE);
+            GC_collect_a_little_inner(1);
+        }
+    }
+    GC_notify_full_gc();
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats) {
+        GET_TIME(start_time);
+        GC_log_printf("Initiating full world-stop collection!\n");
+      }
+#   endif
+    GC_promote_black_lists();
+    /* Make sure all blocks have been reclaimed, so sweep routines      */
+    /* don't see cleared mark bits.                                     */
+    /* If we're guaranteed to finish, then this is unnecessary.         */
+    /* In the find_leak case, we have to finish to guarantee that       */
+    /* previously unmarked objects are not reported as leaks.           */
+#       ifdef PARALLEL_MARK
+          if (GC_parallel)
+            GC_wait_for_reclaim();
+#       endif
+        if ((GC_find_leak || stop_func != GC_never_stop_func)
+            && !GC_reclaim_all(stop_func, FALSE)) {
+            /* Aborted.  So far everything is still consistent. */
+            return(FALSE);
+        }
+    GC_invalidate_mark_state();  /* Flush mark stack.   */
+    GC_clear_marks();
+#   ifdef SAVE_CALL_CHAIN
+        GC_save_callers(GC_last_stack);
+#   endif
+    GC_is_full_gc = TRUE;
+    if (!GC_stopped_mark(stop_func)) {
+      if (!GC_incremental) {
+        /* We're partially done and have no way to complete or use      */
+        /* current work.  Reestablish invariants as cheaply as          */
+        /* possible.                                                    */
+        GC_invalidate_mark_state();
+        GC_unpromote_black_lists();
+      } /* else we claim the world is already still consistent.  We'll  */
+        /* finish incrementally.                                        */
+      return(FALSE);
+    }
+    GC_finish_collection();
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats) {
+        GET_TIME(current_time);
+        GC_log_printf("Complete collection took %lu msecs\n",
+                      MS_TIME_DIFF(current_time,start_time));
+      }
+#   endif
+    return(TRUE);
+}
+
+/*
+ * Perform n units of garbage collection work.  A unit is intended to touch
+ * roughly GC_RATE pages.  Every once in a while, we do more than that.
+ * This needs to be a fairly large number with our current incremental
+ * GC strategy, since otherwise we allocate too much during GC, and the
+ * cleanup gets expensive.
+ */
+#ifndef GC_RATE
+# define GC_RATE 10
+#endif
+#ifndef MAX_PRIOR_ATTEMPTS
+# define MAX_PRIOR_ATTEMPTS 1
+#endif
+        /* Maximum number of prior attempts at world stop marking       */
+        /* A value of 1 means that we finish the second time, no matter */
+        /* how long it takes.  Doesn't count the initial root scan      */
+        /* for a full GC.                                               */
+
+STATIC int GC_deficit = 0;/* The number of extra calls to GC_mark_some  */
+                          /* that we have made.                         */
+
+GC_INNER void GC_collect_a_little_inner(int n)
+{
+    int i;
+    IF_CANCEL(int cancel_state;)
+
+    if (GC_dont_gc) return;
+    DISABLE_CANCEL(cancel_state);
+    if (GC_incremental && GC_collection_in_progress()) {
+        for (i = GC_deficit; i < GC_RATE*n; i++) {
+            if (GC_mark_some((ptr_t)0)) {
+                /* Need to finish a collection */
+#               ifdef SAVE_CALL_CHAIN
+                    GC_save_callers(GC_last_stack);
+#               endif
+#               ifdef PARALLEL_MARK
+                    if (GC_parallel)
+                      GC_wait_for_reclaim();
+#               endif
+                if (GC_n_attempts < MAX_PRIOR_ATTEMPTS
+                    && GC_time_limit != GC_TIME_UNLIMITED) {
+#                 ifndef NO_CLOCK
+                    GET_TIME(GC_start_time);
+#                 endif
+                  if (!GC_stopped_mark(GC_timeout_stop_func)) {
+                    GC_n_attempts++;
+                    break;
+                  }
+                } else {
+                  /* FIXME: If possible, GC_default_stop_func should be */
+                  /* used here.                                         */
+                  (void)GC_stopped_mark(GC_never_stop_func);
+                }
+                GC_finish_collection();
+                break;
+            }
+        }
+        if (GC_deficit > 0) GC_deficit -= GC_RATE*n;
+        if (GC_deficit < 0) GC_deficit = 0;
+    } else {
+        GC_maybe_gc();
+    }
+    RESTORE_CANCEL(cancel_state);
+}
+
+GC_INNER void (*GC_check_heap)(void) = 0;
+GC_INNER void (*GC_print_all_smashed)(void) = 0;
+
+GC_API int GC_CALL GC_collect_a_little(void)
+{
+    int result;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    GC_collect_a_little_inner(1);
+    result = (int)GC_collection_in_progress();
+    UNLOCK();
+    if (!result && GC_debugging_started) GC_print_all_smashed();
+    return(result);
+}
+
+#ifndef SMALL_CONFIG
+  /* Variables for world-stop average delay time statistic computation. */
+  /* "divisor" is incremented every world-stop and halved when reached  */
+  /* its maximum (or upon "total_time" overflow).                       */
+  static unsigned world_stopped_total_time = 0;
+  static unsigned world_stopped_total_divisor = 0;
+# ifndef MAX_TOTAL_TIME_DIVISOR
+    /* We shall not use big values here (so "outdated" delay time       */
+    /* values would have less impact on "average" delay time value than */
+    /* newer ones).                                                     */
+#   define MAX_TOTAL_TIME_DIVISOR 1000
+# endif
+#endif
+
+/*
+ * Assumes lock is held.  We stop the world and mark from all roots.
+ * If stop_func() ever returns TRUE, we may fail and return FALSE.
+ * Increment GC_gc_no if we succeed.
+ */
+STATIC GC_bool GC_stopped_mark(GC_stop_func stop_func)
+{
+    unsigned i;
+#   ifndef SMALL_CONFIG
+      CLOCK_TYPE start_time = 0; /* initialized to prevent warning. */
+      CLOCK_TYPE current_time;
+#   endif
+
+#   if !defined(REDIRECT_MALLOC) && (defined(MSWIN32) || defined(MSWINCE))
+        GC_add_current_malloc_heap();
+#   endif
+#   if defined(REGISTER_LIBRARIES_EARLY)
+        GC_cond_register_dynamic_libraries();
+#   endif
+
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats)
+        GET_TIME(start_time);
+#   endif
+
+    STOP_WORLD();
+#   ifdef THREAD_LOCAL_ALLOC
+      GC_world_stopped = TRUE;
+#   endif
+    if (GC_print_stats) {
+        /* Output blank line for convenience here */
+        GC_log_printf(
+              "\n--> Marking for collection %lu after %lu allocated bytes\n",
+              (unsigned long)GC_gc_no + 1, (unsigned long) GC_bytes_allocd);
+    }
+#   ifdef MAKE_BACK_GRAPH
+      if (GC_print_back_height) {
+        GC_build_back_graph();
+      }
+#   endif
+
+    /* Mark from all roots.  */
+        /* Minimize junk left in my registers and on the stack */
+            GC_clear_a_few_frames();
+            GC_noop(0,0,0,0,0,0);
+        GC_initiate_gc();
+        for (i = 0;;i++) {
+          if ((*stop_func)()) {
+            if (GC_print_stats) {
+              GC_log_printf("Abandoned stopped marking after %u iterations\n",
+                            i);
+            }
+            GC_deficit = i;     /* Give the mutator a chance.   */
+#           ifdef THREAD_LOCAL_ALLOC
+              GC_world_stopped = FALSE;
+#           endif
+            START_WORLD();
+            return(FALSE);
+          }
+          if (GC_mark_some(GC_approx_sp())) break;
+        }
+
+    GC_gc_no++;
+    if (GC_print_stats) {
+      GC_log_printf(
+             "Collection %lu reclaimed %ld bytes ---> heapsize = %lu bytes\n",
+             (unsigned long)(GC_gc_no - 1), (long)GC_bytes_found,
+             (unsigned long)GC_heapsize);
+    }
+
+    /* Check all debugged objects for consistency */
+        if (GC_debugging_started) {
+            (*GC_check_heap)();
+        }
+
+#   ifdef THREAD_LOCAL_ALLOC
+      GC_world_stopped = FALSE;
+#   endif
+    START_WORLD();
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats) {
+        unsigned long time_diff;
+        unsigned total_time, divisor;
+        GET_TIME(current_time);
+        time_diff = MS_TIME_DIFF(current_time,start_time);
+
+        /* Compute new world-stop delay total time */
+        total_time = world_stopped_total_time;
+        divisor = world_stopped_total_divisor;
+        if ((int)total_time < 0 || divisor >= MAX_TOTAL_TIME_DIVISOR) {
+          /* Halve values if overflow occurs */
+          total_time >>= 1;
+          divisor >>= 1;
+        }
+        total_time += time_diff < (((unsigned)-1) >> 1) ?
+                        (unsigned)time_diff : ((unsigned)-1) >> 1;
+        /* Update old world_stopped_total_time and its divisor */
+        world_stopped_total_time = total_time;
+        world_stopped_total_divisor = ++divisor;
+
+        GC_ASSERT(divisor != 0);
+        GC_log_printf(
+                "World-stopped marking took %lu msecs (%u in average)\n",
+                time_diff, total_time / divisor);
+      }
+#   endif
+    return(TRUE);
+}
+
+/* Set all mark bits for the free list whose first entry is q   */
+GC_INNER void GC_set_fl_marks(ptr_t q)
+{
+   struct hblk *h, *last_h;
+   hdr *hhdr;
+   IF_PER_OBJ(size_t sz;)
+   unsigned bit_no;
+
+   if (q != NULL) {
+     h = HBLKPTR(q);
+     last_h = h;
+     hhdr = HDR(h);
+     IF_PER_OBJ(sz = hhdr->hb_sz;)
+
+     for (;;) {
+        bit_no = MARK_BIT_NO((ptr_t)q - (ptr_t)h, sz);
+        if (!mark_bit_from_hdr(hhdr, bit_no)) {
+          set_mark_bit_from_hdr(hhdr, bit_no);
+          ++hhdr -> hb_n_marks;
+        }
+
+        if (q == NULL)
+          panic("Cant be null");
+        
+        q = obj_link(q);
+        if (q == NULL)
+          break;
+
+        h = HBLKPTR(q);
+        if (h != last_h) {
+          last_h = h;
+          hhdr = HDR(h);
+          IF_PER_OBJ(sz = hhdr->hb_sz;)
+        }
+     }
+   }
+}
+
+#if defined(GC_ASSERTIONS) && defined(THREADS) && defined(THREAD_LOCAL_ALLOC)
+  /* Check that all mark bits for the free list whose first entry is    */
+  /* (*pfreelist) are set.  Check skipped if points to a special value. */
+  void GC_check_fl_marks(void **pfreelist)
+  {
+#   ifdef AO_HAVE_load_acquire_read
+      AO_t *list = (AO_t *)AO_load_acquire_read((AO_t *)pfreelist);
+                /* Atomic operations are used because the world is running. */
+      AO_t *prev;
+      AO_t *p;
+
+      if ((word)list <= HBLKSIZE) return;
+
+      prev = (AO_t *)pfreelist;
+      for (p = list; p != NULL;) {
+        AO_t *next;
+
+        if (!GC_is_marked((ptr_t)p)) {
+          GC_err_printf("Unmarked object %p on list %p\n",
+                        (void *)p, (void *)list);
+          ABORT("Unmarked local free list entry");
+        }
+
+        /* While traversing the free-list, it re-reads the pointer to   */
+        /* the current node before accepting its next pointer and       */
+        /* bails out if the latter has changed.  That way, it won't     */
+        /* try to follow the pointer which might be been modified       */
+        /* after the object was returned to the client.  It might       */
+        /* perform the mark-check on the just allocated object but      */
+        /* that should be harmless.                                     */
+        next = (AO_t *)AO_load_acquire_read(p);
+        if (AO_load(prev) != (AO_t)p)
+          break;
+        prev = p;
+        p = next;
+      }
+#   else
+      /* FIXME: Not implemented (just skipped). */
+      (void)pfreelist;
+#   endif
+  }
+#endif /* GC_ASSERTIONS && THREAD_LOCAL_ALLOC */
+
+/* Clear all mark bits for the free list whose first entry is q */
+/* Decrement GC_bytes_found by number of bytes on free list.    */
+STATIC void GC_clear_fl_marks(ptr_t q)
+{
+   struct hblk *h, *last_h;
+   hdr *hhdr;
+   size_t sz;
+   unsigned bit_no;
+
+   if (q != NULL) {
+     h = HBLKPTR(q);
+     last_h = h;
+     hhdr = HDR(h);
+     sz = hhdr->hb_sz;  /* Normally set only once. */
+
+     for (;;) {
+        bit_no = MARK_BIT_NO((ptr_t)q - (ptr_t)h, sz);
+        if (mark_bit_from_hdr(hhdr, bit_no)) {
+          size_t n_marks = hhdr -> hb_n_marks - 1;
+          clear_mark_bit_from_hdr(hhdr, bit_no);
+#         ifdef PARALLEL_MARK
+            /* Appr. count, don't decrement to zero! */
+            if (0 != n_marks || !GC_parallel) {
+              hhdr -> hb_n_marks = n_marks;
+            }
+#         else
+            hhdr -> hb_n_marks = n_marks;
+#         endif
+        }
+        GC_bytes_found -= sz;
+
+        q = obj_link(q);
+        if (q == NULL)
+          break;
+
+        h = HBLKPTR(q);
+        if (h != last_h) {
+          last_h = h;
+          hhdr = HDR(h);
+          sz = hhdr->hb_sz;
+        }
+     }
+   }
+}
+
+#if defined(GC_ASSERTIONS) && defined(THREADS) && defined(THREAD_LOCAL_ALLOC)
+  void GC_check_tls(void);
+#endif
+
+/* Finish up a collection.  Assumes mark bits are consistent, lock is   */
+/* held, but the world is otherwise running.                            */
+STATIC void GC_finish_collection(void)
+{
+#   ifndef SMALL_CONFIG
+      CLOCK_TYPE start_time = 0; /* initialized to prevent warning. */
+      CLOCK_TYPE finalize_time = 0;
+      CLOCK_TYPE done_time;
+#   endif
+
+#   if defined(GC_ASSERTIONS) && defined(THREADS) \
+       && defined(THREAD_LOCAL_ALLOC) && !defined(DBG_HDRS_ALL)
+        /* Check that we marked some of our own data.           */
+        /* FIXME: Add more checks.                              */
+        GC_check_tls();
+#   endif
+
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats)
+        GET_TIME(start_time);
+#   endif
+
+    GC_bytes_found = 0;
+#   if defined(LINUX) && defined(__ELF__) && !defined(SMALL_CONFIG)
+        if (GETENV("GC_PRINT_ADDRESS_MAP") != 0) {
+          GC_print_address_map();
+        }
+#   endif
+    COND_DUMP;
+    if (GC_find_leak) {
+      /* Mark all objects on the free list.  All objects should be      */
+      /* marked when we're done.                                        */
+      word size;        /* current object size  */
+      unsigned kind;
+      ptr_t q;
+
+      for (kind = 0; kind < GC_n_kinds; kind++) {
+        for (size = 1; size <= MAXOBJGRANULES; size++) {
+          q = GC_obj_kinds[kind].ok_freelist[size];
+          if (q != 0) GC_set_fl_marks(q);
+        }
+      }
+      GC_start_reclaim(TRUE);
+        /* The above just checks; it doesn't really reclaim anything.   */
+    }
+
+    GC_finalize();
+#   ifdef STUBBORN_ALLOC
+      GC_clean_changing_list();
+#   endif
+
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats)
+        GET_TIME(finalize_time);
+#   endif
+
+    if (GC_print_back_height) {
+#     ifdef MAKE_BACK_GRAPH
+        GC_traverse_back_graph();
+#     elif !defined(SMALL_CONFIG)
+        GC_err_printf("Back height not available: "
+                      "Rebuild collector with -DMAKE_BACK_GRAPH\n");
+#     endif
+    }
+
+    /* Clear free list mark bits, in case they got accidentally marked   */
+    /* (or GC_find_leak is set and they were intentionally marked).      */
+    /* Also subtract memory remaining from GC_bytes_found count.         */
+    /* Note that composite objects on free list are cleared.             */
+    /* Thus accidentally marking a free list is not a problem;  only     */
+    /* objects on the list itself will be marked, and that's fixed here. */
+    {
+      word size;        /* current object size          */
+      ptr_t q;          /* pointer to current object    */
+      unsigned kind;
+
+      for (kind = 0; kind < GC_n_kinds; kind++) {
+        for (size = 1; size <= MAXOBJGRANULES; size++) {
+          q = GC_obj_kinds[kind].ok_freelist[size];
+          if (q != 0) GC_clear_fl_marks(q);
+        }
+      }
+    }
+
+    if (GC_print_stats == VERBOSE)
+        GC_log_printf("Bytes recovered before sweep - f.l. count = %ld\n",
+                      (long)GC_bytes_found);
+
+    /* Reconstruct free lists to contain everything not marked */
+    GC_start_reclaim(FALSE);
+    if (GC_print_stats) {
+      GC_log_printf("Heap contains %lu pointer-containing "
+                    "+ %lu pointer-free reachable bytes\n",
+                    (unsigned long)GC_composite_in_use,
+                    (unsigned long)GC_atomic_in_use);
+    }
+    if (GC_is_full_gc) {
+        GC_used_heap_size_after_full = USED_HEAP_SIZE;
+        GC_need_full_gc = FALSE;
+    } else {
+        GC_need_full_gc = USED_HEAP_SIZE - GC_used_heap_size_after_full
+                            > min_bytes_allocd();
+    }
+
+    if (GC_print_stats == VERBOSE) {
+#     ifdef USE_MUNMAP
+        GC_log_printf("Immediately reclaimed %ld bytes in heap"
+                      " of size %lu bytes (%lu unmapped)\n",
+                      (long)GC_bytes_found, (unsigned long)GC_heapsize,
+                      (unsigned long)GC_unmapped_bytes);
+#     else
+        GC_log_printf(
+                "Immediately reclaimed %ld bytes in heap of size %lu bytes\n",
+                (long)GC_bytes_found, (unsigned long)GC_heapsize);
+#     endif
+    }
+
+    /* Reset or increment counters for next cycle */
+    GC_n_attempts = 0;
+    GC_is_full_gc = FALSE;
+    GC_bytes_allocd_before_gc += GC_bytes_allocd;
+    GC_non_gc_bytes_at_gc = GC_non_gc_bytes;
+    GC_bytes_allocd = 0;
+    GC_bytes_dropped = 0;
+    GC_bytes_freed = 0;
+    GC_finalizer_bytes_freed = 0;
+
+#   ifdef USE_MUNMAP
+      GC_unmap_old();
+#   endif
+
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats) {
+        GET_TIME(done_time);
+
+        /* A convenient place to output finalization statistics. */
+        GC_print_finalization_stats();
+
+        GC_log_printf("Finalize plus initiate sweep took %lu + %lu msecs\n",
+                      MS_TIME_DIFF(finalize_time,start_time),
+                      MS_TIME_DIFF(done_time,finalize_time));
+      }
+#   endif
+}
+
+/* If stop_func == 0 then GC_default_stop_func is used instead.         */
+STATIC GC_bool GC_try_to_collect_general(GC_stop_func stop_func,
+                                         GC_bool force_unmap)
+{
+    GC_bool result;
+#   ifdef USE_MUNMAP
+      int old_unmap_threshold;
+#   endif
+    IF_CANCEL(int cancel_state;)
+    DCL_LOCK_STATE;
+
+    if (!GC_is_initialized) GC_init();
+    if (GC_debugging_started) GC_print_all_smashed();
+    GC_INVOKE_FINALIZERS();
+    LOCK();
+    DISABLE_CANCEL(cancel_state);
+#   ifdef USE_MUNMAP
+      old_unmap_threshold = GC_unmap_threshold;
+      if (force_unmap ||
+          (GC_force_unmap_on_gcollect && old_unmap_threshold > 0))
+        GC_unmap_threshold = 1; /* unmap as much as possible */
+#   endif
+    ENTER_GC();
+    /* Minimize junk left in my registers */
+      GC_noop(0,0,0,0,0,0);
+    result = GC_try_to_collect_inner(stop_func != 0 ? stop_func :
+                                     GC_default_stop_func);
+    EXIT_GC();
+#   ifdef USE_MUNMAP
+      GC_unmap_threshold = old_unmap_threshold; /* restore */
+#   endif
+    RESTORE_CANCEL(cancel_state);
+    UNLOCK();
+    if (result) {
+        if (GC_debugging_started) GC_print_all_smashed();
+        GC_INVOKE_FINALIZERS();
+    }
+    return(result);
+}
+
+/* Externally callable routines to invoke full, stop-the-world collection. */
+GC_API int GC_CALL GC_try_to_collect(GC_stop_func stop_func)
+{
+    GC_ASSERT(stop_func != 0);
+    return (int)GC_try_to_collect_general(stop_func, FALSE);
+}
+
+GC_API void GC_CALL GC_gcollect(void)
+{
+    /* 0 is passed as stop_func to get GC_default_stop_func value       */
+    /* while holding the allocation lock (to prevent data races).       */
+    (void)GC_try_to_collect_general(0, FALSE);
+    if (GC_have_errors) GC_print_all_errors();
+}
+
+GC_API void GC_CALL GC_gcollect_and_unmap(void)
+{
+    (void)GC_try_to_collect_general(GC_never_stop_func, TRUE);
+}
+
+GC_INNER word GC_n_heap_sects = 0;
+                        /* Number of sections currently in heap. */
+
+#ifdef USE_PROC_FOR_LIBRARIES
+  GC_INNER word GC_n_memory = 0;
+                        /* Number of GET_MEM allocated memory sections. */
+#endif
+
+#ifdef USE_PROC_FOR_LIBRARIES
+  /* Add HBLKSIZE aligned, GET_MEM-generated block to GC_our_memory. */
+  /* Defined to do nothing if USE_PROC_FOR_LIBRARIES not set.       */
+  GC_INNER void GC_add_to_our_memory(ptr_t p, size_t bytes)
+  {
+    if (0 == p) return;
+    if (GC_n_memory >= MAX_HEAP_SECTS)
+      ABORT("Too many GC-allocated memory sections: Increase MAX_HEAP_SECTS");
+    GC_our_memory[GC_n_memory].hs_start = p;
+    GC_our_memory[GC_n_memory].hs_bytes = bytes;
+    GC_n_memory++;
+  }
+#endif
+
+/*
+ * Use the chunk of memory starting at p of size bytes as part of the heap.
+ * Assumes p is HBLKSIZE aligned, and bytes is a multiple of HBLKSIZE.
+ */
+GC_INNER void GC_add_to_heap(struct hblk *p, size_t bytes)
+{
+    hdr * phdr;
+    word endp;
+
+    if (GC_n_heap_sects >= MAX_HEAP_SECTS) {
+        ABORT("Too many heap sections: Increase MAXHINCR or MAX_HEAP_SECTS");
+    }
+    while ((word)p <= HBLKSIZE) {
+        /* Can't handle memory near address zero. */
+        ++p;
+        bytes -= HBLKSIZE;
+        if (0 == bytes) return;
+    }
+    endp = (word)p + bytes;
+    if (endp <= (word)p) {
+        /* Address wrapped. */
+        bytes -= HBLKSIZE;
+        if (0 == bytes) return;
+        endp -= HBLKSIZE;
+    }
+    phdr = GC_install_header(p);
+    if (0 == phdr) {
+        /* This is extremely unlikely. Can't add it.  This will         */
+        /* almost certainly result in a 0 return from the allocator,    */
+        /* which is entirely appropriate.                               */
+        return;
+    }
+    GC_ASSERT(endp > (word)p && endp == (word)p + bytes);
+    GC_heap_sects[GC_n_heap_sects].hs_start = (ptr_t)p;
+    GC_heap_sects[GC_n_heap_sects].hs_bytes = bytes;
+    GC_n_heap_sects++;
+    phdr -> hb_sz = bytes;
+    phdr -> hb_flags = 0;
+    GC_freehblk(p);
+    GC_heapsize += bytes;
+
+    /* Normally the caller calculates a new GC_collect_at_heapsize,
+     * but this is also called directly from alloc_mark_stack, so
+     * adjust here. It will be recalculated when called from
+     * GC_expand_hp_inner.
+     */
+    GC_collect_at_heapsize += bytes;
+    if (GC_collect_at_heapsize < GC_heapsize /* wrapped */)
+       GC_collect_at_heapsize = (word)(-1);
+
+    if ((ptr_t)p <= (ptr_t)GC_least_plausible_heap_addr
+        || GC_least_plausible_heap_addr == 0) {
+        GC_least_plausible_heap_addr = (void *)((ptr_t)p - sizeof(word));
+                /* Making it a little smaller than necessary prevents   */
+                /* us from getting a false hit from the variable        */
+                /* itself.  There's some unintentional reflection       */
+                /* here.                                                */
+    }
+    if ((ptr_t)p + bytes >= (ptr_t)GC_greatest_plausible_heap_addr) {
+        GC_greatest_plausible_heap_addr = (void *)endp;
+    }
+}
+
+#if !defined(NO_DEBUGGING)
+  void GC_print_heap_sects(void)
+  {
+    unsigned i;
+
+    GC_printf("Total heap size: %lu\n", (unsigned long)GC_heapsize);
+    for (i = 0; i < GC_n_heap_sects; i++) {
+      ptr_t start = GC_heap_sects[i].hs_start;
+      size_t len = GC_heap_sects[i].hs_bytes;
+      struct hblk *h;
+      unsigned nbl = 0;
+
+      for (h = (struct hblk *)start; h < (struct hblk *)(start + len); h++) {
+        if (GC_is_black_listed(h, HBLKSIZE)) nbl++;
+      }
+      GC_printf("Section %d from %p to %p %lu/%lu blacklisted\n",
+                i, start, start + len,
+                (unsigned long)nbl, (unsigned long)(len/HBLKSIZE));
+    }
+  }
+#endif
+
+void * GC_least_plausible_heap_addr = (void *)ONES;
+void * GC_greatest_plausible_heap_addr = 0;
+
+GC_INLINE word GC_max(word x, word y)
+{
+    return(x > y? x : y);
+}
+
+GC_INLINE word GC_min(word x, word y)
+{
+    return(x < y? x : y);
+}
+
+GC_API void GC_CALL GC_set_max_heap_size(GC_word n)
+{
+    GC_max_heapsize = n;
+}
+
+GC_word GC_max_retries = 0;
+
+/*
+ * this explicitly increases the size of the heap.  It is used
+ * internally, but may also be invoked from GC_expand_hp by the user.
+ * The argument is in units of HBLKSIZE.
+ * Tiny values of n are rounded up.
+ * Returns FALSE on failure.
+ */
+GC_INNER GC_bool GC_expand_hp_inner(word n)
+{
+    word bytes;
+    struct hblk * space;
+    word expansion_slop;        /* Number of bytes by which we expect the */
+                                /* heap to expand soon.                   */
+
+    if (n < MINHINCR) n = MINHINCR;
+    bytes = n * HBLKSIZE;
+    /* Make sure bytes is a multiple of GC_page_size */
+      {
+        word mask = GC_page_size - 1;
+        bytes += mask;
+        bytes &= ~mask;
+      }
+
+    if (GC_max_heapsize != 0 && GC_heapsize + bytes > GC_max_heapsize) {
+        /* Exceeded self-imposed limit */
+        return(FALSE);
+    }
+    space = GET_MEM(bytes);
+    GC_add_to_our_memory((ptr_t)space, bytes);
+    if (space == 0) {
+        if (GC_print_stats) {
+            GC_log_printf("Failed to expand heap by %ld bytes\n",
+                          (unsigned long)bytes);
+        }
+        return(FALSE);
+    }
+    if (GC_print_stats) {
+      GC_log_printf("Increasing heap size by %lu after %lu allocated bytes\n",
+                    (unsigned long)bytes, (unsigned long)GC_bytes_allocd);
+    }
+    /* Adjust heap limits generously for blacklisting to work better.   */
+    /* GC_add_to_heap performs minimal adjustment needed for            */
+    /* correctness.                                                     */
+    expansion_slop = min_bytes_allocd() + 4*MAXHINCR*HBLKSIZE;
+    if ((GC_last_heap_addr == 0 && !((word)space & SIGNB))
+        || (GC_last_heap_addr != 0 && GC_last_heap_addr < (ptr_t)space)) {
+        /* Assume the heap is growing up */
+        word new_limit = (word)space + bytes + expansion_slop;
+        if (new_limit > (word)space) {
+          GC_greatest_plausible_heap_addr =
+            (void *)GC_max((word)GC_greatest_plausible_heap_addr,
+                           (word)new_limit);
+        }
+    } else {
+        /* Heap is growing down */
+        word new_limit = (word)space - expansion_slop;
+        if (new_limit < (word)space) {
+          GC_least_plausible_heap_addr =
+            (void *)GC_min((word)GC_least_plausible_heap_addr,
+                           (word)space - expansion_slop);
+        }
+    }
+    GC_prev_heap_addr = GC_last_heap_addr;
+    GC_last_heap_addr = (ptr_t)space;
+    GC_add_to_heap(space, bytes);
+    /* Force GC before we are likely to allocate past expansion_slop */
+      GC_collect_at_heapsize =
+         GC_heapsize + expansion_slop - 2*MAXHINCR*HBLKSIZE;
+      if (GC_collect_at_heapsize < GC_heapsize /* wrapped */)
+         GC_collect_at_heapsize = (word)(-1);
+    return(TRUE);
+}
+
+/* Really returns a bool, but it's externally visible, so that's clumsy. */
+/* Arguments is in bytes.  Includes GC_init() call.                      */
+GC_API int GC_CALL GC_expand_hp(size_t bytes)
+{
+    int result;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    if (!GC_is_initialized) GC_init();
+    result = (int)GC_expand_hp_inner(divHBLKSZ((word)bytes));
+    if (result) GC_requested_heapsize += bytes;
+    UNLOCK();
+    return(result);
+}
+
+GC_INNER unsigned GC_fail_count = 0;
+                        /* How many consecutive GC/expansion failures?  */
+                        /* Reset by GC_allochblk.                       */
+
+/* Collect or expand heap in an attempt make the indicated number of    */
+/* free blocks available.  Should be called until the blocks are        */
+/* available (setting retry value to TRUE unless this is the first call */
+/* in a loop) or until it fails by returning FALSE.                     */
+GC_INNER GC_bool GC_collect_or_expand(word needed_blocks,
+                                      GC_bool ignore_off_page,
+                                      GC_bool retry)
+{
+    GC_bool gc_not_stopped = TRUE;
+    word blocks_to_get;
+    IF_CANCEL(int cancel_state;)
+
+    DISABLE_CANCEL(cancel_state);
+    if (!GC_incremental && !GC_dont_gc &&
+        ((GC_dont_expand && GC_bytes_allocd > 0) || GC_should_collect())) {
+      /* Try to do a full collection using 'default' stop_func (unless  */
+      /* nothing has been allocated since the latest collection or heap */
+      /* expansion is disabled).                                        */
+      gc_not_stopped = GC_try_to_collect_inner(
+                        GC_bytes_allocd > 0 && (!GC_dont_expand || !retry) ?
+                        GC_default_stop_func : GC_never_stop_func);
+      if (gc_not_stopped == TRUE || !retry) {
+        /* Either the collection hasn't been aborted or this is the     */
+        /* first attempt (in a loop).                                   */
+        RESTORE_CANCEL(cancel_state);
+        return(TRUE);
+      }
+    }
+
+    blocks_to_get = GC_heapsize/(HBLKSIZE*GC_free_space_divisor)
+                        + needed_blocks;
+    if (blocks_to_get > MAXHINCR) {
+      word slop;
+
+      /* Get the minimum required to make it likely that we can satisfy */
+      /* the current request in the presence of black-listing.          */
+      /* This will probably be more than MAXHINCR.                      */
+      if (ignore_off_page) {
+        slop = 4;
+      } else {
+        slop = 2 * divHBLKSZ(BL_LIMIT);
+        if (slop > needed_blocks) slop = needed_blocks;
+      }
+      if (needed_blocks + slop > MAXHINCR) {
+        blocks_to_get = needed_blocks + slop;
+      } else {
+        blocks_to_get = MAXHINCR;
+      }
+    }
+
+    if (!GC_expand_hp_inner(blocks_to_get)
+        && !GC_expand_hp_inner(needed_blocks)) {
+      if (gc_not_stopped == FALSE) {
+        /* Don't increment GC_fail_count here (and no warning).     */
+        GC_gcollect_inner();
+        GC_ASSERT(GC_bytes_allocd == 0);
+      } else if (GC_fail_count++ < GC_max_retries) {
+        WARN("Out of Memory!  Trying to continue ...\n", 0);
+        GC_gcollect_inner();
+      } else {
+#       if !defined(AMIGA) || !defined(GC_AMIGA_FASTALLOC)
+          WARN("Out of Memory! Heap size: %" GC_PRIdPTR " MiB."
+               " Returning NULL!\n", (GC_heapsize - GC_unmapped_bytes) >> 20);
+#       endif
+        RESTORE_CANCEL(cancel_state);
+        return(FALSE);
+      }
+    } else if (GC_fail_count && GC_print_stats) {
+      GC_log_printf("Memory available again...\n");
+    }
+    RESTORE_CANCEL(cancel_state);
+    return(TRUE);
+}
+
+/*
+ * Make sure the object free list for size gran (in granules) is not empty.
+ * Return a pointer to the first object on the free list.
+ * The object MUST BE REMOVED FROM THE FREE LIST BY THE CALLER.
+ * Assumes we hold the allocator lock.
+ */
+GC_INNER ptr_t GC_allocobj(size_t gran, int kind)
+{
+    void ** flh = &(GC_obj_kinds[kind].ok_freelist[gran]);
+    GC_bool tried_minor = FALSE;
+    GC_bool retry = FALSE;
+
+    if (gran == 0) return(0);
+
+    while (*flh == 0) {
+      ENTER_GC();
+      /* Do our share of marking work */
+        if(TRUE_INCREMENTAL) GC_collect_a_little_inner(1);
+      /* Sweep blocks for objects of this size */
+        GC_continue_reclaim(gran, kind);
+      EXIT_GC();
+      if (*flh == 0) {
+        GC_new_hblk(gran, kind);
+      }
+      if (*flh == 0) {
+        ENTER_GC();
+        if (GC_incremental && GC_time_limit == GC_TIME_UNLIMITED
+            && !tried_minor) {
+          GC_collect_a_little_inner(1);
+          tried_minor = TRUE;
+        } else {
+          if (!GC_collect_or_expand(1, FALSE, retry)) {
+            EXIT_GC();
+            return(0);
+          }
+          retry = TRUE;
+        }
+        EXIT_GC();
+      }
+    }
+    /* Successful allocation; reset failure count.      */
+    GC_fail_count = 0;
+
+    return(*flh);
+}
diff --git a/src/gc/bdwgc/alpha_mach_dep.S b/src/gc/bdwgc/alpha_mach_dep.S
new file mode 100644
index 0000000..d4def24
--- /dev/null
+++ b/src/gc/bdwgc/alpha_mach_dep.S
@@ -0,0 +1,86 @@
+	.arch ev6
+
+        .text
+        .align  4
+        .globl  GC_push_regs
+        .ent    GC_push_regs 2
+GC_push_regs:
+	ldgp    $gp, 0($27)
+	lda     $sp, -16($sp)
+	stq     $26, 0($sp)
+        .mask   0x04000000, 0
+        .frame  $sp, 16, $26, 0
+
+/* $0		integer result                                                */
+/* $1-$8	temp regs - not preserved cross calls                         */
+/* $9-$15	call saved regs                                               */
+/* $16-$21	argument regs - not preserved cross calls                     */
+/* $22-$28	temp regs - not preserved cross calls                         */
+/* $29		global pointer - not preserved cross calls                    */
+/* $30		stack pointer                                                 */
+
+# define call_push(x)			\
+	mov   x, $16;			\
+	jsr   $26, GC_push_one;		\
+	ldgp  $gp, 0($26)
+	
+        call_push($9)
+        call_push($10)
+        call_push($11)
+        call_push($12)
+        call_push($13)
+        call_push($14)
+        call_push($15)
+
+/* $f0-$f1	floating point results                                        */
+/* $f2-$f9	call saved regs                                               */
+/* $f10-$f30	temp regs - not preserved cross calls                         */
+
+	/* Use the most efficient transfer method for this hardware. */
+	/* Bit 1 detects the FIX extension, which includes ftoit. */
+	amask	2, $0
+	bne	$0, $use_stack
+
+#undef call_push
+#define call_push(x)			\
+	ftoit	x, $16;			\
+	jsr	$26, GC_push_one;	\
+	ldgp	$gp, 0($26)
+
+	call_push($f2)
+	call_push($f3)
+	call_push($f4)
+	call_push($f5)
+	call_push($f6)
+	call_push($f7)
+	call_push($f8)
+	call_push($f9)
+
+	ldq     $26, 0($sp)
+	lda     $sp, 16($sp)
+	ret     $31, ($26), 1
+
+	.align	4
+$use_stack:
+
+#undef call_push
+#define call_push(x)			\
+	stt	x, 8($sp);		\
+	ldq	$16, 8($sp);		\
+	jsr	$26, GC_push_one;	\
+	ldgp	$gp, 0($26)
+
+	call_push($f2)
+	call_push($f3)
+	call_push($f4)
+	call_push($f5)
+	call_push($f6)
+	call_push($f7)
+	call_push($f8)
+	call_push($f9)
+
+	ldq     $26, 0($sp)
+	lda     $sp, 16($sp)
+	ret     $31, ($26), 1
+
+	.end    GC_push_regs
diff --git a/src/gc/bdwgc/atomic_ops_malloc.h b/src/gc/bdwgc/atomic_ops_malloc.h
new file mode 100644
index 0000000..41987ed
--- /dev/null
+++ b/src/gc/bdwgc/atomic_ops_malloc.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Almost lock-free malloc implementation based on stack implementation. */
+/* See README.malloc file for detailed usage rules.                      */
+
+#ifndef AO_ATOMIC_H
+#define AO_ATOMIC_H
+
+#include <stdlib.h>     /* For size_t */
+
+#include "atomic_ops_stack.h"
+
+#ifdef AO_STACK_IS_LOCK_FREE
+# define AO_MALLOC_IS_LOCK_FREE
+#endif
+
+void AO_free(void *);
+
+void * AO_malloc(size_t);
+
+/* Allow use of mmap to grow the heap.  No-op on some platforms.        */
+void AO_malloc_enable_mmap(void);
+
+#endif /* !AO_ATOMIC_H */
diff --git a/src/gc/bdwgc/atomic_ops_stack.h b/src/gc/bdwgc/atomic_ops_stack.h
new file mode 100644
index 0000000..6c8b5bb
--- /dev/null
+++ b/src/gc/bdwgc/atomic_ops_stack.h
@@ -0,0 +1,188 @@
+/*
+ * The implementation of the routines described here is covered by the GPL.
+ * This header file is covered by the following license:
+ */
+
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Almost lock-free LIFO linked lists (linked stacks).  */
+#ifndef AO_STACK_H
+#define AO_STACK_H
+
+#include "atomic_ops.h"
+
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && !defined(AO_HAVE_compare_double_and_swap) \
+    && defined(AO_HAVE_compare_and_swap)
+# define AO_USE_ALMOST_LOCK_FREE
+#else
+  /* If we have no compare-and-swap operation defined, we assume        */
+  /* that we will actually be using CAS emulation.  If we do that,      */
+  /* it's cheaper to use the version-based implementation.              */
+# define AO_STACK_IS_LOCK_FREE
+#endif
+
+/*
+ * These are not guaranteed to be completely lock-free.
+ * List insertion may spin under extremely unlikely conditions.
+ * It cannot deadlock due to recursive reentry unless AO_list_remove
+ * is called while at least AO_BL_SIZE activations of
+ * AO_list_remove are currently active in the same thread, i.e.
+ * we must have at least AO_BL_SIZE recursive signal handler
+ * invocations.
+ *
+ * All operations take an AO_list_aux argument.  It is safe to
+ * share a single AO_list_aux structure among all lists, but that
+ * may increase contention.  Any given list must always be accessed
+ * with the same AO_list_aux structure.
+ *
+ * We make some machine-dependent assumptions:
+ *   - We have a compare-and-swap operation.
+ *   - At least _AO_N_BITS low order bits in pointers are
+ *     zero and normally unused.
+ *   - size_t and pointers have the same size.
+ *
+ * We do use a fully lock-free implementation if double-width
+ * compare-and-swap operations are available.
+ */
+
+#ifdef AO_USE_ALMOST_LOCK_FREE
+/* The number of low order pointer bits we can use for a small  */
+/* version number.                                              */
+# if defined(__LP64__) || defined(_LP64) || defined(_WIN64)
+   /* WIN64 isn't really supported yet. */
+#  define AO_N_BITS 3
+# else
+#  define AO_N_BITS 2
+# endif
+
+# define AO_BIT_MASK ((1 << AO_N_BITS) - 1)
+/*
+ * AO_stack_aux should be treated as opaque.
+ * It is fully defined here, so it can be allocated, and to facilitate
+ * debugging.
+ */
+#ifndef AO_BL_SIZE
+#  define AO_BL_SIZE 2
+#endif
+
+#if AO_BL_SIZE > (1 << AO_N_BITS)
+#  error AO_BL_SIZE too big
+#endif
+
+typedef struct AO__stack_aux {
+  volatile AO_t AO_stack_bl[AO_BL_SIZE];
+} AO_stack_aux;
+
+/* The stack implementation knows only about the location of    */
+/* link fields in nodes, and nothing about the rest of the      */
+/* stack elements.  Link fields hold an AO_t, which is not      */
+/* necessarily a real pointer.  This converts the AO_t to a     */
+/* real (AO_t *) which is either o, or points at the link       */
+/* field in the next node.                                      */
+#define AO_REAL_NEXT_PTR(x) (AO_t *)((x) & ~AO_BIT_MASK)
+
+/* The following two routines should not normally be used directly.     */
+/* We make them visible here for the rare cases in which it makes sense */
+/* to share the an AO_stack_aux between stacks.                         */
+void
+AO_stack_push_explicit_aux_release(volatile AO_t *list, AO_t *x,
+                                  AO_stack_aux *);
+
+AO_t *
+AO_stack_pop_explicit_aux_acquire(volatile AO_t *list, AO_stack_aux *);
+
+/* And now AO_stack_t for the real interface:                           */
+
+typedef struct AO__stack {
+  volatile AO_t AO_ptr;
+  AO_stack_aux AO_aux;
+} AO_stack_t;
+
+#define AO_STACK_INITIALIZER {0}
+
+AO_INLINE void AO_stack_init(AO_stack_t *list)
+{
+# if AO_BL_SIZE == 2
+    list -> AO_aux.AO_stack_bl[0] = 0;
+    list -> AO_aux.AO_stack_bl[1] = 0;
+# else
+    int i;
+    for (i = 0; i < AO_BL_SIZE; ++i)
+      list -> AO_aux.AO_stack_bl[i] = 0;
+# endif
+  list -> AO_ptr = 0;
+}
+
+/* Convert an AO_stack_t to a pointer to the link field in      */
+/* the first element.                                           */
+#define AO_REAL_HEAD_PTR(x) AO_REAL_NEXT_PTR((x).AO_ptr)
+
+#define AO_stack_push_release(l, e) \
+        AO_stack_push_explicit_aux_release(&((l)->AO_ptr), e, &((l)->AO_aux))
+#define AO_HAVE_stack_push_release
+
+#define AO_stack_pop_acquire(l) \
+        AO_stack_pop_explicit_aux_acquire(&((l)->AO_ptr), &((l)->AO_aux))
+#define AO_HAVE_stack_pop_acquire
+
+# else /* Use fully non-blocking data structure, wide CAS       */
+
+#ifndef AO_HAVE_double_t
+  /* Can happen if we're using CAS emulation, since we don't want to    */
+  /* force that here, in case other atomic_ops clients don't want it.   */
+# include "atomic_ops/sysdeps/standard_ao_double_t.h"
+#endif
+
+typedef volatile AO_double_t AO_stack_t;
+/* AO_val1 is version, AO_val2 is pointer.      */
+
+#define AO_STACK_INITIALIZER {0}
+
+AO_INLINE void AO_stack_init(AO_stack_t *list)
+{
+  list -> AO_val1 = 0;
+  list -> AO_val2 = 0;
+}
+
+#define AO_REAL_HEAD_PTR(x) (AO_t *)((x).AO_val2)
+#define AO_REAL_NEXT_PTR(x) (AO_t *)(x)
+
+void AO_stack_push_release(AO_stack_t *list, AO_t *new_element);
+#define AO_HAVE_stack_push_release
+AO_t * AO_stack_pop_acquire(AO_stack_t *list);
+#define AO_HAVE_stack_pop_acquire
+
+#endif /* Wide CAS case */
+
+#if defined(AO_HAVE_stack_push_release) && !defined(AO_HAVE_stack_push)
+# define AO_stack_push(l, e) AO_stack_push_release(l, e)
+# define AO_HAVE_stack_push
+#endif
+
+#if defined(AO_HAVE_stack_pop_acquire) && !defined(AO_HAVE_stack_pop)
+# define AO_stack_pop(l) AO_stack_pop_acquire(l)
+# define AO_HAVE_stack_pop
+#endif
+
+#endif /* !AO_STACK_H */
diff --git a/src/gc/bdwgc/backgraph.c b/src/gc/bdwgc/backgraph.c
new file mode 100644
index 0000000..637a11e
--- /dev/null
+++ b/src/gc/bdwgc/backgraph.c
@@ -0,0 +1,483 @@
+/*
+ * Copyright (c) 2001 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "private/dbg_mlc.h"
+
+/*
+ * This implements a full, though not well-tuned, representation of the
+ * backwards points-to graph.  This is used to test for non-GC-robust
+ * data structures; the code is not used during normal garbage collection.
+ *
+ * One restriction is that we drop all back-edges from nodes with very
+ * high in-degree, and simply add them add them to a list of such
+ * nodes.  They are then treated as permanent roots.  Id this by itself
+ * doesn't introduce a space leak, then such nodes can't contribute to
+ * a growing space leak.
+ */
+
+#ifdef MAKE_BACK_GRAPH
+
+#define MAX_IN  10      /* Maximum in-degree we handle directly */
+
+/* #include <unistd.h> */
+
+#if !defined(DBG_HDRS_ALL) || (ALIGNMENT != CPP_WORDSZ/8) /* || !defined(UNIX_LIKE) */
+# error The configuration does not support MAKE_BACK_GRAPH
+#endif
+
+/* We store single back pointers directly in the object's oh_bg_ptr field. */
+/* If there is more than one ptr to an object, we store q | FLAG_MANY,     */
+/* where q is a pointer to a back_edges object.                            */
+/* Every once in a while we use a back_edges object even for a single      */
+/* pointer, since we need the other fields in the back_edges structure to  */
+/* be present in some fraction of the objects.  Otherwise we get serious   */
+/* performance issues.                                                     */
+#define FLAG_MANY 2
+
+typedef struct back_edges_struct {
+  word n_edges; /* Number of edges, including those in continuation     */
+                /* structures.                                          */
+  unsigned short flags;
+#       define RETAIN 1 /* Directly points to a reachable object;       */
+                        /* retain for next GC.                          */
+  unsigned short height_gc_no;
+                /* If height > 0, then the GC_gc_no value when it       */
+                /* was computed.  If it was computed this cycle, then   */
+                /* it is current.  If it was computed during the        */
+                /* last cycle, then it represents the old height,       */
+                /* which is only saved for live objects referenced by   */
+                /* dead ones.  This may grow due to refs from newly     */
+                /* dead objects.                                        */
+  signed_word height;
+                /* Longest path through unreachable nodes to this node  */
+                /* that we found using depth first search.              */
+
+#   define HEIGHT_UNKNOWN ((signed_word)(-2))
+#   define HEIGHT_IN_PROGRESS ((signed_word)(-1))
+  ptr_t edges[MAX_IN];
+  struct back_edges_struct *cont;
+                /* Pointer to continuation structure; we use only the   */
+                /* edges field in the continuation.                     */
+                /* also used as free list link.                         */
+} back_edges;
+
+/* Allocate a new back edge structure.  Should be more sophisticated    */
+/* if this were production code.                                        */
+#define MAX_BACK_EDGE_STRUCTS 100000
+static back_edges *back_edge_space = 0;
+STATIC int GC_n_back_edge_structs = 0;
+                                /* Serves as pointer to never used      */
+                                /* back_edges space.                    */
+static back_edges *avail_back_edges = 0;
+                                /* Pointer to free list of deallocated  */
+                                /* back_edges structures.               */
+
+static back_edges * new_back_edges(void)
+{
+  if (0 == back_edge_space) {
+    back_edge_space = (back_edges *)GET_MEM(
+                        ROUNDUP_PAGESIZE_IF_MMAP(MAX_BACK_EDGE_STRUCTS
+                                                  * sizeof(back_edges)));
+    if (NULL == back_edge_space)
+      ABORT("Insufficient memory for back edges");
+    GC_add_to_our_memory((ptr_t)back_edge_space,
+                         MAX_BACK_EDGE_STRUCTS*sizeof(back_edges));
+  }
+  if (0 != avail_back_edges) {
+    back_edges * result = avail_back_edges;
+    avail_back_edges = result -> cont;
+    result -> cont = 0;
+    return result;
+  }
+  if (GC_n_back_edge_structs >= MAX_BACK_EDGE_STRUCTS - 1) {
+    ABORT("Needed too much space for back edges: adjust "
+          "MAX_BACK_EDGE_STRUCTS");
+  }
+  return back_edge_space + (GC_n_back_edge_structs++);
+}
+
+/* Deallocate p and its associated continuation structures.     */
+static void deallocate_back_edges(back_edges *p)
+{
+   back_edges *last = p;
+
+   while (0 != last -> cont) last = last -> cont;
+   last -> cont = avail_back_edges;
+   avail_back_edges = p;
+}
+
+/* Table of objects that are currently on the depth-first search        */
+/* stack.  Only objects with in-degree one are in this table.           */
+/* Other objects are identified using HEIGHT_IN_PROGRESS.               */
+/* FIXME: This data structure NEEDS IMPROVEMENT.                        */
+#define INITIAL_IN_PROGRESS 10000
+static ptr_t * in_progress_space = 0;
+static size_t in_progress_size = 0;
+static size_t n_in_progress = 0;
+
+static void push_in_progress(ptr_t p)
+{
+  if (n_in_progress >= in_progress_size) {
+    if (in_progress_size == 0) {
+      in_progress_size = ROUNDUP_PAGESIZE_IF_MMAP(INITIAL_IN_PROGRESS
+                                                        * sizeof(ptr_t))
+                                / sizeof(ptr_t);
+      in_progress_space = (ptr_t *)GET_MEM(in_progress_size * sizeof(ptr_t));
+      GC_add_to_our_memory((ptr_t)in_progress_space,
+                           in_progress_size * sizeof(ptr_t));
+    } else {
+      ptr_t * new_in_progress_space;
+      in_progress_size *= 2;
+      new_in_progress_space = (ptr_t *)
+                                GET_MEM(in_progress_size * sizeof(ptr_t));
+      GC_add_to_our_memory((ptr_t)new_in_progress_space,
+                           in_progress_size * sizeof(ptr_t));
+      if (new_in_progress_space != NULL)
+        BCOPY(in_progress_space, new_in_progress_space,
+              n_in_progress * sizeof(ptr_t));
+      in_progress_space = new_in_progress_space;
+      /* FIXME: This just drops the old space.  */
+    }
+  }
+  if (in_progress_space == 0)
+      ABORT("MAKE_BACK_GRAPH: Out of in-progress space: "
+            "Huge linear data structure?");
+  in_progress_space[n_in_progress++] = p;
+}
+
+static GC_bool is_in_progress(ptr_t p)
+{
+  size_t i;
+  for (i = 0; i < n_in_progress; ++i) {
+    if (in_progress_space[i] == p) return TRUE;
+  }
+  return FALSE;
+}
+
+GC_INLINE void pop_in_progress(ptr_t p)
+{
+  --n_in_progress;
+  GC_ASSERT(in_progress_space[n_in_progress] == p);
+}
+
+#define GET_OH_BG_PTR(p) \
+                (ptr_t)GC_REVEAL_POINTER(((oh *)(p)) -> oh_bg_ptr)
+#define SET_OH_BG_PTR(p,q) (((oh *)(p)) -> oh_bg_ptr = GC_HIDE_POINTER(q))
+
+/* Execute s once for each predecessor q of p in the points-to graph.   */
+/* s should be a bracketed statement.  We declare q.                    */
+#define FOR_EACH_PRED(q, p, s) \
+  { \
+    ptr_t q = GET_OH_BG_PTR(p); \
+    if (!((word)q & FLAG_MANY)) { \
+      if (q && !((word)q & 1)) s \
+              /* !((word)q & 1) checks for a misinterpreted freelist link */ \
+    } else { \
+      back_edges *orig_be_ = (back_edges *)((word)q & ~FLAG_MANY); \
+      back_edges *be_ = orig_be_; \
+      int local_; \
+      word total_; \
+      word n_edges_ = be_ -> n_edges; \
+      for (total_ = 0, local_ = 0; total_ < n_edges_; ++local_, ++total_) { \
+          if (local_ == MAX_IN) { \
+              be_ = be_ -> cont; \
+              local_ = 0; \
+          } \
+          q = be_ -> edges[local_]; s \
+      } \
+    } \
+  }
+
+/* Ensure that p has a back_edges structure associated with it. */
+static void ensure_struct(ptr_t p)
+{
+  ptr_t old_back_ptr = GET_OH_BG_PTR(p);
+
+  if (!((word)old_back_ptr & FLAG_MANY)) {
+    back_edges *be = new_back_edges();
+    be -> flags = 0;
+    if (0 == old_back_ptr) {
+      be -> n_edges = 0;
+    } else {
+      be -> n_edges = 1;
+      be -> edges[0] = old_back_ptr;
+    }
+    be -> height = HEIGHT_UNKNOWN;
+    be -> height_gc_no = (unsigned short)(GC_gc_no - 1);
+    GC_ASSERT(be >= back_edge_space);
+    SET_OH_BG_PTR(p, (word)be | FLAG_MANY);
+  }
+}
+
+/* Add the (forward) edge from p to q to the backward graph.  Both p    */
+/* q are pointers to the object base, i.e. pointers to an oh.           */
+static void add_edge(ptr_t p, ptr_t q)
+{
+    ptr_t old_back_ptr = GET_OH_BG_PTR(q);
+    back_edges * be, *be_cont;
+    word i;
+    static unsigned random_number = 13;
+#   define GOT_LUCKY_NUMBER (((++random_number) & 0x7f) == 0)
+      /* A not very random number we use to occasionally allocate a     */
+      /* back_edges structure even for a single backward edge.  This    */
+      /* prevents us from repeatedly tracing back through very long     */
+      /* chains, since we will have some place to store height and      */
+      /* in_progress flags along the way.                               */
+
+    GC_ASSERT(p == GC_base(p) && q == GC_base(q));
+    if (!GC_HAS_DEBUG_INFO(q) || !GC_HAS_DEBUG_INFO(p)) {
+      /* This is really a misinterpreted free list link, since we saw   */
+      /* a pointer to a free list.  Don't overwrite it!                 */
+      return;
+    }
+    if (0 == old_back_ptr) {
+        SET_OH_BG_PTR(q, p);
+        if (GOT_LUCKY_NUMBER) ensure_struct(q);
+        return;
+    }
+    /* Check whether it was already in the list of predecessors. */
+      FOR_EACH_PRED(pred, q, { if (p == pred) return; });
+    ensure_struct(q);
+    old_back_ptr = GET_OH_BG_PTR(q);
+    be = (back_edges *)((word)old_back_ptr & ~FLAG_MANY);
+    for (i = be -> n_edges, be_cont = be; i > MAX_IN; i -= MAX_IN)
+        be_cont = be_cont -> cont;
+    if (i == MAX_IN) {
+        be_cont -> cont = new_back_edges();
+        be_cont = be_cont -> cont;
+        i = 0;
+    }
+    be_cont -> edges[i] = p;
+    be -> n_edges++;
+#   ifdef DEBUG_PRINT_BIG_N_EDGES
+      if (GC_print_stats == VERBOSE && be -> n_edges == 100) {
+        GC_err_printf("The following object has big in-degree:\n");
+        GC_print_heap_obj(q);
+      }
+#   endif
+}
+
+typedef void (*per_object_func)(ptr_t p, size_t n_bytes, word gc_descr);
+
+static void per_object_helper(struct hblk *h, word fn)
+{
+  hdr * hhdr = HDR(h);
+  size_t sz = hhdr -> hb_sz;
+  word descr = hhdr -> hb_descr;
+  per_object_func f = (per_object_func)fn;
+  int i = 0;
+
+  do {
+    f((ptr_t)(h -> hb_body + i), sz, descr);
+    i += (int)sz;
+  } while ((word)i + sz <= BYTES_TO_WORDS(HBLKSIZE));
+}
+
+GC_INLINE void GC_apply_to_each_object(per_object_func f)
+{
+  GC_apply_to_all_blocks(per_object_helper, (word)f);
+}
+
+/*ARGSUSED*/
+static void reset_back_edge(ptr_t p, size_t n_bytes, word gc_descr)
+{
+  /* Skip any free list links, or dropped blocks */
+  if (GC_HAS_DEBUG_INFO(p)) {
+    ptr_t old_back_ptr = GET_OH_BG_PTR(p);
+    if ((word)old_back_ptr & FLAG_MANY) {
+      back_edges *be = (back_edges *)((word)old_back_ptr & ~FLAG_MANY);
+      if (!(be -> flags & RETAIN)) {
+        deallocate_back_edges(be);
+        SET_OH_BG_PTR(p, 0);
+      } else {
+
+        GC_ASSERT(GC_is_marked(p));
+
+        /* Back edges may point to objects that will not be retained.   */
+        /* Delete them for now, but remember the height.                */
+        /* Some will be added back at next GC.                          */
+          be -> n_edges = 0;
+          if (0 != be -> cont) {
+            deallocate_back_edges(be -> cont);
+            be -> cont = 0;
+          }
+
+        GC_ASSERT(GC_is_marked(p));
+
+        /* We only retain things for one GC cycle at a time.            */
+          be -> flags &= ~RETAIN;
+      }
+    } else /* Simple back pointer */ {
+      /* Clear to avoid dangling pointer. */
+      SET_OH_BG_PTR(p, 0);
+    }
+  }
+}
+
+static void add_back_edges(ptr_t p, size_t n_bytes, word gc_descr)
+{
+  word *currentp = (word *)(p + sizeof(oh));
+
+  /* For now, fix up non-length descriptors conservatively.     */
+    if((gc_descr & GC_DS_TAGS) != GC_DS_LENGTH) {
+      gc_descr = n_bytes;
+    }
+  while (currentp < (word *)(p + gc_descr)) {
+    word current = *currentp++;
+    FIXUP_POINTER(current);
+    if (current >= (word)GC_least_plausible_heap_addr &&
+        current <= (word)GC_greatest_plausible_heap_addr) {
+       ptr_t target = GC_base((void *)current);
+       if (0 != target) {
+         add_edge(p, target);
+       }
+    }
+  }
+}
+
+/* Rebuild the representation of the backward reachability graph.       */
+/* Does not examine mark bits.  Can be called before GC.                */
+GC_INNER void GC_build_back_graph(void)
+{
+  GC_apply_to_each_object(add_back_edges);
+}
+
+/* Return an approximation to the length of the longest simple path     */
+/* through unreachable objects to p.  We refer to this as the height    */
+/* of p.                                                                */
+static word backwards_height(ptr_t p)
+{
+  word result;
+  ptr_t back_ptr = GET_OH_BG_PTR(p);
+  back_edges *be;
+
+  if (0 == back_ptr) return 1;
+  if (!((word)back_ptr & FLAG_MANY)) {
+    if (is_in_progress(p)) return 0; /* DFS back edge, i.e. we followed */
+                                     /* an edge to an object already    */
+                                     /* on our stack: ignore            */
+    push_in_progress(p);
+    result = backwards_height(back_ptr)+1;
+    pop_in_progress(p);
+    return result;
+  }
+  be = (back_edges *)((word)back_ptr & ~FLAG_MANY);
+  if (be -> height >= 0 && be -> height_gc_no == (unsigned short)GC_gc_no)
+      return be -> height;
+  /* Ignore back edges in DFS */
+    if (be -> height == HEIGHT_IN_PROGRESS) return 0;
+  result = (be -> height > 0? be -> height : 1);
+  be -> height = HEIGHT_IN_PROGRESS;
+  FOR_EACH_PRED(q, p, {
+    word this_height;
+    if (GC_is_marked(q) && !(FLAG_MANY & (word)GET_OH_BG_PTR(p))) {
+      if (GC_print_stats)
+          GC_log_printf("Found bogus pointer from %p to %p\n", q, p);
+        /* Reachable object "points to" unreachable one.                */
+        /* Could be caused by our lax treatment of GC descriptors.      */
+      this_height = 1;
+    } else {
+        this_height = backwards_height(q);
+    }
+    if (this_height >= result) result = this_height + 1;
+  });
+  be -> height = result;
+  be -> height_gc_no = (unsigned short)GC_gc_no;
+  return result;
+}
+
+STATIC word GC_max_height = 0;
+STATIC ptr_t GC_deepest_obj = NULL;
+
+/* Compute the maximum height of every unreachable predecessor p of a   */
+/* reachable object.  Arrange to save the heights of all such objects p */
+/* so that they can be used in calculating the height of objects in the */
+/* next GC.                                                             */
+/* Set GC_max_height to be the maximum height we encounter, and         */
+/* GC_deepest_obj to be the corresponding object.                       */
+/*ARGSUSED*/
+static void update_max_height(ptr_t p, size_t n_bytes, word gc_descr)
+{
+  if (GC_is_marked(p) && GC_HAS_DEBUG_INFO(p)) {
+    word p_height = 0;
+    ptr_t p_deepest_obj = 0;
+    ptr_t back_ptr;
+    back_edges *be = 0;
+
+    /* If we remembered a height last time, use it as a minimum.        */
+    /* It may have increased due to newly unreachable chains pointing   */
+    /* to p, but it can't have decreased.                               */
+    back_ptr = GET_OH_BG_PTR(p);
+    if (0 != back_ptr && ((word)back_ptr & FLAG_MANY)) {
+      be = (back_edges *)((word)back_ptr & ~FLAG_MANY);
+      if (be -> height != HEIGHT_UNKNOWN) p_height = be -> height;
+    }
+    FOR_EACH_PRED(q, p, {
+      if (!GC_is_marked(q) && GC_HAS_DEBUG_INFO(q)) {
+        word q_height;
+
+        q_height = backwards_height(q);
+        if (q_height > p_height) {
+          p_height = q_height;
+          p_deepest_obj = q;
+        }
+      }
+    });
+    if (p_height > 0) {
+      /* Remember the height for next time. */
+        if (be == 0) {
+          ensure_struct(p);
+          back_ptr = GET_OH_BG_PTR(p);
+          be = (back_edges *)((word)back_ptr & ~FLAG_MANY);
+        }
+        be -> flags |= RETAIN;
+        be -> height = p_height;
+        be -> height_gc_no = (unsigned short)GC_gc_no;
+    }
+    if (p_height > GC_max_height) {
+        GC_max_height = p_height;
+        GC_deepest_obj = p_deepest_obj;
+    }
+  }
+}
+
+STATIC word GC_max_max_height = 0;
+
+GC_INNER void GC_traverse_back_graph(void)
+{
+  GC_max_height = 0;
+  GC_apply_to_each_object(update_max_height);
+  if (0 != GC_deepest_obj)
+    GC_set_mark_bit(GC_deepest_obj);  /* Keep it until we can print it. */
+}
+
+void GC_print_back_graph_stats(void)
+{
+  GC_printf("Maximum backwards height of reachable objects at GC %lu is %ld\n",
+            (unsigned long) GC_gc_no, (unsigned long)GC_max_height);
+  if (GC_max_height > GC_max_max_height) {
+    GC_max_max_height = GC_max_height;
+    GC_printf("The following unreachable object is last in a longest chain "
+              "of unreachable objects:\n");
+    GC_print_heap_obj(GC_deepest_obj);
+  }
+  if (GC_print_stats) {
+    GC_log_printf("Needed max total of %d back-edge structs\n",
+                  GC_n_back_edge_structs);
+  }
+  GC_apply_to_each_object(reset_back_edge);
+  GC_deepest_obj = 0;
+}
+
+#endif /* MAKE_BACK_GRAPH */
diff --git a/src/gc/bdwgc/bdwgc.c b/src/gc/bdwgc/bdwgc.c
new file mode 100644
index 0000000..60be14b
--- /dev/null
+++ b/src/gc/bdwgc/bdwgc.c
@@ -0,0 +1,145 @@
+/* 
+ * This file is part of the Nautilus AeroKernel developed
+ * by the Hobbes and V3VEE Projects with funding from the 
+ * United States National  Science Foundation and the Department of Energy.  
+ *
+ * The V3VEE Project is a joint project between Northwestern University
+ * and the University of New Mexico.  The Hobbes Project is a collaboration
+ * led by Sandia National Laboratories that includes several national 
+ * laboratories and universities. You can find out more at:
+ * http://www.v3vee.org  and
+ * http://xstack.sandia.gov/hobbes
+ *
+ * Copyright (c) 2017, Matt George <11georgem@gmail.com>
+ * Copyright (c) 2017, The V3VEE Project  <http://www.v3vee.org> 
+ *                     The Hobbes Project <http://xstack.sandia.gov/hobbes>
+ * All rights reserved.
+ *
+ * Authors:  Matt George <11georgem@gmail.com>
+ *
+ * This is free software.  You are permitted to use,
+ * redistribute, and modify it as specified in the file "LICENSE.txt".
+ */
+
+
+
+
+struct nk_thread;
+
+#include <nautilus/thread.h>
+#include <gc/bdwgc/bdwgc.h>
+#include "bdwgc_internal.h"
+#include "gc.h"
+
+
+typedef unsigned long word;
+
+extern NK_LOCK_T GC_allocate_ml;
+#define LOCK() NK_LOCK(&GC_allocate_ml)
+#define UNLOCK() NK_UNLOCK(&GC_allocate_ml)
+#define DCL_LOCK_STATE
+
+
+//#define MALLOC(x) ({ void *p  = malloc((x)+2*PAD); if (!p) { panic("Failed to Allocate %d bytes\n",x); } memset(p,0,(x)+2*PAD); p+PAD; })
+
+extern nk_tls_key_t GC_thread_key;
+extern int  keys_initialized;
+
+
+void GC_init_thread_local(GC_tlfs p); // included from thread_local_alloc.h
+
+/* Called from GC_inner_start_routine().  Defined in this file to       */
+/* minimize the number of include files in pthread_start.c (because     */
+/* sem_t and sem_post() are not used that file directly).               */
+nk_thread_t *
+GC_start_rtn_prepare_thread(void *(**pstart)(void *),
+                            void **pstart_arg,
+                            struct GC_stack_base *sb, void *arg)
+{
+  struct start_info * si = arg;
+  nk_thread_t * me = get_cur_thread();
+  DCL_LOCK_STATE;
+
+  BDWGC_DEBUG("Starting thread %p, sp = %p\n", me, &arg);
+
+  LOCK();
+  //me = GC_register_my_thread_inner(sb, me);
+  BDWGC_SPECIFIC_THREAD_STATE(me) -> traced_stack_sect = NULL;
+  BDWGC_SPECIFIC_THREAD_STATE(me) -> flags = si -> flags;
+
+  GC_init_thread_local(&(BDWGC_SPECIFIC_THREAD_STATE(me) -> tlfs));
+
+  UNLOCK();
+  
+  *pstart = si -> start_routine;
+
+  //BDWGC_DEBUG("start_routine = %p\n", (void *)(signed_word)(*pstart));
+
+  *pstart_arg = si -> arg;
+  //  sem_post(&(si -> registered));      /* Last action on si.   */
+  /* OK to deallocate.    */
+  return me;
+}
+
+
+void *nk_gc_bdwgc_thread_state_init(struct nk_thread *thread)
+{
+  bdwgc_thread_state * s = (bdwgc_thread_state *)malloc(sizeof(bdwgc_thread_state));
+
+  if (!s) { 
+      BDWGC_ERROR("Failed to allocate per-thread GC state for thread %ld\n",thread->tid);
+      return 0;
+  }
+
+  s -> traced_stack_sect = NULL;
+  s -> thread_blocked = false;
+  /* if (0 != nk_tls_key_create(&GC_thread_key, 0)) { */
+  /*   panic("Failed to create key for local allocator"); */
+  /* } */
+
+  /* if (0 != nk_tls_set(GC_thread_key, tlfs)) { */
+  /*   panic("Failed to set thread specific allocation pointers"); */
+  /* } */
+
+  int i;
+  for (i = 1; i < GC_TINY_FREELISTS; ++i) {
+    s -> tlfs.ptrfree_freelists[i] = (void *)(unsigned long)1;
+    s -> tlfs.normal_freelists[i]  = (void *)(unsigned long)1;
+  }
+  /* Set up the size 0 free lists.    */
+  /* We now handle most of them like regular free lists, to ensure    */
+  /* That explicit deallocation works.  */
+  s -> tlfs.ptrfree_freelists[0] = (void *)(unsigned long)1;
+  s -> tlfs.normal_freelists[0]  = (void *)(unsigned long)1;
+  
+  BDWGC_DEBUG("Initialized per-thread GC state at %p for thread %ld\n", s,thread->tid);
+
+  return s;
+}
+
+void nk_gc_bdwgc_thread_state_deinit(struct nk_thread *t)
+{
+    BDWGC_DEBUG("Freeing per-thread GC state at %p for thread %ld\n", t->gc_state, t->tid);
+    free(t->gc_state);
+}
+
+
+int  nk_gc_bdwgc_init()
+{
+    //    GC_COND_INIT();
+    BDWGC_INFO("inited\n");
+    return 0;
+}
+void nk_gc_bdwgc_deinit()
+{
+    BDWGC_INFO("deinited\n");
+}
+
+#ifdef NAUT_CONFIG_TEST_BDWGC
+int  nk_gc_bdwgc_test()
+{
+    extern int bdwgc_test();
+
+    return bdwgc_test();
+}
+#endif
diff --git a/src/gc/bdwgc/blacklst.c b/src/gc/bdwgc/blacklst.c
new file mode 100644
index 0000000..de3199d
--- /dev/null
+++ b/src/gc/bdwgc/blacklst.c
@@ -0,0 +1,289 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+/*
+ * We maintain several hash tables of hblks that have had false hits.
+ * Each contains one bit per hash bucket;  If any page in the bucket
+ * has had a false hit, we assume that all of them have.
+ * See the definition of page_hash_table in gc_private.h.
+ * False hits from the stack(s) are much more dangerous than false hits
+ * from elsewhere, since the former can pin a large object that spans the
+ * block, even though it does not start on the dangerous block.
+ */
+
+/*
+ * Externally callable routines are:
+
+ * GC_add_to_black_list_normal
+ * GC_add_to_black_list_stack
+ * GC_promote_black_lists
+ * GC_is_black_listed
+ *
+ * All require that the allocator lock is held.
+ */
+
+/* Pointers to individual tables.  We replace one table by another by   */
+/* switching these pointers.                                            */
+STATIC word * GC_old_normal_bl = NULL;
+                /* Nonstack false references seen at last full          */
+                /* collection.                                          */
+STATIC word * GC_incomplete_normal_bl = NULL;
+                /* Nonstack false references seen since last            */
+                /* full collection.                                     */
+STATIC word * GC_old_stack_bl = NULL;
+STATIC word * GC_incomplete_stack_bl = NULL;
+
+STATIC word GC_total_stack_black_listed = 0;
+                        /* Number of bytes on stack blacklist.  */
+
+GC_INNER word GC_black_list_spacing = MINHINCR * HBLKSIZE;
+                        /* Initial rough guess. */
+
+STATIC void GC_clear_bl(word *);
+
+GC_INNER void GC_default_print_heap_obj_proc(ptr_t p)
+{
+    ptr_t base = GC_base(p);
+    GC_err_printf("start: %p, appr. length: %ld", base,
+                  (unsigned long)GC_size(base));
+}
+
+GC_INNER void (*GC_print_heap_obj)(ptr_t p) = GC_default_print_heap_obj_proc;
+
+#ifdef PRINT_BLACK_LIST
+STATIC void GC_print_source_ptr(ptr_t p)
+{
+    ptr_t base = GC_base(p);
+    if (0 == base) {
+        if (0 == p) {
+            GC_err_printf("in register");
+        } else {
+            GC_err_printf("in root set");
+        }
+    } else {
+        GC_err_printf("in object at ");
+        /* FIXME: We can't call the debug version of GC_print_heap_obj  */
+        /* (with PRINT_CALL_CHAIN) here because the lock is held and    */
+        /* the world is stopped.                                        */
+        GC_default_print_heap_obj_proc(base);
+    }
+}
+#endif
+
+GC_INNER void GC_bl_init_no_interiors(void)
+{
+  if (GC_incomplete_normal_bl == 0) {
+    GC_old_normal_bl = (word *)GC_scratch_alloc(sizeof(page_hash_table));
+    GC_incomplete_normal_bl = (word *)GC_scratch_alloc(
+                                                  sizeof(page_hash_table));
+    if (GC_old_normal_bl == 0 || GC_incomplete_normal_bl == 0) {
+      GC_err_printf("Insufficient memory for black list\n");
+      EXIT();
+    }
+    GC_clear_bl(GC_old_normal_bl);
+    GC_clear_bl(GC_incomplete_normal_bl);
+  }
+}
+
+GC_INNER void GC_bl_init(void)
+{
+    if (!GC_all_interior_pointers) {
+      GC_bl_init_no_interiors();
+    }
+    GC_old_stack_bl = (word *)GC_scratch_alloc(sizeof(page_hash_table));
+    GC_incomplete_stack_bl = (word *)GC_scratch_alloc(sizeof(page_hash_table));
+    if (GC_old_stack_bl == 0 || GC_incomplete_stack_bl == 0) {
+        GC_err_printf("Insufficient memory for black list\n");
+        EXIT();
+    }
+    GC_clear_bl(GC_old_stack_bl);
+    GC_clear_bl(GC_incomplete_stack_bl);
+}
+
+STATIC void GC_clear_bl(word *doomed)
+{
+    BZERO(doomed, sizeof(page_hash_table));
+}
+
+STATIC void GC_copy_bl(word *old, word *new)
+{
+    BCOPY(old, new, sizeof(page_hash_table));
+}
+
+static word total_stack_black_listed(void);
+
+/* Signal the completion of a collection.  Turn the incomplete black    */
+/* lists into new black lists, etc.                                     */
+GC_INNER void GC_promote_black_lists(void)
+{
+    word * very_old_normal_bl = GC_old_normal_bl;
+    word * very_old_stack_bl = GC_old_stack_bl;
+
+    GC_old_normal_bl = GC_incomplete_normal_bl;
+    GC_old_stack_bl = GC_incomplete_stack_bl;
+    if (!GC_all_interior_pointers) {
+      GC_clear_bl(very_old_normal_bl);
+    }
+    GC_clear_bl(very_old_stack_bl);
+    GC_incomplete_normal_bl = very_old_normal_bl;
+    GC_incomplete_stack_bl = very_old_stack_bl;
+    GC_total_stack_black_listed = total_stack_black_listed();
+    if (GC_print_stats == VERBOSE)
+        GC_log_printf("%ld bytes in heap blacklisted for interior pointers\n",
+                      (unsigned long)GC_total_stack_black_listed);
+    if (GC_total_stack_black_listed != 0) {
+        GC_black_list_spacing =
+                HBLKSIZE*(GC_heapsize/GC_total_stack_black_listed);
+    }
+    if (GC_black_list_spacing < 3 * HBLKSIZE) {
+        GC_black_list_spacing = 3 * HBLKSIZE;
+    }
+    if (GC_black_list_spacing > MAXHINCR * HBLKSIZE) {
+        GC_black_list_spacing = MAXHINCR * HBLKSIZE;
+        /* Makes it easier to allocate really huge blocks, which otherwise */
+        /* may have problems with nonuniform blacklist distributions.      */
+        /* This way we should always succeed immediately after growing the */
+        /* heap.                                                           */
+    }
+}
+
+GC_INNER void GC_unpromote_black_lists(void)
+{
+    if (!GC_all_interior_pointers) {
+      GC_copy_bl(GC_old_normal_bl, GC_incomplete_normal_bl);
+    }
+    GC_copy_bl(GC_old_stack_bl, GC_incomplete_stack_bl);
+}
+
+/* P is not a valid pointer reference, but it falls inside      */
+/* the plausible heap bounds.                                   */
+/* Add it to the normal incomplete black list if appropriate.   */
+#ifdef PRINT_BLACK_LIST
+  GC_INNER void GC_add_to_black_list_normal(word p, ptr_t source)
+#else
+  GC_INNER void GC_add_to_black_list_normal(word p)
+#endif
+{
+  if (GC_modws_valid_offsets[p & (sizeof(word)-1)]) {
+    word index = PHT_HASH((word)p);
+
+    if (HDR(p) == 0 || get_pht_entry_from_index(GC_old_normal_bl, index)) {
+#     ifdef PRINT_BLACK_LIST
+        if (!get_pht_entry_from_index(GC_incomplete_normal_bl, index)) {
+          GC_err_printf("Black listing (normal) %p referenced from %p ",
+                        (ptr_t)p, source);
+          GC_print_source_ptr(source);
+          GC_err_puts("\n");
+        }
+#     endif
+      set_pht_entry_from_index(GC_incomplete_normal_bl, index);
+    } /* else this is probably just an interior pointer to an allocated */
+      /* object, and isn't worth black listing.                         */
+  }
+}
+
+/* And the same for false pointers from the stack. */
+#ifdef PRINT_BLACK_LIST
+  GC_INNER void GC_add_to_black_list_stack(word p, ptr_t source)
+#else
+  GC_INNER void GC_add_to_black_list_stack(word p)
+#endif
+{
+  word index = PHT_HASH((word)p);
+
+  if (HDR(p) == 0 || get_pht_entry_from_index(GC_old_stack_bl, index)) {
+#   ifdef PRINT_BLACK_LIST
+      if (!get_pht_entry_from_index(GC_incomplete_stack_bl, index)) {
+        GC_err_printf("Black listing (stack) %p referenced from %p ",
+                      (ptr_t)p, source);
+        GC_print_source_ptr(source);
+        GC_err_puts("\n");
+      }
+#   endif
+    set_pht_entry_from_index(GC_incomplete_stack_bl, index);
+  }
+}
+
+/*
+ * Is the block starting at h of size len bytes black listed?   If so,
+ * return the address of the next plausible r such that (r, len) might not
+ * be black listed.  (R may not actually be in the heap.  We guarantee only
+ * that every smaller value of r after h is also black listed.)
+ * If (h,len) is not black listed, return 0.
+ * Knows about the structure of the black list hash tables.
+ */
+struct hblk * GC_is_black_listed(struct hblk *h, word len)
+{
+    word index = PHT_HASH((word)h);
+    word i;
+    word nblocks;
+
+    if (!GC_all_interior_pointers
+        && (get_pht_entry_from_index(GC_old_normal_bl, index)
+            || get_pht_entry_from_index(GC_incomplete_normal_bl, index))) {
+      return (h+1);
+    }
+
+    nblocks = divHBLKSZ(len);
+    for (i = 0;;) {
+        if (GC_old_stack_bl[divWORDSZ(index)] == 0
+            && GC_incomplete_stack_bl[divWORDSZ(index)] == 0) {
+            /* An easy case */
+          i += WORDSZ - modWORDSZ(index);
+        } else {
+          if (get_pht_entry_from_index(GC_old_stack_bl, index)
+              || get_pht_entry_from_index(GC_incomplete_stack_bl, index)) {
+            return(h+i+1);
+          }
+          i++;
+        }
+        if (i >= nblocks) break;
+        index = PHT_HASH((word)(h+i));
+    }
+    return(0);
+}
+
+/* Return the number of blacklisted blocks in a given range.    */
+/* Used only for statistical purposes.                          */
+/* Looks only at the GC_incomplete_stack_bl.                    */
+STATIC word GC_number_stack_black_listed(struct hblk *start,
+                                         struct hblk *endp1)
+{
+    register struct hblk * h;
+    word result = 0;
+
+    for (h = start; h < endp1; h++) {
+        word index = PHT_HASH((word)h);
+
+        if (get_pht_entry_from_index(GC_old_stack_bl, index)) result++;
+    }
+    return(result);
+}
+
+/* Return the total number of (stack) black-listed bytes. */
+static word total_stack_black_listed(void)
+{
+    register unsigned i;
+    word total = 0;
+
+    for (i = 0; i < GC_n_heap_sects; i++) {
+        struct hblk * start = (struct hblk *) GC_heap_sects[i].hs_start;
+        struct hblk * endp1 = start + GC_heap_sects[i].hs_bytes/HBLKSIZE;
+
+        total += GC_number_stack_black_listed(start, endp1);
+    }
+    return(total * HBLKSIZE);
+}
diff --git a/src/gc/bdwgc/checksums.c b/src/gc/bdwgc/checksums.c
new file mode 100644
index 0000000..3f2af93
--- /dev/null
+++ b/src/gc/bdwgc/checksums.c
@@ -0,0 +1,228 @@
+/*
+ * Copyright (c) 1992-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifdef CHECKSUMS
+
+/* This is debugging code intended to verify the results of dirty bit   */
+/* computations. Works only in a single threaded environment.           */
+/* We assume that stubborn objects are changed only when they are       */
+/* enabled for writing.  (Certain kinds of writing are actually         */
+/* safe under other conditions.)                                        */
+#define NSUMS 10000
+
+#define OFFSET 0x10000
+
+typedef struct {
+        GC_bool new_valid;
+        word old_sum;
+        word new_sum;
+        struct hblk * block;    /* Block to which this refers + OFFSET  */
+                                /* to hide it from collector.           */
+} page_entry;
+
+page_entry GC_sums[NSUMS];
+
+STATIC word GC_faulted[NSUMS] = { 0 };
+                /* Record of pages on which we saw a write fault.       */
+
+STATIC size_t GC_n_faulted = 0;
+
+void GC_record_fault(struct hblk * h)
+{
+    word page = (word)h;
+
+    page += GC_page_size - 1;
+    page &= ~(GC_page_size - 1);
+    if (GC_n_faulted >= NSUMS) ABORT("write fault log overflowed");
+    GC_faulted[GC_n_faulted++] = page;
+}
+
+STATIC GC_bool GC_was_faulted(struct hblk *h)
+{
+    size_t i;
+    word page = (word)h;
+
+    page += GC_page_size - 1;
+    page &= ~(GC_page_size - 1);
+    for (i = 0; i < GC_n_faulted; ++i) {
+        if (GC_faulted[i] == page) return TRUE;
+    }
+    return FALSE;
+}
+
+STATIC word GC_checksum(struct hblk *h)
+{
+    word *p = (word *)h;
+    word *lim = (word *)(h+1);
+    word result = 0;
+
+    while (p < lim) {
+        result += *p++;
+    }
+    return(result | 0x80000000 /* doesn't look like pointer */);
+}
+
+#ifdef STUBBORN_ALLOC
+  /* Check whether a stubborn object from the given block appears on    */
+  /* the appropriate free list.                                         */
+  STATIC GC_bool GC_on_free_list(struct hblk *h)
+  {
+    hdr * hhdr = HDR(h);
+    size_t sz = BYTES_TO_WORDS(hhdr -> hb_sz);
+    ptr_t p;
+
+    if (sz > MAXOBJWORDS) return(FALSE);
+    for (p = GC_sobjfreelist[sz]; p != 0; p = obj_link(p)) {
+        if (HBLKPTR(p) == h) return(TRUE);
+    }
+    return(FALSE);
+  }
+#endif
+
+int GC_n_dirty_errors = 0;
+int GC_n_faulted_dirty_errors = 0;
+int GC_n_changed_errors = 0;
+int GC_n_clean = 0;
+int GC_n_dirty = 0;
+
+STATIC void GC_update_check_page(struct hblk *h, int index)
+{
+    page_entry *pe = GC_sums + index;
+    hdr * hhdr = HDR(h);
+    struct hblk *b;
+
+    if (pe -> block != 0 && pe -> block != h + OFFSET) ABORT("goofed");
+    pe -> old_sum = pe -> new_sum;
+    pe -> new_sum = GC_checksum(h);
+#   if !defined(MSWIN32) && !defined(MSWINCE)
+        if (pe -> new_sum != 0x80000000 && !GC_page_was_ever_dirty(h)) {
+            GC_err_printf("GC_page_was_ever_dirty(%p) is wrong\n", h);
+        }
+#   endif
+    if (GC_page_was_dirty(h)) {
+        GC_n_dirty++;
+    } else {
+        GC_n_clean++;
+    }
+    b = h;
+    while (IS_FORWARDING_ADDR_OR_NIL(hhdr) && hhdr != 0) {
+        b -= (word)hhdr;
+        hhdr = HDR(b);
+    }
+    if (pe -> new_valid
+        && hhdr != 0 && hhdr -> hb_descr != 0 /* may contain pointers */
+        && pe -> old_sum != pe -> new_sum) {
+        if (!GC_page_was_dirty(h) || !GC_page_was_ever_dirty(h)) {
+            GC_bool was_faulted = GC_was_faulted(h);
+            /* Set breakpoint here */GC_n_dirty_errors++;
+            if (was_faulted) GC_n_faulted_dirty_errors++;
+        }
+#       ifdef STUBBORN_ALLOC
+          if (!HBLK_IS_FREE(hhdr)
+            && hhdr -> hb_obj_kind == STUBBORN
+            && !GC_page_was_changed(h)
+            && !GC_on_free_list(h)) {
+            /* if GC_on_free_list(h) then reclaim may have touched it   */
+            /* without any allocations taking place.                    */
+            /* Set breakpoint here */GC_n_changed_errors++;
+          }
+#       endif
+    }
+    pe -> new_valid = TRUE;
+    pe -> block = h + OFFSET;
+}
+
+word GC_bytes_in_used_blocks = 0;
+
+/*ARGSUSED*/
+STATIC void GC_add_block(struct hblk *h, word dummy)
+{
+   hdr * hhdr = HDR(h);
+   size_t bytes = hhdr -> hb_sz;
+
+   bytes += HBLKSIZE-1;
+   bytes &= ~(HBLKSIZE-1);
+   GC_bytes_in_used_blocks += bytes;
+}
+
+STATIC void GC_check_blocks(void)
+{
+    word bytes_in_free_blocks = GC_large_free_bytes;
+
+    GC_bytes_in_used_blocks = 0;
+    GC_apply_to_all_blocks(GC_add_block, (word)0);
+    if (GC_print_stats)
+      GC_log_printf("GC_bytes_in_used_blocks = %lu,"
+                    " bytes_in_free_blocks = %lu, heapsize = %lu\n",
+                    (unsigned long)GC_bytes_in_used_blocks,
+                    (unsigned long)bytes_in_free_blocks,
+                    (unsigned long)GC_heapsize);
+    if (GC_bytes_in_used_blocks + bytes_in_free_blocks != GC_heapsize) {
+        GC_err_printf("LOST SOME BLOCKS!!\n");
+    }
+}
+
+/* Should be called immediately after GC_read_dirty and GC_read_changed. */
+void GC_check_dirty(void)
+{
+    int index;
+    unsigned i;
+    struct hblk *h;
+    ptr_t start;
+
+    GC_check_blocks();
+
+    GC_n_dirty_errors = 0;
+    GC_n_faulted_dirty_errors = 0;
+    GC_n_changed_errors = 0;
+    GC_n_clean = 0;
+    GC_n_dirty = 0;
+
+    index = 0;
+    for (i = 0; i < GC_n_heap_sects; i++) {
+        start = GC_heap_sects[i].hs_start;
+        for (h = (struct hblk *)start;
+             h < (struct hblk *)(start + GC_heap_sects[i].hs_bytes);
+             h++) {
+             GC_update_check_page(h, index);
+             index++;
+             if (index >= NSUMS) goto out;
+        }
+    }
+out:
+    if (GC_print_stats)
+      GC_log_printf("Checked %lu clean and %lu dirty pages\n",
+                    (unsigned long)GC_n_clean, (unsigned long)GC_n_dirty);
+    if (GC_n_dirty_errors > 0) {
+        GC_err_printf("Found %d dirty bit errors (%d were faulted)\n",
+                      GC_n_dirty_errors, GC_n_faulted_dirty_errors);
+    }
+    if (GC_n_changed_errors > 0) {
+        GC_err_printf("Found %lu changed bit errors\n",
+                      (unsigned long)GC_n_changed_errors);
+        GC_err_printf(
+                "These may be benign (provoked by nonpointer changes)\n");
+#       ifdef THREADS
+          GC_err_printf(
+            "Also expect 1 per thread currently allocating a stubborn obj\n");
+#       endif
+    }
+    for (i = 0; i < GC_n_faulted; ++i) {
+        GC_faulted[i] = 0; /* Don't expose block pointers to GC */
+    }
+    GC_n_faulted = 0;
+}
+
+#endif /* CHECKSUMS */
diff --git a/src/gc/bdwgc/darwin_stop_world.c b/src/gc/bdwgc/darwin_stop_world.c
new file mode 100644
index 0000000..f05dc68
--- /dev/null
+++ b/src/gc/bdwgc/darwin_stop_world.c
@@ -0,0 +1,667 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2010 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/pthread_support.h"
+
+/* This probably needs more porting work to ppc64. */
+
+#if defined(GC_DARWIN_THREADS)
+
+/* From "Inside Mac OS X - Mach-O Runtime Architecture" published by Apple
+   Page 49:
+   "The space beneath the stack pointer, where a new stack frame would normally
+   be allocated, is called the red zone. This area as shown in Figure 3-2 may
+   be used for any purpose as long as a new stack frame does not need to be
+   added to the stack."
+
+   Page 50: "If a leaf procedure's red zone usage would exceed 224 bytes, then
+   it must set up a stack frame just like routines that call other routines."
+*/
+#ifdef POWERPC
+# if CPP_WORDSZ == 32
+#   define PPC_RED_ZONE_SIZE 224
+# elif CPP_WORDSZ == 64
+#   define PPC_RED_ZONE_SIZE 320
+# endif
+#endif
+
+#ifndef DARWIN_DONT_PARSE_STACK
+
+typedef struct StackFrame {
+  unsigned long savedSP;
+  unsigned long savedCR;
+  unsigned long savedLR;
+  unsigned long reserved[2];
+  unsigned long savedRTOC;
+} StackFrame;
+
+GC_INNER ptr_t GC_FindTopOfStack(unsigned long stack_start)
+{
+  StackFrame *frame;
+
+  if (stack_start == 0) {
+# ifdef POWERPC
+#   if CPP_WORDSZ == 32
+      __asm__ __volatile__ ("lwz %0,0(r1)" : "=r" (frame));
+#   else
+      __asm__ __volatile__ ("ld %0,0(r1)" : "=r" (frame));
+#   endif
+# endif
+  } else {
+    frame = (StackFrame *)stack_start;
+  }
+
+# ifdef DEBUG_THREADS
+    /* GC_printf("FindTopOfStack start at sp = %p\n", frame); */
+# endif
+  while (frame->savedSP != 0) {
+    /* if there are no more stack frames, stop */
+
+    frame = (StackFrame*)frame->savedSP;
+
+    /* we do these next two checks after going to the next frame
+       because the LR for the first stack frame in the loop
+       is not set up on purpose, so we shouldn't check it. */
+    if ((frame->savedLR & ~0x3) == 0 || (frame->savedLR & ~0x3) == ~0x3)
+      break; /* if the next LR is bogus, stop */
+  }
+# ifdef DEBUG_THREADS
+    /* GC_printf("FindTopOfStack finish at sp = %p\n", frame); */
+# endif
+  return (ptr_t)frame;
+}
+
+#endif /* !DARWIN_DONT_PARSE_STACK */
+
+/* GC_query_task_threads controls whether to obtain the list of */
+/* the threads from the kernel or to use GC_threads table.      */
+#ifdef GC_NO_THREADS_DISCOVERY
+# define GC_query_task_threads FALSE
+#elif defined(GC_DISCOVER_TASK_THREADS)
+# define GC_query_task_threads TRUE
+#else
+  STATIC GC_bool GC_query_task_threads = FALSE;
+#endif /* !GC_NO_THREADS_DISCOVERY */
+
+/* Use implicit threads registration (all task threads excluding the GC */
+/* special ones are stoped and scanned).  Should be called before       */
+/* GC_INIT() (or, at least, before going multi-threaded).  Deprecated.  */
+GC_API void GC_CALL GC_use_threads_discovery(void)
+{
+# if defined(GC_NO_THREADS_DISCOVERY) || defined(DARWIN_DONT_PARSE_STACK)
+    ABORT("Darwin task-threads-based stop and push unsupported");
+# else
+    GC_ASSERT(!GC_need_to_lock);
+#   ifndef GC_DISCOVER_TASK_THREADS
+      GC_query_task_threads = TRUE;
+#   endif
+    GC_init_parallel(); /* just to be consistent with Win32 one */
+# endif
+}
+
+/* Evaluates the stack range for a given thread.  Returns the lower     */
+/* bound and sets *phi to the upper one.                                */
+STATIC ptr_t GC_stack_range_for(ptr_t *phi, thread_act_t thread, GC_thread p,
+                                GC_bool thread_blocked, mach_port_t my_thread)
+{
+  ptr_t lo;
+  if (thread == my_thread) {
+    GC_ASSERT(!thread_blocked);
+    lo = GC_approx_sp();
+#   ifndef DARWIN_DONT_PARSE_STACK
+      *phi = GC_FindTopOfStack(0);
+#   endif
+
+  } else if (thread_blocked) {
+    lo = p->stop_info.stack_ptr;
+#   ifndef DARWIN_DONT_PARSE_STACK
+      *phi = p->topOfStack;
+#   endif
+
+  } else {
+    /* MACHINE_THREAD_STATE_COUNT does not seem to be defined       */
+    /* everywhere.  Hence we use our own version.  Alternatively,   */
+    /* we could use THREAD_STATE_MAX (but seems to be not optimal). */
+    kern_return_t kern_result;
+    mach_msg_type_number_t thread_state_count = GC_MACH_THREAD_STATE_COUNT;
+    GC_THREAD_STATE_T state;
+
+    /* Get the thread state (registers, etc) */
+    kern_result = thread_get_state(thread, GC_MACH_THREAD_STATE,
+                                   (natural_t *)&state,
+                                   &thread_state_count);
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread_get_state returns value = %d\n", kern_result);
+#   endif
+    if (kern_result != KERN_SUCCESS)
+      ABORT("thread_get_state failed");
+
+#   if defined(I386)
+      lo = (void *)state.THREAD_FLD(esp);
+#     ifndef DARWIN_DONT_PARSE_STACK
+        *phi = GC_FindTopOfStack(state.THREAD_FLD(esp));
+#     endif
+      GC_push_one(state.THREAD_FLD(eax));
+      GC_push_one(state.THREAD_FLD(ebx));
+      GC_push_one(state.THREAD_FLD(ecx));
+      GC_push_one(state.THREAD_FLD(edx));
+      GC_push_one(state.THREAD_FLD(edi));
+      GC_push_one(state.THREAD_FLD(esi));
+      GC_push_one(state.THREAD_FLD(ebp));
+
+#   elif defined(X86_64)
+      lo = (void *)state.THREAD_FLD(rsp);
+#     ifndef DARWIN_DONT_PARSE_STACK
+        *phi = GC_FindTopOfStack(state.THREAD_FLD(rsp));
+#     endif
+      GC_push_one(state.THREAD_FLD(rax));
+      GC_push_one(state.THREAD_FLD(rbx));
+      GC_push_one(state.THREAD_FLD(rcx));
+      GC_push_one(state.THREAD_FLD(rdx));
+      GC_push_one(state.THREAD_FLD(rdi));
+      GC_push_one(state.THREAD_FLD(rsi));
+      GC_push_one(state.THREAD_FLD(rbp));
+      /* GC_push_one(state.THREAD_FLD(rsp)); */
+      GC_push_one(state.THREAD_FLD(r8));
+      GC_push_one(state.THREAD_FLD(r9));
+      GC_push_one(state.THREAD_FLD(r10));
+      GC_push_one(state.THREAD_FLD(r11));
+      GC_push_one(state.THREAD_FLD(r12));
+      GC_push_one(state.THREAD_FLD(r13));
+      GC_push_one(state.THREAD_FLD(r14));
+      GC_push_one(state.THREAD_FLD(r15));
+
+#   elif defined(POWERPC)
+      lo = (void *)(state.THREAD_FLD(r1) - PPC_RED_ZONE_SIZE);
+#     ifndef DARWIN_DONT_PARSE_STACK
+        *phi = GC_FindTopOfStack(state.THREAD_FLD(r1));
+#     endif
+      GC_push_one(state.THREAD_FLD(r0));
+      GC_push_one(state.THREAD_FLD(r2));
+      GC_push_one(state.THREAD_FLD(r3));
+      GC_push_one(state.THREAD_FLD(r4));
+      GC_push_one(state.THREAD_FLD(r5));
+      GC_push_one(state.THREAD_FLD(r6));
+      GC_push_one(state.THREAD_FLD(r7));
+      GC_push_one(state.THREAD_FLD(r8));
+      GC_push_one(state.THREAD_FLD(r9));
+      GC_push_one(state.THREAD_FLD(r10));
+      GC_push_one(state.THREAD_FLD(r11));
+      GC_push_one(state.THREAD_FLD(r12));
+      GC_push_one(state.THREAD_FLD(r13));
+      GC_push_one(state.THREAD_FLD(r14));
+      GC_push_one(state.THREAD_FLD(r15));
+      GC_push_one(state.THREAD_FLD(r16));
+      GC_push_one(state.THREAD_FLD(r17));
+      GC_push_one(state.THREAD_FLD(r18));
+      GC_push_one(state.THREAD_FLD(r19));
+      GC_push_one(state.THREAD_FLD(r20));
+      GC_push_one(state.THREAD_FLD(r21));
+      GC_push_one(state.THREAD_FLD(r22));
+      GC_push_one(state.THREAD_FLD(r23));
+      GC_push_one(state.THREAD_FLD(r24));
+      GC_push_one(state.THREAD_FLD(r25));
+      GC_push_one(state.THREAD_FLD(r26));
+      GC_push_one(state.THREAD_FLD(r27));
+      GC_push_one(state.THREAD_FLD(r28));
+      GC_push_one(state.THREAD_FLD(r29));
+      GC_push_one(state.THREAD_FLD(r30));
+      GC_push_one(state.THREAD_FLD(r31));
+
+#   elif defined(ARM32)
+      lo = (void *)state.__sp;
+#     ifndef DARWIN_DONT_PARSE_STACK
+        *phi = GC_FindTopOfStack(state.__sp);
+#     endif
+      GC_push_one(state.__r[0]);
+      GC_push_one(state.__r[1]);
+      GC_push_one(state.__r[2]);
+      GC_push_one(state.__r[3]);
+      GC_push_one(state.__r[4]);
+      GC_push_one(state.__r[5]);
+      GC_push_one(state.__r[6]);
+      GC_push_one(state.__r[7]);
+      GC_push_one(state.__r[8]);
+      GC_push_one(state.__r[9]);
+      GC_push_one(state.__r[10]);
+      GC_push_one(state.__r[11]);
+      GC_push_one(state.__r[12]);
+      /* GC_push_one(state.__sp); */
+      GC_push_one(state.__lr);
+      /* GC_push_one(state.__pc); */
+      GC_push_one(state.__cpsr);
+
+#   else
+#     error FIXME for non-x86 || ppc || arm architectures
+#   endif
+  } /* thread != my_thread */
+
+# ifdef DARWIN_DONT_PARSE_STACK
+    /* p is guaranteed to be non-NULL regardless of GC_query_task_threads. */
+    *phi = (p->flags & MAIN_THREAD) != 0 ? GC_stackbottom : p->stack_end;
+# endif
+# ifdef DEBUG_THREADS
+    GC_log_printf("Darwin: Stack for thread 0x%lx = [%p,%p)\n",
+                  (unsigned long)thread, lo, *phi);
+# endif
+  return lo;
+}
+
+GC_INNER void GC_push_all_stacks(void)
+{
+  int i;
+  ptr_t lo, hi;
+  task_t my_task = current_task();
+  mach_port_t my_thread = mach_thread_self();
+  GC_bool found_me = FALSE;
+  int nthreads = 0;
+  word total_size = 0;
+  mach_msg_type_number_t listcount = (mach_msg_type_number_t)THREAD_TABLE_SZ;
+  if (!GC_thr_initialized)
+    GC_thr_init();
+
+# ifndef DARWIN_DONT_PARSE_STACK
+    if (GC_query_task_threads) {
+      kern_return_t kern_result;
+      thread_act_array_t act_list = 0;
+
+      /* Obtain the list of the threads from the kernel.  */
+      kern_result = task_threads(my_task, &act_list, &listcount);
+      if (kern_result != KERN_SUCCESS)
+        ABORT("task_threads failed");
+
+      for (i = 0; i < (int)listcount; i++) {
+        thread_act_t thread = act_list[i];
+        lo = GC_stack_range_for(&hi, thread, NULL, FALSE, my_thread);
+        GC_ASSERT(lo <= hi);
+        total_size += hi - lo;
+        GC_push_all_stack(lo, hi);
+        nthreads++;
+        if (thread == my_thread)
+          found_me = TRUE;
+        mach_port_deallocate(my_task, thread);
+      } /* for (i=0; ...) */
+
+      vm_deallocate(my_task, (vm_address_t)act_list,
+                    sizeof(thread_t) * listcount);
+    } else
+# endif /* !DARWIN_DONT_PARSE_STACK */
+  /* else */ {
+    for (i = 0; i < (int)listcount; i++) {
+      GC_thread p;
+      for (p = GC_threads[i]; p != NULL; p = p->next)
+        if ((p->flags & FINISHED) == 0) {
+          thread_act_t thread = (thread_act_t)p->stop_info.mach_thread;
+          lo = GC_stack_range_for(&hi, thread, p, (GC_bool)p->thread_blocked,
+                                  my_thread);
+          GC_ASSERT(lo <= hi);
+          total_size += hi - lo;
+          GC_push_all_stack_sections(lo, hi, p->traced_stack_sect);
+          nthreads++;
+          if (thread == my_thread)
+            found_me = TRUE;
+        }
+    } /* for (i=0; ...) */
+  }
+
+  mach_port_deallocate(my_task, my_thread);
+  if (GC_print_stats == VERBOSE)
+    GC_log_printf("Pushed %d thread stacks\n", nthreads);
+  if (!found_me && !GC_in_thread_creation)
+    ABORT("Collecting from unknown thread");
+  GC_total_stacksize = total_size;
+}
+
+#ifndef GC_NO_THREADS_DISCOVERY
+
+# ifdef MPROTECT_VDB
+    STATIC mach_port_t GC_mach_handler_thread = 0;
+    STATIC GC_bool GC_use_mach_handler_thread = FALSE;
+
+    GC_INNER void GC_darwin_register_mach_handler_thread(mach_port_t thread)
+    {
+      GC_mach_handler_thread = thread;
+      GC_use_mach_handler_thread = TRUE;
+    }
+# endif /* MPROTECT_VDB */
+
+# ifndef GC_MAX_MACH_THREADS
+#   define GC_MAX_MACH_THREADS THREAD_TABLE_SZ
+# endif
+
+  struct GC_mach_thread {
+    thread_act_t thread;
+    GC_bool already_suspended;
+  };
+
+  struct GC_mach_thread GC_mach_threads[GC_MAX_MACH_THREADS];
+  STATIC int GC_mach_threads_count = 0;
+  /* FIXME: it is better to implement GC_mach_threads as a hash set.  */
+
+/* returns true if there's a thread in act_list that wasn't in old_list */
+STATIC GC_bool GC_suspend_thread_list(thread_act_array_t act_list, int count,
+                                      thread_act_array_t old_list,
+                                      int old_count, mach_port_t my_thread)
+{
+  int i;
+  int j = -1;
+  GC_bool changed = FALSE;
+
+  for (i = 0; i < count; i++) {
+    thread_act_t thread = act_list[i];
+    GC_bool found;
+    struct thread_basic_info info;
+    mach_msg_type_number_t outCount;
+    kern_return_t kern_result;
+
+    if (thread == my_thread
+#       ifdef MPROTECT_VDB
+          || (GC_mach_handler_thread == thread && GC_use_mach_handler_thread)
+#       endif
+        ) {
+      /* Don't add our and the handler threads. */
+      continue;
+    }
+#   ifdef PARALLEL_MARK
+      if (GC_is_mach_marker(thread))
+        continue; /* ignore the parallel marker threads */
+#   endif
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("Attempting to suspend thread 0x%lx\n",
+                    (unsigned long)thread);
+#   endif
+    /* find the current thread in the old list */
+    found = FALSE;
+    {
+      int last_found = j; /* remember the previous found thread index */
+
+      /* Search for the thread starting from the last found one first.  */
+      while (++j < old_count)
+        if (old_list[j] == thread) {
+          found = TRUE;
+          break;
+        }
+      if (!found) {
+        /* If not found, search in the rest (beginning) of the list.    */
+        for (j = 0; j < last_found; j++)
+          if (old_list[j] == thread) {
+            found = TRUE;
+            break;
+          }
+
+        if (!found) {
+          /* add it to the GC_mach_threads list */
+          if (GC_mach_threads_count == GC_MAX_MACH_THREADS)
+            ABORT("Too many threads");
+          GC_mach_threads[GC_mach_threads_count].thread = thread;
+          /* default is not suspended */
+          GC_mach_threads[GC_mach_threads_count].already_suspended = FALSE;
+          changed = TRUE;
+        }
+      }
+    }
+
+    outCount = THREAD_INFO_MAX;
+    kern_result = thread_info(thread, THREAD_BASIC_INFO,
+                              (thread_info_t)&info, &outCount);
+    if (kern_result != KERN_SUCCESS) {
+      /* The thread may have quit since the thread_threads() call we  */
+      /* mark already suspended so it's not dealt with anymore later. */
+      if (!found)
+        GC_mach_threads[GC_mach_threads_count++].already_suspended = TRUE;
+      continue;
+    }
+#   ifdef DEBUG_THREADS
+      GC_log_printf("Thread state for 0x%lx = %d\n", (unsigned long)thread,
+                    info.run_state);
+#   endif
+    if (info.suspend_count != 0) {
+      /* thread is already suspended. */
+      if (!found)
+        GC_mach_threads[GC_mach_threads_count++].already_suspended = TRUE;
+      continue;
+    }
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("Suspending 0x%lx\n", (unsigned long)thread);
+#   endif
+    kern_result = thread_suspend(thread);
+    if (kern_result != KERN_SUCCESS) {
+      /* The thread may have quit since the thread_threads() call we  */
+      /* mark already suspended so it's not dealt with anymore later. */
+      if (!found)
+        GC_mach_threads[GC_mach_threads_count++].already_suspended = TRUE;
+      continue;
+    }
+    if (!found)
+      GC_mach_threads_count++;
+  }
+  return changed;
+}
+
+#endif /* !GC_NO_THREADS_DISCOVERY */
+
+/* Caller holds allocation lock.        */
+GC_INNER void GC_stop_world(void)
+{
+  unsigned i;
+  task_t my_task = current_task();
+  mach_port_t my_thread = mach_thread_self();
+  kern_return_t kern_result;
+
+# ifdef DEBUG_THREADS
+    GC_log_printf("Stopping the world from thread 0x%lx\n",
+                  (unsigned long)my_thread);
+# endif
+# ifdef PARALLEL_MARK
+    if (GC_parallel) {
+      /* Make sure all free list construction has stopped before we     */
+      /* start.  No new construction can start, since free list         */
+      /* construction is required to acquire and release the GC lock    */
+      /* before it starts, and we have the lock.                        */
+      GC_acquire_mark_lock();
+      GC_ASSERT(GC_fl_builder_count == 0);
+      /* We should have previously waited for it to become zero. */
+    }
+# endif /* PARALLEL_MARK */
+
+  if (GC_query_task_threads) {
+#   ifndef GC_NO_THREADS_DISCOVERY
+      GC_bool changed;
+      thread_act_array_t act_list, prev_list;
+      mach_msg_type_number_t listcount, prevcount;
+
+      /* Clear out the mach threads list table.  We do not need to      */
+      /* really clear GC_mach_threads[] as it is used only in the range */
+      /* from 0 to GC_mach_threads_count-1, inclusive.                  */
+      GC_mach_threads_count = 0;
+
+      /* Loop stopping threads until you have gone over the whole list  */
+      /* twice without a new one appearing.  thread_create() won't      */
+      /* return (and thus the thread stop) until the new thread exists, */
+      /* so there is no window whereby you could stop a thread,         */
+      /* recognize it is stopped, but then have a new thread it created */
+      /* before stopping show up later.                                 */
+      changed = TRUE;
+      prev_list = NULL;
+      prevcount = 0;
+      do {
+        kern_result = task_threads(my_task, &act_list, &listcount);
+
+        if (kern_result == KERN_SUCCESS) {
+          changed = GC_suspend_thread_list(act_list, listcount, prev_list,
+                                           prevcount, my_thread);
+
+          if (prev_list != NULL) {
+            for (i = 0; i < prevcount; i++)
+              mach_port_deallocate(my_task, prev_list[i]);
+
+            vm_deallocate(my_task, (vm_address_t)prev_list,
+                          sizeof(thread_t) * prevcount);
+          }
+
+          /* Repeat while having changes. */
+          prev_list = act_list;
+          prevcount = listcount;
+        }
+      } while (changed);
+
+      GC_ASSERT(prev_list != 0);
+      for (i = 0; i < prevcount; i++)
+        mach_port_deallocate(my_task, prev_list[i]);
+      vm_deallocate(my_task, (vm_address_t)act_list,
+                    sizeof(thread_t) * listcount);
+#   endif /* !GC_NO_THREADS_DISCOVERY */
+
+  } else {
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      GC_thread p;
+
+      for (p = GC_threads[i]; p != NULL; p = p->next) {
+        if ((p->flags & FINISHED) == 0 && !p->thread_blocked &&
+             p->stop_info.mach_thread != my_thread) {
+
+          kern_result = thread_suspend(p->stop_info.mach_thread);
+          if (kern_result != KERN_SUCCESS)
+            ABORT("thread_suspend failed");
+        }
+      }
+    }
+  }
+
+# ifdef MPROTECT_VDB
+    if(GC_incremental) {
+      GC_mprotect_stop();
+    }
+# endif
+# ifdef PARALLEL_MARK
+    if (GC_parallel)
+      GC_release_mark_lock();
+# endif
+
+# ifdef DEBUG_THREADS
+    GC_log_printf("World stopped from 0x%lx\n", (unsigned long)my_thread);
+# endif
+  mach_port_deallocate(my_task, my_thread);
+}
+
+GC_INLINE void GC_thread_resume(thread_act_t thread)
+{
+  kern_return_t kern_result;
+# if defined(DEBUG_THREADS) || defined(GC_ASSERTIONS)
+    struct thread_basic_info info;
+    mach_msg_type_number_t outCount = THREAD_INFO_MAX;
+    kern_result = thread_info(thread, THREAD_BASIC_INFO,
+                              (thread_info_t)&info, &outCount);
+    if (kern_result != KERN_SUCCESS)
+      ABORT("thread_info failed");
+# endif
+# ifdef DEBUG_THREADS
+    GC_log_printf("Resuming thread 0x%lx with state %d\n",
+                  (unsigned long)thread, info.run_state);
+# endif
+  /* Resume the thread */
+  kern_result = thread_resume(thread);
+  if (kern_result != KERN_SUCCESS)
+    ABORT("thread_resume failed");
+}
+
+/* Caller holds allocation lock, and has held it continuously since     */
+/* the world stopped.                                                   */
+GC_INNER void GC_start_world(void)
+{
+  task_t my_task = current_task();
+  int i;
+# ifdef DEBUG_THREADS
+    GC_log_printf("World starting\n");
+# endif
+# ifdef MPROTECT_VDB
+    if(GC_incremental) {
+      GC_mprotect_resume();
+    }
+# endif
+
+  if (GC_query_task_threads) {
+#   ifndef GC_NO_THREADS_DISCOVERY
+      int j = GC_mach_threads_count;
+      kern_return_t kern_result;
+      thread_act_array_t act_list;
+      mach_msg_type_number_t listcount;
+
+      kern_result = task_threads(my_task, &act_list, &listcount);
+      if (kern_result != KERN_SUCCESS)
+        ABORT("task_threads failed");
+
+      for (i = 0; i < (int)listcount; i++) {
+        thread_act_t thread = act_list[i];
+        int last_found = j;        /* The thread index found during the   */
+                                   /* previous iteration (count value     */
+                                   /* means no thread found yet).         */
+
+        /* Search for the thread starting from the last found one first.  */
+        while (++j < GC_mach_threads_count) {
+          if (GC_mach_threads[j].thread == thread)
+            break;
+        }
+        if (j >= GC_mach_threads_count) {
+          /* If not found, search in the rest (beginning) of the list.    */
+          for (j = 0; j < last_found; j++) {
+            if (GC_mach_threads[j].thread == thread)
+              break;
+          }
+        }
+
+        if (j != last_found) {
+          /* The thread is found in GC_mach_threads.      */
+          if (GC_mach_threads[j].already_suspended) {
+#           ifdef DEBUG_THREADS
+              GC_log_printf("Not resuming already suspended thread 0x%lx\n",
+                            (unsigned long)thread);
+#           endif
+          } else {
+            GC_thread_resume(thread);
+          }
+        }
+
+        mach_port_deallocate(my_task, thread);
+      }
+      vm_deallocate(my_task, (vm_address_t)act_list,
+                    sizeof(thread_t) * listcount);
+#   endif /* !GC_NO_THREADS_DISCOVERY */
+
+  } else {
+    mach_port_t my_thread = mach_thread_self();
+
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      GC_thread p;
+      for (p = GC_threads[i]; p != NULL; p = p->next) {
+        if ((p->flags & FINISHED) == 0 && !p->thread_blocked &&
+             p->stop_info.mach_thread != my_thread)
+          GC_thread_resume(p->stop_info.mach_thread);
+      }
+    }
+
+    mach_port_deallocate(my_task, my_thread);
+  }
+
+# ifdef DEBUG_THREADS
+    GC_log_printf("World started\n");
+# endif
+}
+
+#endif /* GC_DARWIN_THREADS */
diff --git a/src/gc/bdwgc/dbg_mlc.c b/src/gc/bdwgc/dbg_mlc.c
new file mode 100644
index 0000000..1d53a96
--- /dev/null
+++ b/src/gc/bdwgc/dbg_mlc.c
@@ -0,0 +1,1214 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1997 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ * Copyright (C) 2007 Free Software Foundation, Inc
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/dbg_mlc.h"
+
+
+#ifdef NAUT
+# include <nautilus/naut_string.h>
+# include <nautilus/errno.h>
+#else
+#include <string.h>
+# ifndef MSWINCE && 
+#  include <errno.h>
+# endif
+#endif
+
+#ifndef SHORT_DBG_HDRS
+  /* Check whether object with base pointer p has debugging info. */
+  /* p is assumed to point to a legitimate object in our part     */
+  /* of the heap.                                                 */
+  /* This excludes the check as to whether the back pointer is    */
+  /* odd, which is added by the GC_HAS_DEBUG_INFO macro.          */
+  /* Note that if DBG_HDRS_ALL is set, uncollectible objects      */
+  /* on free lists may not have debug information set.  Thus it's */
+  /* not always safe to return TRUE (1), even if the client does  */
+  /* its part.  Return -1 if the object with debug info has been  */
+  /* marked as deallocated.                                       */
+  GC_INNER int GC_has_other_debug_info(ptr_t p)
+  {
+    ptr_t body = (ptr_t)((oh *)p + 1);
+    word sz = GC_size(p);
+
+    if (HBLKPTR(p) != HBLKPTR((ptr_t)body)
+        || sz < DEBUG_BYTES + EXTRA_BYTES) {
+      return 0;
+    }
+    if (((oh *)p) -> oh_sf != (START_FLAG ^ (word)body)
+        && ((word *)p)[BYTES_TO_WORDS(sz)-1] != (END_FLAG ^ (word)body)) {
+      return 0;
+    }
+    if (((oh *)p)->oh_sz == sz) {
+      /* Object may have had debug info, but has been deallocated     */
+      return -1;
+    }
+    return 1;
+  }
+#endif /* !SHORT_DBG_HDRS */
+
+#ifdef KEEP_BACK_PTRS
+
+# include <stdlib.h>
+
+# if defined(__GLIBC__) || defined(SOLARIS) \
+     || defined(HPUX) || defined(IRIX5) || defined(OSF1)
+#   define RANDOM() random()
+# else
+#   define RANDOM() (long)rand()
+# endif
+
+  /* Store back pointer to source in dest, if that appears to be possible. */
+  /* This is not completely safe, since we may mistakenly conclude that    */
+  /* dest has a debugging wrapper.  But the error probability is very      */
+  /* small, and this shouldn't be used in production code.                 */
+  /* We assume that dest is the real base pointer.  Source will usually    */
+  /* be a pointer to the interior of an object.                            */
+  GC_INNER void GC_store_back_pointer(ptr_t source, ptr_t dest)
+  {
+    if (GC_HAS_DEBUG_INFO(dest)) {
+      ((oh *)dest) -> oh_back_ptr = HIDE_BACK_PTR(source);
+    }
+  }
+
+  GC_INNER void GC_marked_for_finalization(ptr_t dest)
+  {
+    GC_store_back_pointer(MARKED_FOR_FINALIZATION, dest);
+  }
+
+  /* Store information about the object referencing dest in *base_p     */
+  /* and *offset_p.                                                     */
+  /*   source is root ==> *base_p = address, *offset_p = 0              */
+  /*   source is heap object ==> *base_p != 0, *offset_p = offset       */
+  /*   Returns 1 on success, 0 if source couldn't be determined.        */
+  /* Dest can be any address within a heap object.                      */
+  GC_API GC_ref_kind GC_CALL GC_get_back_ptr_info(void *dest, void **base_p,
+                                                  size_t *offset_p)
+  {
+    oh * hdr = (oh *)GC_base(dest);
+    ptr_t bp;
+    ptr_t bp_base;
+
+#   ifdef LINT2
+      /* Explicitly instruct the code analysis tool that                */
+      /* GC_get_back_ptr_info is not expected to be called with an      */
+      /* incorrect "dest" value.                                        */
+      if (!hdr) ABORT("Invalid GC_get_back_ptr_info argument");
+#   endif
+    if (!GC_HAS_DEBUG_INFO((ptr_t) hdr)) return GC_NO_SPACE;
+    bp = GC_REVEAL_POINTER(hdr -> oh_back_ptr);
+    if (MARKED_FOR_FINALIZATION == bp) return GC_FINALIZER_REFD;
+    if (MARKED_FROM_REGISTER == bp) return GC_REFD_FROM_REG;
+    if (NOT_MARKED == bp) return GC_UNREFERENCED;
+#   if ALIGNMENT == 1
+      /* Heuristically try to fix off by 1 errors we introduced by      */
+      /* insisting on even addresses.                                   */
+      {
+        ptr_t alternate_ptr = bp + 1;
+        ptr_t target = *(ptr_t *)bp;
+        ptr_t alternate_target = *(ptr_t *)alternate_ptr;
+
+        if (alternate_target >= GC_least_plausible_heap_addr
+            && alternate_target <= GC_greatest_plausible_heap_addr
+            && (target < GC_least_plausible_heap_addr
+                || target > GC_greatest_plausible_heap_addr)) {
+            bp = alternate_ptr;
+        }
+      }
+#   endif
+    bp_base = GC_base(bp);
+    if (0 == bp_base) {
+      *base_p = bp;
+      *offset_p = 0;
+      return GC_REFD_FROM_ROOT;
+    } else {
+      if (GC_HAS_DEBUG_INFO(bp_base)) bp_base += sizeof(oh);
+      *base_p = bp_base;
+      *offset_p = bp - bp_base;
+      return GC_REFD_FROM_HEAP;
+    }
+  }
+
+  /* Generate a random heap address.            */
+  /* The resulting address is in the heap, but  */
+  /* not necessarily inside a valid object.     */
+  GC_API void * GC_CALL GC_generate_random_heap_address(void)
+  {
+    size_t i;
+    word heap_offset = RANDOM();
+    if (GC_heapsize > RAND_MAX) {
+        heap_offset *= RAND_MAX;
+        heap_offset += RANDOM();
+    }
+    heap_offset %= GC_heapsize;
+        /* This doesn't yield a uniform distribution, especially if     */
+        /* e.g. RAND_MAX = 1.5* GC_heapsize.  But for typical cases,    */
+        /* it's not too bad.                                            */
+    for (i = 0; i < GC_n_heap_sects; ++ i) {
+        size_t size = GC_heap_sects[i].hs_bytes;
+        if (heap_offset < size) {
+            return GC_heap_sects[i].hs_start + heap_offset;
+        } else {
+            heap_offset -= size;
+        }
+    }
+    ABORT("GC_generate_random_heap_address: size inconsistency");
+    /*NOTREACHED*/
+    return 0;
+  }
+
+  /* Generate a random address inside a valid marked heap object. */
+  GC_API void * GC_CALL GC_generate_random_valid_address(void)
+  {
+    ptr_t result;
+    ptr_t base;
+    do {
+      result = GC_generate_random_heap_address();
+      base = GC_base(result);
+    } while (base == 0 || !GC_is_marked(base));
+    return result;
+  }
+
+  /* Print back trace for p */
+  GC_API void GC_CALL GC_print_backtrace(void *p)
+  {
+    void *current = p;
+    int i;
+    GC_ref_kind source;
+    size_t offset;
+    void *base;
+
+    GC_print_heap_obj(GC_base(current));
+    GC_err_printf("\n");
+    for (i = 0; ; ++i) {
+      source = GC_get_back_ptr_info(current, &base, &offset);
+      if (GC_UNREFERENCED == source) {
+        GC_err_printf("Reference could not be found\n");
+        goto out;
+      }
+      if (GC_NO_SPACE == source) {
+        GC_err_printf("No debug info in object: Can't find reference\n");
+        goto out;
+      }
+      GC_err_printf("Reachable via %d levels of pointers from ", i);
+      switch(source) {
+        case GC_REFD_FROM_ROOT:
+          GC_err_printf("root at %p\n\n", base);
+          goto out;
+        case GC_REFD_FROM_REG:
+          GC_err_printf("root in register\n\n");
+          goto out;
+        case GC_FINALIZER_REFD:
+          GC_err_printf("list of finalizable objects\n\n");
+          goto out;
+        case GC_REFD_FROM_HEAP:
+          GC_err_printf("offset %ld in object:\n", (unsigned long)offset);
+          /* Take GC_base(base) to get real base, i.e. header. */
+          GC_print_heap_obj(GC_base(base));
+          GC_err_printf("\n");
+          break;
+        default:
+          GC_err_printf("INTERNAL ERROR: UNEXPECTED SOURCE!!!!\n");
+          goto out;
+      }
+      current = base;
+    }
+    out:;
+  }
+
+  /* Force a garbage collection and generate a backtrace from a */
+  /* random heap address.                                       */
+  GC_INNER void GC_generate_random_backtrace_no_gc(void)
+  {
+    void * current;
+    current = GC_generate_random_valid_address();
+    GC_printf("\n****Chosen address %p in object\n", current);
+    GC_print_backtrace(current);
+  }
+
+  GC_API void GC_CALL GC_generate_random_backtrace(void)
+  {
+    if (GC_try_to_collect(GC_never_stop_func) == 0) {
+      GC_err_printf("Cannot generate a backtrace: "
+                    "garbage collection is disabled!\n");
+      return;
+    }
+    GC_generate_random_backtrace_no_gc();
+  }
+
+#endif /* KEEP_BACK_PTRS */
+
+# define CROSSES_HBLK(p, sz) \
+        (((word)(p + sizeof(oh) + sz - 1) ^ (word)p) >= HBLKSIZE)
+
+/* Store debugging info into p.  Return displaced pointer.         */
+/* This version assumes we do hold the allocation lock.            */
+STATIC ptr_t GC_store_debug_info_inner(ptr_t p, word sz, const char *string,
+                                       int linenum)
+{
+    word * result = (word *)((oh *)p + 1);
+
+    GC_ASSERT(GC_size(p) >= sizeof(oh) + sz);
+    GC_ASSERT(!(SMALL_OBJ(sz) && CROSSES_HBLK(p, sz)));
+#   ifdef KEEP_BACK_PTRS
+      ((oh *)p) -> oh_back_ptr = HIDE_BACK_PTR(NOT_MARKED);
+#   endif
+#   ifdef MAKE_BACK_GRAPH
+      ((oh *)p) -> oh_bg_ptr = HIDE_BACK_PTR((ptr_t)0);
+#   endif
+    ((oh *)p) -> oh_string = string;
+    ((oh *)p) -> oh_int = (word)linenum;
+#   ifndef SHORT_DBG_HDRS
+      ((oh *)p) -> oh_sz = sz;
+      ((oh *)p) -> oh_sf = START_FLAG ^ (word)result;
+      ((word *)p)[BYTES_TO_WORDS(GC_size(p))-1] =
+         result[SIMPLE_ROUNDED_UP_WORDS(sz)] = END_FLAG ^ (word)result;
+#   endif
+    return((ptr_t)result);
+}
+
+GC_INNER ptr_t GC_store_debug_info(ptr_t p, word sz, const char *string,
+                                   int linenum)
+{
+    ptr_t result;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    result = GC_store_debug_info_inner(p, sz, string, linenum);
+    UNLOCK();
+    return result;
+}
+
+#ifndef SHORT_DBG_HDRS
+  /* Check the object with debugging info at ohdr.      */
+  /* Return NULL if it's OK.  Else return clobbered     */
+  /* address.                                           */
+  STATIC ptr_t GC_check_annotated_obj(oh *ohdr)
+  {
+    ptr_t body = (ptr_t)(ohdr + 1);
+    word gc_sz = GC_size((ptr_t)ohdr);
+    if (ohdr -> oh_sz + DEBUG_BYTES > gc_sz) {
+        return((ptr_t)(&(ohdr -> oh_sz)));
+    }
+    if (ohdr -> oh_sf != (START_FLAG ^ (word)body)) {
+        return((ptr_t)(&(ohdr -> oh_sf)));
+    }
+    if (((word *)ohdr)[BYTES_TO_WORDS(gc_sz)-1] != (END_FLAG ^ (word)body)) {
+        return((ptr_t)((word *)ohdr + BYTES_TO_WORDS(gc_sz)-1));
+    }
+    if (((word *)body)[SIMPLE_ROUNDED_UP_WORDS(ohdr -> oh_sz)]
+        != (END_FLAG ^ (word)body)) {
+        return((ptr_t)((word *)body + SIMPLE_ROUNDED_UP_WORDS(ohdr->oh_sz)));
+    }
+    return(0);
+  }
+#endif /* !SHORT_DBG_HDRS */
+
+STATIC GC_describe_type_fn GC_describe_type_fns[MAXOBJKINDS] = {0};
+
+GC_API void GC_CALL GC_register_describe_type_fn(int kind,
+                                                 GC_describe_type_fn fn)
+{
+  GC_describe_type_fns[kind] = fn;
+}
+
+/* Print a type description for the object whose client-visible address */
+/* is p.                                                                */
+STATIC void GC_print_type(ptr_t p)
+{
+    hdr * hhdr = GC_find_header(p);
+    char buffer[GC_TYPE_DESCR_LEN + 1];
+    int kind = hhdr -> hb_obj_kind;
+
+    if (0 != GC_describe_type_fns[kind] && GC_is_marked(GC_base(p))) {
+        /* This should preclude free list objects except with   */
+        /* thread-local allocation.                             */
+        buffer[GC_TYPE_DESCR_LEN] = 0;
+        (GC_describe_type_fns[kind])(p, buffer);
+        GC_ASSERT(buffer[GC_TYPE_DESCR_LEN] == 0);
+        GC_err_puts(buffer);
+    } else {
+        switch(kind) {
+          case PTRFREE:
+            GC_err_puts("PTRFREE");
+            break;
+          case NORMAL:
+            GC_err_puts("NORMAL");
+            break;
+          case UNCOLLECTABLE:
+            GC_err_puts("UNCOLLECTABLE");
+            break;
+#         ifdef ATOMIC_UNCOLLECTABLE
+            case AUNCOLLECTABLE:
+              GC_err_puts("ATOMIC UNCOLLECTABLE");
+              break;
+#         endif
+          case STUBBORN:
+            GC_err_puts("STUBBORN");
+            break;
+          default:
+            GC_err_printf("kind=%d descr=0x%lx", kind,
+                          (unsigned long)(hhdr -> hb_descr));
+        }
+    }
+}
+
+#define GET_OH_LINENUM(ohdr) ((int)(ohdr)->oh_int)
+
+/* Print a human-readable description of the object to stderr. p points */
+/* to somewhere inside an object with the debugging info.               */
+STATIC void GC_print_obj(ptr_t p)
+{
+    oh * ohdr = (oh *)GC_base(p);
+
+    GC_ASSERT(I_DONT_HOLD_LOCK());
+#   ifdef LINT2
+      if (!ohdr) ABORT("Invalid GC_print_obj argument");
+#   endif
+    GC_err_printf("%p (", ((ptr_t)ohdr + sizeof(oh)));
+    GC_err_puts(ohdr -> oh_string);
+#   ifdef SHORT_DBG_HDRS
+      GC_err_printf(":%d, ", GET_OH_LINENUM(ohdr));
+#   else
+      GC_err_printf(":%d, sz=%lu, ",
+                    GET_OH_LINENUM(ohdr), (unsigned long)(ohdr -> oh_sz));
+#   endif
+    GC_print_type((ptr_t)(ohdr + 1));
+    GC_err_puts(")\n");
+    PRINT_CALL_CHAIN(ohdr);
+}
+
+STATIC void GC_debug_print_heap_obj_proc(ptr_t p)
+{
+    GC_ASSERT(I_DONT_HOLD_LOCK());
+    if (GC_HAS_DEBUG_INFO(p)) {
+        GC_print_obj(p);
+    } else {
+        GC_default_print_heap_obj_proc(p);
+    }
+}
+
+#ifndef SHORT_DBG_HDRS
+  /* Use GC_err_printf and friends to print a description of the object */
+  /* whose client-visible address is p, and which was smashed at        */
+  /* clobbered_addr.                                                    */
+  STATIC void GC_print_smashed_obj(const char *msg, ptr_t p,
+                                   ptr_t clobbered_addr)
+  {
+    oh * ohdr = (oh *)GC_base(p);
+
+    GC_ASSERT(I_DONT_HOLD_LOCK());
+#   ifdef LINT2
+      if (!ohdr) ABORT("Invalid GC_print_smashed_obj argument");
+#   endif
+    if (clobbered_addr <= (ptr_t)(&(ohdr -> oh_sz))
+        || ohdr -> oh_string == 0) {
+        GC_err_printf(
+                "%s %p in or near object at %p(<smashed>, appr. sz = %lu)\n",
+                msg, clobbered_addr, p,
+                (unsigned long)(GC_size((ptr_t)ohdr) - DEBUG_BYTES));
+    } else {
+        GC_err_printf("%s %p in or near object at %p (%s:%d, sz=%lu)\n",
+                msg, clobbered_addr, p,
+                (word)(ohdr -> oh_string) < HBLKSIZE ? "(smashed string)" :
+                ohdr -> oh_string[0] == '\0' ? "EMPTY(smashed?)" :
+                                                ohdr -> oh_string,
+                GET_OH_LINENUM(ohdr), (unsigned long)(ohdr -> oh_sz));
+        PRINT_CALL_CHAIN(ohdr);
+    }
+  }
+#endif
+
+#ifndef SHORT_DBG_HDRS
+  STATIC void GC_check_heap_proc (void);
+  STATIC void GC_print_all_smashed_proc (void);
+#else
+  STATIC void GC_do_nothing(void) {}
+#endif
+
+GC_INNER void GC_start_debugging(void)
+{
+# ifndef SHORT_DBG_HDRS
+    GC_check_heap = GC_check_heap_proc;
+    GC_print_all_smashed = GC_print_all_smashed_proc;
+# else
+    GC_check_heap = GC_do_nothing;
+    GC_print_all_smashed = GC_do_nothing;
+# endif
+  GC_print_heap_obj = GC_debug_print_heap_obj_proc;
+  GC_debugging_started = TRUE;
+  GC_register_displacement((word)sizeof(oh));
+}
+
+size_t GC_debug_header_size = sizeof(oh);
+
+GC_API void GC_CALL GC_debug_register_displacement(size_t offset)
+{
+    GC_register_displacement(offset);
+    GC_register_displacement((word)sizeof(oh) + offset);
+}
+
+#ifdef GC_ADD_CALLER
+# if defined(HAVE_DLADDR) && defined(GC_RETURN_ADDR_PARENT)
+#   include <dlfcn.h>
+
+    STATIC void GC_caller_func_offset(word ad, const char **symp, int *offp)
+    {
+      Dl_info caller;
+
+      if (ad && dladdr((void *)ad, &caller) && caller.dli_sname != NULL) {
+        *symp = caller.dli_sname;
+        *offp = (int)((char *)ad - (char *)caller.dli_saddr);
+      }
+      if (NULL == *symp) {
+        *symp = "unknown";
+      }
+    }
+# else
+#   define GC_caller_func_offset(ad, symp, offp) (void)(*(symp) = "unknown")
+# endif
+#endif /* GC_ADD_CALLER */
+
+GC_API void * GC_CALL GC_debug_malloc(size_t lb, GC_EXTRA_PARAMS)
+{
+    void * result;
+
+    /* Note that according to malloc() specification, if size is 0 then */
+    /* malloc() returns either NULL, or a unique pointer value that can */
+    /* later be successfully passed to free(). We always do the latter. */
+    result = GC_malloc(lb + DEBUG_BYTES);
+#   ifdef GC_ADD_CALLER
+      if (s == NULL) {
+        GC_caller_func_offset(ra, &s, &i);
+      }
+#   endif
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc(%lu) returning NULL (",
+                      (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%ld)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+GC_API void * GC_CALL GC_debug_malloc_ignore_off_page(size_t lb,
+                                                      GC_EXTRA_PARAMS)
+{
+    void * result = GC_malloc_ignore_off_page(lb + DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc_ignore_off_page(%lu) returning NULL (",
+                       (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+GC_API void * GC_CALL GC_debug_malloc_atomic_ignore_off_page(size_t lb,
+                                                             GC_EXTRA_PARAMS)
+{
+    void * result = GC_malloc_atomic_ignore_off_page(lb + DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc_atomic_ignore_off_page(%lu)"
+                      " returning NULL (", (unsigned long)lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+#ifdef DBG_HDRS_ALL
+  /* An allocation function for internal use.  Normally internally      */
+  /* allocated objects do not have debug information.  But in this      */
+  /* case, we need to make sure that all objects have debug headers.    */
+  /* We assume debugging was started in collector initialization, and   */
+  /* we already hold the GC lock.                                       */
+  GC_INNER void * GC_debug_generic_malloc_inner(size_t lb, int k)
+  {
+    void * result = GC_generic_malloc_inner(lb + DEBUG_BYTES, k);
+
+    if (result == 0) {
+        GC_err_printf("GC internal allocation (%lu bytes) returning NULL\n",
+                       (unsigned long) lb);
+        return(0);
+    }
+    ADD_CALL_CHAIN(result, GC_RETURN_ADDR);
+    return (GC_store_debug_info_inner(result, (word)lb, "INTERNAL", 0));
+  }
+
+  GC_INNER void * GC_debug_generic_malloc_inner_ignore_off_page(size_t lb,
+                                                                int k)
+  {
+    void * result = GC_generic_malloc_inner_ignore_off_page(
+                                                lb + DEBUG_BYTES, k);
+
+    if (result == 0) {
+        GC_err_printf("GC internal allocation (%lu bytes) returning NULL\n",
+                       (unsigned long) lb);
+        return(0);
+    }
+    ADD_CALL_CHAIN(result, GC_RETURN_ADDR);
+    return (GC_store_debug_info_inner(result, (word)lb, "INTERNAL", 0));
+  }
+#endif /* DBG_HDRS_ALL */
+
+#ifdef STUBBORN_ALLOC
+  GC_API void * GC_CALL GC_debug_malloc_stubborn(size_t lb, GC_EXTRA_PARAMS)
+  {
+    void * result = GC_malloc_stubborn(lb + DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc(%lu) returning NULL (",
+                      (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+  }
+
+  GC_API void GC_CALL GC_debug_change_stubborn(void *p)
+  {
+    void * q = GC_base(p);
+    hdr * hhdr;
+
+    if (q == 0) {
+        GC_err_printf("Bad argument: %p to GC_debug_change_stubborn\n", p);
+        ABORT("GC_debug_change_stubborn: bad arg");
+    }
+    hhdr = HDR(q);
+    if (hhdr -> hb_obj_kind != STUBBORN) {
+        GC_err_printf("GC_debug_change_stubborn arg not stubborn: %p\n", p);
+        ABORT("GC_debug_change_stubborn: arg not stubborn");
+    }
+    GC_change_stubborn(q);
+  }
+
+  GC_API void GC_CALL GC_debug_end_stubborn_change(void *p)
+  {
+    void * q = GC_base(p);
+    hdr * hhdr;
+
+    if (q == 0) {
+        GC_err_printf("Bad argument: %p to GC_debug_end_stubborn_change\n", p);
+        ABORT("GC_debug_end_stubborn_change: bad arg");
+    }
+    hhdr = HDR(q);
+    if (hhdr -> hb_obj_kind != STUBBORN) {
+        GC_err_printf("debug_end_stubborn_change arg not stubborn: %p\n", p);
+        ABORT("GC_debug_end_stubborn_change: arg not stubborn");
+    }
+    GC_end_stubborn_change(q);
+  }
+
+#else /* !STUBBORN_ALLOC */
+
+  GC_API void * GC_CALL GC_debug_malloc_stubborn(size_t lb, GC_EXTRA_PARAMS)
+  {
+    return GC_debug_malloc(lb, OPT_RA s, i);
+  }
+
+  /*ARGSUSED*/
+  GC_API void GC_CALL GC_debug_change_stubborn(void *p) {}
+
+  /*ARGSUSED*/
+  GC_API void GC_CALL GC_debug_end_stubborn_change(void *p) {}
+#endif /* !STUBBORN_ALLOC */
+
+GC_API void * GC_CALL GC_debug_malloc_atomic(size_t lb, GC_EXTRA_PARAMS)
+{
+    void * result = GC_malloc_atomic(lb + DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc_atomic(%lu) returning NULL (",
+                      (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+GC_API char * GC_CALL GC_debug_strdup(const char *str, GC_EXTRA_PARAMS)
+{
+  char *copy;
+  size_t lb;
+  if (str == NULL) {
+    if (GC_find_leak)
+      GC_err_printf("strdup(NULL) behavior is undefined\n");
+    return NULL;
+  }
+
+  lb = strlen(str) + 1;
+  copy = GC_debug_malloc_atomic(lb, OPT_RA s, i);
+  if (copy == NULL) {
+# if !defined(MSWINCE) && !defined(NAUT)
+        errno = ENOMEM;
+#   endif
+    return NULL;
+  }
+# ifndef MSWINCE
+    strcpy(copy, str);
+# else
+    /* strcpy() is deprecated in WinCE */
+    memcpy(copy, str, lb);
+# endif
+  return copy;
+}
+
+GC_API char * GC_CALL GC_debug_strndup(const char *str, size_t size,
+                                       GC_EXTRA_PARAMS)
+{
+  char *copy;
+  size_t len = strlen(str); /* str is expected to be non-NULL  */
+  if (len > size)
+    len = size;
+  copy = GC_debug_malloc_atomic(len + 1, OPT_RA s, i);
+  if (copy == NULL) {
+#   if !defined(MSWINCE) && !defined(NAUT)
+      errno = ENOMEM;
+#   endif
+    return NULL;
+  }
+  BCOPY(str, copy, len);
+  copy[len] = '\0';
+  return copy;
+}
+
+#ifdef GC_REQUIRE_WCSDUP
+# include <wchar.h> /* for wcslen() */
+
+  GC_API wchar_t * GC_CALL GC_debug_wcsdup(const wchar_t *str, GC_EXTRA_PARAMS)
+  {
+    size_t lb = (wcslen(str) + 1) * sizeof(wchar_t);
+    wchar_t *copy = GC_debug_malloc_atomic(lb, OPT_RA s, i);
+    if (copy == NULL) {
+#     ifndef MSWINCE
+        errno = ENOMEM;
+#     endif
+      return NULL;
+    }
+    BCOPY(str, copy, lb);
+    return copy;
+  }
+#endif /* GC_REQUIRE_WCSDUP */
+
+GC_API void * GC_CALL GC_debug_malloc_uncollectable(size_t lb,
+                                                    GC_EXTRA_PARAMS)
+{
+    void * result = GC_malloc_uncollectable(lb + UNCOLLECTABLE_DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf("GC_debug_malloc_uncollectable(%lu) returning NULL (",
+                      (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+#ifdef ATOMIC_UNCOLLECTABLE
+  GC_API void * GC_CALL GC_debug_malloc_atomic_uncollectable(size_t lb,
+                                                             GC_EXTRA_PARAMS)
+  {
+    void * result =
+        GC_malloc_atomic_uncollectable(lb + UNCOLLECTABLE_DEBUG_BYTES);
+
+    if (result == 0) {
+        GC_err_printf(
+                "GC_debug_malloc_atomic_uncollectable(%lu) returning NULL (",
+                (unsigned long) lb);
+        GC_err_puts(s);
+        GC_err_printf(":%lu)\n", (unsigned long)i);
+        return(0);
+    }
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+  }
+#endif /* ATOMIC_UNCOLLECTABLE */
+
+#ifndef GC_FREED_MEM_MARKER
+# if CPP_WORDSZ == 32
+#   define GC_FREED_MEM_MARKER 0xdeadbeef
+# else
+#   define GC_FREED_MEM_MARKER GC_WORD_C(0xEFBEADDEdeadbeef)
+# endif
+#endif
+
+GC_API void GC_CALL GC_debug_free(void * p)
+{
+    ptr_t base;
+    if (0 == p) return;
+
+    base = GC_base(p);
+    if (base == 0) {
+      GC_err_printf("Attempt to free invalid pointer %p\n", p);
+      ABORT("Invalid pointer passed to free()");
+    }
+    if ((ptr_t)p - (ptr_t)base != sizeof(oh)) {
+      GC_err_printf(
+               "GC_debug_free called on pointer %p w/o debugging info\n", p);
+    }
+    else {
+#     ifndef SHORT_DBG_HDRS
+        ptr_t clobbered = GC_check_annotated_obj((oh *)base);
+        word sz = GC_size(base);
+        if (clobbered != 0) {
+          GC_have_errors = TRUE;
+          if (((oh *)base) -> oh_sz == sz) {
+            GC_print_smashed_obj(
+                  "GC_debug_free: found previously deallocated (?) object at",
+                  p, clobbered);
+            return; /* ignore double free */
+          } else {
+            GC_print_smashed_obj("GC_debug_free: found smashed location at",
+                                 p, clobbered);
+          }
+        }
+        /* Invalidate size (mark the object as deallocated) */
+        ((oh *)base) -> oh_sz = sz;
+#     endif /* SHORT_DBG_HDRS */
+    }
+    if (GC_find_leak
+#       ifndef SHORT_DBG_HDRS
+          && ((ptr_t)p - (ptr_t)base != sizeof(oh) || !GC_findleak_delay_free)
+#       endif
+        ) {
+      GC_free(base);
+    } else {
+      hdr * hhdr = HDR(p);
+      if (hhdr -> hb_obj_kind == UNCOLLECTABLE
+#         ifdef ATOMIC_UNCOLLECTABLE
+            || hhdr -> hb_obj_kind == AUNCOLLECTABLE
+#         endif
+          ) {
+        GC_free(base);
+      } else {
+        size_t i;
+        size_t obj_sz = BYTES_TO_WORDS(hhdr -> hb_sz - sizeof(oh));
+
+        for (i = 0; i < obj_sz; ++i)
+          ((word *)p)[i] = GC_FREED_MEM_MARKER;
+        GC_ASSERT((word *)p + i == (word *)(base + hhdr -> hb_sz));
+        
+      }
+    } /* !GC_find_leak */
+}
+
+#if defined(THREADS) && defined(DBG_HDRS_ALL)
+  /* Used internally; we assume it's called correctly.    */
+  GC_INNER void GC_debug_free_inner(void * p)
+  {
+    ptr_t base = GC_base(p);
+    GC_ASSERT((ptr_t)p - (ptr_t)base == sizeof(oh));
+#   ifdef LINT2
+      if (!base) ABORT("Invalid GC_debug_free_inner argument");
+#   endif
+#   ifndef SHORT_DBG_HDRS
+      /* Invalidate size */
+      ((oh *)base) -> oh_sz = GC_size(base);
+#   endif
+    GC_free_inner(base);
+  }
+#endif
+
+GC_API void * GC_CALL GC_debug_realloc(void * p, size_t lb, GC_EXTRA_PARAMS)
+{
+    void * base;
+    void * result;
+    hdr * hhdr;
+
+    if (p == 0) {
+      return GC_debug_malloc(lb, OPT_RA s, i);
+    }
+#   ifdef GC_ADD_CALLER
+      if (s == NULL) {
+        GC_caller_func_offset(ra, &s, &i);
+      }
+#   endif
+    base = GC_base(p);
+    if (base == 0) {
+        GC_err_printf("Attempt to reallocate invalid pointer %p\n", p);
+        ABORT("Invalid pointer passed to realloc()");
+    }
+    if ((ptr_t)p - (ptr_t)base != sizeof(oh)) {
+        GC_err_printf(
+              "GC_debug_realloc called on pointer %p w/o debugging info\n", p);
+        return(GC_realloc(p, lb));
+    }
+    hhdr = HDR(base);
+    switch (hhdr -> hb_obj_kind) {
+#    ifdef STUBBORN_ALLOC
+      case STUBBORN:
+        result = GC_debug_malloc_stubborn(lb, OPT_RA s, i);
+        break;
+#    endif
+      case NORMAL:
+        result = GC_debug_malloc(lb, OPT_RA s, i);
+        break;
+      case PTRFREE:
+        result = GC_debug_malloc_atomic(lb, OPT_RA s, i);
+        break;
+      case UNCOLLECTABLE:
+        result = GC_debug_malloc_uncollectable(lb, OPT_RA s, i);
+        break;
+#    ifdef ATOMIC_UNCOLLECTABLE
+      case AUNCOLLECTABLE:
+        result = GC_debug_malloc_atomic_uncollectable(lb, OPT_RA s, i);
+        break;
+#    endif
+      default:
+        result = NULL; /* initialized to prevent warning. */
+        GC_err_printf("GC_debug_realloc: encountered bad kind\n");
+        ABORT("Bad kind");
+    }
+
+    if (result != NULL) {
+      size_t old_sz;
+#     ifdef SHORT_DBG_HDRS
+        old_sz = GC_size(base) - sizeof(oh);
+#     else
+        old_sz = ((oh *)base) -> oh_sz;
+#     endif
+      BCOPY(p, result, old_sz < lb ? old_sz : lb);
+      GC_debug_free(p);
+    }
+    return(result);
+}
+
+#ifndef SHORT_DBG_HDRS
+
+/* List of smashed (clobbered) locations.  We defer printing these,     */
+/* since we can't always print them nicely with the allocation lock     */
+/* held.  We put them here instead of in GC_arrays, since it may be     */
+/* useful to be able to look at them with the debugger.                 */
+#ifndef MAX_SMASHED
+# define MAX_SMASHED 20
+#endif
+STATIC ptr_t GC_smashed[MAX_SMASHED] = {0};
+STATIC unsigned GC_n_smashed = 0;
+
+STATIC void GC_add_smashed(ptr_t smashed)
+{
+    GC_ASSERT(GC_is_marked(GC_base(smashed)));
+    /* FIXME: Prevent adding an object while printing smashed list.     */
+    GC_smashed[GC_n_smashed] = smashed;
+    if (GC_n_smashed < MAX_SMASHED - 1) ++GC_n_smashed;
+      /* In case of overflow, we keep the first MAX_SMASHED-1   */
+      /* entries plus the last one.                             */
+    GC_have_errors = TRUE;
+}
+
+/* Print all objects on the list.  Clear the list.      */
+STATIC void GC_print_all_smashed_proc(void)
+{
+    unsigned i;
+
+    GC_ASSERT(I_DONT_HOLD_LOCK());
+    if (GC_n_smashed == 0) return;
+    GC_err_printf("GC_check_heap_block: found smashed heap objects:\n");
+    for (i = 0; i < GC_n_smashed; ++i) {
+        GC_print_smashed_obj("", (ptr_t)GC_base(GC_smashed[i]) + sizeof(oh),
+                             GC_smashed[i]);
+        GC_smashed[i] = 0;
+    }
+    GC_n_smashed = 0;
+    GC_err_printf("\n");
+}
+
+/* Check all marked objects in the given block for validity     */
+/* Avoid GC_apply_to_each_object for performance reasons.       */
+/*ARGSUSED*/
+STATIC void GC_check_heap_block(struct hblk *hbp, word dummy)
+{
+    struct hblkhdr * hhdr = HDR(hbp);
+    size_t sz = hhdr -> hb_sz;
+    size_t bit_no;
+    char *p, *plim;
+
+    p = hbp->hb_body;
+    if (sz > MAXOBJBYTES) {
+      plim = p;
+    } else {
+      plim = hbp->hb_body + HBLKSIZE - sz;
+    }
+    /* go through all words in block */
+    for (bit_no = 0; p <= plim; bit_no += MARK_BIT_OFFSET(sz), p += sz) {
+      if (mark_bit_from_hdr(hhdr, bit_no) && GC_HAS_DEBUG_INFO((ptr_t)p)) {
+        ptr_t clobbered = GC_check_annotated_obj((oh *)p);
+        if (clobbered != 0)
+          GC_add_smashed(clobbered);
+      }
+    }
+}
+
+/* This assumes that all accessible objects are marked, and that        */
+/* I hold the allocation lock.  Normally called by collector.           */
+STATIC void GC_check_heap_proc(void)
+{
+  GC_STATIC_ASSERT((sizeof(oh) & (GRANULE_BYTES - 1)) == 0);
+  /* FIXME: Should we check for twice that alignment?   */
+  GC_apply_to_all_blocks(GC_check_heap_block, 0);
+}
+
+GC_INNER GC_bool GC_check_leaked(ptr_t base)
+{
+  size_t i;
+  size_t obj_sz;
+  word *p;
+
+  if (
+#     if defined(KEEP_BACK_PTRS) || defined(MAKE_BACK_GRAPH)
+        (*(word *)base & 1) != 0 &&
+#     endif
+      GC_has_other_debug_info(base) >= 0)
+    return TRUE; /* object has leaked */
+
+  /* Validate freed object's content. */
+  p = (word *)(base + sizeof(oh));
+  obj_sz = BYTES_TO_WORDS(HDR(base)->hb_sz - sizeof(oh));
+  for (i = 0; i < obj_sz; ++i)
+    if (p[i] != GC_FREED_MEM_MARKER) {
+        GC_set_mark_bit(base); /* do not reclaim it in this cycle */
+        GC_add_smashed((ptr_t)(&p[i])); /* alter-after-free detected */
+        break; /* don't report any other smashed locations in the object */
+    }
+
+  return FALSE; /* GC_debug_free() has been called */
+}
+
+#endif /* !SHORT_DBG_HDRS */
+
+struct closure {
+    GC_finalization_proc cl_fn;
+    void * cl_data;
+};
+
+STATIC void * GC_make_closure(GC_finalization_proc fn, void * data)
+{
+    struct closure * result =
+#   ifdef DBG_HDRS_ALL
+      (struct closure *) GC_debug_malloc(sizeof (struct closure),
+                                         GC_EXTRAS);
+#   else
+      (struct closure *) GC_malloc(sizeof (struct closure));
+#   endif
+    if (result != 0) {
+      result -> cl_fn = fn;
+      result -> cl_data = data;
+    }
+    return((void *)result);
+}
+
+/* An auxiliary fns to make finalization work correctly with displaced  */
+/* pointers introduced by the debugging allocators.                     */
+STATIC void GC_CALLBACK GC_debug_invoke_finalizer(void * obj, void * data)
+{
+    struct closure * cl = (struct closure *) data;
+    (*(cl -> cl_fn))((void *)((char *)obj + sizeof(oh)), cl -> cl_data);
+}
+
+/* Special finalizer_proc value to detect GC_register_finalizer() failure. */
+#define OFN_UNSET (GC_finalization_proc)(signed_word)-1
+
+/* Set ofn and ocd to reflect the values we got back.   */
+static void store_old(void *obj, GC_finalization_proc my_old_fn,
+                      struct closure *my_old_cd, GC_finalization_proc *ofn,
+                      void **ocd)
+{
+    if (0 != my_old_fn) {
+      if (my_old_fn == OFN_UNSET) {
+        /* register_finalizer() failed; (*ofn) and (*ocd) are unchanged. */
+        return;
+      }
+      if (my_old_fn != GC_debug_invoke_finalizer) {
+        GC_err_printf("Debuggable object at %p had a non-debug finalizer\n",
+                      obj);
+        /* This should probably be fatal. */
+      } else {
+        if (ofn) *ofn = my_old_cd -> cl_fn;
+        if (ocd) *ocd = my_old_cd -> cl_data;
+      }
+    } else {
+      if (ofn) *ofn = 0;
+      if (ocd) *ocd = 0;
+    }
+}
+
+GC_API void GC_CALL GC_debug_register_finalizer(void * obj,
+                                        GC_finalization_proc fn,
+                                        void * cd, GC_finalization_proc *ofn,
+                                        void * *ocd)
+{
+    GC_finalization_proc my_old_fn = OFN_UNSET;
+    void * my_old_cd;
+    ptr_t base = GC_base(obj);
+    if (0 == base) {
+        /* We won't collect it, hence finalizer wouldn't be run. */
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        return;
+    }
+    if ((ptr_t)obj - base != sizeof(oh)) {
+        GC_err_printf(
+            "GC_debug_register_finalizer called with non-base-pointer %p\n",
+            obj);
+    }
+    if (0 == fn) {
+      GC_register_finalizer(base, 0, 0, &my_old_fn, &my_old_cd);
+    } else {
+      cd = GC_make_closure(fn, cd);
+      if (cd == 0) return; /* out of memory */
+      GC_register_finalizer(base, GC_debug_invoke_finalizer,
+                            cd, &my_old_fn, &my_old_cd);
+    }
+    store_old(obj, my_old_fn, (struct closure *)my_old_cd, ofn, ocd);
+}
+
+GC_API void GC_CALL GC_debug_register_finalizer_no_order
+                                    (void * obj, GC_finalization_proc fn,
+                                     void * cd, GC_finalization_proc *ofn,
+                                     void * *ocd)
+{
+    GC_finalization_proc my_old_fn = OFN_UNSET;
+    void * my_old_cd;
+    ptr_t base = GC_base(obj);
+    if (0 == base) {
+        /* We won't collect it, hence finalizer wouldn't be run. */
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        return;
+    }
+    if ((ptr_t)obj - base != sizeof(oh)) {
+        GC_err_printf(
+          "GC_debug_register_finalizer_no_order called with "
+          "non-base-pointer %p\n",
+          obj);
+    }
+    if (0 == fn) {
+      GC_register_finalizer_no_order(base, 0, 0, &my_old_fn, &my_old_cd);
+    } else {
+      cd = GC_make_closure(fn, cd);
+      if (cd == 0) return; /* out of memory */
+      GC_register_finalizer_no_order(base, GC_debug_invoke_finalizer,
+                                     cd, &my_old_fn, &my_old_cd);
+    }
+    store_old(obj, my_old_fn, (struct closure *)my_old_cd, ofn, ocd);
+}
+
+GC_API void GC_CALL GC_debug_register_finalizer_unreachable
+                                    (void * obj, GC_finalization_proc fn,
+                                     void * cd, GC_finalization_proc *ofn,
+                                     void * *ocd)
+{
+    GC_finalization_proc my_old_fn = OFN_UNSET;
+    void * my_old_cd;
+    ptr_t base = GC_base(obj);
+    if (0 == base) {
+        /* We won't collect it, hence finalizer wouldn't be run. */
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        return;
+    }
+    if ((ptr_t)obj - base != sizeof(oh)) {
+        GC_err_printf(
+            "GC_debug_register_finalizer_unreachable called with "
+            "non-base-pointer %p\n",
+            obj);
+    }
+    if (0 == fn) {
+      GC_register_finalizer_unreachable(base, 0, 0, &my_old_fn, &my_old_cd);
+    } else {
+      cd = GC_make_closure(fn, cd);
+      if (cd == 0) return; /* out of memory */
+      GC_register_finalizer_unreachable(base, GC_debug_invoke_finalizer,
+                                        cd, &my_old_fn, &my_old_cd);
+    }
+    store_old(obj, my_old_fn, (struct closure *)my_old_cd, ofn, ocd);
+}
+
+GC_API void GC_CALL GC_debug_register_finalizer_ignore_self
+                                    (void * obj, GC_finalization_proc fn,
+                                     void * cd, GC_finalization_proc *ofn,
+                                     void * *ocd)
+{
+    GC_finalization_proc my_old_fn = OFN_UNSET;
+    void * my_old_cd;
+    ptr_t base = GC_base(obj);
+    if (0 == base) {
+        /* We won't collect it, hence finalizer wouldn't be run. */
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        return;
+    }
+    if ((ptr_t)obj - base != sizeof(oh)) {
+        GC_err_printf(
+            "GC_debug_register_finalizer_ignore_self called with "
+            "non-base-pointer %p\n", obj);
+    }
+    if (0 == fn) {
+      GC_register_finalizer_ignore_self(base, 0, 0, &my_old_fn, &my_old_cd);
+    } else {
+      cd = GC_make_closure(fn, cd);
+      if (cd == 0) return; /* out of memory */
+      GC_register_finalizer_ignore_self(base, GC_debug_invoke_finalizer,
+                                        cd, &my_old_fn, &my_old_cd);
+    }
+    store_old(obj, my_old_fn, (struct closure *)my_old_cd, ofn, ocd);
+}
+
+GC_API void * GC_CALL GC_debug_malloc_replacement(size_t lb)
+{
+    return GC_debug_malloc(lb, GC_DBG_EXTRAS);
+}
+
+GC_API void * GC_CALL GC_debug_realloc_replacement(void *p, size_t lb)
+{
+    return GC_debug_realloc(p, lb, GC_DBG_EXTRAS);
+}
diff --git a/src/gc/bdwgc/extra/AmigaOS.c b/src/gc/bdwgc/extra/AmigaOS.c
new file mode 100644
index 0000000..f3254bc
--- /dev/null
+++ b/src/gc/bdwgc/extra/AmigaOS.c
@@ -0,0 +1,623 @@
+
+
+/******************************************************************
+
+  AmigaOS-specific routines for GC.
+  This file is normally included from os_dep.c
+
+******************************************************************/
+
+
+#if !defined(GC_AMIGA_DEF) && !defined(GC_AMIGA_SB) && !defined(GC_AMIGA_DS) && !defined(GC_AMIGA_AM)
+# include "gc_priv.h"
+# include <stdio.h>
+# include <signal.h>
+# define GC_AMIGA_DEF
+# define GC_AMIGA_SB
+# define GC_AMIGA_DS
+# define GC_AMIGA_AM
+#endif
+
+
+#ifdef GC_AMIGA_DEF
+
+# ifndef __GNUC__
+#   include <exec/exec.h>
+# endif
+# include <proto/exec.h>
+# include <proto/dos.h>
+# include <dos/dosextens.h>
+# include <workbench/startup.h>
+
+#endif
+
+
+
+
+#ifdef GC_AMIGA_SB
+
+/******************************************************************
+   Find the base of the stack.
+******************************************************************/
+
+ptr_t GC_get_main_stack_base()
+{
+    struct Process *proc = (struct Process*)SysBase->ThisTask;
+ 
+    /* Reference: Amiga Guru Book Pages: 42,567,574 */
+    if (proc->pr_Task.tc_Node.ln_Type==NT_PROCESS
+        && proc->pr_CLI != NULL) {
+	/* first ULONG is StackSize */
+	/*longPtr = proc->pr_ReturnAddr;
+	size = longPtr[0];*/
+
+	return (char *)proc->pr_ReturnAddr + sizeof(ULONG);
+    } else {
+	return (char *)proc->pr_Task.tc_SPUpper;
+    }
+}
+
+#if 0 /* old version */
+ptr_t GC_get_stack_base()
+{
+    extern struct WBStartup *_WBenchMsg;
+    extern long __base;
+    extern long __stack;
+    struct Task *task;
+    struct Process *proc;
+    struct CommandLineInterface *cli;
+    long size;
+
+    if ((task = FindTask(0)) == 0) {
+	GC_err_puts("Cannot find own task structure\n");
+	ABORT("task missing");
+    }
+    proc = (struct Process *)task;
+    cli = BADDR(proc->pr_CLI);
+
+    if (_WBenchMsg != 0 || cli == 0) {
+	size = (char *)task->tc_SPUpper - (char *)task->tc_SPLower;
+    } else {
+	size = cli->cli_DefaultStack * 4;
+    }
+    return (ptr_t)(__base + GC_max(size, __stack));
+}
+#endif
+
+
+#endif
+
+
+#ifdef GC_AMIGA_DS
+/******************************************************************
+   Register data segments.
+******************************************************************/
+
+   void GC_register_data_segments()
+   {
+     struct Process	*proc;
+     struct CommandLineInterface *cli;
+     BPTR myseglist;
+     ULONG *data;
+ 
+     int	num;
+
+
+#    ifdef __GNUC__
+        ULONG dataSegSize;
+        GC_bool found_segment = FALSE;
+	extern char __data_size[];
+
+	dataSegSize=__data_size+8;
+	/* Can`t find the Location of __data_size, because
+           it`s possible that is it, inside the segment. */
+
+#     endif
+
+	proc= (struct Process*)SysBase->ThisTask;
+
+	/* Reference: Amiga Guru Book Pages: 538ff,565,573
+		     and XOper.asm */
+	if (proc->pr_Task.tc_Node.ln_Type==NT_PROCESS) {
+	  if (proc->pr_CLI == NULL) {
+	    myseglist = proc->pr_SegList;
+	  } else {
+	    /* ProcLoaded	'Loaded as a command: '*/
+	    cli = BADDR(proc->pr_CLI);
+	    myseglist = cli->cli_Module;
+	  }
+	} else {
+	  ABORT("Not a Process.");
+ 	}
+
+	if (myseglist == NULL) {
+	    ABORT("Arrrgh.. can't find segments, aborting");
+ 	}
+
+	/* xoper hunks Shell Process */
+
+	num=0;
+        for (data = (ULONG *)BADDR(myseglist); data != NULL;
+             data = (ULONG *)BADDR(data[0])) {
+	  if (((ULONG) GC_register_data_segments < (ULONG) &data[1]) ||
+	      ((ULONG) GC_register_data_segments > (ULONG) &data[1] + data[-1])) {
+#             ifdef __GNUC__
+		if (dataSegSize == data[-1]) {
+		  found_segment = TRUE;
+		}
+# 	      endif
+	      GC_add_roots_inner((char *)&data[1],
+				 ((char *)&data[1]) + data[-1], FALSE);
+          }
+          ++num;
+        } /* for */
+# 	ifdef __GNUC__
+	   if (!found_segment) {
+	     ABORT("Can`t find correct Segments.\nSolution: Use an newer version of ixemul.library");
+	   }
+# 	endif
+  }
+
+#if 0 /* old version */
+  void GC_register_data_segments()
+  {
+    extern struct WBStartup *_WBenchMsg;
+    struct Process	*proc;
+    struct CommandLineInterface *cli;
+    BPTR myseglist;
+    ULONG *data;
+
+    if ( _WBenchMsg != 0 ) {
+	if ((myseglist = _WBenchMsg->sm_Segment) == 0) {
+	    GC_err_puts("No seglist from workbench\n");
+	    return;
+	}
+    } else {
+	if ((proc = (struct Process *)FindTask(0)) == 0) {
+	    GC_err_puts("Cannot find process structure\n");
+	    return;
+	}
+	if ((cli = BADDR(proc->pr_CLI)) == 0) {
+	    GC_err_puts("No CLI\n");
+	    return;
+	}
+	if ((myseglist = cli->cli_Module) == 0) {
+	    GC_err_puts("No seglist from CLI\n");
+	    return;
+	}
+    }
+
+    for (data = (ULONG *)BADDR(myseglist); data != 0;
+         data = (ULONG *)BADDR(data[0])) {
+#        ifdef AMIGA_SKIP_SEG
+           if (((ULONG) GC_register_data_segments < (ULONG) &data[1]) ||
+           ((ULONG) GC_register_data_segments > (ULONG) &data[1] + data[-1])) {
+#	 else
+      	   {
+#	 endif /* AMIGA_SKIP_SEG */
+          GC_add_roots_inner((char *)&data[1],
+          		     ((char *)&data[1]) + data[-1], FALSE);
+         }
+    }
+  }
+#endif /* old version */
+
+
+#endif
+
+
+
+#ifdef GC_AMIGA_AM
+
+#ifndef GC_AMIGA_FASTALLOC
+
+void *GC_amiga_allocwrapper(size_t size,void *(*AllocFunction)(size_t size2)){
+	return (*AllocFunction)(size);
+}
+
+void *(*GC_amiga_allocwrapper_do)(size_t size,void *(*AllocFunction)(size_t size2))
+	=GC_amiga_allocwrapper;
+
+#else
+
+
+
+
+void *GC_amiga_allocwrapper_firsttime(size_t size,void *(*AllocFunction)(size_t size2));
+
+void *(*GC_amiga_allocwrapper_do)(size_t size,void *(*AllocFunction)(size_t size2))
+	=GC_amiga_allocwrapper_firsttime;
+
+
+/******************************************************************
+   Amiga-specific routines to obtain memory, and force GC to give
+   back fast-mem whenever possible.
+	These hacks makes gc-programs go many times faster when
+   the Amiga is low on memory, and are therefore strictly necessary.
+
+   -Kjetil S. Matheussen, 2000.
+******************************************************************/
+
+
+
+/* List-header for all allocated memory. */
+
+struct GC_Amiga_AllocedMemoryHeader{
+	ULONG size;
+	struct GC_Amiga_AllocedMemoryHeader *next;
+};
+struct GC_Amiga_AllocedMemoryHeader *GC_AMIGAMEM=(struct GC_Amiga_AllocedMemoryHeader *)(int)~(NULL);
+
+
+
+/* Type of memory. Once in the execution of a program, this might change to MEMF_ANY|MEMF_CLEAR */
+
+ULONG GC_AMIGA_MEMF = MEMF_FAST | MEMF_CLEAR;
+
+
+/* Prevents GC_amiga_get_mem from allocating memory if this one is TRUE. */
+#ifndef GC_AMIGA_ONLYFAST
+BOOL GC_amiga_dontalloc=FALSE;
+#endif
+
+#ifdef GC_AMIGA_PRINTSTATS
+int succ=0,succ2=0;
+int nsucc=0,nsucc2=0;
+int nullretries=0;
+int numcollects=0;
+int chipa=0;
+int allochip=0;
+int allocfast=0;
+int cur0=0;
+int cur1=0;
+int cur10=0;
+int cur50=0;
+int cur150=0;
+int cur151=0;
+int ncur0=0;
+int ncur1=0;
+int ncur10=0;
+int ncur50=0;
+int ncur150=0;
+int ncur151=0;
+#endif
+
+/* Free everything at program-end. */
+
+void GC_amiga_free_all_mem(void){
+	struct GC_Amiga_AllocedMemoryHeader *gc_am=(struct GC_Amiga_AllocedMemoryHeader *)(~(int)(GC_AMIGAMEM));
+	struct GC_Amiga_AllocedMemoryHeader *temp;
+
+#ifdef GC_AMIGA_PRINTSTATS
+	printf("\n\n"
+		"%d bytes of chip-mem, and %d bytes of fast-mem where allocated from the OS.\n",
+		allochip,allocfast
+	);
+	printf(
+		"%d bytes of chip-mem were returned from the GC_AMIGA_FASTALLOC supported allocating functions.\n",
+		chipa
+	);
+	printf("\n");
+	printf("GC_gcollect was called %d times to avoid returning NULL or start allocating with the MEMF_ANY flag.\n",numcollects);
+	printf("%d of them was a success. (the others had to use allocation from the OS.)\n",nullretries);
+	printf("\n");
+	printf("Succeded forcing %d gc-allocations (%d bytes) of chip-mem to be fast-mem.\n",succ,succ2);
+	printf("Failed forcing %d gc-allocations (%d bytes) of chip-mem to be fast-mem.\n",nsucc,nsucc2);
+	printf("\n");
+	printf(
+		"Number of retries before succeding a chip->fast force:\n"
+		"0: %d, 1: %d, 2-9: %d, 10-49: %d, 50-149: %d, >150: %d\n",
+		cur0,cur1,cur10,cur50,cur150,cur151
+	);
+	printf(
+		"Number of retries before giving up a chip->fast force:\n"
+		"0: %d, 1: %d, 2-9: %d, 10-49: %d, 50-149: %d, >150: %d\n",
+		ncur0,ncur1,ncur10,ncur50,ncur150,ncur151
+	);
+#endif
+
+	while(gc_am!=NULL){
+		temp=gc_am->next;
+		FreeMem(gc_am,gc_am->size);
+		gc_am=(struct GC_Amiga_AllocedMemoryHeader *)(~(int)(temp));
+	}
+}
+
+#ifndef GC_AMIGA_ONLYFAST
+
+/* All memory with address lower than this one is chip-mem. */
+
+char *chipmax;
+
+
+/*
+ * Always set to the last size of memory tried to be allocated.
+ * Needed to ensure allocation when the size is bigger than 100000.
+ *
+ */
+size_t latestsize;
+
+#endif
+
+
+/*
+ * The actual function that is called with the GET_MEM macro.
+ *
+ */
+
+void *GC_amiga_get_mem(size_t size){
+	struct GC_Amiga_AllocedMemoryHeader *gc_am;
+
+#ifndef GC_AMIGA_ONLYFAST
+	if(GC_amiga_dontalloc==TRUE){
+//		printf("rejected, size: %d, latestsize: %d\n",size,latestsize);
+		return NULL;
+	}
+
+	// We really don't want to use chip-mem, but if we must, then as little as possible.
+	if(GC_AMIGA_MEMF==(MEMF_ANY|MEMF_CLEAR) && size>100000 && latestsize<50000) return NULL;
+#endif
+
+	gc_am=AllocMem((ULONG)(size + sizeof(struct GC_Amiga_AllocedMemoryHeader)),GC_AMIGA_MEMF);
+	if(gc_am==NULL) return NULL;
+
+	gc_am->next=GC_AMIGAMEM;
+	gc_am->size=size + sizeof(struct GC_Amiga_AllocedMemoryHeader);
+	GC_AMIGAMEM=(struct GC_Amiga_AllocedMemoryHeader *)(~(int)(gc_am));
+
+//	printf("Allocated %d (%d) bytes at address: %x. Latest: %d\n",size,tot,gc_am,latestsize);
+
+#ifdef GC_AMIGA_PRINTSTATS
+	if((char *)gc_am<chipmax){
+		allochip+=size;
+	}else{
+		allocfast+=size;
+	}
+#endif
+
+	return gc_am+1;
+
+}
+
+
+
+
+#ifndef GC_AMIGA_ONLYFAST
+
+/* Tries very hard to force GC to find fast-mem to return. Done recursively
+ * to hold the rejected memory-pointers reachable from the collector in an
+ * easy way.
+ *
+ */
+#ifdef GC_AMIGA_RETRY
+void *GC_amiga_rec_alloc(size_t size,void *(*AllocFunction)(size_t size2),const int rec){
+	void *ret;
+
+	ret=(*AllocFunction)(size);
+
+#ifdef GC_AMIGA_PRINTSTATS
+	if((char *)ret>chipmax || ret==NULL){
+		if(ret==NULL){
+			nsucc++;
+			nsucc2+=size;
+			if(rec==0) ncur0++;
+			if(rec==1) ncur1++;
+			if(rec>1 && rec<10) ncur10++;
+			if(rec>=10 && rec<50) ncur50++;
+			if(rec>=50 && rec<150) ncur150++;
+			if(rec>=150) ncur151++;
+		}else{
+			succ++;
+			succ2+=size;
+			if(rec==0) cur0++;
+			if(rec==1) cur1++;
+			if(rec>1 && rec<10) cur10++;
+			if(rec>=10 && rec<50) cur50++;
+			if(rec>=50 && rec<150) cur150++;
+			if(rec>=150) cur151++;
+		}
+	}
+#endif
+
+	if (((char *)ret)<=chipmax && ret!=NULL && (rec<(size>500000?9:size/5000))){
+		ret=GC_amiga_rec_alloc(size,AllocFunction,rec+1);
+//		GC_free(ret2);
+	}
+
+	return ret;
+}
+#endif
+
+
+/* The allocating-functions defined inside the Amiga-blocks in gc.h is called
+ * via these functions.
+ */
+
+
+void *GC_amiga_allocwrapper_any(size_t size,void *(*AllocFunction)(size_t size2)){
+	void *ret,*ret2;
+
+	GC_amiga_dontalloc=TRUE;	// Pretty tough thing to do, but its indeed necessary.
+	latestsize=size;
+
+	ret=(*AllocFunction)(size);
+
+	if(((char *)ret) <= chipmax){
+		if(ret==NULL){
+			//Give GC access to allocate memory.
+#ifdef GC_AMIGA_GC
+			if(!GC_dont_gc){
+				GC_gcollect();
+#ifdef GC_AMIGA_PRINTSTATS
+				numcollects++;
+#endif
+				ret=(*AllocFunction)(size);
+			}
+#endif
+			if(ret==NULL){
+				GC_amiga_dontalloc=FALSE;
+				ret=(*AllocFunction)(size);
+				if(ret==NULL){
+					WARN("Out of Memory!  Returning NIL!\n", 0);
+				}
+			}
+#ifdef GC_AMIGA_PRINTSTATS
+			else{
+				nullretries++;
+			}
+			if(ret!=NULL && (char *)ret<=chipmax) chipa+=size;
+#endif
+		}
+#ifdef GC_AMIGA_RETRY
+		else{
+			/* We got chip-mem. Better try again and again and again etc., we might get fast-mem sooner or later... */
+			/* Using gctest to check the effectiveness of doing this, does seldom give a very good result. */
+			/* However, real programs doesn't normally rapidly allocate and deallocate. */
+//			printf("trying to force... %d bytes... ",size);
+			if(
+				AllocFunction!=GC_malloc_uncollectable
+#ifdef ATOMIC_UNCOLLECTABLE
+				&& AllocFunction!=GC_malloc_atomic_uncollectable
+#endif
+			){
+				ret2=GC_amiga_rec_alloc(size,AllocFunction,0);
+			}else{
+				ret2=(*AllocFunction)(size);
+#ifdef GC_AMIGA_PRINTSTATS
+				if((char *)ret2<chipmax || ret2==NULL){
+					nsucc++;
+					nsucc2+=size;
+					ncur0++;
+				}else{
+					succ++;
+					succ2+=size;
+					cur0++;
+				}
+#endif
+			}
+			if(((char *)ret2)>chipmax){
+//				printf("Succeeded.\n");
+				GC_free(ret);
+				ret=ret2;
+			}else{
+				GC_free(ret2);
+//				printf("But did not succeed.\n");
+			}
+		}
+#endif
+	}
+
+	GC_amiga_dontalloc=FALSE;
+
+	return ret;
+}
+
+
+
+void (*GC_amiga_toany)(void)=NULL;
+
+void GC_amiga_set_toany(void (*func)(void)){
+	GC_amiga_toany=func;
+}
+
+#endif // !GC_AMIGA_ONLYFAST
+
+
+void *GC_amiga_allocwrapper_fast(size_t size,void *(*AllocFunction)(size_t size2)){
+	void *ret;
+
+	ret=(*AllocFunction)(size);
+
+	if(ret==NULL){
+		// Enable chip-mem allocation.
+//		printf("ret==NULL\n");
+#ifdef GC_AMIGA_GC
+		if(!GC_dont_gc){
+			GC_gcollect();
+#ifdef GC_AMIGA_PRINTSTATS
+			numcollects++;
+#endif
+			ret=(*AllocFunction)(size);
+		}
+#endif
+		if(ret==NULL){
+#ifndef GC_AMIGA_ONLYFAST
+			GC_AMIGA_MEMF=MEMF_ANY | MEMF_CLEAR;
+			if(GC_amiga_toany!=NULL) (*GC_amiga_toany)();
+			GC_amiga_allocwrapper_do=GC_amiga_allocwrapper_any;
+			return GC_amiga_allocwrapper_any(size,AllocFunction);
+#endif
+		}
+#ifdef GC_AMIGA_PRINTSTATS
+		else{
+			nullretries++;
+		}
+#endif
+	}
+
+	return ret;
+}
+
+void *GC_amiga_allocwrapper_firsttime(size_t size,void *(*AllocFunction)(size_t size2)){
+	atexit(&GC_amiga_free_all_mem);
+	chipmax=(char *)SysBase->MaxLocMem;		// For people still having SysBase in chip-mem, this might speed up a bit.
+	GC_amiga_allocwrapper_do=GC_amiga_allocwrapper_fast;
+	return GC_amiga_allocwrapper_fast(size,AllocFunction);
+}
+
+
+#endif //GC_AMIGA_FASTALLOC
+
+
+
+/*
+ * The wrapped realloc function.
+ *
+ */
+void *GC_amiga_realloc(void *old_object,size_t new_size_in_bytes){
+#ifndef GC_AMIGA_FASTALLOC
+	return GC_realloc(old_object,new_size_in_bytes);
+#else
+	void *ret;
+	latestsize=new_size_in_bytes;
+	ret=GC_realloc(old_object,new_size_in_bytes);
+	if(ret==NULL && GC_AMIGA_MEMF==(MEMF_FAST | MEMF_CLEAR)){
+		/* Out of fast-mem. */
+#ifdef GC_AMIGA_GC
+		if(!GC_dont_gc){
+			GC_gcollect();
+#ifdef GC_AMIGA_PRINTSTATS
+			numcollects++;
+#endif
+			ret=GC_realloc(old_object,new_size_in_bytes);
+		}
+#endif
+		if(ret==NULL){
+#ifndef GC_AMIGA_ONLYFAST
+			GC_AMIGA_MEMF=MEMF_ANY | MEMF_CLEAR;
+			if(GC_amiga_toany!=NULL) (*GC_amiga_toany)();
+			GC_amiga_allocwrapper_do=GC_amiga_allocwrapper_any;
+			ret=GC_realloc(old_object,new_size_in_bytes);
+#endif
+		}
+#ifdef GC_AMIGA_PRINTSTATS
+		else{
+			nullretries++;
+		}
+#endif
+	}
+	if(ret==NULL){
+		WARN("Out of Memory!  Returning NIL!\n", 0);
+	}
+#ifdef GC_AMIGA_PRINTSTATS
+	if(((char *)ret)<chipmax && ret!=NULL){
+		chipa+=new_size_in_bytes;
+	}
+#endif
+	return ret;
+#endif
+}
+
+#endif //GC_AMIGA_AM
+
+
diff --git a/src/gc/bdwgc/extra/MacOS.c b/src/gc/bdwgc/extra/MacOS.c
new file mode 100644
index 0000000..b56bea7
--- /dev/null
+++ b/src/gc/bdwgc/extra/MacOS.c
@@ -0,0 +1,156 @@
+/*
+	MacOS.c
+	
+	Some routines for the Macintosh OS port of the Hans-J. Boehm, Alan J. Demers
+	garbage collector.
+	
+	<Revision History>
+	
+	11/22/94  pcb  StripAddress the temporary memory handle for 24-bit mode.
+	11/30/94  pcb  Tracking all memory usage so we can deallocate it all at once.
+	02/10/96  pcb  Added routine to perform a final collection when
+unloading shared library.
+	
+	by Patrick C. Beard.
+ */
+/* Boehm, February 15, 1996 2:55 pm PST */
+
+#include <Resources.h>
+#include <Memory.h>
+#include <LowMem.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include "gc.h"
+#include "gc_priv.h"
+
+// use 'CODE' resource 0 to get exact location of the beginning of global space.
+
+typedef struct {
+	unsigned long aboveA5;
+	unsigned long belowA5;
+	unsigned long JTSize;
+	unsigned long JTOffset;
+} *CodeZeroPtr, **CodeZeroHandle;
+
+void* GC_MacGetDataStart()
+{
+	CodeZeroHandle code0 = (CodeZeroHandle)GetResource('CODE', 0);
+	if (code0) {
+		long belowA5Size = (**code0).belowA5;
+		ReleaseResource((Handle)code0);
+		return (LMGetCurrentA5() - belowA5Size);
+	}
+	fprintf(stderr, "Couldn't load the jump table.");
+	exit(-1);
+	return 0;
+}
+
+/* track the use of temporary memory so it can be freed all at once. */
+
+typedef struct TemporaryMemoryBlock TemporaryMemoryBlock, **TemporaryMemoryHandle;
+
+struct TemporaryMemoryBlock {
+	TemporaryMemoryHandle nextBlock;
+	char data[];
+};
+
+static TemporaryMemoryHandle theTemporaryMemory = NULL;
+static Boolean firstTime = true;
+
+void GC_MacFreeTemporaryMemory(void);
+
+Ptr GC_MacTemporaryNewPtr(size_t size, Boolean clearMemory)
+{
+	static Boolean firstTime = true;
+	OSErr result;
+	TemporaryMemoryHandle tempMemBlock;
+	Ptr tempPtr = nil;
+
+	tempMemBlock = (TemporaryMemoryHandle)TempNewHandle(size + sizeof(TemporaryMemoryBlock), &result);
+	if (tempMemBlock && result == noErr) {
+		HLockHi((Handle)tempMemBlock);
+		tempPtr = (**tempMemBlock).data;
+		if (clearMemory) memset(tempPtr, 0, size);
+		tempPtr = StripAddress(tempPtr);
+
+		// keep track of the allocated blocks.
+		(**tempMemBlock).nextBlock = theTemporaryMemory;
+		theTemporaryMemory = tempMemBlock;
+	}
+	
+#     if !defined(SHARED_LIBRARY_BUILD)
+	// install an exit routine to clean up the memory used at the end.
+	if (firstTime) {
+		atexit(&GC_MacFreeTemporaryMemory);
+		firstTime = false;
+	}
+#     endif
+	
+	return tempPtr;
+}
+
+extern word GC_fo_entries; 
+
+static void perform_final_collection()
+{
+  unsigned i;
+  word last_fo_entries = 0;
+  
+  /* adjust the stack bottom, because CFM calls us from another stack
+     location. */
+     GC_stackbottom = (ptr_t)&i;
+
+  /* try to collect and finalize everything in sight */
+    for (i = 0; i < 2 || GC_fo_entries < last_fo_entries; i++) {
+        last_fo_entries = GC_fo_entries;
+        GC_gcollect();
+    }
+}
+
+
+void GC_MacFreeTemporaryMemory()
+{
+# if defined(SHARED_LIBRARY_BUILD)
+    /* if possible, collect all memory, and invoke all finalizers. */
+      perform_final_collection();
+# endif
+
+    if (theTemporaryMemory != NULL) {
+	long totalMemoryUsed = 0;
+	TemporaryMemoryHandle tempMemBlock = theTemporaryMemory;
+	while (tempMemBlock != NULL) {
+		TemporaryMemoryHandle nextBlock = (**tempMemBlock).nextBlock;
+		totalMemoryUsed += GetHandleSize((Handle)tempMemBlock);
+		DisposeHandle((Handle)tempMemBlock);
+		tempMemBlock = nextBlock;
+	}
+	theTemporaryMemory = NULL;
+
+#       if !defined(SHARED_LIBRARY_BUILD)
+	  if (GC_print_stats) {
+            fprintf(stdout, "[total memory used:  %ld bytes.]\n",
+                  totalMemoryUsed);
+            fprintf(stdout, "[total collections:  %ld.]\n", GC_gc_no);
+	  }
+#       endif
+    }
+}
+
+#if __option(far_data)
+
+  void* GC_MacGetDataEnd()
+  {
+	CodeZeroHandle code0 = (CodeZeroHandle)GetResource('CODE', 0);
+	if (code0) {
+		long aboveA5Size = (**code0).aboveA5;
+		ReleaseResource((Handle)code0);
+		return (LMGetCurrentA5() + aboveA5Size);
+	}
+	fprintf(stderr, "Couldn't load the jump table.");
+	exit(-1);
+	return 0;
+  }
+
+#endif /* __option(far_data) */
diff --git a/src/gc/bdwgc/extra/add_gc_prefix.c b/src/gc/bdwgc/extra/add_gc_prefix.c
new file mode 100644
index 0000000..8646a8e
--- /dev/null
+++ b/src/gc/bdwgc/extra/add_gc_prefix.c
@@ -0,0 +1,24 @@
+# include <stdio.h>
+# include <gc.h>
+ 
+#ifndef GC_ALPHA_VERSION
+# define GC_ALPHA_VERSION GC_TMP_ALPHA_VERSION
+#endif
+
+int main(argc, argv, envp)
+int argc;
+char ** argv;
+char ** envp;
+{
+    int i;
+    
+    for (i = 1; i < argc; i++) {
+      if (GC_ALPHA_VERSION == GC_NOT_ALPHA) {
+	printf("gc%d.%d/%s ", GC_VERSION_MAJOR, GC_VERSION_MINOR, argv[i]);
+      } else {
+	printf("gc%d.%dalpha%d/%s ", GC_VERSION_MAJOR,
+	       GC_VERSION_MINOR, GC_ALPHA_VERSION, argv[i]);
+      }
+    }
+    return(0);
+}
diff --git a/src/gc/bdwgc/extra/gc.c b/src/gc/bdwgc/extra/gc.c
new file mode 100644
index 0000000..d660433
--- /dev/null
+++ b/src/gc/bdwgc/extra/gc.c
@@ -0,0 +1,82 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* This file could be used for the following purposes:          */
+/* - get the complete GC as a single link object file (module); */
+/* - enable more compiler optimizations.                        */
+
+/* Tip: to get the highest level of compiler optimizations, the typical */
+/* compiler options (GCC) to use are:                                   */
+/* -O3 -fno-strict-aliasing -march=native -Wall -fprofile-generate/use  */
+
+/* Warning: GCC for Linux (for C++ clients only): Use -fexceptions both */
+/* for GC and the client otherwise GC_thread_exit_proc() is not         */
+/* guaranteed to be invoked (see the comments in pthread_start.c).      */
+
+
+#define GC_INNER STATIC
+#define GC_EXTERN GC_INNER
+                /* STATIC is defined in gcconfig.h. */
+
+/* Small files go first... */
+#include "../backgraph.c"
+#include "../blacklst.c"
+#include "../checksums.c"
+#include "../gcj_mlc.c"
+#include "../headers.c"
+#include "../malloc.c"
+#include "../new_hblk.c"
+#include "../obj_map.c"
+#include "../ptr_chck.c"
+#include "../stubborn.c"
+
+#include "../allchblk.c"
+#include "../alloc.c"
+#include "../dbg_mlc.c"
+#include "../finalize.c"
+#include "../mallocx.c"
+#include "../mark.c"
+#include "../mark_rts.c"
+#include "../reclaim.c"
+#include "../typd_mlc.c"
+
+#include "../misc.c"
+#include "../os_dep.c"
+#include "../thread_local_alloc.c"
+
+/* Most platform-specific files go here... */
+#include "../darwin_stop_world.c"
+#include "../dyn_load.c"
+#include "../gc_dlopen.c"
+#include "../mach_dep.c"
+#include "../pcr_interface.c"
+#include "../pthread_stop_world.c"
+#include "../pthread_support.c"
+#include "../specific.c"
+#include "../win32_threads.c"
+
+#include "../pthread_start.c"
+
+/* Restore pthread calls redirection (if altered in             */
+/* pthread_stop_world.c, pthread_support.c or win32_threads.c). */
+/* This is only useful if directly included from application    */
+/* (instead of linking gc).                                     */
+#ifndef GC_NO_THREAD_REDIRECTS
+# include "gc_pthread_redirects.h"
+#endif
+
+/* real_malloc.c, extra/MacOS.c, extra/msvc_dbg.c are not included. */
diff --git a/src/gc/bdwgc/extra/gcname.c b/src/gc/bdwgc/extra/gcname.c
new file mode 100644
index 0000000..11cbeb1
--- /dev/null
+++ b/src/gc/bdwgc/extra/gcname.c
@@ -0,0 +1,17 @@
+#include <stdio.h>
+#include <gc.h>
+
+#ifndef GC_ALPHA_VERSION
+# define GC_ALPHA_VERSION GC_TMP_ALPHA_VERSION
+#endif
+
+int main()
+{
+    if (GC_ALPHA_VERSION == GC_NOT_ALPHA) {
+	printf("gc%d.%d", GC_VERSION_MAJOR, GC_VERSION_MINOR);
+    } else {
+	printf("gc%d.%dalpha%d", GC_VERSION_MAJOR,
+				 GC_VERSION_MINOR, GC_ALPHA_VERSION);
+    }
+    return 0;
+}
diff --git a/src/gc/bdwgc/extra/if_mach.c b/src/gc/bdwgc/extra/if_mach.c
new file mode 100644
index 0000000..d6e0a70
--- /dev/null
+++ b/src/gc/bdwgc/extra/if_mach.c
@@ -0,0 +1,25 @@
+/* Conditionally execute a command based on machine and OS from gcconfig.h */
+
+# include "private/gcconfig.h"
+# include <stdio.h>
+# include <string.h>
+# include <unistd.h>
+
+int main(int argc, char **argv, char **envp)
+{
+    if (argc < 4) goto Usage;
+    if (strcmp(MACH_TYPE, argv[1]) != 0) return(0);
+    if (strcmp(OS_TYPE, "") != 0 && strcmp(argv[2], "") != 0
+        && strcmp(OS_TYPE, argv[2]) != 0) return(0);
+    fprintf(stderr, "^^^^Starting command^^^^\n");
+    fflush(stdout);
+    execvp(argv[3], argv+3);
+    perror("Couldn't execute");
+    
+Usage:
+    fprintf(stderr, "Usage: %s mach_type os_type command\n", argv[0]);
+    fprintf(stderr, "Currently mach_type = %s, os_type = %s\n",
+    	    MACH_TYPE, OS_TYPE);
+    return(1);
+}
+
diff --git a/src/gc/bdwgc/extra/if_not_there.c b/src/gc/bdwgc/extra/if_not_there.c
new file mode 100644
index 0000000..7af6fba
--- /dev/null
+++ b/src/gc/bdwgc/extra/if_not_there.c
@@ -0,0 +1,38 @@
+/* Conditionally execute a command based if the file argv[1] doesn't exist */
+/* Except for execvp, we stick to ANSI C.				   */
+# include "private/gcconfig.h"
+# include <stdio.h>
+# include <stdlib.h>
+# include <unistd.h>
+#ifdef __DJGPP__
+#include <dirent.h>
+#endif /* __DJGPP__ */
+
+int main(int argc, char **argv, char **envp)
+{
+    FILE * f;
+#ifdef __DJGPP__
+    DIR * d;
+#endif /* __DJGPP__ */
+    if (argc < 3) goto Usage;
+    if ((f = fopen(argv[1], "rb")) != 0
+        || (f = fopen(argv[1], "r")) != 0) {
+        fclose(f);
+        return(0);
+    }
+#ifdef __DJGPP__
+    if ((d = opendir(argv[1])) != 0) {
+	    closedir(d);
+	    return(0);
+    }
+#endif
+    printf("^^^^Starting command^^^^\n");
+    fflush(stdout);
+    execvp(argv[2], argv+2);
+    exit(1);
+    
+Usage:
+    fprintf(stderr, "Usage: %s file_name command\n", argv[0]);
+    return(1);
+}
+
diff --git a/src/gc/bdwgc/extra/msvc_dbg.c b/src/gc/bdwgc/extra/msvc_dbg.c
new file mode 100644
index 0000000..5bc216a
--- /dev/null
+++ b/src/gc/bdwgc/extra/msvc_dbg.c
@@ -0,0 +1,381 @@
+/*
+  Copyright (c) 2004 Andrei Polushin
+
+  Permission is hereby granted, free of charge,  to any person obtaining a copy
+  of this software and associated documentation files (the "Software"), to deal
+  in the Software without restriction,  including without limitation the rights
+  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+  copies of the Software, and to permit persons to whom the Software is
+  furnished to do so, subject to the following conditions:
+
+  The above copyright notice and this permission notice shall be included in
+  all copies or substantial portions of the Software.
+
+  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+  THE SOFTWARE.
+*/
+
+#if !defined(_M_AMD64) && defined(_MSC_VER)
+
+/* X86_64 is currently missing some meachine-dependent code below.  */
+
+#include "private/msvc_dbg.h"
+
+#include "gc.h"
+
+#define WIN32_LEAN_AND_MEAN
+#include <windows.h>
+
+#pragma pack(push, 8)
+#include <imagehlp.h>
+#pragma pack(pop)
+
+#pragma comment(lib, "dbghelp.lib")
+#pragma optimize("gy", off)
+
+typedef GC_word word;
+#define GC_ULONG_PTR word
+
+#ifdef _WIN64
+        typedef GC_ULONG_PTR ULONG_ADDR;
+#else
+        typedef ULONG        ULONG_ADDR;
+#endif
+
+static HANDLE GetSymHandle()
+{
+  static HANDLE symHandle = NULL;
+  if (!symHandle) {
+    BOOL bRet = SymInitialize(symHandle = GetCurrentProcess(), NULL, FALSE);
+    if (bRet) {
+      DWORD dwOptions = SymGetOptions();
+      dwOptions &= ~SYMOPT_UNDNAME;
+      dwOptions |= SYMOPT_LOAD_LINES;
+      SymSetOptions(dwOptions);
+    }
+  }
+  return symHandle;
+}
+
+static void* CALLBACK FunctionTableAccess(HANDLE hProcess,
+                                          ULONG_ADDR dwAddrBase)
+{
+  return SymFunctionTableAccess(hProcess, dwAddrBase);
+}
+
+static ULONG_ADDR CALLBACK GetModuleBase(HANDLE hProcess, ULONG_ADDR dwAddress)
+{
+  MEMORY_BASIC_INFORMATION memoryInfo;
+  ULONG_ADDR dwAddrBase = SymGetModuleBase(hProcess, dwAddress);
+  if (dwAddrBase) {
+    return dwAddrBase;
+  }
+  if (VirtualQueryEx(hProcess, (void*)(GC_ULONG_PTR)dwAddress, &memoryInfo,
+                     sizeof(memoryInfo))) {
+    char filePath[_MAX_PATH];
+    char curDir[_MAX_PATH];
+    char exePath[_MAX_PATH];
+    DWORD size = GetModuleFileNameA((HINSTANCE)memoryInfo.AllocationBase,
+                                    filePath, sizeof(filePath));
+
+    /* Save and restore current directory around SymLoadModule, see KB  */
+    /* article Q189780.                                                 */
+    GetCurrentDirectoryA(sizeof(curDir), curDir);
+    GetModuleFileNameA(NULL, exePath, sizeof(exePath));
+#if defined(_MSC_VER) && _MSC_VER == 1200
+    /* use strcat for VC6 */
+    strcat(exePath, "\\..");
+#else
+    strcat_s(exePath, sizeof(exePath), "\\..");
+#endif /* _MSC_VER >= 1200 */
+    SetCurrentDirectoryA(exePath);
+#ifdef _DEBUG
+    GetCurrentDirectoryA(sizeof(exePath), exePath);
+#endif
+    SymLoadModule(hProcess, NULL, size ? filePath : NULL, NULL,
+                  (ULONG_ADDR)(GC_ULONG_PTR)memoryInfo.AllocationBase, 0);
+    SetCurrentDirectoryA(curDir);
+  }
+  return (ULONG_ADDR)(GC_ULONG_PTR)memoryInfo.AllocationBase;
+}
+
+static ULONG_ADDR CheckAddress(void* address)
+{
+  ULONG_ADDR dwAddress = (ULONG_ADDR)(GC_ULONG_PTR)address;
+  GetModuleBase(GetSymHandle(), dwAddress);
+  return dwAddress;
+}
+
+size_t GetStackFrames(size_t skip, void* frames[], size_t maxFrames)
+{
+  HANDLE hProcess = GetSymHandle();
+  HANDLE hThread = GetCurrentThread();
+  CONTEXT context;
+  context.ContextFlags = CONTEXT_FULL;
+  if (!GetThreadContext(hThread, &context)) {
+    return 0;
+  }
+  /* GetThreadContext might return invalid context for the current thread. */
+#if defined(_M_IX86)
+    __asm mov context.Ebp, ebp
+#endif
+  return GetStackFramesFromContext(hProcess, hThread, &context, skip + 1,
+                                   frames, maxFrames);
+}
+
+size_t GetStackFramesFromContext(HANDLE hProcess, HANDLE hThread,
+                                 CONTEXT* context, size_t skip,
+                                 void* frames[], size_t maxFrames)
+{
+  size_t frameIndex;
+  DWORD machineType;
+  STACKFRAME stackFrame = { 0 };
+  stackFrame.AddrPC.Mode      = AddrModeFlat;
+#if defined(_M_IX86)
+  machineType                 = IMAGE_FILE_MACHINE_I386;
+  stackFrame.AddrPC.Offset    = context->Eip;
+  stackFrame.AddrStack.Mode   = AddrModeFlat;
+  stackFrame.AddrStack.Offset = context->Esp;
+  stackFrame.AddrFrame.Mode   = AddrModeFlat;
+  stackFrame.AddrFrame.Offset = context->Ebp;
+#elif defined(_M_MRX000)
+  machineType                 = IMAGE_FILE_MACHINE_R4000;
+  stackFrame.AddrPC.Offset    = context->Fir;
+#elif defined(_M_ALPHA)
+  machineType                 = IMAGE_FILE_MACHINE_ALPHA;
+  stackFrame.AddrPC.Offset    = (unsigned long)context->Fir;
+#elif defined(_M_PPC)
+  machineType                 = IMAGE_FILE_MACHINE_POWERPC;
+  stackFrame.AddrPC.Offset    = context->Iar;
+#elif defined(_M_IA64)
+  machineType                 = IMAGE_FILE_MACHINE_IA64;
+  stackFrame.AddrPC.Offset    = context->StIIP;
+#elif defined(_M_ALPHA64)
+  machineType                 = IMAGE_FILE_MACHINE_ALPHA64;
+  stackFrame.AddrPC.Offset    = context->Fir;
+#else
+#error Unknown CPU
+#endif
+  for (frameIndex = 0; frameIndex < maxFrames; ) {
+    BOOL bRet = StackWalk(machineType, hProcess, hThread, &stackFrame,
+                    &context, NULL, FunctionTableAccess, GetModuleBase, NULL);
+    if (!bRet) {
+      break;
+    }
+    if (skip) {
+      skip--;
+    } else {
+      frames[frameIndex++] = (void*)(GC_ULONG_PTR)stackFrame.AddrPC.Offset;
+    }
+  }
+  return frameIndex;
+}
+
+size_t GetModuleNameFromAddress(void* address, char* moduleName, size_t size)
+{
+  if (size) *moduleName = 0;
+  {
+    const char* sourceName;
+    IMAGEHLP_MODULE moduleInfo = { sizeof (moduleInfo) };
+    if (!SymGetModuleInfo(GetSymHandle(), CheckAddress(address),
+                          &moduleInfo)) {
+      return 0;
+    }
+    sourceName = strrchr(moduleInfo.ImageName, '\\');
+    if (sourceName) {
+      sourceName++;
+    } else {
+      sourceName = moduleInfo.ImageName;
+    }
+    if (size) {
+      strncpy(moduleName, sourceName, size)[size - 1] = 0;
+    }
+    return strlen(sourceName);
+  }
+}
+
+size_t GetModuleNameFromStack(size_t skip, char* moduleName, size_t size)
+{
+  void* address = NULL;
+  GetStackFrames(skip + 1, &address, 1);
+  if (address) {
+    return GetModuleNameFromAddress(address, moduleName, size);
+  }
+  return 0;
+}
+
+size_t GetSymbolNameFromAddress(void* address, char* symbolName, size_t size,
+                                size_t* offsetBytes)
+{
+  if (size) *symbolName = 0;
+  if (offsetBytes) *offsetBytes = 0;
+  __try {
+    ULONG_ADDR dwOffset = 0;
+    union {
+      IMAGEHLP_SYMBOL sym;
+      char symNameBuffer[sizeof(IMAGEHLP_SYMBOL) + MAX_SYM_NAME];
+    } u;
+    u.sym.SizeOfStruct  = sizeof(u.sym);
+    u.sym.MaxNameLength = sizeof(u.symNameBuffer) - sizeof(u.sym);
+
+    if (!SymGetSymFromAddr(GetSymHandle(), CheckAddress(address), &dwOffset,
+                           &u.sym)) {
+      return 0;
+    } else {
+      const char* sourceName = u.sym.Name;
+      char undName[1024];
+      if (UnDecorateSymbolName(u.sym.Name, undName, sizeof(undName),
+                UNDNAME_NO_MS_KEYWORDS | UNDNAME_NO_ACCESS_SPECIFIERS)) {
+        sourceName = undName;
+      } else if (SymUnDName(&u.sym, undName, sizeof(undName))) {
+        sourceName = undName;
+      }
+      if (offsetBytes) {
+        *offsetBytes = dwOffset;
+      }
+      if (size) {
+        strncpy(symbolName, sourceName, size)[size - 1] = 0;
+      }
+      return strlen(sourceName);
+    }
+  } __except (EXCEPTION_EXECUTE_HANDLER) {
+    SetLastError(GetExceptionCode());
+  }
+  return 0;
+}
+
+size_t GetSymbolNameFromStack(size_t skip, char* symbolName, size_t size,
+                              size_t* offsetBytes)
+{
+  void* address = NULL;
+  GetStackFrames(skip + 1, &address, 1);
+  if (address) {
+    return GetSymbolNameFromAddress(address, symbolName, size, offsetBytes);
+  }
+  return 0;
+}
+
+size_t GetFileLineFromAddress(void* address, char* fileName, size_t size,
+                              size_t* lineNumber, size_t* offsetBytes)
+{
+  if (size) *fileName = 0;
+  if (lineNumber) *lineNumber = 0;
+  if (offsetBytes) *offsetBytes = 0;
+  {
+    char* sourceName;
+    IMAGEHLP_LINE line = { sizeof (line) };
+    GC_ULONG_PTR dwOffset = 0;
+    if (!SymGetLineFromAddr(GetSymHandle(), CheckAddress(address), &dwOffset,
+                            &line)) {
+      return 0;
+    }
+    if (lineNumber) {
+      *lineNumber = line.LineNumber;
+    }
+    if (offsetBytes) {
+      *offsetBytes = dwOffset;
+    }
+    sourceName = line.FileName;
+    /* TODO: resolve relative filenames, found in 'source directories'  */
+    /* registered with MSVC IDE.                                        */
+    if (size) {
+      strncpy(fileName, sourceName, size)[size - 1] = 0;
+    }
+    return strlen(sourceName);
+  }
+}
+
+size_t GetFileLineFromStack(size_t skip, char* fileName, size_t size,
+                            size_t* lineNumber, size_t* offsetBytes)
+{
+  void* address = NULL;
+  GetStackFrames(skip + 1, &address, 1);
+  if (address) {
+    return GetFileLineFromAddress(address, fileName, size, lineNumber,
+                                  offsetBytes);
+  }
+  return 0;
+}
+
+size_t GetDescriptionFromAddress(void* address, const char* format,
+                                 char* buffer, size_t size)
+{
+  char*const begin = buffer;
+  char*const end = buffer + size;
+  size_t line_number = 0;
+  char   str[128];
+
+  if (size) {
+    *buffer = 0;
+  }
+  buffer += GetFileLineFromAddress(address, buffer, size, &line_number, NULL);
+  size = end < buffer ? 0 : end - buffer;
+
+  if (line_number) {
+    wsprintf(str, "(%d) : ", line_number);
+    if (size) {
+      strncpy(buffer, str, size)[size - 1] = 0;
+    }
+    buffer += strlen(str);
+    size = end < buffer ? 0 : end - buffer;
+  }
+
+  if (size) {
+    strncpy(buffer, "at ", size)[size - 1] = 0;
+  }
+  buffer += strlen("at ");
+  size = end < buffer ? 0 : end - buffer;
+
+  buffer += GetSymbolNameFromAddress(address, buffer, size, NULL);
+  size = end < buffer ? 0 : end - buffer;
+
+  if (size) {
+    strncpy(buffer, " in ", size)[size - 1] = 0;
+  }
+  buffer += strlen(" in ");
+  size = end < buffer ? 0 : end - buffer;
+
+  buffer += GetModuleNameFromAddress(address, buffer, size);
+  size = end < buffer ? 0 : end - buffer;
+
+  return buffer - begin;
+}
+
+size_t GetDescriptionFromStack(void* const frames[], size_t count,
+                               const char* format, char* description[],
+                               size_t size)
+{
+  char*const begin = (char*)description;
+  char*const end = begin + size;
+  char* buffer = begin + (count + 1) * sizeof(char*);
+  size_t i;
+  for (i = 0; i < count; ++i) {
+    if (description) description[i] = buffer;
+    size = end < buffer ? 0 : end - buffer;
+    buffer += 1 + GetDescriptionFromAddress(frames[i], NULL, buffer, size);
+  }
+  if (description) description[count] = NULL;
+  return buffer - begin;
+}
+
+/* Compatibility with <execinfo.h> */
+
+int backtrace(void* addresses[], int count)
+{
+  return GetStackFrames(1, addresses, count);
+}
+
+char** backtrace_symbols(void*const* addresses, int count)
+{
+  size_t size = GetDescriptionFromStack(addresses, count, NULL, NULL, 0);
+  char** symbols = (char**)malloc(size);
+  GetDescriptionFromStack(addresses, count, NULL, symbols, size);
+  return symbols;
+}
+
+#endif /* !_M_AMD64 */
diff --git a/src/gc/bdwgc/extra/setjmp_t.c b/src/gc/bdwgc/extra/setjmp_t.c
new file mode 100644
index 0000000..fb2ec48
--- /dev/null
+++ b/src/gc/bdwgc/extra/setjmp_t.c
@@ -0,0 +1,145 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* Check whether setjmp actually saves registers in jmp_buf. */
+/* If it doesn't, the generic mark_regs code won't work.     */
+/* Compilers vary as to whether they will put x in a         */
+/* (callee-save) register without -O.  The code is           */
+/* contrived such that any decent compiler should put x in   */
+/* a callee-save register with -O.  Thus it is               */
+/* recommended that this be run optimized.  (If the machine  */
+/* has no callee-save registers, then the generic code is    */
+/* safe, but this will not be noticed by this piece of       */
+/* code.)  This test appears to be far from perfect.         */
+
+
+#ifdef NAUT
+# include <nautilus/printk.h>
+# include <nautilus/setjmp.h>
+#else
+# include <stdio.h>
+# include <setjmp.h>
+# include <string.h>
+#endif
+
+# include "private/gc_priv.h"
+
+#ifdef OS2
+/* GETPAGESIZE() is set to getpagesize() by default, but that   */
+/* doesn't really exist, and the collector doesn't need it.     */
+#define INCL_DOSFILEMGR
+#define INCL_DOSMISC
+#define INCL_DOSERRORS
+#include <os2.h>
+
+int getpagesize(void)
+{
+    ULONG result[1];
+
+    if (DosQuerySysInfo(QSV_PAGE_SIZE, QSV_PAGE_SIZE,
+                        (void *)result, sizeof(ULONG)) != NO_ERROR) {
+        fprintf(stderr, "DosQuerySysInfo failed\n");
+        result[0] = 4096;
+    }
+    return((int)(result[0]));
+}
+#endif
+
+struct {
+  char a_a;
+  char * a_b;
+} a;
+
+int * nested_sp(void)
+{
+    volatile int sp;
+    sp = (int)&sp;
+    return (int *)sp;
+}
+
+int main(void)
+{
+    volatile word sp;
+    long ps = GETPAGESIZE();
+    jmp_buf b;
+    register int x = (int)strlen("a");  /* 1, slightly disguised */
+    static int y = 0;
+
+    sp = (word)(&sp);
+    printf("This appears to be a %s running %s\n", MACH_TYPE, OS_TYPE);
+    if (nested_sp() < (int *)sp) {
+      printf("Stack appears to grow down, which is the default.\n");
+      printf("A good guess for STACKBOTTOM on this machine is 0x%lx.\n",
+             ((unsigned long)sp + ps) & ~(ps-1));
+    } else {
+      printf("Stack appears to grow up.\n");
+      printf("Define STACK_GROWS_UP in gc_private.h\n");
+      printf("A good guess for STACKBOTTOM on this machine is 0x%lx.\n",
+             ((unsigned long)sp + ps) & ~(ps-1));
+    }
+    printf("Note that this may vary between machines of ostensibly\n");
+    printf("the same architecture (e.g. Sun 3/50s and 3/80s).\n");
+    printf("On many machines the value is not fixed.\n");
+    printf("A good guess for ALIGNMENT on this machine is %ld.\n",
+           (unsigned long)(&(a.a_b))-(unsigned long)(&a));
+
+    printf("The following is a very dubious test of one root marking"
+           " strategy.\n");
+    printf("Results may not be accurate/useful:\n");
+    /* Encourage the compiler to keep x in a callee-save register */
+    x = 2*x-1;
+    printf("");
+    x = 2*x-1;
+    setjmp(b);
+    if (y == 1) {
+      if (x == 2) {
+        printf("Setjmp-based generic mark_regs code probably wont work.\n");
+        printf("But we rarely try that anymore.  If you have getcontect()\n");
+        printf("this probably doesn't matter.\n");
+      } else if (x == 1) {
+          printf("Setjmp-based register marking code may work.\n");
+      } else {
+          printf("Very strange setjmp implementation.\n");
+      }
+    }
+    y++;
+    x = 2;
+    if (y == 1) longjmp(b,1);
+    printf("Some GC internal configuration stuff: \n");
+    printf("\tWORDSZ = %d, ALIGNMENT = %d, GC_GRANULE_BYTES = %d\n",
+           WORDSZ, ALIGNMENT, GC_GRANULE_BYTES);
+    printf("\tUsing one mark ");
+#   if defined(USE_MARK_BYTES)
+      printf("byte");
+#   else
+      printf("bit");
+#   endif
+    printf(" per ");
+#   if defined(MARK_BIT_PER_OBJ)
+      printf("object.\n");
+#   elif defined(MARK_BIT_PER_GRANULE)
+      printf("granule.\n");
+#   endif
+#   ifdef THREAD_LOCAL_ALLOC
+      printf("Thread local allocation enabled.\n");
+#   endif
+#   ifdef PARALLEL_MARK
+      printf("Parallel marking enabled.\n");
+#   endif
+    return(0);
+}
+
+int g(int x)
+{
+    return(x);
+}
diff --git a/src/gc/bdwgc/extra/threadlibs.c b/src/gc/bdwgc/extra/threadlibs.c
new file mode 100644
index 0000000..6d84efd
--- /dev/null
+++ b/src/gc/bdwgc/extra/threadlibs.c
@@ -0,0 +1,83 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2010 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+# include "gc_config_macros.h"
+# include "private/gcconfig.h"
+
+# include <stdio.h>
+
+int main(void)
+{
+#   if defined(GC_USE_LD_WRAP)
+        printf("-Wl,--wrap -Wl,dlopen "
+               "-Wl,--wrap -Wl,pthread_create -Wl,--wrap -Wl,pthread_join "
+               "-Wl,--wrap -Wl,pthread_detach -Wl,--wrap -Wl,pthread_sigmask "
+               "-Wl,--wrap -Wl,pthread_exit -Wl,--wrap -Wl,pthread_cancel\n");
+#   endif
+#   if (defined(GC_LINUX_THREADS) && !defined(PLATFORM_ANDROID)) \
+        || defined(GC_IRIX_THREADS) || defined(GC_DARWIN_THREADS) \
+        || defined(GC_AIX_THREADS) || defined(GC_GNU_THREADS)
+#       ifdef GC_USE_DLOPEN_WRAP
+          printf("-ldl ");
+#       endif
+        printf("-lpthread\n");
+#   endif
+#   if defined(GC_OPENBSD_THREADS)
+        printf("-pthread\n");
+#   endif
+#   if defined(GC_FREEBSD_THREADS)
+#       ifdef GC_USE_DLOPEN_WRAP
+          printf("-ldl ");
+#       endif
+#       if (__FREEBSD_version >= 500000)
+          printf("-lpthread\n");
+#       else
+          printf("-pthread\n");
+#       endif
+#   endif
+#   if defined(GC_NETBSD_THREADS)
+          printf("-lpthread -lrt\n");
+#   endif
+
+#   if defined(GC_HPUX_THREADS) || defined(GC_OSF1_THREADS)
+        printf("-lpthread -lrt\n");
+#   endif
+#   if defined(GC_SOLARIS_THREADS)
+        printf("-lthread -lposix4\n");
+                /* Is this right for recent versions? */
+#   endif
+#   if defined(GC_WIN32_THREADS) && defined(CYGWIN32)
+        printf("-lpthread\n");
+#   endif
+#   if defined(GC_WIN32_PTHREADS)
+#      ifdef PTW32_STATIC_LIB
+         /* assume suffix s for static version of the win32 pthread library */
+         printf("-lpthreadGC2s -lws2_32\n");
+#      else
+         printf("-lpthreadGC2\n");
+#      endif
+#   endif
+#   if defined(GC_OSF1_THREADS)
+        printf("-pthread -lrt"); /* DOB: must be -pthread, not -lpthread */
+#   endif
+    /* You need GCC 3.0.3 to build this one!            */
+    /* DG/UX native gcc doesn't know what "-pthread" is */
+#   if defined(GC_DGUX386_THREADS)
+        printf("-ldl -pthread\n");
+#   endif
+    return 0;
+}
diff --git a/src/gc/bdwgc/finalize.c b/src/gc/bdwgc/finalize.c
new file mode 100644
index 0000000..325de81
--- /dev/null
+++ b/src/gc/bdwgc/finalize.c
@@ -0,0 +1,948 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1996 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (C) 2007 Free Software Foundation, Inc
+
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_pmark.h"
+
+#ifdef FINALIZE_ON_DEMAND
+  int GC_finalize_on_demand = 1;
+#else
+  int GC_finalize_on_demand = 0;
+#endif
+
+#ifdef JAVA_FINALIZATION
+  int GC_java_finalization = 1;
+#else
+  int GC_java_finalization = 0;
+#endif
+
+/* Type of mark procedure used for marking from finalizable object.     */
+/* This procedure normally does not mark the object, only its           */
+/* descendants.                                                         */
+typedef void (* finalization_mark_proc)(ptr_t /* finalizable_obj_ptr */);
+
+#define HASH3(addr,size,log_size) \
+        ((((word)(addr) >> 3) ^ ((word)(addr) >> (3 + (log_size)))) \
+         & ((size) - 1))
+#define HASH2(addr,log_size) HASH3(addr, 1 << log_size, log_size)
+
+struct hash_chain_entry {
+    word hidden_key;
+    struct hash_chain_entry * next;
+};
+
+static struct disappearing_link {
+    struct hash_chain_entry prolog;
+#   define dl_hidden_link prolog.hidden_key
+                                /* Field to be cleared.         */
+#   define dl_next(x) (struct disappearing_link *)((x) -> prolog.next)
+#   define dl_set_next(x,y) (x)->prolog.next = (struct hash_chain_entry *)(y)
+
+    word dl_hidden_obj;         /* Pointer to object base       */
+} **dl_head = 0;
+
+static signed_word log_dl_table_size = -1;
+                        /* Binary log of                                */
+                        /* current size of array pointed to by dl_head. */
+                        /* -1 ==> size is 0.                            */
+
+STATIC word GC_dl_entries = 0;
+                        /* Number of entries currently in disappearing  */
+                        /* link table.                                  */
+
+static struct finalizable_object {
+    struct hash_chain_entry prolog;
+#   define fo_hidden_base prolog.hidden_key
+                                /* Pointer to object base.      */
+                                /* No longer hidden once object */
+                                /* is on finalize_now queue.    */
+#   define fo_next(x) (struct finalizable_object *)((x) -> prolog.next)
+#   define fo_set_next(x,y) (x)->prolog.next = (struct hash_chain_entry *)(y)
+    GC_finalization_proc fo_fn; /* Finalizer.                   */
+    ptr_t fo_client_data;
+    word fo_object_size;        /* In bytes.                    */
+    finalization_mark_proc fo_mark_proc;        /* Mark-through procedure */
+} **fo_head = 0;
+
+STATIC struct finalizable_object * GC_finalize_now = 0;
+        /* List of objects that should be finalized now.        */
+
+static signed_word log_fo_table_size = -1;
+
+word GC_fo_entries = 0; /* used also in extra/MacOS.c */
+
+GC_INNER void GC_push_finalizer_structures(void)
+{
+    GC_push_all((ptr_t)(&dl_head), (ptr_t)(&dl_head) + sizeof(word));
+    GC_push_all((ptr_t)(&fo_head), (ptr_t)(&fo_head) + sizeof(word));
+    GC_push_all((ptr_t)(&GC_finalize_now),
+                (ptr_t)(&GC_finalize_now) + sizeof(word));
+}
+
+/* Double the size of a hash table. *size_ptr is the log of its current */
+/* size.  May be a no-op.                                               */
+/* *table is a pointer to an array of hash headers.  If we succeed, we  */
+/* update both *table and *log_size_ptr.                                */
+/* Lock is held.                                                        */
+STATIC void GC_grow_table(struct hash_chain_entry ***table,
+                          signed_word *log_size_ptr)
+{
+    register word i;
+    register struct hash_chain_entry *p;
+    signed_word log_old_size = *log_size_ptr;
+    signed_word log_new_size = log_old_size + 1;
+    word old_size = ((log_old_size == -1)? 0: (1 << log_old_size));
+    word new_size = (word)1 << log_new_size;
+    /* FIXME: Power of 2 size often gets rounded up to one more page. */
+    struct hash_chain_entry **new_table = (struct hash_chain_entry **)
+        GC_INTERNAL_MALLOC_IGNORE_OFF_PAGE(
+                (size_t)new_size * sizeof(struct hash_chain_entry *), NORMAL);
+
+    if (new_table == 0) {
+        if (*table == 0) {
+            ABORT("Insufficient space for initial table allocation");
+        } else {
+            return;
+        }
+    }
+    for (i = 0; i < old_size; i++) {
+      p = (*table)[i];
+      while (p != 0) {
+        ptr_t real_key = GC_REVEAL_POINTER(p -> hidden_key);
+        struct hash_chain_entry *next = p -> next;
+        size_t new_hash = HASH3(real_key, new_size, log_new_size);
+
+        p -> next = new_table[new_hash];
+        new_table[new_hash] = p;
+        p = next;
+      }
+    }
+    *log_size_ptr = log_new_size;
+    *table = new_table;
+}
+
+GC_API int GC_CALL GC_register_disappearing_link(void * * link)
+{
+    ptr_t base;
+
+    base = (ptr_t)GC_base((void *)link);
+    if (base == 0)
+        ABORT("Bad arg to GC_register_disappearing_link");
+    return(GC_general_register_disappearing_link(link, base));
+}
+
+GC_API int GC_CALL GC_general_register_disappearing_link(void * * link,
+                                                         void * obj)
+{
+    struct disappearing_link *curr_dl;
+    size_t index;
+    struct disappearing_link * new_dl;
+    DCL_LOCK_STATE;
+
+    if (((word)link & (ALIGNMENT-1)) || link == NULL)
+        ABORT("Bad arg to GC_general_register_disappearing_link");
+    LOCK();
+    GC_ASSERT(obj != NULL);
+    GC_ASSERT(GC_base(obj) == obj);
+    if (log_dl_table_size == -1
+        || GC_dl_entries > ((word)1 << log_dl_table_size)) {
+        GC_grow_table((struct hash_chain_entry ***)(&dl_head),
+                      &log_dl_table_size);
+        if (GC_print_stats) {
+            GC_log_printf("Grew dl table to %u entries\n",
+                      (1 << (unsigned)log_dl_table_size));
+        }
+    }
+    index = HASH2(link, log_dl_table_size);
+    for (curr_dl = dl_head[index]; curr_dl != 0; curr_dl = dl_next(curr_dl)) {
+        if (curr_dl -> dl_hidden_link == GC_HIDE_POINTER(link)) {
+            curr_dl -> dl_hidden_obj = GC_HIDE_POINTER(obj);
+            UNLOCK();
+            return GC_DUPLICATE;
+        }
+    }
+    new_dl = (struct disappearing_link *)
+        GC_INTERNAL_MALLOC(sizeof(struct disappearing_link),NORMAL);
+    if (0 == new_dl) {
+      GC_oom_func oom_fn = GC_oom_fn;
+      UNLOCK();
+      new_dl = (struct disappearing_link *)
+                (*oom_fn)(sizeof(struct disappearing_link));
+      if (0 == new_dl) {
+        return GC_NO_MEMORY;
+      }
+      /* It's not likely we'll make it here, but ... */
+      LOCK();
+      /* Recalculate index since the table may grow.    */
+      index = HASH2(link, log_dl_table_size);
+      /* Check again that our disappearing link not in the table. */
+      for (curr_dl = dl_head[index]; curr_dl != 0;
+           curr_dl = dl_next(curr_dl)) {
+        if (curr_dl -> dl_hidden_link == GC_HIDE_POINTER(link)) {
+          curr_dl -> dl_hidden_obj = GC_HIDE_POINTER(obj);
+          UNLOCK();
+#         ifndef DBG_HDRS_ALL
+            /* Free unused new_dl returned by GC_oom_fn() */
+            GC_free((void *)new_dl);
+#         endif
+          return GC_DUPLICATE;
+        }
+      }
+    }
+    new_dl -> dl_hidden_obj = GC_HIDE_POINTER(obj);
+    new_dl -> dl_hidden_link = GC_HIDE_POINTER(link);
+    dl_set_next(new_dl, dl_head[index]);
+    dl_head[index] = new_dl;
+    GC_dl_entries++;
+    UNLOCK();
+    return GC_SUCCESS;
+}
+
+GC_API int GC_CALL GC_unregister_disappearing_link(void * * link)
+{
+    struct disappearing_link *curr_dl, *prev_dl;
+    size_t index;
+    DCL_LOCK_STATE;
+
+    if (((word)link & (ALIGNMENT-1)) != 0) return(0); /* Nothing to do. */
+
+    LOCK();
+    index = HASH2(link, log_dl_table_size);
+    prev_dl = 0; curr_dl = dl_head[index];
+    while (curr_dl != 0) {
+        if (curr_dl -> dl_hidden_link == GC_HIDE_POINTER(link)) {
+            if (prev_dl == 0) {
+                dl_head[index] = dl_next(curr_dl);
+            } else {
+                dl_set_next(prev_dl, dl_next(curr_dl));
+            }
+            GC_dl_entries--;
+            UNLOCK();
+#           ifdef DBG_HDRS_ALL
+              dl_set_next(curr_dl, 0);
+#           else
+              GC_free((void *)curr_dl);
+#           endif
+            return(1);
+        }
+        prev_dl = curr_dl;
+        curr_dl = dl_next(curr_dl);
+    }
+    UNLOCK();
+    return(0);
+}
+
+/* Possible finalization_marker procedures.  Note that mark stack       */
+/* overflow is handled by the caller, and is not a disaster.            */
+STATIC void GC_normal_finalize_mark_proc(ptr_t p)
+{
+    hdr * hhdr = HDR(p);
+
+    PUSH_OBJ(p, hhdr, GC_mark_stack_top,
+             &(GC_mark_stack[GC_mark_stack_size]));
+}
+
+/* This only pays very partial attention to the mark descriptor.        */
+/* It does the right thing for normal and atomic objects, and treats    */
+/* most others as normal.                                               */
+STATIC void GC_ignore_self_finalize_mark_proc(ptr_t p)
+{
+    hdr * hhdr = HDR(p);
+    word descr = hhdr -> hb_descr;
+    ptr_t q;
+    word r;
+    ptr_t scan_limit;
+    ptr_t target_limit = p + hhdr -> hb_sz - 1;
+
+    if ((descr & GC_DS_TAGS) == GC_DS_LENGTH) {
+       scan_limit = p + descr - sizeof(word);
+    } else {
+       scan_limit = target_limit + 1 - sizeof(word);
+    }
+    for (q = p; q <= scan_limit; q += ALIGNMENT) {
+        r = *(word *)q;
+        if ((ptr_t)r < p || (ptr_t)r > target_limit) {
+            GC_PUSH_ONE_HEAP(r, q);
+        }
+    }
+}
+
+/*ARGSUSED*/
+STATIC void GC_null_finalize_mark_proc(ptr_t p) {}
+
+/* Possible finalization_marker procedures.  Note that mark stack       */
+/* overflow is handled by the caller, and is not a disaster.            */
+
+/* GC_unreachable_finalize_mark_proc is an alias for normal marking,    */
+/* but it is explicitly tested for, and triggers different              */
+/* behavior.  Objects registered in this way are not finalized          */
+/* if they are reachable by other finalizable objects, even if those    */
+/* other objects specify no ordering.                                   */
+STATIC void GC_unreachable_finalize_mark_proc(ptr_t p)
+{
+    GC_normal_finalize_mark_proc(p);
+}
+
+/* Register a finalization function.  See gc.h for details.     */
+/* The last parameter is a procedure that determines            */
+/* marking for finalization ordering.  Any objects marked       */
+/* by that procedure will be guaranteed to not have been        */
+/* finalized when this finalizer is invoked.                    */
+STATIC void GC_register_finalizer_inner(void * obj,
+                                        GC_finalization_proc fn, void *cd,
+                                        GC_finalization_proc *ofn, void **ocd,
+                                        finalization_mark_proc mp)
+{
+    ptr_t base;
+    struct finalizable_object * curr_fo, * prev_fo;
+    size_t index;
+    struct finalizable_object *new_fo = 0;
+    hdr *hhdr = NULL; /* initialized to prevent warning. */
+    GC_oom_func oom_fn;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    if (log_fo_table_size == -1
+        || GC_fo_entries > ((word)1 << log_fo_table_size)) {
+        GC_grow_table((struct hash_chain_entry ***)(&fo_head),
+                      &log_fo_table_size);
+        if (GC_print_stats) {
+            GC_log_printf("Grew fo table to %u entries\n",
+                          (1 << (unsigned)log_fo_table_size));
+        }
+    }
+    /* in the THREADS case we hold allocation lock.             */
+    base = (ptr_t)obj;
+    for (;;) {
+      index = HASH2(base, log_fo_table_size);
+      prev_fo = 0; curr_fo = fo_head[index];
+      while (curr_fo != 0) {
+        GC_ASSERT(GC_size(curr_fo) >= sizeof(struct finalizable_object));
+        if (curr_fo -> fo_hidden_base == GC_HIDE_POINTER(base)) {
+          /* Interruption by a signal in the middle of this     */
+          /* should be safe.  The client may see only *ocd      */
+          /* updated, but we'll declare that to be his problem. */
+          if (ocd) *ocd = (void *) (curr_fo -> fo_client_data);
+          if (ofn) *ofn = curr_fo -> fo_fn;
+          /* Delete the structure for base. */
+          if (prev_fo == 0) {
+            fo_head[index] = fo_next(curr_fo);
+          } else {
+            fo_set_next(prev_fo, fo_next(curr_fo));
+          }
+          if (fn == 0) {
+            GC_fo_entries--;
+            /* May not happen if we get a signal.  But a high   */
+            /* estimate will only make the table larger than    */
+            /* necessary.                                       */
+#           if !defined(THREADS) && !defined(DBG_HDRS_ALL)
+              GC_free((void *)curr_fo);
+#           endif
+          } else {
+            curr_fo -> fo_fn = fn;
+            curr_fo -> fo_client_data = (ptr_t)cd;
+            curr_fo -> fo_mark_proc = mp;
+            /* Reinsert it.  We deleted it first to maintain    */
+            /* consistency in the event of a signal.            */
+            if (prev_fo == 0) {
+              fo_head[index] = curr_fo;
+            } else {
+              fo_set_next(prev_fo, curr_fo);
+            }
+          }
+          UNLOCK();
+#         ifndef DBG_HDRS_ALL
+            if (EXPECT(new_fo != 0, FALSE)) {
+              /* Free unused new_fo returned by GC_oom_fn() */
+              GC_free((void *)new_fo);
+            }
+#         endif
+          return;
+        }
+        prev_fo = curr_fo;
+        curr_fo = fo_next(curr_fo);
+      }
+      if (EXPECT(new_fo != 0, FALSE)) {
+        /* new_fo is returned by GC_oom_fn(), so fn != 0 and hhdr != 0. */
+        break;
+      }
+      if (fn == 0) {
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        UNLOCK();
+        return;
+      }
+      GET_HDR(base, hhdr);
+      if (EXPECT(0 == hhdr, FALSE)) {
+        /* We won't collect it, hence finalizer wouldn't be run. */
+        if (ocd) *ocd = 0;
+        if (ofn) *ofn = 0;
+        UNLOCK();
+        return;
+      }
+      new_fo = (struct finalizable_object *)
+        GC_INTERNAL_MALLOC(sizeof(struct finalizable_object),NORMAL);
+      if (EXPECT(new_fo != 0, TRUE))
+        break;
+      oom_fn = GC_oom_fn;
+      UNLOCK();
+      new_fo = (struct finalizable_object *)
+                (*oom_fn)(sizeof(struct finalizable_object));
+      if (0 == new_fo) {
+        /* No enough memory.  *ocd and *ofn remains unchanged.  */
+        return;
+      }
+      /* It's not likely we'll make it here, but ... */
+      LOCK();
+      /* Recalculate index since the table may grow and         */
+      /* check again that our finalizer is not in the table.    */
+    }
+    GC_ASSERT(GC_size(new_fo) >= sizeof(struct finalizable_object));
+    if (ocd) *ocd = 0;
+    if (ofn) *ofn = 0;
+    new_fo -> fo_hidden_base = GC_HIDE_POINTER(base);
+    new_fo -> fo_fn = fn;
+    new_fo -> fo_client_data = (ptr_t)cd;
+    new_fo -> fo_object_size = hhdr -> hb_sz;
+    new_fo -> fo_mark_proc = mp;
+    fo_set_next(new_fo, fo_head[index]);
+    GC_fo_entries++;
+    fo_head[index] = new_fo;
+    UNLOCK();
+}
+
+GC_API void GC_CALL GC_register_finalizer(void * obj,
+                                  GC_finalization_proc fn, void * cd,
+                                  GC_finalization_proc *ofn, void ** ocd)
+{
+    GC_register_finalizer_inner(obj, fn, cd, ofn,
+                                ocd, GC_normal_finalize_mark_proc);
+}
+
+GC_API void GC_CALL GC_register_finalizer_ignore_self(void * obj,
+                               GC_finalization_proc fn, void * cd,
+                               GC_finalization_proc *ofn, void ** ocd)
+{
+    GC_register_finalizer_inner(obj, fn, cd, ofn,
+                                ocd, GC_ignore_self_finalize_mark_proc);
+}
+
+GC_API void GC_CALL GC_register_finalizer_no_order(void * obj,
+                               GC_finalization_proc fn, void * cd,
+                               GC_finalization_proc *ofn, void ** ocd)
+{
+    GC_register_finalizer_inner(obj, fn, cd, ofn,
+                                ocd, GC_null_finalize_mark_proc);
+}
+
+static GC_bool need_unreachable_finalization = FALSE;
+        /* Avoid the work if this isn't used.   */
+
+GC_API void GC_CALL GC_register_finalizer_unreachable(void * obj,
+                               GC_finalization_proc fn, void * cd,
+                               GC_finalization_proc *ofn, void ** ocd)
+{
+    need_unreachable_finalization = TRUE;
+    GC_ASSERT(GC_java_finalization);
+    GC_register_finalizer_inner(obj, fn, cd, ofn,
+                                ocd, GC_unreachable_finalize_mark_proc);
+}
+
+#ifndef NO_DEBUGGING
+  void GC_dump_finalization(void)
+  {
+    struct disappearing_link * curr_dl;
+    struct finalizable_object * curr_fo;
+    ptr_t real_ptr, real_link;
+    int dl_size = (log_dl_table_size == -1 ) ? 0 : (1 << log_dl_table_size);
+    int fo_size = (log_fo_table_size == -1 ) ? 0 : (1 << log_fo_table_size);
+    int i;
+
+    GC_printf("Disappearing links:\n");
+    for (i = 0; i < dl_size; i++) {
+      for (curr_dl = dl_head[i]; curr_dl != 0; curr_dl = dl_next(curr_dl)) {
+        real_ptr = GC_REVEAL_POINTER(curr_dl -> dl_hidden_obj);
+        real_link = GC_REVEAL_POINTER(curr_dl -> dl_hidden_link);
+        GC_printf("Object: %p, Link:%p\n", real_ptr, real_link);
+      }
+    }
+    GC_printf("Finalizers:\n");
+    for (i = 0; i < fo_size; i++) {
+      for (curr_fo = fo_head[i]; curr_fo != 0; curr_fo = fo_next(curr_fo)) {
+        real_ptr = GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+        GC_printf("Finalizable object: %p\n", real_ptr);
+      }
+    }
+  }
+#endif /* !NO_DEBUGGING */
+
+#ifndef SMALL_CONFIG
+  STATIC word GC_old_dl_entries = 0; /* for stats printing */
+#endif
+
+#ifndef THREADS
+  /* Global variables to minimize the level of recursion when a client  */
+  /* finalizer allocates memory.                                        */
+  STATIC int GC_finalizer_nested = 0;
+                        /* Only the lowest byte is used, the rest is    */
+                        /* padding for proper global data alignment     */
+                        /* required for some compilers (like Watcom).   */
+  STATIC unsigned GC_finalizer_skipped = 0;
+
+  /* Checks and updates the level of finalizers recursion.              */
+  /* Returns NULL if GC_invoke_finalizers() should not be called by the */
+  /* collector (to minimize the risk of a deep finalizers recursion),   */
+  /* otherwise returns a pointer to GC_finalizer_nested.                */
+  STATIC unsigned char *GC_check_finalizer_nested(void)
+  {
+    unsigned nesting_level = *(unsigned char *)&GC_finalizer_nested;
+    if (nesting_level) {
+      /* We are inside another GC_invoke_finalizers().          */
+      /* Skip some implicitly-called GC_invoke_finalizers()     */
+      /* depending on the nesting (recursion) level.            */
+      if (++GC_finalizer_skipped < (1U << nesting_level)) return NULL;
+      GC_finalizer_skipped = 0;
+    }
+    *(char *)&GC_finalizer_nested = (char)(nesting_level + 1);
+    return (unsigned char *)&GC_finalizer_nested;
+  }
+#endif /* THREADS */
+
+/* Called with held lock (but the world is running).                    */
+/* Cause disappearing links to disappear and unreachable objects to be  */
+/* enqueued for finalization.                                           */
+GC_INNER void GC_finalize(void)
+{
+    struct disappearing_link * curr_dl, * prev_dl, * next_dl;
+    struct finalizable_object * curr_fo, * prev_fo, * next_fo;
+    ptr_t real_ptr, real_link;
+    size_t i;
+    size_t dl_size = (log_dl_table_size == -1 ) ? 0 : (1 << log_dl_table_size);
+    size_t fo_size = (log_fo_table_size == -1 ) ? 0 : (1 << log_fo_table_size);
+
+#   ifndef SMALL_CONFIG
+      /* Save current GC_dl_entries value for stats printing */
+      GC_old_dl_entries = GC_dl_entries;
+#   endif
+
+  /* Make disappearing links disappear */
+    for (i = 0; i < dl_size; i++) {
+      curr_dl = dl_head[i];
+      prev_dl = 0;
+      while (curr_dl != 0) {
+        real_ptr = GC_REVEAL_POINTER(curr_dl -> dl_hidden_obj);
+        real_link = GC_REVEAL_POINTER(curr_dl -> dl_hidden_link);
+        if (!GC_is_marked(real_ptr)) {
+            *(word *)real_link = 0;
+            next_dl = dl_next(curr_dl);
+            if (prev_dl == 0) {
+                dl_head[i] = next_dl;
+            } else {
+                dl_set_next(prev_dl, next_dl);
+            }
+            GC_clear_mark_bit((ptr_t)curr_dl);
+            GC_dl_entries--;
+            curr_dl = next_dl;
+        } else {
+            prev_dl = curr_dl;
+            curr_dl = dl_next(curr_dl);
+        }
+      }
+    }
+  /* Mark all objects reachable via chains of 1 or more pointers        */
+  /* from finalizable objects.                                          */
+    GC_ASSERT(GC_mark_state == MS_NONE);
+    for (i = 0; i < fo_size; i++) {
+      for (curr_fo = fo_head[i]; curr_fo != 0; curr_fo = fo_next(curr_fo)) {
+        GC_ASSERT(GC_size(curr_fo) >= sizeof(struct finalizable_object));
+        real_ptr = GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+        if (!GC_is_marked(real_ptr)) {
+            GC_MARKED_FOR_FINALIZATION(real_ptr);
+            GC_MARK_FO(real_ptr, curr_fo -> fo_mark_proc);
+            if (GC_is_marked(real_ptr)) {
+                WARN("Finalization cycle involving %p\n", real_ptr);
+            }
+        }
+      }
+    }
+  /* Enqueue for finalization all objects that are still                */
+  /* unreachable.                                                       */
+    GC_bytes_finalized = 0;
+    for (i = 0; i < fo_size; i++) {
+      curr_fo = fo_head[i];
+      prev_fo = 0;
+      while (curr_fo != 0) {
+        real_ptr = GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+        if (!GC_is_marked(real_ptr)) {
+            if (!GC_java_finalization) {
+              GC_set_mark_bit(real_ptr);
+            }
+            /* Delete from hash table */
+              next_fo = fo_next(curr_fo);
+              if (prev_fo == 0) {
+                fo_head[i] = next_fo;
+              } else {
+                fo_set_next(prev_fo, next_fo);
+              }
+              GC_fo_entries--;
+            /* Add to list of objects awaiting finalization.    */
+              fo_set_next(curr_fo, GC_finalize_now);
+              GC_finalize_now = curr_fo;
+              /* unhide object pointer so any future collections will   */
+              /* see it.                                                */
+              curr_fo -> fo_hidden_base =
+                        (word)GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+              GC_bytes_finalized +=
+                        curr_fo -> fo_object_size
+                        + sizeof(struct finalizable_object);
+            GC_ASSERT(GC_is_marked(GC_base((ptr_t)curr_fo)));
+            curr_fo = next_fo;
+        } else {
+            prev_fo = curr_fo;
+            curr_fo = fo_next(curr_fo);
+        }
+      }
+    }
+
+  if (GC_java_finalization) {
+    /* make sure we mark everything reachable from objects finalized
+       using the no_order mark_proc */
+      for (curr_fo = GC_finalize_now;
+         curr_fo != NULL; curr_fo = fo_next(curr_fo)) {
+        real_ptr = (ptr_t)curr_fo -> fo_hidden_base;
+        if (!GC_is_marked(real_ptr)) {
+            if (curr_fo -> fo_mark_proc == GC_null_finalize_mark_proc) {
+                GC_MARK_FO(real_ptr, GC_normal_finalize_mark_proc);
+            }
+            if (curr_fo -> fo_mark_proc != GC_unreachable_finalize_mark_proc) {
+                GC_set_mark_bit(real_ptr);
+            }
+        }
+      }
+
+    /* now revive finalize-when-unreachable objects reachable from
+       other finalizable objects */
+      if (need_unreachable_finalization) {
+        curr_fo = GC_finalize_now;
+        prev_fo = 0;
+        while (curr_fo != 0) {
+          next_fo = fo_next(curr_fo);
+          if (curr_fo -> fo_mark_proc == GC_unreachable_finalize_mark_proc) {
+            real_ptr = (ptr_t)curr_fo -> fo_hidden_base;
+            if (!GC_is_marked(real_ptr)) {
+              GC_set_mark_bit(real_ptr);
+            } else {
+              if (prev_fo == 0)
+                GC_finalize_now = next_fo;
+              else
+                fo_set_next(prev_fo, next_fo);
+
+              curr_fo -> fo_hidden_base =
+                                GC_HIDE_POINTER(curr_fo -> fo_hidden_base);
+              GC_bytes_finalized -=
+                  curr_fo->fo_object_size + sizeof(struct finalizable_object);
+
+              i = HASH2(real_ptr, log_fo_table_size);
+              fo_set_next (curr_fo, fo_head[i]);
+              GC_fo_entries++;
+              fo_head[i] = curr_fo;
+              curr_fo = prev_fo;
+            }
+          }
+          prev_fo = curr_fo;
+          curr_fo = next_fo;
+        }
+      }
+  }
+
+  /* Remove dangling disappearing links. */
+    for (i = 0; i < dl_size; i++) {
+      curr_dl = dl_head[i];
+      prev_dl = 0;
+      while (curr_dl != 0) {
+        real_link = GC_base(GC_REVEAL_POINTER(curr_dl -> dl_hidden_link));
+        if (real_link != 0 && !GC_is_marked(real_link)) {
+            next_dl = dl_next(curr_dl);
+            if (prev_dl == 0) {
+                dl_head[i] = next_dl;
+            } else {
+                dl_set_next(prev_dl, next_dl);
+            }
+            GC_clear_mark_bit((ptr_t)curr_dl);
+            GC_dl_entries--;
+            curr_dl = next_dl;
+        } else {
+            prev_dl = curr_dl;
+            curr_dl = dl_next(curr_dl);
+        }
+      }
+    }
+  if (GC_fail_count) {
+    /* Don't prevent running finalizers if there has been an allocation */
+    /* failure recently.                                                */
+#   ifdef THREADS
+      GC_reset_finalizer_nested();
+#   else
+      GC_finalizer_nested = 0;
+#   endif
+  }
+}
+
+#ifndef JAVA_FINALIZATION_NOT_NEEDED
+
+  /* Enqueue all remaining finalizers to be run - Assumes lock is held. */
+  STATIC void GC_enqueue_all_finalizers(void)
+  {
+    struct finalizable_object * curr_fo, * prev_fo, * next_fo;
+    ptr_t real_ptr;
+    register int i;
+    int fo_size;
+
+    fo_size = (log_fo_table_size == -1 ) ? 0 : (1 << log_fo_table_size);
+    GC_bytes_finalized = 0;
+    for (i = 0; i < fo_size; i++) {
+        curr_fo = fo_head[i];
+        prev_fo = 0;
+      while (curr_fo != 0) {
+          real_ptr = GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+          GC_MARK_FO(real_ptr, GC_normal_finalize_mark_proc);
+          GC_set_mark_bit(real_ptr);
+
+          /* Delete from hash table */
+          next_fo = fo_next(curr_fo);
+          if (prev_fo == 0) {
+              fo_head[i] = next_fo;
+          } else {
+              fo_set_next(prev_fo, next_fo);
+          }
+          GC_fo_entries--;
+
+          /* Add to list of objects awaiting finalization.      */
+          fo_set_next(curr_fo, GC_finalize_now);
+          GC_finalize_now = curr_fo;
+
+          /* unhide object pointer so any future collections will       */
+          /* see it.                                            */
+          curr_fo -> fo_hidden_base =
+                        (word)GC_REVEAL_POINTER(curr_fo -> fo_hidden_base);
+          GC_bytes_finalized +=
+                curr_fo -> fo_object_size + sizeof(struct finalizable_object);
+          curr_fo = next_fo;
+        }
+    }
+  }
+
+  /* Invoke all remaining finalizers that haven't yet been run.
+   * This is needed for strict compliance with the Java standard,
+   * which can make the runtime guarantee that all finalizers are run.
+   * Unfortunately, the Java standard implies we have to keep running
+   * finalizers until there are no more left, a potential infinite loop.
+   * YUCK.
+   * Note that this is even more dangerous than the usual Java
+   * finalizers, in that objects reachable from static variables
+   * may have been finalized when these finalizers are run.
+   * Finalizers run at this point must be prepared to deal with a
+   * mostly broken world.
+   * This routine is externally callable, so is called without
+   * the allocation lock.
+   */
+  GC_API void GC_CALL GC_finalize_all(void)
+  {
+    DCL_LOCK_STATE;
+
+    LOCK();
+    while (GC_fo_entries > 0) {
+      GC_enqueue_all_finalizers();
+      UNLOCK();
+      GC_invoke_finalizers();
+      /* Running the finalizers in this thread is arguably not a good   */
+      /* idea when we should be notifying another thread to run them.   */
+      /* But otherwise we don't have a great way to wait for them to    */
+      /* run.                                                           */
+      LOCK();
+    }
+    UNLOCK();
+  }
+
+#endif /* !JAVA_FINALIZATION_NOT_NEEDED */
+
+/* Returns true if it is worth calling GC_invoke_finalizers. (Useful if */
+/* finalizers can only be called from some kind of `safe state' and     */
+/* getting into that safe state is expensive.)                          */
+GC_API int GC_CALL GC_should_invoke_finalizers(void)
+{
+    return GC_finalize_now != 0;
+}
+
+/* Invoke finalizers for all objects that are ready to be finalized.    */
+/* Should be called without allocation lock.                            */
+GC_API int GC_CALL GC_invoke_finalizers(void)
+{
+    struct finalizable_object * curr_fo;
+    int count = 0;
+    word bytes_freed_before = 0; /* initialized to prevent warning. */
+    DCL_LOCK_STATE;
+
+    while (GC_finalize_now != 0) {
+#       ifdef THREADS
+            LOCK();
+#       endif
+        if (count == 0) {
+            bytes_freed_before = GC_bytes_freed;
+            /* Don't do this outside, since we need the lock. */
+        }
+        curr_fo = GC_finalize_now;
+#       ifdef THREADS
+            if (curr_fo != 0) GC_finalize_now = fo_next(curr_fo);
+            UNLOCK();
+            if (curr_fo == 0) break;
+#       else
+            GC_finalize_now = fo_next(curr_fo);
+#       endif
+        fo_set_next(curr_fo, 0);
+        (*(curr_fo -> fo_fn))((ptr_t)(curr_fo -> fo_hidden_base),
+                              curr_fo -> fo_client_data);
+        curr_fo -> fo_client_data = 0;
+        ++count;
+#       ifdef UNDEFINED
+            /* This is probably a bad idea.  It throws off accounting if */
+            /* nearly all objects are finalizable.  O.w. it shouldn't    */
+            /* matter.                                                   */
+            GC_free((void *)curr_fo);
+#       endif
+    }
+    /* bytes_freed_before is initialized whenever count != 0 */
+    if (count != 0 && bytes_freed_before != GC_bytes_freed) {
+        LOCK();
+        GC_finalizer_bytes_freed += (GC_bytes_freed - bytes_freed_before);
+        UNLOCK();
+    }
+    return count;
+}
+
+/* All accesses to it should be synchronized to avoid data races.       */
+GC_finalizer_notifier_proc GC_finalizer_notifier =
+        (GC_finalizer_notifier_proc)0;
+
+static GC_word last_finalizer_notification = 0;
+
+GC_INNER void GC_notify_or_invoke_finalizers(void)
+{
+    GC_finalizer_notifier_proc notifier_fn = 0;
+#   if defined(KEEP_BACK_PTRS) || defined(MAKE_BACK_GRAPH)
+      static word last_back_trace_gc_no = 1;    /* Skip first one. */
+#   endif
+    DCL_LOCK_STATE;
+
+#   if defined(THREADS) && !defined(KEEP_BACK_PTRS) \
+       && !defined(MAKE_BACK_GRAPH)
+      /* Quick check (while unlocked) for an empty finalization queue.  */
+      if (GC_finalize_now == 0) return;
+#   endif
+    LOCK();
+
+    /* This is a convenient place to generate backtraces if appropriate, */
+    /* since that code is not callable with the allocation lock.         */
+#   if defined(KEEP_BACK_PTRS) || defined(MAKE_BACK_GRAPH)
+      if (GC_gc_no > last_back_trace_gc_no) {
+#       ifdef KEEP_BACK_PTRS
+          long i;
+          /* Stops when GC_gc_no wraps; that's OK.      */
+          last_back_trace_gc_no = (word)(-1);  /* disable others. */
+          for (i = 0; i < GC_backtraces; ++i) {
+              /* FIXME: This tolerates concurrent heap mutation,        */
+              /* which may cause occasional mysterious results.         */
+              /* We need to release the GC lock, since GC_print_callers */
+              /* acquires it.  It probably shouldn't.                   */
+              UNLOCK();
+              GC_generate_random_backtrace_no_gc();
+              LOCK();
+          }
+          last_back_trace_gc_no = GC_gc_no;
+#       endif
+#       ifdef MAKE_BACK_GRAPH
+          if (GC_print_back_height) {
+            UNLOCK();
+            GC_print_back_graph_stats();
+            LOCK();
+          }
+#       endif
+      }
+#   endif
+    if (GC_finalize_now == 0) {
+      UNLOCK();
+      return;
+    }
+
+    if (!GC_finalize_on_demand) {
+      unsigned char *pnested = GC_check_finalizer_nested();
+      UNLOCK();
+      /* Skip GC_invoke_finalizers() if nested */
+      if (pnested != NULL) {
+        (void) GC_invoke_finalizers();
+        *pnested = 0; /* Reset since no more finalizers. */
+#       ifndef THREADS
+          GC_ASSERT(GC_finalize_now == 0);
+#       endif   /* Otherwise GC can run concurrently and add more */
+      }
+      return;
+    }
+
+    /* These variables require synchronization to avoid data races.     */
+    if (last_finalizer_notification != GC_gc_no) {
+        last_finalizer_notification = GC_gc_no;
+        notifier_fn = GC_finalizer_notifier;
+    }
+    UNLOCK();
+    if (notifier_fn != 0)
+        (*notifier_fn)(); /* Invoke the notifier */
+}
+
+GC_API void * GC_CALL GC_call_with_alloc_lock(GC_fn_type fn,
+                                              void * client_data)
+{
+    void * result;
+    DCL_LOCK_STATE;
+
+#   ifdef THREADS
+      LOCK();
+      /* FIXME - This looks wrong!! */
+      SET_LOCK_HOLDER();
+#   endif
+    result = (*fn)(client_data);
+#   ifdef THREADS
+#     ifndef GC_ASSERTIONS
+        UNSET_LOCK_HOLDER();
+#     endif /* o.w. UNLOCK() does it implicitly */
+      UNLOCK();
+#   endif
+    return(result);
+}
+
+#ifndef SMALL_CONFIG
+  GC_INNER void GC_print_finalization_stats(void)
+  {
+    struct finalizable_object *fo = GC_finalize_now;
+    unsigned long ready = 0;
+
+    GC_log_printf(
+        "%lu finalization table entries; %lu disappearing links alive\n",
+        (unsigned long)GC_fo_entries, (unsigned long)GC_dl_entries);
+    for (; 0 != fo; fo = fo_next(fo)) ++ready;
+    GC_log_printf("%lu objects are eligible for immediate finalization; "
+                  "%ld links cleared\n",
+                  ready, (long)GC_old_dl_entries - (long)GC_dl_entries);
+  }
+#endif /* !SMALL_CONFIG */
diff --git a/src/gc/bdwgc/gc_dlopen.c b/src/gc/bdwgc/gc_dlopen.c
new file mode 100644
index 0000000..f76230e
--- /dev/null
+++ b/src/gc/bdwgc/gc_dlopen.c
@@ -0,0 +1,103 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1997 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Original author: Bill Janssen
+ * Heavily modified by Hans Boehm and others
+ */
+
+#include "private/gc_priv.h"
+
+/* This used to be in dyn_load.c.  It was extracted into a separate     */
+/* file to avoid having to link against libdl.{a,so} if the client      */
+/* doesn't call dlopen.  Of course this fails if the collector is in    */
+/* a dynamic library. -HB                                               */
+#if defined(GC_PTHREADS) && !defined(GC_NO_DLOPEN)
+
+#undef GC_MUST_RESTORE_REDEFINED_DLOPEN
+#if defined(dlopen) && !defined(GC_USE_LD_WRAP)
+  /* To support various threads pkgs, gc.h interposes on dlopen by      */
+  /* defining "dlopen" to be "GC_dlopen", which is implemented below.   */
+  /* However, both GC_FirstDLOpenedLinkMap() and GC_dlopen() use the    */
+  /* real system dlopen() in their implementation. We first remove      */
+  /* gc.h's dlopen definition and restore it later, after GC_dlopen().  */
+# undef dlopen
+# define GC_MUST_RESTORE_REDEFINED_DLOPEN
+#endif
+
+/* Make sure we're not in the middle of a collection, and make sure we  */
+/* don't start any.  This is invoked prior to a dlopen call to avoid    */
+/* synchronization issues.  We can't just acquire the allocation lock,  */
+/* since startup code in dlopen may try to allocate.  This solution     */
+/* risks heap growth (or, even, heap overflow) in the presence of many  */
+/* dlopen calls in either a multi-threaded environment, or if the       */
+/* library initialization code allocates substantial amounts of GC'ed   */
+/* memory.                                                              */
+#ifndef USE_PROC_FOR_LIBRARIES
+  static void disable_gc_for_dlopen(void)
+  {
+    DCL_LOCK_STATE;
+    LOCK();
+    while (GC_incremental && GC_collection_in_progress()) {
+      GC_collect_a_little_inner(1000);
+    }
+    ++GC_dont_gc;
+    UNLOCK();
+  }
+#endif
+
+/* Redefine dlopen to guarantee mutual exclusion with           */
+/* GC_register_dynamic_libraries.  Should probably happen for   */
+/* other operating systems, too.                                */
+
+/* This is similar to WRAP/REAL_FUNC() in pthread_support.c.    */
+#ifdef GC_USE_LD_WRAP
+# define WRAP_DLFUNC(f) __wrap_##f
+# define REAL_DLFUNC(f) __real_##f
+  void * REAL_DLFUNC(dlopen)(const char *, int);
+#else
+# define WRAP_DLFUNC(f) GC_##f
+# define REAL_DLFUNC(f) f
+#endif
+
+GC_API void * WRAP_DLFUNC(dlopen)(const char *path, int mode)
+{
+  void * result;
+
+# ifndef USE_PROC_FOR_LIBRARIES
+    /* Disable collections.  This solution risks heap growth (or,       */
+    /* even, heap overflow) but there seems no better solutions.        */
+    disable_gc_for_dlopen();
+# endif
+  result = REAL_DLFUNC(dlopen)(path, mode);
+# ifndef USE_PROC_FOR_LIBRARIES
+    GC_enable(); /* undoes disable_gc_for_dlopen */
+# endif
+  return(result);
+}
+
+#ifdef GC_USE_LD_WRAP
+  /* Define GC_ function as an alias for the plain one, which will be   */
+  /* intercepted.  This allows files which include gc.h, and hence      */
+  /* generate references to the GC_ symbol, to see the right symbol.    */
+  GC_API void *GC_dlopen(const char *path, int mode)
+  {
+    return dlopen(path, mode);
+  }
+#endif /* GC_USE_LD_WRAP */
+
+#ifdef GC_MUST_RESTORE_REDEFINED_DLOPEN
+# define dlopen GC_dlopen
+#endif
+
+#endif  /* GC_PTHREADS && !GC_NO_DLOPEN */
diff --git a/src/gc/bdwgc/gcj_mlc.c b/src/gc/bdwgc/gcj_mlc.c
new file mode 100644
index 0000000..3467a39
--- /dev/null
+++ b/src/gc/bdwgc/gcj_mlc.c
@@ -0,0 +1,278 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "private/gc_pmark.h"  /* includes gc_priv.h */
+
+#ifdef GC_GCJ_SUPPORT
+
+/*
+ * This is an allocator interface tuned for gcj (the GNU static
+ * java compiler).
+ *
+ * Each allocated object has a pointer in its first word to a vtable,
+ * which for our purposes is simply a structure describing the type of
+ * the object.
+ * This descriptor structure contains a GC marking descriptor at offset
+ * MARK_DESCR_OFFSET.
+ *
+ * It is hoped that this interface may also be useful for other systems,
+ * possibly with some tuning of the constants.  But the immediate goal
+ * is to get better gcj performance.
+ *
+ * We assume:
+ *  1) Counting on explicit initialization of this interface is OK;
+ *  2) FASTLOCK is not a significant win.
+ */
+
+#include "gc_gcj.h"
+#include "private/dbg_mlc.h"
+
+#ifdef GC_ASSERTIONS
+  GC_INNER /* variable is also used in thread_local_alloc.c */
+#else
+  STATIC
+#endif
+GC_bool GC_gcj_malloc_initialized = FALSE;
+
+int GC_gcj_kind = 0;    /* Object kind for objects with descriptors     */
+                        /* in "vtable".                                 */
+int GC_gcj_debug_kind = 0;
+                        /* The kind of objects that is always marked    */
+                        /* with a mark proc call.                       */
+
+GC_INNER ptr_t * GC_gcjobjfreelist = NULL;
+
+STATIC ptr_t * GC_gcjdebugobjfreelist = NULL;
+
+/*ARGSUSED*/
+STATIC struct GC_ms_entry * GC_gcj_fake_mark_proc(word * addr,
+                                        struct GC_ms_entry *mark_stack_ptr,
+                                        struct GC_ms_entry *mark_stack_limit,
+                                        word env)
+{
+    ABORT("No client gcj mark proc is specified");
+    return mark_stack_ptr;
+}
+
+/* Caller does not hold allocation lock. */
+GC_API void GC_CALL GC_init_gcj_malloc(int mp_index,
+                                       void * /* really GC_mark_proc */mp)
+{
+    GC_bool ignore_gcj_info;
+    DCL_LOCK_STATE;
+
+    if (mp == 0)        /* In case GC_DS_PROC is unused.        */
+      mp = (void *)(word)GC_gcj_fake_mark_proc;
+
+    GC_init();  /* In case it's not already done.       */
+    LOCK();
+    if (GC_gcj_malloc_initialized) {
+      UNLOCK();
+      return;
+    }
+    GC_gcj_malloc_initialized = TRUE;
+#   ifdef GC_IGNORE_GCJ_INFO
+      /* This is useful for debugging on platforms with missing getenv(). */
+      ignore_gcj_info = 1;
+#   else
+      ignore_gcj_info = (0 != GETENV("GC_IGNORE_GCJ_INFO"));
+#   endif
+    if (GC_print_stats && ignore_gcj_info) {
+        GC_log_printf("Gcj-style type information is disabled!\n");
+    }
+    GC_ASSERT(GC_mark_procs[mp_index] == (GC_mark_proc)0); /* unused */
+    GC_mark_procs[mp_index] = (GC_mark_proc)(word)mp;
+    if ((unsigned)mp_index >= GC_n_mark_procs)
+        ABORT("GC_init_gcj_malloc: bad index");
+    /* Set up object kind gcj-style indirect descriptor. */
+      GC_gcjobjfreelist = (ptr_t *)GC_new_free_list_inner();
+      if (ignore_gcj_info) {
+        /* Use a simple length-based descriptor, thus forcing a fully   */
+        /* conservative scan.                                           */
+        GC_gcj_kind = GC_new_kind_inner((void **)GC_gcjobjfreelist,
+                                        (0 | GC_DS_LENGTH),
+                                        TRUE, TRUE);
+      } else {
+        GC_gcj_kind = GC_new_kind_inner(
+                        (void **)GC_gcjobjfreelist,
+                        (((word)(-(signed_word)MARK_DESCR_OFFSET
+                                 - GC_INDIR_PER_OBJ_BIAS))
+                         | GC_DS_PER_OBJECT),
+                        FALSE, TRUE);
+      }
+    /* Set up object kind for objects that require mark proc call.      */
+      if (ignore_gcj_info) {
+        GC_gcj_debug_kind = GC_gcj_kind;
+        GC_gcjdebugobjfreelist = GC_gcjobjfreelist;
+      } else {
+        GC_gcjdebugobjfreelist = (ptr_t *)GC_new_free_list_inner();
+        GC_gcj_debug_kind = GC_new_kind_inner(
+                                (void **)GC_gcjdebugobjfreelist,
+                                GC_MAKE_PROC(mp_index,
+                                             1 /* allocated with debug info */),
+                                FALSE, TRUE);
+      }
+    UNLOCK();
+}
+
+#define GENERAL_MALLOC_INNER(lb,k) \
+    GC_clear_stack(GC_generic_malloc_inner(lb, k))
+
+#define GENERAL_MALLOC_INNER_IOP(lb,k) \
+    GC_clear_stack(GC_generic_malloc_inner_ignore_off_page(lb, k))
+
+/* We need a mechanism to release the lock and invoke finalizers.       */
+/* We don't really have an opportunity to do this on a rarely executed  */
+/* path on which the lock is not held.  Thus we check at a              */
+/* rarely executed point at which it is safe to release the lock.       */
+/* We do this even where we could just call GC_INVOKE_FINALIZERS,       */
+/* since it's probably cheaper and certainly more uniform.              */
+/* FIXME - Consider doing the same elsewhere?                           */
+static void maybe_finalize(void)
+{
+   static word last_finalized_no = 0;
+   DCL_LOCK_STATE;
+
+   if (GC_gc_no == last_finalized_no) return;
+   if (!GC_is_initialized) return;
+   UNLOCK();
+   GC_INVOKE_FINALIZERS();
+   LOCK();
+   last_finalized_no = GC_gc_no;
+}
+
+/* Allocate an object, clear it, and store the pointer to the   */
+/* type structure (vtable in gcj).                              */
+/* This adds a byte at the end of the object if GC_malloc would.*/
+#ifdef THREAD_LOCAL_ALLOC
+  GC_INNER void * GC_core_gcj_malloc(size_t lb,
+                                     void * ptr_to_struct_containing_descr)
+#else
+  GC_API void * GC_CALL GC_gcj_malloc(size_t lb,
+                                      void * ptr_to_struct_containing_descr)
+#endif
+{
+    ptr_t op;
+    ptr_t * opp;
+    word lg;
+    DCL_LOCK_STATE;
+
+    if(SMALL_OBJ(lb)) {
+        lg = GC_size_map[lb];
+        opp = &(GC_gcjobjfreelist[lg]);
+        LOCK();
+        op = *opp;
+        if(EXPECT(op == 0, FALSE)) {
+            maybe_finalize();
+            op = (ptr_t)GENERAL_MALLOC_INNER((word)lb, GC_gcj_kind);
+            if (0 == op) {
+                GC_oom_func oom_fn = GC_oom_fn;
+                UNLOCK();
+                return((*oom_fn)(lb));
+            }
+        } else {
+            *opp = obj_link(op);
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+        }
+        *(void **)op = ptr_to_struct_containing_descr;
+        GC_ASSERT(((void **)op)[1] == 0);
+        UNLOCK();
+    } else {
+        LOCK();
+        maybe_finalize();
+        op = (ptr_t)GENERAL_MALLOC_INNER((word)lb, GC_gcj_kind);
+        if (0 == op) {
+            GC_oom_func oom_fn = GC_oom_fn;
+            UNLOCK();
+            return((*oom_fn)(lb));
+        }
+        *(void **)op = ptr_to_struct_containing_descr;
+        UNLOCK();
+    }
+    return((void *) op);
+}
+
+/* Similar to GC_gcj_malloc, but add debug info.  This is allocated     */
+/* with GC_gcj_debug_kind.                                              */
+GC_API void * GC_CALL GC_debug_gcj_malloc(size_t lb,
+                void * ptr_to_struct_containing_descr, GC_EXTRA_PARAMS)
+{
+    void * result;
+    DCL_LOCK_STATE;
+
+    /* We're careful to avoid extra calls, which could          */
+    /* confuse the backtrace.                                   */
+    LOCK();
+    maybe_finalize();
+    result = GC_generic_malloc_inner(lb + DEBUG_BYTES, GC_gcj_debug_kind);
+    if (result == 0) {
+        GC_oom_func oom_fn = GC_oom_fn;
+        UNLOCK();
+        GC_err_printf("GC_debug_gcj_malloc(%ld, %p) returning NULL (",
+                      (unsigned long)lb, ptr_to_struct_containing_descr);
+        GC_err_puts(s);
+        GC_err_printf(":%d)\n", i);
+        return((*oom_fn)(lb));
+    }
+    *((void **)((ptr_t)result + sizeof(oh))) = ptr_to_struct_containing_descr;
+    UNLOCK();
+    if (!GC_debugging_started) {
+        GC_start_debugging();
+    }
+    ADD_CALL_CHAIN(result, ra);
+    return (GC_store_debug_info(result, (word)lb, s, i));
+}
+
+/* There is no THREAD_LOCAL_ALLOC for GC_gcj_malloc_ignore_off_page().  */
+GC_API void * GC_CALL GC_gcj_malloc_ignore_off_page(size_t lb,
+                                     void * ptr_to_struct_containing_descr)
+{
+    ptr_t op;
+    ptr_t * opp;
+    word lg;
+    DCL_LOCK_STATE;
+
+    if(SMALL_OBJ(lb)) {
+        lg = GC_size_map[lb];
+        opp = &(GC_gcjobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) == 0 ) {
+            maybe_finalize();
+            op = (ptr_t)GENERAL_MALLOC_INNER_IOP(lb, GC_gcj_kind);
+            if (0 == op) {
+                GC_oom_func oom_fn = GC_oom_fn;
+                UNLOCK();
+                return((*oom_fn)(lb));
+            }
+        } else {
+            *opp = obj_link(op);
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+        }
+    } else {
+        LOCK();
+        maybe_finalize();
+        op = (ptr_t)GENERAL_MALLOC_INNER_IOP(lb, GC_gcj_kind);
+        if (0 == op) {
+            GC_oom_func oom_fn = GC_oom_fn;
+            UNLOCK();
+            return((*oom_fn)(lb));
+        }
+    }
+    *(void **)op = ptr_to_struct_containing_descr;
+    UNLOCK();
+    return((void *) op);
+}
+
+#endif  /* GC_GCJ_SUPPORT */
diff --git a/src/gc/bdwgc/headers.c b/src/gc/bdwgc/headers.c
new file mode 100644
index 0000000..96ef713
--- /dev/null
+++ b/src/gc/bdwgc/headers.c
@@ -0,0 +1,403 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+/*
+ * This implements:
+ * 1. allocation of heap block headers
+ * 2. A map from addresses to heap block addresses to heap block headers
+ *
+ * Access speed is crucial.  We implement an index structure based on a 2
+ * level tree.
+ */
+
+STATIC bottom_index * GC_all_bottom_indices = 0;
+                                /* Pointer to first (lowest addr) */
+                                /* bottom_index.                  */
+
+STATIC bottom_index * GC_all_bottom_indices_end = 0;
+                                /* Pointer to last (highest addr) */
+                                /* bottom_index.                  */
+
+/* Non-macro version of header location routine */
+GC_INNER hdr * GC_find_header(ptr_t h)
+{
+#   ifdef HASH_TL
+        hdr * result;
+        GET_HDR(h, result);
+        return(result);
+#   else
+        return(HDR_INNER(h));
+#   endif
+}
+
+/* Handle a header cache miss.  Returns a pointer to the        */
+/* header corresponding to p, if p can possibly be a valid      */
+/* object pointer, and 0 otherwise.                             */
+/* GUARANTEED to return 0 for a pointer past the first page     */
+/* of an object unless both GC_all_interior_pointers is set     */
+/* and p is in fact a valid object pointer.                     */
+/* Never returns a pointer to a free hblk.                      */
+GC_INNER hdr *
+#ifdef PRINT_BLACK_LIST
+  GC_header_cache_miss(ptr_t p, hdr_cache_entry *hce, ptr_t source)
+#else
+  GC_header_cache_miss(ptr_t p, hdr_cache_entry *hce)
+#endif
+{
+  hdr *hhdr;
+  HC_MISS();
+  GET_HDR(p, hhdr);
+  if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+    if (GC_all_interior_pointers) {
+      if (hhdr != 0) {
+        ptr_t current = p;
+
+        current = (ptr_t)HBLKPTR(current);
+        do {
+            current = current - HBLKSIZE*(word)hhdr;
+            hhdr = HDR(current);
+        } while(IS_FORWARDING_ADDR_OR_NIL(hhdr));
+        /* current points to near the start of the large object */
+        if (hhdr -> hb_flags & IGNORE_OFF_PAGE)
+            return 0;
+        if (HBLK_IS_FREE(hhdr)
+            || p - current >= (ptrdiff_t)(hhdr->hb_sz)) {
+            GC_ADD_TO_BLACK_LIST_NORMAL(p, source);
+            /* Pointer past the end of the block */
+            return 0;
+        }
+      } else {
+        GC_ADD_TO_BLACK_LIST_NORMAL(p, source);
+        /* And return zero: */
+      }
+      GC_ASSERT(hhdr == 0 || !HBLK_IS_FREE(hhdr));
+      return hhdr;
+      /* Pointers past the first page are probably too rare     */
+      /* to add them to the cache.  We don't.                   */
+      /* And correctness relies on the fact that we don't.      */
+    } else {
+      if (hhdr == 0) {
+        GC_ADD_TO_BLACK_LIST_NORMAL(p, source);
+      }
+      return 0;
+    }
+  } else {
+    if (HBLK_IS_FREE(hhdr)) {
+      GC_ADD_TO_BLACK_LIST_NORMAL(p, source);
+      return 0;
+    } else {
+      hce -> block_addr = (word)(p) >> LOG_HBLKSIZE;
+      hce -> hce_hdr = hhdr;
+      return hhdr;
+    }
+  }
+}
+
+/* Routines to dynamically allocate collector data structures that will */
+/* never be freed.                                                       */
+
+static ptr_t scratch_free_ptr = 0;
+
+/* GC_scratch_last_end_ptr is end point of last obtained scratch area.  */
+/* GC_scratch_end_ptr is end point of current scratch area.             */
+
+GC_INNER ptr_t GC_scratch_alloc(size_t bytes)
+{
+    register ptr_t result = scratch_free_ptr;
+
+    bytes += GRANULE_BYTES-1;
+    bytes &= ~(GRANULE_BYTES-1);
+    scratch_free_ptr += bytes;
+    if (scratch_free_ptr <= GC_scratch_end_ptr) {
+        return(result);
+    }
+    {
+        word bytes_to_get = MINHINCR * HBLKSIZE;
+
+        if (bytes_to_get <= bytes) {
+          /* Undo the damage, and get memory directly */
+            bytes_to_get = ROUNDUP_PAGESIZE_IF_MMAP(bytes);
+            result = (ptr_t)GET_MEM(bytes_to_get);
+            GC_add_to_our_memory(result, bytes_to_get);
+            scratch_free_ptr -= bytes;
+            if (result != NULL) {
+                GC_scratch_last_end_ptr = result + bytes;
+            }
+            return(result);
+        }
+
+        bytes_to_get = ROUNDUP_PAGESIZE_IF_MMAP(bytes_to_get); /* for safety */
+        result = (ptr_t)GET_MEM(bytes_to_get);
+        GC_add_to_our_memory(result, bytes_to_get);
+        if (result == 0) {
+            if (GC_print_stats)
+                GC_log_printf("Out of memory - trying to allocate less\n");
+            scratch_free_ptr -= bytes;
+            bytes_to_get = ROUNDUP_PAGESIZE_IF_MMAP(bytes);
+            result = (ptr_t)GET_MEM(bytes_to_get);
+            GC_add_to_our_memory(result, bytes_to_get);
+            return result;
+        }
+        scratch_free_ptr = result;
+        GC_scratch_end_ptr = scratch_free_ptr + bytes_to_get;
+        GC_scratch_last_end_ptr = GC_scratch_end_ptr;
+        return(GC_scratch_alloc(bytes));
+    }
+}
+
+static hdr * hdr_free_list = 0;
+
+/* Return an uninitialized header */
+static hdr * alloc_hdr(void)
+{
+    register hdr * result;
+
+    if (hdr_free_list == 0) {
+        result = (hdr *) GC_scratch_alloc((word)(sizeof(hdr)));
+    } else {
+        result = hdr_free_list;
+        hdr_free_list = (hdr *) (result -> hb_next);
+    }
+    return(result);
+}
+
+GC_INLINE void free_hdr(hdr * hhdr)
+{
+    hhdr -> hb_next = (struct hblk *) hdr_free_list;
+    hdr_free_list = hhdr;
+}
+
+#ifdef COUNT_HDR_CACHE_HITS
+  /* Used for debugging/profiling (the symbols are externally visible). */
+  word GC_hdr_cache_hits = 0;
+  word GC_hdr_cache_misses = 0;
+#endif
+
+GC_INNER void GC_init_headers(void)
+{
+    register unsigned i;
+
+    GC_all_nils = (bottom_index *)GC_scratch_alloc((word)sizeof(bottom_index));
+    if (GC_all_nils == NULL) {
+      GC_err_printf("Insufficient memory for GC_all_nils\n");
+      EXIT();
+    }
+    BZERO(GC_all_nils, sizeof(bottom_index));
+    for (i = 0; i < TOP_SZ; i++) {
+        GC_top_index[i] = GC_all_nils;
+    }
+}
+
+/* Make sure that there is a bottom level index block for address addr  */
+/* Return FALSE on failure.                                             */
+static GC_bool get_index(word addr)
+{
+    word hi = (word)(addr) >> (LOG_BOTTOM_SZ + LOG_HBLKSIZE);
+    bottom_index * r;
+    bottom_index * p;
+    bottom_index ** prev;
+    bottom_index *pi;
+
+#   ifdef HASH_TL
+      word i = TL_HASH(hi);
+      bottom_index * old;
+
+      old = p = GC_top_index[i];
+      while(p != GC_all_nils) {
+          if (p -> key == hi) return(TRUE);
+          p = p -> hash_link;
+      }
+      r = (bottom_index*)GC_scratch_alloc((word)(sizeof (bottom_index)));
+      if (r == 0) return(FALSE);
+      BZERO(r, sizeof (bottom_index));
+      r -> hash_link = old;
+      GC_top_index[i] = r;
+#   else
+      if (GC_top_index[hi] != GC_all_nils) return(TRUE);
+      r = (bottom_index*)GC_scratch_alloc((word)(sizeof (bottom_index)));
+      if (r == 0) return(FALSE);
+      GC_top_index[hi] = r;
+      BZERO(r, sizeof (bottom_index));
+#   endif
+    r -> key = hi;
+    /* Add it to the list of bottom indices */
+      prev = &GC_all_bottom_indices;    /* pointer to p */
+      pi = 0;                           /* bottom_index preceding p */
+      while ((p = *prev) != 0 && p -> key < hi) {
+        pi = p;
+        prev = &(p -> asc_link);
+      }
+      r -> desc_link = pi;
+      if (0 == p) {
+        GC_all_bottom_indices_end = r;
+      } else {
+        p -> desc_link = r;
+      }
+      r -> asc_link = p;
+      *prev = r;
+    return(TRUE);
+}
+
+/* Install a header for block h.        */
+/* The header is uninitialized.         */
+/* Returns the header or 0 on failure.  */
+GC_INNER struct hblkhdr * GC_install_header(struct hblk *h)
+{
+    hdr * result;
+
+    if (!get_index((word) h)) return(0);
+    result = alloc_hdr();
+    if (result) {
+      SET_HDR(h, result);
+#     ifdef USE_MUNMAP
+        result -> hb_last_reclaimed = (unsigned short)GC_gc_no;
+#     endif
+    }
+    return(result);
+}
+
+/* Set up forwarding counts for block h of size sz */
+GC_INNER GC_bool GC_install_counts(struct hblk *h, size_t sz/* bytes */)
+{
+    struct hblk * hbp;
+    word i;
+
+    for (hbp = h; (char *)hbp < (char *)h + sz; hbp += BOTTOM_SZ) {
+        if (!get_index((word) hbp)) return(FALSE);
+    }
+    if (!get_index((word)h + sz - 1)) return(FALSE);
+    for (hbp = h + 1; (char *)hbp < (char *)h + sz; hbp += 1) {
+        i = HBLK_PTR_DIFF(hbp, h);
+        SET_HDR(hbp, (hdr *)(i > MAX_JUMP? MAX_JUMP : i));
+    }
+    return(TRUE);
+}
+
+/* Remove the header for block h */
+GC_INNER void GC_remove_header(struct hblk *h)
+{
+    hdr **ha;
+    GET_HDR_ADDR(h, ha);
+    free_hdr(*ha);
+    *ha = 0;
+}
+
+/* Remove forwarding counts for h */
+GC_INNER void GC_remove_counts(struct hblk *h, size_t sz/* bytes */)
+{
+    register struct hblk * hbp;
+    for (hbp = h+1; (char *)hbp < (char *)h + sz; hbp += 1) {
+        SET_HDR(hbp, 0);
+    }
+}
+
+/* Apply fn to all allocated blocks */
+/*VARARGS1*/
+void GC_apply_to_all_blocks(void (*fn)(struct hblk *h, word client_data),
+                            word client_data)
+{
+    signed_word j;
+    bottom_index * index_p;
+
+    for (index_p = GC_all_bottom_indices; index_p != 0;
+         index_p = index_p -> asc_link) {
+        for (j = BOTTOM_SZ-1; j >= 0;) {
+            if (!IS_FORWARDING_ADDR_OR_NIL(index_p->index[j])) {
+                if (!HBLK_IS_FREE(index_p->index[j])) {
+                    (*fn)(((struct hblk *)
+                              (((index_p->key << LOG_BOTTOM_SZ) + (word)j)
+                               << LOG_HBLKSIZE)),
+                          client_data);
+                }
+                j--;
+             } else if (index_p->index[j] == 0) {
+                j--;
+             } else {
+                j -= (signed_word)(index_p->index[j]);
+             }
+         }
+     }
+}
+
+/* Get the next valid block whose address is at least h */
+/* Return 0 if there is none.                           */
+GC_INNER struct hblk * GC_next_used_block(struct hblk *h)
+{
+    register bottom_index * bi;
+    register word j = ((word)h >> LOG_HBLKSIZE) & (BOTTOM_SZ-1);
+
+    GET_BI(h, bi);
+    if (bi == GC_all_nils) {
+        register word hi = (word)h >> (LOG_BOTTOM_SZ + LOG_HBLKSIZE);
+        bi = GC_all_bottom_indices;
+        while (bi != 0 && bi -> key < hi) bi = bi -> asc_link;
+        j = 0;
+    }
+    while(bi != 0) {
+        while (j < BOTTOM_SZ) {
+            hdr * hhdr = bi -> index[j];
+            if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+                j++;
+            } else {
+                if (!HBLK_IS_FREE(hhdr)) {
+                    return((struct hblk *)
+                              (((bi -> key << LOG_BOTTOM_SZ) + j)
+                               << LOG_HBLKSIZE));
+                } else {
+                    j += divHBLKSZ(hhdr -> hb_sz);
+                }
+            }
+        }
+        j = 0;
+        bi = bi -> asc_link;
+    }
+    return(0);
+}
+
+/* Get the last (highest address) block whose address is        */
+/* at most h.  Return 0 if there is none.                       */
+/* Unlike the above, this may return a free block.              */
+GC_INNER struct hblk * GC_prev_block(struct hblk *h)
+{
+    register bottom_index * bi;
+    register signed_word j = ((word)h >> LOG_HBLKSIZE) & (BOTTOM_SZ-1);
+
+    GET_BI(h, bi);
+    if (bi == GC_all_nils) {
+        register word hi = (word)h >> (LOG_BOTTOM_SZ + LOG_HBLKSIZE);
+        bi = GC_all_bottom_indices_end;
+        while (bi != 0 && bi -> key > hi) bi = bi -> desc_link;
+        j = BOTTOM_SZ - 1;
+    }
+    while(bi != 0) {
+        while (j >= 0) {
+            hdr * hhdr = bi -> index[j];
+            if (0 == hhdr) {
+                --j;
+            } else if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+                j -= (signed_word)hhdr;
+            } else {
+                return((struct hblk *)
+                          (((bi -> key << LOG_BOTTOM_SZ) + j)
+                               << LOG_HBLKSIZE));
+            }
+        }
+        j = BOTTOM_SZ - 1;
+        bi = bi -> desc_link;
+    }
+    return(0);
+}
diff --git a/src/gc/bdwgc/include/atomic_ops.h b/src/gc/bdwgc/include/atomic_ops.h
new file mode 100644
index 0000000..b4aadfa
--- /dev/null
+++ b/src/gc/bdwgc/include/atomic_ops.h
@@ -0,0 +1,389 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ATOMIC_OPS_H
+
+#define ATOMIC_OPS_H
+
+#include <assert.h>
+#include <stddef.h>
+
+/* We define various atomic operations on memory in a           */
+/* machine-specific way.  Unfortunately, this is complicated    */
+/* by the fact that these may or may not be combined with       */
+/* various memory barriers.  Thus the actual operations we      */
+/* define have the form AO_<atomic-op>_<barrier>, for all       */
+/* plausible combinations of <atomic-op> and <barrier>.         */
+/* This of course results in a mild combinatorial explosion.    */
+/* To deal with it, we try to generate derived                  */
+/* definitions for as many of the combinations as we can, as    */
+/* automatically as possible.                                   */
+/*                                                              */
+/* Our assumption throughout is that the programmer will        */
+/* specify the least demanding operation and memory barrier     */
+/* that will guarantee correctness for the implementation.      */
+/* Our job is to find the least expensive way to implement it   */
+/* on the applicable hardware.  In many cases that will         */
+/* involve, for example, a stronger memory barrier, or a        */
+/* combination of hardware primitives.                          */
+/*                                                              */
+/* Conventions:                                                 */
+/* "plain" atomic operations are not guaranteed to include      */
+/* a barrier.  The suffix in the name specifies the barrier     */
+/* type.  Suffixes are:                                         */
+/* _release: Earlier operations may not be delayed past it.     */
+/* _acquire: Later operations may not move ahead of it.         */
+/* _read: Subsequent reads must follow this operation and       */
+/*        preceding reads.                                      */
+/* _write: Earlier writes precede both this operation and       */
+/*        later writes.                                         */
+/* _full: Ordered with respect to both earlier and later memops.*/
+/* _release_write: Ordered with respect to earlier writes.      */
+/* _acquire_read: Ordered with respect to later reads.          */
+/*                                                              */
+/* Currently we try to define the following atomic memory       */
+/* operations, in combination with the above barriers:          */
+/* AO_nop                                                       */
+/* AO_load                                                      */
+/* AO_store                                                     */
+/* AO_test_and_set (binary)                                     */
+/* AO_fetch_and_add                                             */
+/* AO_fetch_and_add1                                            */
+/* AO_fetch_and_sub1                                            */
+/* AO_or                                                        */
+/* AO_compare_and_swap                                          */
+/*                                                              */
+/* Note that atomicity guarantees are valid only if both        */
+/* readers and writers use AO_ operations to access the         */
+/* shared value, while ordering constraints are intended to     */
+/* apply all memory operations.  If a location can potentially  */
+/* be accessed simultaneously from multiple threads, and one of */
+/* those accesses may be a write access, then all such          */
+/* accesses to that location should be through AO_ primitives.  */
+/* However if AO_ operations enforce sufficient ordering to     */
+/* ensure that a location x cannot be accessed concurrently,    */
+/* or can only be read concurrently, then x can be accessed     */
+/* via ordinary references and assignments.                     */
+/*                                                              */
+/* Compare_and_exchange takes an address and an expected old    */
+/* value and a new value, and returns an int.  Nonzero          */
+/* indicates that it succeeded.                                 */
+/* Test_and_set takes an address, atomically replaces it by     */
+/* AO_TS_SET, and returns the prior value.                      */
+/* An AO_TS_t location can be reset with the                    */
+/* AO_CLEAR macro, which normally uses AO_store_release.        */
+/* AO_fetch_and_add takes an address and an AO_t increment      */
+/* value.  The AO_fetch_and_add1 and AO_fetch_and_sub1 variants */
+/* are provided, since they allow faster implementations on     */
+/* some hardware. AO_or atomically ors an AO_t value into a     */
+/* memory location, but does not provide access to the original.*/
+/*                                                              */
+/* We expect this list to grow slowly over time.                */
+/*                                                              */
+/* Note that AO_nop_full is a full memory barrier.              */
+/*                                                              */
+/* Note that if some data is initialized with                   */
+/*      data.x = ...; data.y = ...; ...                         */
+/*      AO_store_release_write(&data_is_initialized, 1)         */
+/* then data is guaranteed to be initialized after the test     */
+/*      if (AO_load_acquire_read(&data_is_initialized)) ...     */
+/* succeeds.  Furthermore, this should generate near-optimal    */
+/* code on all common platforms.                                */
+/*                                                              */
+/* All operations operate on unsigned AO_t, which               */
+/* is the natural word size, and usually unsigned long.         */
+/* It is possible to check whether a particular operation op    */
+/* is available on a particular platform by checking whether    */
+/* AO_HAVE_op is defined.  We make heavy use of these macros    */
+/* internally.                                                  */
+
+/* The rest of this file basically has three sections:          */
+/*                                                              */
+/* Some utility and default definitions.                        */
+/*                                                              */
+/* The architecture dependent section:                          */
+/* This defines atomic operations that have direct hardware     */
+/* support on a particular platform, mostly by including the    */
+/* appropriate compiler- and hardware-dependent file.           */
+/*                                                              */
+/* The synthesis section:                                       */
+/* This tries to define other atomic operations in terms of     */
+/* those that are explicitly available on the platform.         */
+/* This section is hardware independent.                        */
+/* We make no attempt to synthesize operations in ways that     */
+/* effectively introduce locks, except for the debugging/demo   */
+/* pthread-based implementation at the beginning.  A more       */
+/* realistic implementation that falls back to locks could be   */
+/* added as a higher layer.  But that would sacrifice           */
+/* usability from signal handlers.                              */
+/* The synthesis section is implemented almost entirely in      */
+/* atomic_ops/generalize.h.                                     */
+
+/* Some common defaults.  Overridden for some architectures.    */
+#define AO_t size_t
+
+/* The test_and_set primitive returns an AO_TS_VAL_t value.     */
+/* AO_TS_t is the type of an in-memory test-and-set location.   */
+
+#define AO_TS_INITIALIZER (AO_t)AO_TS_CLEAR
+
+/* Platform-dependent stuff:                                    */
+#if defined(__GNUC__) || defined(_MSC_VER) || defined(__INTEL_COMPILER) \
+        || defined(__DMC__) || defined(__WATCOMC__)
+# define AO_INLINE static __inline
+#elif defined(__sun)
+# define AO_INLINE static inline
+#else
+# define AO_INLINE static
+#endif
+
+#if defined(__GNUC__) && !defined(__INTEL_COMPILER)
+# define AO_compiler_barrier() __asm__ __volatile__("" : : : "memory")
+#elif defined(_MSC_VER) || defined(__DMC__) || defined(__BORLANDC__) \
+        || defined(__WATCOMC__)
+# if defined(_AMD64_) || defined(_M_X64) || _MSC_VER >= 1400
+#   if defined(_WIN32_WCE)
+/* #     include <cmnintrin.h> */
+#   elif defined(_MSC_VER)
+#     include <intrin.h>
+#   endif
+#   pragma intrinsic(_ReadWriteBarrier)
+#   define AO_compiler_barrier() _ReadWriteBarrier()
+        /* We assume this does not generate a fence instruction.        */
+        /* The documentation is a bit unclear.                          */
+# else
+#   define AO_compiler_barrier() __asm { }
+        /* The preceding implementation may be preferable here too.     */
+        /* But the documentation warns about VC++ 2003 and earlier.     */
+# endif
+#elif defined(__INTEL_COMPILER)
+# define AO_compiler_barrier() __memory_barrier() /* Too strong? IA64-only? */
+#elif defined(_HPUX_SOURCE)
+# if defined(__ia64)
+#   include <machine/sys/inline.h>
+#   define AO_compiler_barrier() _Asm_sched_fence()
+# else
+    /* FIXME - We dont know how to do this.  This is a guess.   */
+    /* And probably a bad one.                                  */
+    static volatile int AO_barrier_dummy;
+#   define AO_compiler_barrier() (void)(AO_barrier_dummy = AO_barrier_dummy)
+# endif
+#else
+  /* We conjecture that the following usually gives us the right        */
+  /* semantics or an error.                                             */
+# define AO_compiler_barrier() asm("")
+#endif
+
+#if defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/generic_pthread.h"
+#endif /* AO_USE_PTHREAD_DEFS */
+
+#if (defined(__CC_ARM) || defined(__ARMCC__)) && !defined(__GNUC__) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/armcc/arm_v6.h"
+# define AO_GENERALIZE_TWICE
+#endif
+
+#if defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS) \
+    && !defined(__INTEL_COMPILER)
+# if defined(__i386__)
+    /* We don't define AO_USE_SYNC_CAS_BUILTIN for x86 here because     */
+    /* it might require specifying additional options (like -march)     */
+    /* or additional link libraries (if -march is not specified).       */
+#   include "atomic_ops/sysdeps/gcc/x86.h"
+# endif /* __i386__ */
+# if defined(__x86_64__)
+#   if __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 2)
+      /* It is safe to use __sync CAS built-in on this architecture.    */
+#     define AO_USE_SYNC_CAS_BUILTIN
+#   endif
+#   ifdef __ILP32__
+#     ifndef AO_USE_PENTIUM4_INSTRS
+#       define AO_USE_PENTIUM4_INSTRS
+#     endif
+#     include "atomic_ops/sysdeps/gcc/x86.h"
+#   else
+#     include "atomic_ops/sysdeps/gcc/x86_64.h"
+#   endif
+# endif /* __x86_64__ */
+# if defined(__ia64__)
+#   include "atomic_ops/sysdeps/gcc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# endif /* __ia64__ */
+# if defined(__hppa__)
+#   include "atomic_ops/sysdeps/gcc/hppa.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __hppa__ */
+# if defined(__alpha__)
+#   include "atomic_ops/sysdeps/gcc/alpha.h"
+#   define AO_GENERALIZE_TWICE
+# endif /* __alpha__ */
+# if defined(__s390__)
+#   include "atomic_ops/sysdeps/gcc/s390.h"
+# endif /* __s390__ */
+# if defined(__sparc__)
+#   include "atomic_ops/sysdeps/gcc/sparc.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __sparc__ */
+# if defined(__m68k__)
+#   include "atomic_ops/sysdeps/gcc/m68k.h"
+# endif /* __m68k__ */
+# if defined(__powerpc__) || defined(__ppc__) || defined(__PPC__) \
+     || defined(__powerpc64__) || defined(__ppc64__)
+#   include "atomic_ops/sysdeps/gcc/powerpc.h"
+# endif /* __powerpc__ */
+# if defined(__arm__) && !defined(AO_USE_PTHREAD_DEFS)
+#   include "atomic_ops/sysdeps/gcc/arm.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __arm__ */
+# if defined(__cris__) || defined(CRIS)
+#   include "atomic_ops/sysdeps/gcc/cris.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+# if defined(__mips__)
+#   include "atomic_ops/sysdeps/gcc/mips.h"
+# endif /* __mips__ */
+# if defined(__sh__) || defined(SH4)
+#   include "atomic_ops/sysdeps/gcc/sh.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __sh__ */
+# if defined(__avr32__)
+#   include "atomic_ops/sysdeps/gcc/avr32.h"
+# endif
+# if defined(__hexagon__)
+#   include "atomic_ops/sysdeps/gcc/hexagon.h"
+# endif
+#endif /* __GNUC__ && !AO_USE_PTHREAD_DEFS */
+
+#if (defined(__IBMC__) || defined(__IBMCPP__)) && !defined(__GNUC__) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__powerpc__) || defined(__powerpc) || defined(__ppc__) \
+     || defined(__PPC__) || defined(_M_PPC) || defined(_ARCH_PPC) \
+     || defined(_ARCH_PWR)
+#   include "atomic_ops/sysdeps/ibmc/powerpc.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+#endif
+
+#if defined(__INTEL_COMPILER) && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__ia64__)
+#   include "atomic_ops/sysdeps/icc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+# if defined(__GNUC__)
+    /* Intel Compiler in GCC compatible mode */
+#   if defined(__i386__)
+#     include "atomic_ops/sysdeps/gcc/x86.h"
+#   endif /* __i386__ */
+#   if defined(__x86_64__)
+#     if __INTEL_COMPILER > 1110
+#       define AO_USE_SYNC_CAS_BUILTIN
+#     endif
+#     ifdef __ILP32__
+#       ifndef AO_USE_PENTIUM4_INSTRS
+#         define AO_USE_PENTIUM4_INSTRS
+#       endif
+#       include "atomic_ops/sysdeps/gcc/x86.h"
+#     else
+#       include "atomic_ops/sysdeps/gcc/x86_64.h"
+#     endif
+#   endif /* __x86_64__ */
+# endif
+#endif
+
+#if defined(_HPUX_SOURCE) && !defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__ia64)
+#   include "atomic_ops/sysdeps/hpc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# else
+#   include "atomic_ops/sysdeps/hpc/hppa.h"
+#   define AO_CAN_EMUL_CAS
+# endif
+#endif
+
+#if defined(_MSC_VER) || defined(__DMC__) || defined(__BORLANDC__) \
+        || (defined(__WATCOMC__) && defined(__NT__))
+# if defined(_AMD64_) || defined(_M_X64)
+#   include "atomic_ops/sysdeps/msftc/x86_64.h"
+# elif defined(_M_IX86) || defined(x86)
+#   include "atomic_ops/sysdeps/msftc/x86.h"
+# elif defined(_M_ARM) || defined(ARM) || defined(_ARM_)
+#   include "atomic_ops/sysdeps/msftc/arm.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+#endif
+
+#if defined(__sun) && !defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS)
+  /* Note: use -DAO_USE_PTHREAD_DEFS if Sun CC does not handle inline asm. */
+# if defined(__i386)
+#   include "atomic_ops/sysdeps/sunc/x86.h"
+# endif /* __i386 */
+# if defined(__x86_64) || defined(__amd64)
+#   include "atomic_ops/sysdeps/sunc/x86_64.h"
+# endif /* __x86_64 */
+#endif
+
+#if !defined(__GNUC__) && (defined(sparc) || defined(__sparc)) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/sunc/sparc.h"
+# define AO_CAN_EMUL_CAS
+#endif
+
+#if defined(AO_REQUIRE_CAS) && !defined(AO_HAVE_compare_and_swap) \
+    && !defined(AO_HAVE_compare_and_swap_full) \
+    && !defined(AO_HAVE_compare_and_swap_acquire)
+# if defined(AO_CAN_EMUL_CAS)
+#   include "atomic_ops/sysdeps/emul_cas.h"
+# else
+#  error Cannot implement AO_compare_and_swap_full on this architecture.
+# endif
+#endif /* AO_REQUIRE_CAS && !AO_HAVE_compare_and_swap ... */
+
+/* The most common way to clear a test-and-set location         */
+/* at the end of a critical section.                            */
+#if AO_AO_TS_T && !defined(AO_CLEAR)
+# define AO_CLEAR(addr) AO_store_release((AO_TS_t *)(addr), AO_TS_CLEAR)
+#endif
+#if AO_CHAR_TS_T && !defined(AO_CLEAR)
+# define AO_CLEAR(addr) AO_char_store_release((AO_TS_t *)(addr), AO_TS_CLEAR)
+#endif
+
+/* The generalization section.  */
+#if !defined(AO_GENERALIZE_TWICE) && defined(AO_CAN_EMUL_CAS) \
+    && !defined(AO_HAVE_compare_and_swap_full)
+# define AO_GENERALIZE_TWICE
+#endif
+
+/* Theoretically we should repeatedly include atomic_ops/generalize.h.  */
+/* In fact, we observe that this converges after a small fixed number   */
+/* of iterations, usually one.                                          */
+#include "atomic_ops/generalize.h"
+#ifdef AO_GENERALIZE_TWICE
+# include "atomic_ops/generalize.h"
+#endif
+
+/* For compatibility with version 0.4 and earlier       */
+#define AO_TS_T AO_TS_t
+#define AO_T AO_t
+#define AO_TS_VAL AO_TS_VAL_t
+
+#endif /* ATOMIC_OPS_H */
diff --git a/src/gc/bdwgc/include/bdwgc_internal.h b/src/gc/bdwgc/include/bdwgc_internal.h
new file mode 100644
index 0000000..1e82ed5
--- /dev/null
+++ b/src/gc/bdwgc/include/bdwgc_internal.h
@@ -0,0 +1,223 @@
+/* 
+ * This file is part of the Nautilus AeroKernel developed
+ * by the Hobbes and V3VEE Projects with funding from the 
+ * United States National  Science Foundation and the Department of Energy.  
+ *
+ * The V3VEE Project is a joint project between Northwestern University
+ * and the University of New Mexico.  The Hobbes Project is a collaboration
+ * led by Sandia National Laboratories that includes several national 
+ * laboratories and universities. You can find out more at:
+ * http://www.v3vee.org  and
+ * http://xstack.sandia.gov/hobbes
+ *
+ * Copyright (c) 2017, Matt George <11georgem@gmail.com>
+ * Copyright (c) 2017, The V3VEE Project  <http://www.v3vee.org> 
+ *                     The Hobbes Project <http://xstack.sandia.gov/hobbes>
+ * All rights reserved.
+ *
+ * Authors:  Matt George <11georgem@gmail.com>
+ *
+ * This is free software.  You are permitted to use,
+ * redistribute, and modify it as specified in the file "LICENSE.txt".
+ */
+
+// This is a port of the Boehm garbage collector to
+// the Nautilus kernel
+
+
+/** Includes support code for the Boehm Garbage collector. */
+
+#include <nautilus/vc.h>
+#include <nautilus/backtrace.h>
+
+
+#ifndef __BDWGC_INTERNAL__
+#define __BDWGC_INTERNAL__
+
+
+#ifndef NAUT_CONFIG_DEBUG_BDWGC
+#define BDWGC_DEBUG(fmt, args...)
+#else
+#define BDWGC_DEBUG(fmt, args...) DEBUG_PRINT("bdwgc: gc_state=%p: " fmt, !get_cur_thread() ? 0 : get_cur_thread()->gc_state, ##args)
+#endif
+
+#define BDWGC_INFO(fmt, args...) INFO_PRINT("bdwgc: gc_state=%p: " fmt, !get_cur_thread() ? 0 : get_cur_thread()->gc_state, ##args)
+#define BDWGC_WARN(fmt, args...) WARN_PRINT("bdwgc: gc_state=%p: " fmt, !get_cur_thread() ? 0 : get_cur_thread()->gc_state, ##args)
+#define BDWGC_ERROR(fmt, args...) ERROR_PRINT("bdwgc: gc_state=%p: " fmt, !get_cur_thread() ? 0 : get_cur_thread()->gc_state, ##args)
+
+
+#define BDWGC_SPECIFIC_THREAD_STATE(t) ((bdwgc_thread_state*)(t->gc_state))
+#define BDWGC_THREAD_STATE() (BDWGC_SPECIFIC_THREAD_STATE(get_cur_thread()))
+
+#define BDWGC_SPECIFIC_STACK_BOTTOM(t) ((void*)((uint64_t)(t)->stack + (t)->stack_size - sizeof(uint64_t)))
+#define BDWGC_STACK_BOTTOM() (BDWGC_SPECIFIC_STACK_BOTTOM(get_cur_thread()))
+
+
+/* A generic pointer to which we can add        */
+/* byte displacements and which can be used     */
+/* for address comparisons.                     */
+typedef char * ptr_t;
+  
+/* This is used by GC_call_with_gc_active(), GC_push_all_stack_sections(). */
+struct GC_traced_stack_sect_s {
+  ptr_t saved_stack_ptr;
+  struct GC_traced_stack_sect_s *prev;
+};  
+
+/* Originally from gc_tiny_fl.h
+ * 
+ * Constants and data structures for "tiny" free lists.
+ * These are used for thread-local allocation or in-lined allocators.
+ * Each global free list also essentially starts with one of these.
+ * However, global free lists are known to the GC.  "Tiny" free lists
+ * are basically private to the client.  Their contents are viewed as
+ * "in use" and marked accordingly by the core of the GC.
+ *
+ * Note that inlined code might know about the layout of these and the constants
+ * involved.  Thus any change here may invalidate clients, and such changes should
+ * be avoided.  Hence we keep this as simple as possible.
+ */
+
+/*
+ * We always set GC_GRANULE_BYTES to twice the length of a pointer.
+ * This means that all allocation requests are rounded up to the next
+ * multiple of 16 on 64-bit architectures or 8 on 32-bit architectures.
+ * This appears to be a reasonable compromise between fragmentation overhead
+ * and space usage for mark bits (usually mark bytes).
+ * On many 64-bit architectures some memory references require 16-byte
+ * alignment, making this necessary anyway.
+ * For a few 32-bit architecture (e.g. x86), we may also need 16-byte alignment
+ * for certain memory references.  But currently that does not seem to be the
+ * default for all conventional malloc implementations, so we ignore that
+ * problem.
+ * It would always be safe, and often useful, to be able to allocate very
+ * small objects with smaller alignment.  But that would cost us mark bit
+ * space, so we no longer do so.
+ */
+#ifndef GC_GRANULE_BYTES
+/* GC_GRANULE_BYTES should not be overridden in any instances of the GC */
+/* library that may be shared between applications, since it affects	  */
+/* the binary interface to the library.				  */
+# if defined(__LP64__) || defined (_LP64) || defined(_WIN64)    \
+  || defined(__s390x__)                                         \
+  || (defined(__x86_64__) && !defined(__ILP32__))               \
+  || defined(__alpha__) || defined(__powerpc64__)               \
+  || defined(__arch64__)
+#  define GC_GRANULE_BYTES 16
+#  define GC_GRANULE_WORDS 2
+# else
+#  define GC_GRANULE_BYTES 8
+#  define GC_GRANULE_WORDS 2
+# endif
+#endif /* !GC_GRANULE_BYTES */
+
+#if GC_GRANULE_WORDS == 2
+#  define GC_WORDS_TO_GRANULES(n) ((n)>>1)
+#else
+#  define GC_WORDS_TO_GRANULES(n) ((n)*sizeof(void *)/GC_GRANULE_BYTES)
+#endif
+
+/* A "tiny" free list header contains TINY_FREELISTS pointers to 	*/
+/* singly linked lists of objects of different sizes, the ith one	*/
+/* containing objects i granules in size.  Note that there is a list	*/
+/* of size zero objects.						*/
+#ifndef GC_TINY_FREELISTS
+# if GC_GRANULE_BYTES == 16
+#   define GC_TINY_FREELISTS 25
+# else
+#   define GC_TINY_FREELISTS 33	/* Up to and including 256 bytes */
+# endif
+#endif /* !GC_TINY_FREELISTS */
+
+/* The ith free list corresponds to size i*GC_GRANULE_BYTES	*/
+/* Internally to the collector, the index can be computed with	*/
+/* ROUNDED_UP_GRANULES.  Externally, we don't know whether	*/
+/* DONT_ADD_BYTE_AT_END is set, but the client should know.	*/
+
+/* Convert a free list index to the actual size of objects	*/
+/* on that list, including extra space we added.  Not an	*/
+/* inverse of the above.					*/
+#define GC_RAW_BYTES_FROM_INDEX(i) ((i) * GC_GRANULE_BYTES)
+
+  
+/* Originally from thread_local_alloc.h
+ * 
+ * Free lists contain either a pointer or a small count       */
+/* reflecting the number of granules allocated at that        */
+/* size.                                                      */
+/* 0 ==> thread-local allocation in use, free list            */
+/*       empty.                                               */
+/* > 0, <= DIRECT_GRANULES ==> Using global allocation,       */
+/*       too few objects of this size have been               */
+/*       allocated by this thread.                            */
+/* >= HBLKSIZE  => pointer to nonempty free list.             */
+/* > DIRECT_GRANULES, < HBLKSIZE ==> transition to            */
+/*    local alloc, equivalent to 0.                           */
+typedef struct thread_local_freelists {
+  void * ptrfree_freelists[GC_TINY_FREELISTS];
+  void * normal_freelists[GC_TINY_FREELISTS];
+
+  /* Don't use local free lists for up to this much       */
+  /* allocation.                                          */
+  # define DIRECT_GRANULES (HBLKSIZE/GRANULE_BYTES)
+
+  //  # define DIRECT_GRANULES 99999999999
+  
+} *GC_tlfs;
+
+
+
+typedef unsigned int sem_t;
+struct start_info {
+    void *(*start_routine)(void *);
+    void *arg;
+    unsigned long flags;
+    sem_t registered;           /* 1 ==> in our thread table, but       */
+                                /* parent hasn't yet noticed.           */
+};
+
+
+typedef struct {
+    //Extra bookkeeping information the stopping code uses */
+    //struct thread_stop_info stop_info;
+    unsigned char flags;
+
+    /* Protected by GC lock.                */
+    /* Treated as a boolean value.  If set, */
+    /* thread will acquire GC lock before   */
+    /* doing any pointer manipulations, and */
+    /* has set its SP value.  Thus it does  */
+    /* not need to be sent a signal to stop */
+    /* it.                                  */
+    unsigned char thread_blocked;
+
+  /* Used by GC_check_finalizer_nested()  */
+  /* to minimize the level of recursion   */
+  /* when a client finalizer allocates    */
+  /* memory (initially both are 0).       */
+    unsigned short finalizer_skipped;
+    unsigned char finalizer_nested;
+
+  /* Points to the "frame" data held in stack by  */
+  /* the innermost GC_call_with_gc_active() of    */
+  /* this thread.  May be NULL.                   */
+  struct GC_traced_stack_sect_s *traced_stack_sect;
+
+  /* The value returned from the thread.  */
+  /* Used only to avoid premature         */
+  /* reclamation of any data it might     */
+  /* reference.                           */
+  /* This is unfortunately also the       */
+  /* reason we need to intercept join     */
+  /* and detach.                          */
+    void * gc_status;
+    struct thread_local_freelists tlfs;
+
+  //  struct thread_stop_info stop_info;
+  
+} bdwgc_thread_state ;
+
+bdwgc_thread_state * bdwgc_thread_state_init();
+
+
+#endif
diff --git a/src/gc/bdwgc/include/cord.h b/src/gc/bdwgc/include/cord.h
new file mode 100644
index 0000000..83973a4
--- /dev/null
+++ b/src/gc/bdwgc/include/cord.h
@@ -0,0 +1,326 @@
+/*
+ * Copyright (c) 1993-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Author: Hans-J. Boehm (boehm@parc.xerox.com)
+ */
+
+/*
+ * Cords are immutable character strings.  A number of operations
+ * on long cords are much more efficient than their strings.h counterpart.
+ * In particular, concatenation takes constant time independent of the length
+ * of the arguments.  (Cords are represented as trees, with internal
+ * nodes representing concatenation and leaves consisting of either C
+ * strings or a functional description of the string.)
+ *
+ * The following are reasonable applications of cords.  They would perform
+ * unacceptably if C strings were used:
+ * - A compiler that produces assembly language output by repeatedly
+ *   concatenating instructions onto a cord representing the output file.
+ * - A text editor that converts the input file to a cord, and then
+ *   performs editing operations by producing a new cord representing
+ *   the file after each character change (and keeping the old ones in an
+ *   edit history)
+ *
+ * For optimal performance, cords should be built by
+ * concatenating short sections.
+ * This interface is designed for maximum compatibility with C strings.
+ * ASCII NUL characters may be embedded in cords using CORD_from_fn.
+ * This is handled correctly, but CORD_to_char_star will produce a string
+ * with embedded NULs when given such a cord.
+ *
+ * This interface is fairly big, largely for performance reasons.
+ * The most basic constants and functions:
+ *
+ * CORD - the type of a cord;
+ * CORD_EMPTY - empty cord;
+ * CORD_len(cord) - length of a cord;
+ * CORD_cat(cord1,cord2) - concatenation of two cords;
+ * CORD_substr(cord, start, len) - substring (or subcord);
+ * CORD_pos i;  CORD_FOR(i, cord) {  ... CORD_pos_fetch(i) ... } -
+ *    examine each character in a cord.  CORD_pos_fetch(i) is the char.
+ * CORD_fetch(int i) - Retrieve i'th character (slowly).
+ * CORD_cmp(cord1, cord2) - compare two cords.
+ * CORD_from_file(FILE * f) - turn a read-only file into a cord.
+ * CORD_to_char_star(cord) - convert to C string.
+ *   (Non-NULL C constant strings are cords.)
+ * CORD_printf (etc.) - cord version of printf. Use %r for cords.
+ */
+#ifndef CORD_H
+#define CORD_H
+
+#include <stddef.h>
+#include <stdio.h>
+/* Cords have type const char *.  This is cheating quite a bit, and not */
+/* 100% portable.  But it means that nonempty character string          */
+/* constants may be used as cords directly, provided the string is      */
+/* never modified in place.  The empty cord is represented by, and      */
+/* can be written as, 0.                                                */
+
+typedef const char * CORD;
+
+/* An empty cord is always represented as nil   */
+#define CORD_EMPTY 0
+
+/* Is a nonempty cord represented as a C string? */
+#define CORD_IS_STRING(s) (*(s) != '\0')
+
+/* Concatenate two cords.  If the arguments are C strings, they may     */
+/* not be subsequently altered.                                         */
+CORD CORD_cat(CORD x, CORD y);
+
+/* Concatenate a cord and a C string with known length.  Except for the */
+/* empty string case, this is a special case of CORD_cat.  Since the    */
+/* length is known, it can be faster.                                   */
+/* The string y is shared with the resulting CORD.  Hence it should     */
+/* not be altered by the caller.                                        */
+CORD CORD_cat_char_star(CORD x, const char * y, size_t leny);
+
+/* Compute the length of a cord */
+size_t CORD_len(CORD x);
+
+/* Cords may be represented by functions defining the ith character */
+typedef char (* CORD_fn)(size_t i, void * client_data);
+
+/* Turn a functional description into a cord.   */
+CORD CORD_from_fn(CORD_fn fn, void * client_data, size_t len);
+
+/* Return the substring (subcord really) of x with length at most n,    */
+/* starting at position i.  (The initial character has position 0.)     */
+CORD CORD_substr(CORD x, size_t i, size_t n);
+
+/* Return the argument, but rebalanced to allow more efficient          */
+/* character retrieval, substring operations, and comparisons.          */
+/* This is useful only for cords that were built using repeated         */
+/* concatenation.  Guarantees log time access to the result, unless     */
+/* x was obtained through a large number of repeated substring ops      */
+/* or the embedded functional descriptions take longer to evaluate.     */
+/* May reallocate significant parts of the cord.  The argument is not   */
+/* modified; only the result is balanced.                               */
+CORD CORD_balance(CORD x);
+
+/* The following traverse a cord by applying a function to each         */
+/* character.  This is occasionally appropriate, especially where       */
+/* speed is crucial.  But, since C doesn't have nested functions,       */
+/* clients of this sort of traversal are clumsy to write.  Consider     */
+/* the functions that operate on cord positions instead.                */
+
+/* Function to iteratively apply to individual characters in cord.      */
+typedef int (* CORD_iter_fn)(char c, void * client_data);
+
+/* Function to apply to substrings of a cord.  Each substring is a      */
+/* a C character string, not a general cord.                            */
+typedef int (* CORD_batched_iter_fn)(const char * s, void * client_data);
+#define CORD_NO_FN ((CORD_batched_iter_fn)0)
+
+/* Apply f1 to each character in the cord, in ascending order,          */
+/* starting at position i. If                                           */
+/* f2 is not CORD_NO_FN, then multiple calls to f1 may be replaced by   */
+/* a single call to f2.  The parameter f2 is provided only to allow     */
+/* some optimization by the client.  This terminates when the right     */
+/* end of this string is reached, or when f1 or f2 return != 0.  In the */
+/* latter case CORD_iter returns != 0.  Otherwise it returns 0.         */
+/* The specified value of i must be < CORD_len(x).                      */
+int CORD_iter5(CORD x, size_t i, CORD_iter_fn f1,
+               CORD_batched_iter_fn f2, void * client_data);
+
+/* A simpler version that starts at 0, and without f2:  */
+int CORD_iter(CORD x, CORD_iter_fn f1, void * client_data);
+#define CORD_iter(x, f1, cd) CORD_iter5(x, 0, f1, CORD_NO_FN, cd)
+
+/* Similar to CORD_iter5, but end-to-beginning. No provisions for       */
+/* CORD_batched_iter_fn.                                                */
+int CORD_riter4(CORD x, size_t i, CORD_iter_fn f1, void * client_data);
+
+/* A simpler version that starts at the end:    */
+int CORD_riter(CORD x, CORD_iter_fn f1, void * client_data);
+
+/* Functions that operate on cord positions.  The easy way to traverse  */
+/* cords.  A cord position is logically a pair consisting of a cord     */
+/* and an index into that cord.  But it is much faster to retrieve a    */
+/* character based on a position than on an index.  Unfortunately,      */
+/* positions are big (order of a few 100 bytes), so allocate them with  */
+/* caution.                                                             */
+/* Things in cord_pos.h should be treated as opaque, except as          */
+/* described below.  Also note that                                     */
+/* CORD_pos_fetch, CORD_next and CORD_prev have both macro and function */
+/* definitions.  The former may evaluate their argument more than once. */
+#include "private/cord_pos.h"
+
+/*
+        Visible definitions from above:
+
+        typedef <OPAQUE but fairly big> CORD_pos[1];
+
+        * Extract the cord from a position:
+        CORD CORD_pos_to_cord(CORD_pos p);
+
+        * Extract the current index from a position:
+        size_t CORD_pos_to_index(CORD_pos p);
+
+        * Fetch the character located at the given position:
+        char CORD_pos_fetch(CORD_pos p);
+
+        * Initialize the position to refer to the given cord and index.
+        * Note that this is the most expensive function on positions:
+        void CORD_set_pos(CORD_pos p, CORD x, size_t i);
+
+        * Advance the position to the next character.
+        * P must be initialized and valid.
+        * Invalidates p if past end:
+        void CORD_next(CORD_pos p);
+
+        * Move the position to the preceding character.
+        * P must be initialized and valid.
+        * Invalidates p if past beginning:
+        void CORD_prev(CORD_pos p);
+
+        * Is the position valid, i.e. inside the cord?
+        int CORD_pos_valid(CORD_pos p);
+*/
+#define CORD_FOR(pos, cord) \
+    for (CORD_set_pos(pos, cord, 0); CORD_pos_valid(pos); CORD_next(pos))
+
+
+/* An out of memory handler to call.  May be supplied by client.        */
+/* Must not return.                                                     */
+extern void (* CORD_oom_fn)(void);
+
+/* Dump the representation of x to stdout in an implementation defined  */
+/* manner.  Intended for debugging only.                                */
+void CORD_dump(CORD x);
+
+/* The following could easily be implemented by the client.  They are   */
+/* provided in cordxtra.c for convenience.                              */
+
+/* Concatenate a character to the end of a cord.        */
+CORD CORD_cat_char(CORD x, char c);
+
+/* Concatenate n cords. */
+CORD CORD_catn(int n, /* CORD */ ...);
+
+/* Return the character in CORD_substr(x, i, 1)         */
+char CORD_fetch(CORD x, size_t i);
+
+/* Return < 0, 0, or > 0, depending on whether x < y, x = y, x > y      */
+int CORD_cmp(CORD x, CORD y);
+
+/* A generalization that takes both starting positions for the          */
+/* comparison, and a limit on the number of characters to be compared.  */
+int CORD_ncmp(CORD x, size_t x_start, CORD y, size_t y_start, size_t len);
+
+/* Find the first occurrence of s in x at position start or later.      */
+/* Return the position of the first character of s in x, or             */
+/* CORD_NOT_FOUND if there is none.                                     */
+size_t CORD_str(CORD x, size_t start, CORD s);
+
+/* Return a cord consisting of i copies of (possibly NUL) c.  Dangerous */
+/* in conjunction with CORD_to_char_star.                               */
+/* The resulting representation takes constant space, independent of i. */
+CORD CORD_chars(char c, size_t i);
+#define CORD_nul(i) CORD_chars('\0', (i))
+
+/* Turn a file into cord.  The file must be seekable.  Its contents     */
+/* must remain constant.  The file may be accessed as an immediate      */
+/* result of this call and/or as a result of subsequent accesses to     */
+/* the cord.  Short files are likely to be immediately read, but        */
+/* long files are likely to be read on demand, possibly relying on      */
+/* stdio for buffering.                                                 */
+/* We must have exclusive access to the descriptor f, i.e. we may       */
+/* read it at any time, and expect the file pointer to be               */
+/* where we left it.  Normally this should be invoked as                */
+/* CORD_from_file(fopen(...))                                           */
+/* CORD_from_file arranges to close the file descriptor when it is no   */
+/* longer needed (e.g. when the result becomes inaccessible).           */
+/* The file f must be such that ftell reflects the actual character     */
+/* position in the file, i.e. the number of characters that can be      */
+/* or were read with fread.  On UNIX systems this is always true.  On   */
+/* MS Windows systems, f must be opened in binary mode.                 */
+CORD CORD_from_file(FILE * f);
+
+/* Equivalent to the above, except that the entire file will be read    */
+/* and the file pointer will be closed immediately.                     */
+/* The binary mode restriction from above does not apply.               */
+CORD CORD_from_file_eager(FILE * f);
+
+/* Equivalent to the above, except that the file will be read on demand.*/
+/* The binary mode restriction applies.                                 */
+CORD CORD_from_file_lazy(FILE * f);
+
+/* Turn a cord into a C string. The result shares no structure with     */
+/* x, and is thus modifiable.                                           */
+char * CORD_to_char_star(CORD x);
+
+/* Turn a C string into a CORD.  The C string is copied, and so may     */
+/* subsequently be modified.                                            */
+CORD CORD_from_char_star(const char *s);
+
+/* Identical to the above, but the result may share structure with      */
+/* the argument and is thus not modifiable.                             */
+const char * CORD_to_const_char_star(CORD x);
+
+/* Write a cord to a file, starting at the current position.  No        */
+/* trailing NULs are newlines are added.                                */
+/* Returns EOF if a write error occurs, 1 otherwise.                    */
+int CORD_put(CORD x, FILE * f);
+
+/* "Not found" result for the following two functions.                  */
+#define CORD_NOT_FOUND ((size_t)(-1))
+
+/* A vague analog of strchr.  Returns the position (an integer, not     */
+/* a pointer) of the first occurrence of (char) c inside x at position  */
+/* i or later. The value i must be < CORD_len(x).                       */
+size_t CORD_chr(CORD x, size_t i, int c);
+
+/* A vague analog of strrchr.  Returns index of the last occurrence     */
+/* of (char) c inside x at position i or earlier. The value i           */
+/* must be < CORD_len(x).                                               */
+size_t CORD_rchr(CORD x, size_t i, int c);
+
+
+/* The following are also not primitive, but are implemented in         */
+/* cordprnt.c.  They provide functionality similar to the ANSI C        */
+/* functions with corresponding names, but with the following           */
+/* additions and changes:                                               */
+/* 1. A %r conversion specification specifies a CORD argument.  Field   */
+/*    width, precision, etc. have the same semantics as for %s.         */
+/*    (Note that %c, %C, and %S were already taken.)                    */
+/* 2. The format string is represented as a CORD.                       */
+/* 3. CORD_sprintf and CORD_vsprintf assign the result through the 1st  */      /*    argument. Unlike their ANSI C versions, there is no need to guess */
+/*    the correct buffer size.                                          */
+/* 4. Most of the conversions are implement through the native          */
+/*    vsprintf.  Hence they are usually no faster, and                  */
+/*    idiosyncrasies of the native printf are preserved.  However,      */
+/*    CORD arguments to CORD_sprintf and CORD_vsprintf are NOT copied;  */
+/*    the result shares the original structure.  This may make them     */
+/*    very efficient in some unusual applications.                      */
+/*    The format string is copied.                                      */
+/* All functions return the number of characters generated or -1 on     */
+/* error.  This complies with the ANSI standard, but is inconsistent    */
+/* with some older implementations of sprintf.                          */
+
+/* The implementation of these is probably less portable than the rest  */
+/* of this package.                                                     */
+
+#ifndef CORD_NO_IO
+
+#include <stdarg.h>
+
+int CORD_sprintf(CORD * out, CORD format, ...);
+int CORD_vsprintf(CORD * out, CORD format, va_list args);
+int CORD_fprintf(FILE * f, CORD format, ...);
+int CORD_vfprintf(FILE * f, CORD format, va_list args);
+int CORD_printf(CORD format, ...);
+int CORD_vprintf(CORD format, va_list args);
+
+#endif /* CORD_NO_IO */
+
+#endif /* CORD_H */
diff --git a/src/gc/bdwgc/include/ec.h b/src/gc/bdwgc/include/ec.h
new file mode 100644
index 0000000..c829b83
--- /dev/null
+++ b/src/gc/bdwgc/include/ec.h
@@ -0,0 +1,70 @@
+# ifndef EC_H
+# define EC_H
+
+# ifndef CORD_H
+#  include "cord.h"
+# endif
+
+/* Extensible cords are strings that may be destructively appended to.	*/
+/* They allow fast construction of cords from characters that are	*/
+/* being read from a stream.						*/
+/*
+ * A client might look like:
+ *
+ *	{
+ *	    CORD_ec x;
+ *	    CORD result;
+ *	    char c;
+ *	    FILE *f;
+ *
+ *	    ...
+ *	    CORD_ec_init(x);
+ *	    while(...) {
+ *		c = getc(f);
+ *		...
+ *		CORD_ec_append(x, c);
+ *	    }
+ *	    result = CORD_balance(CORD_ec_to_cord(x));
+ *
+ * If a C string is desired as the final result, the call to CORD_balance
+ * may be replaced by a call to CORD_to_char_star.
+ */
+
+# ifndef CORD_BUFSZ
+#   define CORD_BUFSZ 128
+# endif
+
+typedef struct CORD_ec_struct {
+    CORD ec_cord;
+    char * ec_bufptr;
+    char ec_buf[CORD_BUFSZ+1];
+} CORD_ec[1];
+
+/* This structure represents the concatenation of ec_cord with		*/
+/* ec_buf[0 ... (ec_bufptr-ec_buf-1)]					*/
+
+/* Flush the buffer part of the extended chord into ec_cord.	*/
+/* Note that this is almost the only real function, and it is	*/
+/* implemented in 6 lines in cordxtra.c				*/
+void CORD_ec_flush_buf(CORD_ec x);
+      
+/* Convert an extensible cord to a cord. */
+# define CORD_ec_to_cord(x) (CORD_ec_flush_buf(x), (x)[0].ec_cord)
+
+/* Initialize an extensible cord. */
+# define CORD_ec_init(x) ((x)[0].ec_cord = 0, (x)[0].ec_bufptr = (x)[0].ec_buf)
+
+/* Append a character to an extensible cord.	*/
+# define CORD_ec_append(x, c) \
+    {  \
+	if ((x)[0].ec_bufptr == (x)[0].ec_buf + CORD_BUFSZ) { \
+	  	CORD_ec_flush_buf(x); \
+	} \
+	*((x)[0].ec_bufptr)++ = (c); \
+    }
+
+/* Append a cord to an extensible cord.  Structure remains shared with 	*/
+/* original.								*/
+void CORD_ec_append_cord(CORD_ec x, CORD s);
+
+# endif /* EC_H */
diff --git a/src/gc/bdwgc/include/extra/gc.h b/src/gc/bdwgc/include/extra/gc.h
new file mode 100644
index 0000000..55ae4c6
--- /dev/null
+++ b/src/gc/bdwgc/include/extra/gc.h
@@ -0,0 +1,2 @@
+/* This file is installed for backward compatibility. */
+#include <gc/gc.h>
diff --git a/src/gc/bdwgc/include/extra/gc_cpp.h b/src/gc/bdwgc/include/extra/gc_cpp.h
new file mode 100644
index 0000000..36669f9
--- /dev/null
+++ b/src/gc/bdwgc/include/extra/gc_cpp.h
@@ -0,0 +1,2 @@
+/* This file is installed for backward compatibility. */
+#include <gc/gc_cpp.h>
diff --git a/src/gc/bdwgc/include/gc.h b/src/gc/bdwgc/include/gc.h
new file mode 100644
index 0000000..6cd2959
--- /dev/null
+++ b/src/gc/bdwgc/include/gc.h
@@ -0,0 +1,1513 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright 1999 by Hewlett-Packard Company.  All rights reserved.
+ * Copyright (C) 2007 Free Software Foundation, Inc
+ * Copyright (c) 2000-2011 by Hewlett-Packard Development Company.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/*
+ * Note that this defines a large number of tuning hooks, which can
+ * safely be ignored in nearly all cases.  For normal use it suffices
+ * to call only GC_MALLOC and perhaps GC_REALLOC.
+ * For better performance, also look at GC_MALLOC_ATOMIC, and
+ * GC_enable_incremental.  If you need an action to be performed
+ * immediately before an object is collected, look at GC_register_finalizer.
+ * If you are using Solaris threads, look at the end of this file.
+ * Everything else is best ignored unless you encounter performance
+ * problems.
+ */
+
+#ifndef GC_H
+#define GC_H
+
+#include "gc_version.h"
+        /* Define version numbers here to allow test on build machine   */
+        /* for cross-builds.  Note that this defines the header         */
+        /* version number, which may or may not match that of the       */
+        /* dynamic library.  GC_get_version() can be used to obtain     */
+        /* the latter.                                                  */
+
+#include "gc_config_macros.h"
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+typedef void * GC_PTR;  /* preserved only for backward compatibility    */
+
+/* Define word and signed_word to be unsigned and signed types of the   */
+/* size as char * or void *.  There seems to be no way to do this       */
+/* even semi-portably.  The following is probably no better/worse       */
+/* than almost anything else.                                           */
+/* The ANSI standard suggests that size_t and ptrdiff_t might be        */
+/* better choices.  But those had incorrect definitions on some older   */
+/* systems.  Notably "typedef int size_t" is WRONG.                     */
+#ifdef _WIN64
+# ifdef __int64
+    typedef unsigned __int64 GC_word;
+    typedef __int64 GC_signed_word;
+# else
+    typedef unsigned long long GC_word;
+    typedef long long GC_signed_word;
+# endif
+#else
+  typedef unsigned long GC_word;
+  typedef long GC_signed_word;
+#endif
+
+/* Get the GC library version. The returned value is a constant in the  */
+/* form: ((version_major<<16) | (version_minor<<8) | alpha_version).    */
+GC_API unsigned GC_CALL GC_get_version(void);
+
+/* Public read-only variables */
+/* The supplied getter functions are preferred for new code.            */
+
+GC_API GC_word GC_gc_no;/* Counter incremented per collection.          */
+                        /* Includes empty GCs at startup.               */
+GC_API GC_word GC_CALL GC_get_gc_no(void);
+                        /* GC_get_gc_no() is unsynchronized, so         */
+                        /* it requires GC_call_with_alloc_lock() to     */
+                        /* avoid data races on multiprocessors.         */
+
+#ifdef NAUT
+#define GC_THREADS
+#endif
+    
+#ifdef GC_THREADS
+  GC_API int GC_parallel;
+                        /* GC is parallelized for performance on        */
+                        /* multiprocessors.  Currently set only         */
+                        /* implicitly if collector is built with        */
+                        /* PARALLEL_MARK defined and if either:         */
+                        /*  Env variable GC_NPROC is set to > 1, or     */
+                        /*  GC_NPROC is not set and this is an MP.      */
+                        /* If GC_parallel is set, incremental           */
+                        /* collection is only partially functional,     */
+                        /* and may not be desirable. This getter does   */
+                        /* not use or need synchronization (i.e.        */
+                        /* acquiring the GC lock).                      */
+  GC_API int GC_CALL GC_get_parallel(void);
+#endif
+
+
+/* Public R/W variables */
+/* The supplied setter and getter functions are preferred for new code. */
+
+typedef void * (GC_CALLBACK * GC_oom_func)(size_t /* bytes_requested */);
+GC_API GC_oom_func GC_oom_fn;
+                        /* When there is insufficient memory to satisfy */
+                        /* an allocation request, we return             */
+                        /* (*GC_oom_fn)(size).  By default this just    */
+                        /* returns NULL.                                */
+                        /* If it returns, it must return 0 or a valid   */
+                        /* pointer to a previously allocated heap       */
+                        /* object.  GC_oom_fn must not be 0.            */
+                        /* Both the supplied setter and the getter      */
+                        /* acquire the GC lock (to avoid data races).   */
+GC_API void GC_CALL GC_set_oom_fn(GC_oom_func);
+GC_API GC_oom_func GC_CALL GC_get_oom_fn(void);
+
+GC_API int GC_find_leak;
+                        /* Do not actually garbage collect, but simply  */
+                        /* report inaccessible memory that was not      */
+                        /* deallocated with GC_free.  Initial value     */
+                        /* is determined by FIND_LEAK macro.            */
+                        /* The value should not typically be modified   */
+                        /* after GC initialization (and, thus, it does  */
+                        /* not use or need synchronization).            */
+GC_API void GC_CALL GC_set_find_leak(int);
+GC_API int GC_CALL GC_get_find_leak(void);
+
+GC_API int GC_all_interior_pointers;
+                        /* Arrange for pointers to object interiors to  */
+                        /* be recognized as valid.  Typically should    */
+                        /* not be changed after GC initialization (in   */
+                        /* case of calling it after the GC is           */
+                        /* initialized, the setter acquires the GC lock */
+                        /* (to avoid data races).  The initial value    */
+                        /* depends on whether the GC is built with      */
+                        /* ALL_INTERIOR_POINTERS macro defined or not.  */
+                        /* Unless DONT_ADD_BYTE_AT_END is defined, this */
+                        /* also affects whether sizes are increased by  */
+                        /* at least a byte to allow "off the end"       */
+                        /* pointer recognition.  Must be only 0 or 1.   */
+GC_API void GC_CALL GC_set_all_interior_pointers(int);
+GC_API int GC_CALL GC_get_all_interior_pointers(void);
+
+GC_API int GC_finalize_on_demand;
+                        /* If nonzero, finalizers will only be run in   */
+                        /* response to an explicit GC_invoke_finalizers */
+                        /* call.  The default is determined by whether  */
+                        /* the FINALIZE_ON_DEMAND macro is defined      */
+                        /* when the collector is built.                 */
+                        /* The setter and getter are unsynchronized.    */
+GC_API void GC_CALL GC_set_finalize_on_demand(int);
+GC_API int GC_CALL GC_get_finalize_on_demand(void);
+
+GC_API int GC_java_finalization;
+                        /* Mark objects reachable from finalizable      */
+                        /* objects in a separate post-pass.  This makes */
+                        /* it a bit safer to use non-topologically-     */
+                        /* ordered finalization.  Default value is      */
+                        /* determined by JAVA_FINALIZATION macro.       */
+                        /* Enables register_finalizer_unreachable to    */
+                        /* work correctly.                              */
+                        /* The setter and getter are unsynchronized.    */
+GC_API void GC_CALL GC_set_java_finalization(int);
+GC_API int GC_CALL GC_get_java_finalization(void);
+
+typedef void (GC_CALLBACK * GC_finalizer_notifier_proc)(void);
+GC_API GC_finalizer_notifier_proc GC_finalizer_notifier;
+                        /* Invoked by the collector when there are      */
+                        /* objects to be finalized.  Invoked at most    */
+                        /* once per GC cycle.  Never invoked unless     */
+                        /* GC_finalize_on_demand is set.                */
+                        /* Typically this will notify a finalization    */
+                        /* thread, which will call GC_invoke_finalizers */
+                        /* in response.  May be 0 (means no notifier).  */
+                        /* Both the supplied setter and the getter      */
+                        /* acquire the GC lock (to avoid data races).   */
+GC_API void GC_CALL GC_set_finalizer_notifier(GC_finalizer_notifier_proc);
+GC_API GC_finalizer_notifier_proc GC_CALL GC_get_finalizer_notifier(void);
+
+GC_API int GC_dont_gc;  /* != 0 ==> Don't collect.  In versions 6.2a1+, */
+                        /* this overrides explicit GC_gcollect() calls. */
+                        /* Used as a counter, so that nested enabling   */
+                        /* and disabling work correctly.  Should        */
+                        /* normally be updated with GC_enable() and     */
+                        /* GC_disable() calls.  Direct assignment to    */
+                        /* GC_dont_gc is deprecated.  To check whether  */
+                        /* GC is disabled, GC_is_disabled() is          */
+                        /* preferred for new code.                      */
+
+GC_API int GC_dont_expand;
+                        /* Don't expand the heap unless explicitly      */
+                        /* requested or forced to.  The setter and      */
+                        /* getter are unsynchronized.                   */
+GC_API void GC_CALL GC_set_dont_expand(int);
+GC_API int GC_CALL GC_get_dont_expand(void);
+
+GC_API int GC_use_entire_heap;
+                /* Causes the non-incremental collector to use the      */
+                /* entire heap before collecting.  This was the only    */
+                /* option for GC versions < 5.0.  This sometimes        */
+                /* results in more large block fragmentation, since     */
+                /* very large blocks will tend to get broken up         */
+                /* during each GC cycle.  It is likely to result in a   */
+                /* larger working set, but lower collection             */
+                /* frequencies, and hence fewer instructions executed   */
+                /* in the collector.                                    */
+
+GC_API int GC_full_freq;    /* Number of partial collections between    */
+                            /* full collections.  Matters only if       */
+                            /* GC_incremental is set.                   */
+                            /* Full collections are also triggered if   */
+                            /* the collector detects a substantial      */
+                            /* increase in the number of in-use heap    */
+                            /* blocks.  Values in the tens are now      */
+                            /* perfectly reasonable, unlike for         */
+                            /* earlier GC versions.                     */
+                        /* The setter and getter are unsynchronized, so */
+                        /* GC_call_with_alloc_lock() is required to     */
+                        /* avoid data races (if the value is modified   */
+                        /* after the GC is put to multi-threaded mode). */
+GC_API void GC_CALL GC_set_full_freq(int);
+GC_API int GC_CALL GC_get_full_freq(void);
+
+GC_API GC_word GC_non_gc_bytes;
+                        /* Bytes not considered candidates for          */
+                        /* collection.  Used only to control scheduling */
+                        /* of collections.  Updated by                  */
+                        /* GC_malloc_uncollectable and GC_free.         */
+                        /* Wizards only.                                */
+                        /* The setter and getter are unsynchronized, so */
+                        /* GC_call_with_alloc_lock() is required to     */
+                        /* avoid data races (if the value is modified   */
+                        /* after the GC is put to multi-threaded mode). */
+GC_API void GC_CALL GC_set_non_gc_bytes(GC_word);
+GC_API GC_word GC_CALL GC_get_non_gc_bytes(void);
+
+GC_API int GC_no_dls;
+                        /* Don't register dynamic library data segments. */
+                        /* Wizards only.  Should be used only if the     */
+                        /* application explicitly registers all roots.   */
+                        /* (In some environments like Microsoft Windows  */
+                        /* and Apple's Darwin, this may also prevent     */
+                        /* registration of the main data segment as part */
+                        /* of the root set.)                             */
+                        /* The setter and getter are unsynchronized.     */
+GC_API void GC_CALL GC_set_no_dls(int);
+GC_API int GC_CALL GC_get_no_dls(void);
+
+GC_API GC_word GC_free_space_divisor;
+                        /* We try to make sure that we allocate at      */
+                        /* least N/GC_free_space_divisor bytes between  */
+                        /* collections, where N is twice the number     */
+                        /* of traced bytes, plus the number of untraced */
+                        /* bytes (bytes in "atomic" objects), plus      */
+                        /* a rough estimate of the root set size.       */
+                        /* N approximates GC tracing work per GC.       */
+                        /* Initially, GC_free_space_divisor = 3.        */
+                        /* Increasing its value will use less space     */
+                        /* but more collection time.  Decreasing it     */
+                        /* will appreciably decrease collection time    */
+                        /* at the expense of space.                     */
+                        /* The setter and getter are unsynchronized, so */
+                        /* GC_call_with_alloc_lock() is required to     */
+                        /* avoid data races (if the value is modified   */
+                        /* after the GC is put to multi-threaded mode). */
+GC_API void GC_CALL GC_set_free_space_divisor(GC_word);
+GC_API GC_word GC_CALL GC_get_free_space_divisor(void);
+
+GC_API GC_word GC_max_retries;
+                        /* The maximum number of GCs attempted before   */
+                        /* reporting out of memory after heap           */
+                        /* expansion fails.  Initially 0.               */
+                        /* The setter and getter are unsynchronized, so */
+                        /* GC_call_with_alloc_lock() is required to     */
+                        /* avoid data races (if the value is modified   */
+                        /* after the GC is put to multi-threaded mode). */
+GC_API void GC_CALL GC_set_max_retries(GC_word);
+GC_API GC_word GC_CALL GC_get_max_retries(void);
+
+
+GC_API char *GC_stackbottom;    /* Cool end of user stack.              */
+                                /* May be set in the client prior to    */
+                                /* calling any GC_ routines.  This      */
+                                /* avoids some overhead, and            */
+                                /* potentially some signals that can    */
+                                /* confuse debuggers.  Otherwise the    */
+                                /* collector attempts to set it         */
+                                /* automatically.                       */
+                                /* For multi-threaded code, this is the */
+                                /* cold end of the stack for the        */
+                                /* primordial thread.  Portable clients */
+                                /* should use GC_get_stack_base(),      */
+                                /* GC_call_with_gc_active() and         */
+                                /* GC_register_my_thread() instead.     */
+
+GC_API int GC_dont_precollect;  /* Don't collect as part of GC          */
+                                /* initialization.  Should be set only  */
+                                /* if the client wants a chance to      */
+                                /* manually initialize the root set     */
+                                /* before the first collection.         */
+                                /* Interferes with blacklisting.        */
+                                /* Wizards only.  The setter and getter */
+                                /* are unsynchronized (and no external  */
+                                /* locking is needed since the value is */
+                                /* accessed at GC initialization only). */
+GC_API void GC_CALL GC_set_dont_precollect(int);
+GC_API int GC_CALL GC_get_dont_precollect(void);
+
+GC_API unsigned long GC_time_limit;
+                               /* If incremental collection is enabled, */
+                               /* We try to terminate collections       */
+                               /* after this many milliseconds.  Not a  */
+                               /* hard time bound.  Setting this to     */
+                               /* GC_TIME_UNLIMITED will essentially    */
+                               /* disable incremental collection while  */
+                               /* leaving generational collection       */
+                               /* enabled.                              */
+#define GC_TIME_UNLIMITED 999999
+                               /* Setting GC_time_limit to this value   */
+                               /* will disable the "pause time exceeded"*/
+                               /* tests.                                */
+                        /* The setter and getter are unsynchronized, so */
+                        /* GC_call_with_alloc_lock() is required to     */
+                        /* avoid data races (if the value is modified   */
+                        /* after the GC is put to multi-threaded mode). */
+GC_API void GC_CALL GC_set_time_limit(unsigned long);
+GC_API unsigned long GC_CALL GC_get_time_limit(void);
+
+/* Public procedures */
+
+/* Set whether the GC will allocate executable memory pages or not.     */
+/* A non-zero argument instructs the collector to allocate memory with  */
+/* the executable flag on.  Must be called before the collector is      */
+/* initialized.  May have no effect on some platforms.  The default     */
+/* value is controlled by NO_EXECUTE_PERMISSION macro (if present then  */
+/* the flag is off).  Portable clients should have                      */
+/* GC_set_pages_executable(1) call (before GC_INIT) provided they are   */
+/* going to execute code on any of the GC-allocated memory objects.     */
+GC_API void GC_CALL GC_set_pages_executable(int);
+
+/* Returns non-zero value if the GC is set to the allocate-executable   */
+/* mode.  The mode could be changed by GC_set_pages_executable (before  */
+/* GC_INIT) unless the former has no effect on the platform.  Does not  */
+/* use or need synchronization (i.e. acquiring the allocator lock).     */
+GC_API int GC_CALL GC_get_pages_executable(void);
+
+/* Overrides the default handle-fork mode.  Non-zero value means GC     */
+/* should install proper pthread_atfork handlers.  Has effect only if   */
+/* called before GC_INIT.  Clients should invoke GC_set_handle_fork(1)  */
+/* only if going to use fork with GC functions called in the forked     */
+/* child.  (Note that such client and atfork handlers activities are    */
+/* not fully POSIX-compliant.)                                          */
+GC_API void GC_CALL GC_set_handle_fork(int);
+
+/* Initialize the collector.  Portable clients should call GC_INIT()    */
+/* from the main program instead.                                       */
+GC_API void GC_CALL GC_init(void);
+
+/* General purpose allocation routines, with roughly malloc calling     */
+/* conv.  The atomic versions promise that no relevant pointers are     */
+/* contained in the object.  The non-atomic versions guarantee that the */
+/* new object is cleared.  GC_malloc_stubborn promises that no changes  */
+/* to the object will occur after GC_end_stubborn_change has been       */
+/* called on the result of GC_malloc_stubborn.  GC_malloc_uncollectable */
+/* allocates an object that is scanned for pointers to collectible      */
+/* objects, but is not itself collectible.  The object is scanned even  */
+/* if it does not appear to be reachable.  GC_malloc_uncollectable and  */
+/* GC_free called on the resulting object implicitly update             */
+/* GC_non_gc_bytes appropriately.                                       */
+/* Note that the GC_malloc_stubborn support doesn't really exist        */
+/* anymore.  MANUAL_VDB provides comparable functionality.              */
+GC_API void * GC_CALL GC_malloc(size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_malloc_atomic(size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API char * GC_CALL GC_strdup(const char *) GC_ATTR_MALLOC;
+GC_API char * GC_CALL GC_strndup(const char *, size_t) GC_ATTR_MALLOC;
+GC_API void * GC_CALL GC_malloc_uncollectable(size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_malloc_stubborn(size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+
+/* GC_memalign() is not well tested.                                    */
+GC_API void * GC_CALL GC_memalign(size_t /* align */, size_t /* lb */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(2);
+GC_API int GC_CALL GC_posix_memalign(void ** /* memptr */, size_t /* align */,
+                        size_t /* lb */);
+
+/* Explicitly deallocate an object.  Dangerous if used incorrectly.     */
+/* Requires a pointer to the base of an object.                         */
+/* If the argument is stubborn, it should not be changeable when freed. */
+/* An object should not be enabled for finalization when it is          */
+/* explicitly deallocated.                                              */
+/* GC_free(0) is a no-op, as required by ANSI C for free.               */
+GC_API void GC_CALL GC_free(void *);
+
+/* Stubborn objects may be changed only if the collector is explicitly  */
+/* informed.  The collector is implicitly informed of coming change     */
+/* when such an object is first allocated.  The following routines      */
+/* inform the collector that an object will no longer be changed, or    */
+/* that it will once again be changed.  Only non-NULL pointer stores    */
+/* into the object are considered to be changes.  The argument to       */
+/* GC_end_stubborn_change must be exactly the value returned by         */
+/* GC_malloc_stubborn or passed to GC_change_stubborn.  (In the second  */
+/* case, it may be an interior pointer within 512 bytes of the          */
+/* beginning of the objects.)  There is a performance penalty for       */
+/* allowing more than one stubborn object to be changed at once, but it */
+/* is acceptable to do so.  The same applies to dropping stubborn       */
+/* objects that are still changeable.                                   */
+GC_API void GC_CALL GC_change_stubborn(void *);
+GC_API void GC_CALL GC_end_stubborn_change(void *);
+
+/* Return a pointer to the base (lowest address) of an object given     */
+/* a pointer to a location within the object.                           */
+/* I.e., map an interior pointer to the corresponding base pointer.     */
+/* Note that with debugging allocation, this returns a pointer to the   */
+/* actual base of the object, i.e. the debug information, not to        */
+/* the base of the user object.                                         */
+/* Return 0 if displaced_pointer doesn't point to within a valid        */
+/* object.                                                              */
+/* Note that a deallocated object in the garbage collected heap         */
+/* may be considered valid, even if it has been deallocated with        */
+/* GC_free.                                                             */
+GC_API void * GC_CALL GC_base(void * /* displaced_pointer */);
+
+/* Given a pointer to the base of an object, return its size in bytes.  */
+/* The returned size may be slightly larger than what was originally    */
+/* requested.                                                           */
+GC_API size_t GC_CALL GC_size(const void * /* object_addr */);
+
+/* For compatibility with C library.  This is occasionally faster than  */
+/* a malloc followed by a bcopy.  But if you rely on that, either here  */
+/* or with the standard C library, your code is broken.  In my          */
+/* opinion, it shouldn't have been invented, but now we're stuck. -HB   */
+/* The resulting object has the same kind as the original.              */
+/* If the argument is stubborn, the result will have changes enabled.   */
+/* It is an error to have changes enabled for the original object.      */
+/* Follows ANSI conventions for NULL old_object.                        */
+GC_API void * GC_CALL GC_realloc(void * /* old_object */,
+                                 size_t /* new_size_in_bytes */)
+                        /* 'realloc' attr */ GC_ATTR_ALLOC_SIZE(2);
+
+/* Explicitly increase the heap size.   */
+/* Returns 0 on failure, 1 on success.  */
+GC_API int GC_CALL GC_expand_hp(size_t /* number_of_bytes */);
+
+/* Limit the heap size to n bytes.  Useful when you're debugging,       */
+/* especially on systems that don't handle running out of memory well.  */
+/* n == 0 ==> unbounded.  This is the default.  This setter function is */
+/* unsynchronized (so it might require GC_call_with_alloc_lock to avoid */
+/* data races).                                                         */
+GC_API void GC_CALL GC_set_max_heap_size(GC_word /* n */);
+
+/* Inform the collector that a certain section of statically allocated  */
+/* memory contains no pointers to garbage collected memory.  Thus it    */
+/* need not be scanned.  This is sometimes important if the application */
+/* maps large read/write files into the address space, which could be   */
+/* mistaken for dynamic library data segments on some systems.          */
+/* The section (referred to by low_address) must be pointer-aligned.    */
+/* low_address must not be greater than high_address_plus_1.            */
+GC_API void GC_CALL GC_exclude_static_roots(void * /* low_address */,
+                                        void * /* high_address_plus_1 */);
+
+/* Clear the set of root segments.  Wizards only.                       */
+GC_API void GC_CALL GC_clear_roots(void);
+
+/* Add a root segment.  Wizards only.                                   */
+/* Both segment start and end are not needed to be pointer-aligned.     */
+/* low_address must not be greater than high_address_plus_1.            */
+GC_API void GC_CALL GC_add_roots(void * /* low_address */,
+                                 void * /* high_address_plus_1 */);
+
+/* Remove a root segment.  Wizards only.                                */
+/* May be unimplemented on some platforms.                              */
+GC_API void GC_CALL GC_remove_roots(void * /* low_address */,
+                                    void * /* high_address_plus_1 */);
+
+/* Add a displacement to the set of those considered valid by the       */
+/* collector.  GC_register_displacement(n) means that if p was returned */
+/* by GC_malloc, then (char *)p + n will be considered to be a valid    */
+/* pointer to p.  N must be small and less than the size of p.          */
+/* (All pointers to the interior of objects from the stack are          */
+/* considered valid in any case.  This applies to heap objects and      */
+/* static data.)                                                        */
+/* Preferably, this should be called before any other GC procedures.    */
+/* Calling it later adds to the probability of excess memory            */
+/* retention.                                                           */
+/* This is a no-op if the collector has recognition of                  */
+/* arbitrary interior pointers enabled, which is now the default.       */
+GC_API void GC_CALL GC_register_displacement(size_t /* n */);
+
+/* The following version should be used if any debugging allocation is  */
+/* being done.                                                          */
+GC_API void GC_CALL GC_debug_register_displacement(size_t /* n */);
+
+/* Explicitly trigger a full, world-stop collection.    */
+GC_API void GC_CALL GC_gcollect(void);
+
+/* Same as above but ignores the default stop_func setting and tries to */
+/* unmap as much memory as possible (regardless of the corresponding    */
+/* switch setting).  The recommended usage: on receiving a system       */
+/* low-memory event; before retrying a system call failed because of    */
+/* the system is running out of resources.                              */
+GC_API void GC_CALL GC_gcollect_and_unmap(void);
+
+/* Trigger a full world-stopped collection.  Abort the collection if    */
+/* and when stop_func returns a nonzero value.  Stop_func will be       */
+/* called frequently, and should be reasonably fast.  (stop_func is     */
+/* called with the allocation lock held and the world might be stopped; */
+/* it's not allowed for stop_func to manipulate pointers to the garbage */
+/* collected heap or call most of GC functions.)  This works even       */
+/* if virtual dirty bits, and hence incremental collection is not       */
+/* available for this architecture.  Collections can be aborted faster  */
+/* than normal pause times for incremental collection.  However,        */
+/* aborted collections do no useful work; the next collection needs     */
+/* to start from the beginning.  stop_func must not be 0.               */
+/* GC_try_to_collect() returns 0 if the collection was aborted (or the  */
+/* collections are disabled), 1 if it succeeded.                        */
+typedef int (GC_CALLBACK * GC_stop_func)(void);
+GC_API int GC_CALL GC_try_to_collect(GC_stop_func /* stop_func */);
+
+/* Set and get the default stop_func.  The default stop_func is used by */
+/* GC_gcollect() and by implicitly trigged collections (except for the  */
+/* case when handling out of memory).  Must not be 0.                   */
+/* Both the setter and getter acquire the GC lock to avoid data races.  */
+GC_API void GC_CALL GC_set_stop_func(GC_stop_func /* stop_func */);
+GC_API GC_stop_func GC_CALL GC_get_stop_func(void);
+
+/* Return the number of bytes in the heap.  Excludes collector private  */
+/* data structures.  Excludes the unmapped memory (returned to the OS). */
+/* Includes empty blocks and fragmentation loss.  Includes some pages   */
+/* that were allocated but never written.                               */
+/* This is an unsynchronized getter, so it should be called typically   */
+/* with the GC lock held to avoid data races on multiprocessors (the    */
+/* alternative is to use GC_get_heap_usage_safe API call instead).      */
+/* This getter remains lock-free (unsynchronized) for compatibility     */
+/* reason since some existing clients call it from a GC callback        */
+/* holding the allocator lock.  (This API function and the following    */
+/* four ones bellow were made thread-safe in GC v7.2alpha1 and          */
+/* reverted back in v7.2alpha7 for the reason described.)               */
+GC_API size_t GC_CALL GC_get_heap_size(void);
+
+/* Return a lower bound on the number of free bytes in the heap         */
+/* (excluding the unmapped memory space).  This is an unsynchronized    */
+/* getter (see GC_get_heap_size comment regarding thread-safety).       */
+GC_API size_t GC_CALL GC_get_free_bytes(void);
+
+/* Return the size (in bytes) of the unmapped memory (which is returned */
+/* to the OS but could be remapped back by the collector later unless   */
+/* the OS runs out of system/virtual memory). This is an unsynchronized */
+/* getter (see GC_get_heap_size comment regarding thread-safety).       */
+GC_API size_t GC_CALL GC_get_unmapped_bytes(void);
+
+/* Return the number of bytes allocated since the last collection.      */
+/* This is an unsynchronized getter (see GC_get_heap_size comment       */
+/* regarding thread-safety).                                            */
+GC_API size_t GC_CALL GC_get_bytes_since_gc(void);
+
+/* Return the total number of bytes allocated in this process.          */
+/* Never decreases, except due to wrapping.  This is an unsynchronized  */
+/* getter (see GC_get_heap_size comment regarding thread-safety).       */
+GC_API size_t GC_CALL GC_get_total_bytes(void);
+
+/* Return the heap usage information.  This is a thread-safe (atomic)   */
+/* alternative for the five above getters.   (This function acquires    */
+/* the allocator lock thus preventing data racing and returning the     */
+/* consistent result.)  Passing NULL pointer is allowed for any         */
+/* argument.  Returned (filled in) values are of word type.             */
+/* (This API function was introduced in GC v7.2alpha7 at the same time  */
+/* when GC_get_heap_size and the friends were made lock-free again.)    */
+GC_API void GC_CALL GC_get_heap_usage_safe(GC_word * /* pheap_size */,
+                                           GC_word * /* pfree_bytes */,
+                                           GC_word * /* punmapped_bytes */,
+                                           GC_word * /* pbytes_since_gc */,
+                                           GC_word * /* ptotal_bytes */);
+
+/* Disable garbage collection.  Even GC_gcollect calls will be          */
+/* ineffective.                                                         */
+GC_API void GC_CALL GC_disable(void);
+
+/* Return non-zero (TRUE) if and only if garbage collection is disabled */
+/* (i.e., GC_dont_gc value is non-zero).  Does not acquire the lock.    */
+GC_API int GC_CALL GC_is_disabled(void);
+
+/* Try to re-enable garbage collection.  GC_disable() and GC_enable()   */
+/* calls nest.  Garbage collection is enabled if the number of calls to */
+/* both functions is equal.                                             */
+GC_API void GC_CALL GC_enable(void);
+
+/* Enable incremental/generational collection.  Not advisable unless    */
+/* dirty bits are available or most heap objects are pointer-free       */
+/* (atomic) or immutable.  Don't use in leak finding mode.  Ignored if  */
+/* GC_dont_gc is non-zero.  Only the generational piece of this is      */
+/* functional if GC_parallel is TRUE or if GC_time_limit is             */
+/* GC_TIME_UNLIMITED.  Causes thread-local variant of GC_gcj_malloc()   */
+/* to revert to locked allocation.  Must be called before any such      */
+/* GC_gcj_malloc() calls.  For best performance, should be called as    */
+/* early as possible.  On some platforms, calling it later may have     */
+/* adverse effects.                                                     */
+/* Safe to call before GC_INIT().  Includes a  GC_init() call.          */
+GC_API void GC_CALL GC_enable_incremental(void);
+
+/* Does incremental mode write-protect pages?  Returns zero or  */
+/* more of the following, or'ed together:                       */
+#define GC_PROTECTS_POINTER_HEAP  1 /* May protect non-atomic objs.     */
+#define GC_PROTECTS_PTRFREE_HEAP  2
+#define GC_PROTECTS_STATIC_DATA   4 /* Currently never.                 */
+#define GC_PROTECTS_STACK         8 /* Probably impractical.            */
+
+#define GC_PROTECTS_NONE 0
+GC_API int GC_CALL GC_incremental_protection_needs(void);
+
+/* Perform some garbage collection work, if appropriate.        */
+/* Return 0 if there is no more work to be done.                */
+/* Typically performs an amount of work corresponding roughly   */
+/* to marking from one page.  May do more work if further       */
+/* progress requires it, e.g. if incremental collection is      */
+/* disabled.  It is reasonable to call this in a wait loop      */
+/* until it returns 0.                                          */
+GC_API int GC_CALL GC_collect_a_little(void);
+
+/* Allocate an object of size lb bytes.  The client guarantees that     */
+/* as long as the object is live, it will be referenced by a pointer    */
+/* that points to somewhere within the first 256 bytes of the object.   */
+/* (This should normally be declared volatile to prevent the compiler   */
+/* from invalidating this assertion.)  This routine is only useful      */
+/* if a large array is being allocated.  It reduces the chance of       */
+/* accidentally retaining such an array as a result of scanning an      */
+/* integer that happens to be an address inside the array.  (Actually,  */
+/* it reduces the chance of the allocator not finding space for such    */
+/* an array, since it will try hard to avoid introducing such a false   */
+/* reference.)  On a SunOS 4.X or MS Windows system this is recommended */
+/* for arrays likely to be larger than 100K or so.  For other systems,  */
+/* or if the collector is not configured to recognize all interior      */
+/* pointers, the threshold is normally much higher.                     */
+GC_API void * GC_CALL GC_malloc_ignore_off_page(size_t /* lb */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_malloc_atomic_ignore_off_page(size_t /* lb */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+
+#ifdef GC_ADD_CALLER
+# define GC_EXTRAS GC_RETURN_ADDR, __FILE__, __LINE__
+# define GC_EXTRA_PARAMS GC_word ra, const char * s, int i
+#else
+# define GC_EXTRAS __FILE__, __LINE__
+# define GC_EXTRA_PARAMS const char * s, int i
+#endif
+
+/* The following is only defined if the library has been suitably       */
+/* compiled:                                                            */
+GC_API void * GC_CALL GC_malloc_atomic_uncollectable(
+                                                size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_malloc_atomic_uncollectable(size_t,
+                                                           GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+
+/* Debugging (annotated) allocation.  GC_gcollect will check            */
+/* objects allocated in this way for overwrites, etc.                   */
+GC_API void * GC_CALL GC_debug_malloc(size_t /* size_in_bytes */,
+                                      GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_malloc_atomic(size_t /* size_in_bytes */,
+                                             GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API char * GC_CALL GC_debug_strdup(const char *,
+                                      GC_EXTRA_PARAMS) GC_ATTR_MALLOC;
+GC_API char * GC_CALL GC_debug_strndup(const char *, size_t,
+                                       GC_EXTRA_PARAMS) GC_ATTR_MALLOC;
+GC_API void * GC_CALL GC_debug_malloc_uncollectable(
+                        size_t /* size_in_bytes */, GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_malloc_stubborn(size_t /* size_in_bytes */,
+                                               GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_malloc_ignore_off_page(
+                        size_t /* size_in_bytes */, GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_malloc_atomic_ignore_off_page(
+                        size_t /* size_in_bytes */, GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void GC_CALL GC_debug_free(void *);
+GC_API void * GC_CALL GC_debug_realloc(void * /* old_object */,
+                        size_t /* new_size_in_bytes */, GC_EXTRA_PARAMS)
+                        /* 'realloc' attr */ GC_ATTR_ALLOC_SIZE(2);
+GC_API void GC_CALL GC_debug_change_stubborn(void *);
+GC_API void GC_CALL GC_debug_end_stubborn_change(void *);
+
+/* Routines that allocate objects with debug information (like the      */
+/* above), but just fill in dummy file and line number information.     */
+/* Thus they can serve as drop-in malloc/realloc replacements.  This    */
+/* can be useful for two reasons:                                       */
+/* 1) It allows the collector to be built with DBG_HDRS_ALL defined     */
+/*    even if some allocation calls come from 3rd party libraries       */
+/*    that can't be recompiled.                                         */
+/* 2) On some platforms, the file and line information is redundant,    */
+/*    since it can be reconstructed from a stack trace.  On such        */
+/*    platforms it may be more convenient not to recompile, e.g. for    */
+/*    leak detection.  This can be accomplished by instructing the      */
+/*    linker to replace malloc/realloc with these.                      */
+GC_API void * GC_CALL GC_debug_malloc_replacement(size_t /* size_in_bytes */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+GC_API void * GC_CALL GC_debug_realloc_replacement(void * /* object_addr */,
+                                                   size_t /* size_in_bytes */)
+                        /* 'realloc' attr */ GC_ATTR_ALLOC_SIZE(2);
+
+#ifdef GC_DEBUG_REPLACEMENT
+# define GC_MALLOC(sz) GC_debug_malloc_replacement(sz)
+# define GC_REALLOC(old, sz) GC_debug_realloc_replacement(old, sz)
+#elif defined(GC_DEBUG)
+# define GC_MALLOC(sz) GC_debug_malloc(sz, GC_EXTRAS)
+# define GC_REALLOC(old, sz) GC_debug_realloc(old, sz, GC_EXTRAS)
+#else
+# define GC_MALLOC(sz) GC_malloc(sz)
+# define GC_REALLOC(old, sz) GC_realloc(old, sz)
+#endif /* !GC_DEBUG_REPLACEMENT && !GC_DEBUG */
+
+#ifdef GC_DEBUG
+# define GC_MALLOC_ATOMIC(sz) GC_debug_malloc_atomic(sz, GC_EXTRAS)
+# define GC_STRDUP(s) GC_debug_strdup(s, GC_EXTRAS)
+# define GC_STRNDUP(s, sz) GC_debug_strndup(s, sz, GC_EXTRAS)
+# define GC_MALLOC_ATOMIC_UNCOLLECTABLE(sz) \
+                        GC_debug_malloc_atomic_uncollectable(sz, GC_EXTRAS)
+# define GC_MALLOC_UNCOLLECTABLE(sz) \
+                        GC_debug_malloc_uncollectable(sz, GC_EXTRAS)
+# define GC_MALLOC_IGNORE_OFF_PAGE(sz) \
+                        GC_debug_malloc_ignore_off_page(sz, GC_EXTRAS)
+# define GC_MALLOC_ATOMIC_IGNORE_OFF_PAGE(sz) \
+                        GC_debug_malloc_atomic_ignore_off_page(sz, GC_EXTRAS)
+# define GC_FREE(p) GC_debug_free(p)
+# define GC_REGISTER_FINALIZER(p, f, d, of, od) \
+      GC_debug_register_finalizer(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_IGNORE_SELF(p, f, d, of, od) \
+      GC_debug_register_finalizer_ignore_self(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_NO_ORDER(p, f, d, of, od) \
+      GC_debug_register_finalizer_no_order(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_UNREACHABLE(p, f, d, of, od) \
+      GC_debug_register_finalizer_unreachable(p, f, d, of, od)
+# define GC_MALLOC_STUBBORN(sz) GC_debug_malloc_stubborn(sz, GC_EXTRAS)
+# define GC_CHANGE_STUBBORN(p) GC_debug_change_stubborn(p)
+# define GC_END_STUBBORN_CHANGE(p) GC_debug_end_stubborn_change(p)
+# define GC_GENERAL_REGISTER_DISAPPEARING_LINK(link, obj) \
+      GC_general_register_disappearing_link(link, GC_base(obj))
+# define GC_REGISTER_DISPLACEMENT(n) GC_debug_register_displacement(n)
+#else
+# define GC_MALLOC_ATOMIC(sz) GC_malloc_atomic(sz)
+# define GC_STRDUP(s) GC_strdup(s)
+# define GC_STRNDUP(s, sz) GC_strndup(s, sz)
+# define GC_MALLOC_ATOMIC_UNCOLLECTABLE(sz) GC_malloc_atomic_uncollectable(sz)
+# define GC_MALLOC_UNCOLLECTABLE(sz) GC_malloc_uncollectable(sz)
+# define GC_MALLOC_IGNORE_OFF_PAGE(sz) \
+                        GC_malloc_ignore_off_page(sz)
+# define GC_MALLOC_ATOMIC_IGNORE_OFF_PAGE(sz) \
+                        GC_malloc_atomic_ignore_off_page(sz)
+# define GC_FREE(p) GC_free(p)
+# define GC_REGISTER_FINALIZER(p, f, d, of, od) \
+      GC_register_finalizer(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_IGNORE_SELF(p, f, d, of, od) \
+      GC_register_finalizer_ignore_self(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_NO_ORDER(p, f, d, of, od) \
+      GC_register_finalizer_no_order(p, f, d, of, od)
+# define GC_REGISTER_FINALIZER_UNREACHABLE(p, f, d, of, od) \
+      GC_register_finalizer_unreachable(p, f, d, of, od)
+# define GC_MALLOC_STUBBORN(sz) GC_malloc_stubborn(sz)
+# define GC_CHANGE_STUBBORN(p) GC_change_stubborn(p)
+# define GC_END_STUBBORN_CHANGE(p) GC_end_stubborn_change(p)
+# define GC_GENERAL_REGISTER_DISAPPEARING_LINK(link, obj) \
+      GC_general_register_disappearing_link(link, obj)
+# define GC_REGISTER_DISPLACEMENT(n) GC_register_displacement(n)
+#endif /* !GC_DEBUG */
+
+/* The following are included because they are often convenient, and    */
+/* reduce the chance for a misspecified size argument.  But calls may   */
+/* expand to something syntactically incorrect if t is a complicated    */
+/* type expression.  Note that, unlike C++ new operator, these ones     */
+/* may return NULL (if out of memory).                                  */
+#define GC_NEW(t)               ((t*)GC_MALLOC(sizeof(t)))
+#define GC_NEW_ATOMIC(t)        ((t*)GC_MALLOC_ATOMIC(sizeof(t)))
+#define GC_NEW_STUBBORN(t)      ((t*)GC_MALLOC_STUBBORN(sizeof(t)))
+#define GC_NEW_UNCOLLECTABLE(t) ((t*)GC_MALLOC_UNCOLLECTABLE(sizeof(t)))
+
+#ifdef GC_REQUIRE_WCSDUP
+  /* This might be unavailable on some targets (or not needed). */
+  /* wchar_t should be defined in stddef.h */
+  GC_API wchar_t * GC_CALL GC_wcsdup(const wchar_t *) GC_ATTR_MALLOC;
+  GC_API wchar_t * GC_CALL GC_debug_wcsdup(const wchar_t *,
+                                           GC_EXTRA_PARAMS) GC_ATTR_MALLOC;
+# ifdef GC_DEBUG
+#   define GC_WCSDUP(s) GC_debug_wcsdup(s, GC_EXTRAS)
+# else
+#   define GC_WCSDUP(s) GC_wcsdup(s)
+# endif
+#endif /* GC_REQUIRE_WCSDUP */
+
+/* Finalization.  Some of these primitives are grossly unsafe.          */
+/* The idea is to make them both cheap, and sufficient to build         */
+/* a safer layer, closer to Modula-3, Java, or PCedar finalization.     */
+/* The interface represents my conclusions from a long discussion       */
+/* with Alan Demers, Dan Greene, Carl Hauser, Barry Hayes,              */
+/* Christian Jacobi, and Russ Atkinson.  It's not perfect, and          */
+/* probably nobody else agrees with it.     Hans-J. Boehm  3/13/92      */
+typedef void (GC_CALLBACK * GC_finalization_proc)(void * /* obj */,
+                                                  void * /* client_data */);
+
+GC_API void GC_CALL GC_register_finalizer(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+GC_API void GC_CALL GC_debug_register_finalizer(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+        /* When obj is no longer accessible, invoke             */
+        /* (*fn)(obj, cd).  If a and b are inaccessible, and    */
+        /* a points to b (after disappearing links have been    */
+        /* made to disappear), then only a will be              */
+        /* finalized.  (If this does not create any new         */
+        /* pointers to b, then b will be finalized after the    */
+        /* next collection.)  Any finalizable object that       */
+        /* is reachable from itself by following one or more    */
+        /* pointers will not be finalized (or collected).       */
+        /* Thus cycles involving finalizable objects should     */
+        /* be avoided, or broken by disappearing links.         */
+        /* All but the last finalizer registered for an object  */
+        /* is ignored.                                          */
+        /* Finalization may be removed by passing 0 as fn.      */
+        /* Finalizers are implicitly unregistered when they are */
+        /* enqueued for finalization (i.e. become ready to be   */
+        /* finalized).                                          */
+        /* The old finalizer and client data are stored in      */
+        /* *ofn and *ocd.  (ofn and/or ocd may be NULL.         */
+        /* The allocation lock is held while *ofn and *ocd are  */
+        /* updated.  In case of error (no memory to register    */
+        /* new finalizer), *ofn and *ocd remain unchanged.)     */
+        /* Fn is never invoked on an accessible object,         */
+        /* provided hidden pointers are converted to real       */
+        /* pointers only if the allocation lock is held, and    */
+        /* such conversions are not performed by finalization   */
+        /* routines.                                            */
+        /* If GC_register_finalizer is aborted as a result of   */
+        /* a signal, the object may be left with no             */
+        /* finalization, even if neither the old nor new        */
+        /* finalizer were NULL.                                 */
+        /* Obj should be the starting address of an object      */
+        /* allocated by GC_malloc or friends. Obj may also be   */
+        /* NULL or point to something outside GC heap (in this  */
+        /* case, fn is ignored, *ofn and *ocd are set to NULL). */
+        /* Note that any garbage collectible object referenced  */
+        /* by cd will be considered accessible until the        */
+        /* finalizer is invoked.                                */
+
+/* Another versions of the above follow.  It ignores            */
+/* self-cycles, i.e. pointers from a finalizable object to      */
+/* itself.  There is a stylistic argument that this is wrong,   */
+/* but it's unavoidable for C++, since the compiler may         */
+/* silently introduce these.  It's also benign in that specific */
+/* case.  And it helps if finalizable objects are split to      */
+/* avoid cycles.                                                */
+/* Note that cd will still be viewed as accessible, even if it  */
+/* refers to the object itself.                                 */
+GC_API void GC_CALL GC_register_finalizer_ignore_self(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+GC_API void GC_CALL GC_debug_register_finalizer_ignore_self(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+
+/* Another version of the above.  It ignores all cycles.        */
+/* It should probably only be used by Java implementations.     */
+/* Note that cd will still be viewed as accessible, even if it  */
+/* refers to the object itself.                                 */
+GC_API void GC_CALL GC_register_finalizer_no_order(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+GC_API void GC_CALL GC_debug_register_finalizer_no_order(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+
+/* This is a special finalizer that is useful when an object's  */
+/* finalizer must be run when the object is known to be no      */
+/* longer reachable, not even from other finalizable objects.   */
+/* It behaves like "normal" finalization, except that the       */
+/* finalizer is not run while the object is reachable from      */
+/* other objects specifying unordered finalization.             */
+/* Effectively it allows an object referenced, possibly         */
+/* indirectly, from an unordered finalizable object to override */
+/* the unordered finalization request.                          */
+/* This can be used in combination with finalizer_no_order so   */
+/* as to release resources that must not be released while an   */
+/* object can still be brought back to life by other            */
+/* finalizers.                                                  */
+/* Only works if GC_java_finalization is set.  Probably only    */
+/* of interest when implementing a language that requires       */
+/* unordered finalization (e.g. Java, C#).                      */
+GC_API void GC_CALL GC_register_finalizer_unreachable(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+GC_API void GC_CALL GC_debug_register_finalizer_unreachable(void * /* obj */,
+                        GC_finalization_proc /* fn */, void * /* cd */,
+                        GC_finalization_proc * /* ofn */, void ** /* ocd */);
+
+#define GC_NO_MEMORY 2  /* Failure due to lack of memory.       */
+
+/* The following routine may be used to break cycles between    */
+/* finalizable objects, thus causing cyclic finalizable         */
+/* objects to be finalized in the correct order.  Standard      */
+/* use involves calling GC_register_disappearing_link(&p),      */
+/* where p is a pointer that is not followed by finalization    */
+/* code, and should not be considered in determining            */
+/* finalization order.                                          */
+GC_API int GC_CALL GC_register_disappearing_link(void ** /* link */);
+        /* Link should point to a field of a heap allocated     */
+        /* object obj.  *link will be cleared when obj is       */
+        /* found to be inaccessible.  This happens BEFORE any   */
+        /* finalization code is invoked, and BEFORE any         */
+        /* decisions about finalization order are made.         */
+        /* This is useful in telling the finalizer that         */
+        /* some pointers are not essential for proper           */
+        /* finalization.  This may avoid finalization cycles.   */
+        /* Note that obj may be resurrected by another          */
+        /* finalizer, and thus the clearing of *link may        */
+        /* be visible to non-finalization code.                 */
+        /* There's an argument that an arbitrary action should  */
+        /* be allowed here, instead of just clearing a pointer. */
+        /* But this causes problems if that action alters, or   */
+        /* examines connectivity.  Returns GC_DUPLICATE if link */
+        /* was already registered, GC_SUCCESS if registration   */
+        /* succeeded, GC_NO_MEMORY if it failed for lack of     */
+        /* memory, and GC_oom_fn did not handle the problem.    */
+        /* Only exists for backward compatibility.  See below:  */
+
+GC_API int GC_CALL GC_general_register_disappearing_link(void ** /* link */,
+                                                         void * /* obj */);
+        /* A slight generalization of the above. *link is       */
+        /* cleared when obj first becomes inaccessible.  This   */
+        /* can be used to implement weak pointers easily and    */
+        /* safely. Typically link will point to a location      */
+        /* holding a disguised pointer to obj.  (A pointer      */
+        /* inside an "atomic" object is effectively disguised.) */
+        /* In this way, weak pointers are broken before any     */
+        /* object reachable from them gets finalized.           */
+        /* Each link may be registered only with one obj value, */
+        /* i.e. all objects but the last one (link registered   */
+        /* with) are ignored.  This was added after a long      */
+        /* email discussion with John Ellis.                    */
+        /* link must be non-NULL (and be properly aligned).     */
+        /* obj must be a pointer to the first word of an object */
+        /* allocated by GC_malloc or friends.  It is unsafe to  */
+        /* explicitly deallocate the object containing link.    */
+        /* Explicit deallocation of obj may or may not cause    */
+        /* link to eventually be cleared.                       */
+        /* This function can be used to implement certain types */
+        /* of weak pointers.  Note, however, this generally     */
+        /* requires that the allocation lock is held (see       */
+        /* GC_call_with_alloc_lock() below) when the disguised  */
+        /* pointer is accessed.  Otherwise a strong pointer     */
+        /* could be recreated between the time the collector    */
+        /* decides to reclaim the object and the link is        */
+        /* cleared.  Returns GC_SUCCESS if registration         */
+        /* succeeded (a new link is registered), GC_DUPLICATE   */
+        /* if link was already registered (with some object),   */
+        /* GC_NO_MEMORY if registration failed for lack of      */
+        /* memory (and GC_oom_fn did not handle the problem).   */
+
+GC_API int GC_CALL GC_unregister_disappearing_link(void ** /* link */);
+        /* Undoes a registration by either of the above two     */
+        /* routines.  Returns 0 if link was not actually        */
+        /* registered (otherwise returns 1).                    */
+
+/* Returns !=0 if GC_invoke_finalizers has something to do.     */
+GC_API int GC_CALL GC_should_invoke_finalizers(void);
+
+GC_API int GC_CALL GC_invoke_finalizers(void);
+        /* Run finalizers for all objects that are ready to     */
+        /* be finalized.  Return the number of finalizers       */
+        /* that were run.  Normally this is also called         */
+        /* implicitly during some allocations.  If              */
+        /* GC_finalize_on_demand is nonzero, it must be called  */
+        /* explicitly.                                          */
+
+/* Explicitly tell the collector that an object is reachable    */
+/* at a particular program point.  This prevents the argument   */
+/* pointer from being optimized away, even it is otherwise no   */
+/* longer needed.  It should have no visible effect in the      */
+/* absence of finalizers or disappearing links.  But it may be  */
+/* needed to prevent finalizers from running while the          */
+/* associated external resource is still in use.                */
+/* The function is sometimes called keep_alive in other         */
+/* settings.                                                    */
+#if defined(__GNUC__) && !defined(__INTEL_COMPILER)
+# define GC_reachable_here(ptr) \
+                __asm__ __volatile__(" " : : "X"(ptr) : "memory")
+#else
+  GC_API void GC_CALL GC_noop1(GC_word);
+# define GC_reachable_here(ptr) GC_noop1((GC_word)(ptr))
+#endif
+
+/* GC_set_warn_proc can be used to redirect or filter warning messages. */
+/* p may not be a NULL pointer.  Both the setter and the getter acquire */
+/* the GC lock (to avoid data races).                                   */
+typedef void (GC_CALLBACK * GC_warn_proc)(char * /* msg */,
+                                          GC_word /* arg */);
+GC_API void GC_CALL GC_set_warn_proc(GC_warn_proc /* p */);
+/* GC_get_warn_proc returns the current warn_proc.                      */
+GC_API GC_warn_proc GC_CALL GC_get_warn_proc(void);
+
+/* GC_ignore_warn_proc may be used as an argument for GC_set_warn_proc  */
+/* to suppress all warnings (unless statistics printing is turned on).  */
+GC_API void GC_CALLBACK GC_ignore_warn_proc(char *, GC_word);
+
+/* The following is intended to be used by a higher level       */
+/* (e.g. Java-like) finalization facility.  It is expected      */
+/* that finalization code will arrange for hidden pointers to   */
+/* disappear.  Otherwise objects can be accessed after they     */
+/* have been collected.                                         */
+/* Note that putting pointers in atomic objects or in           */
+/* non-pointer slots of "typed" objects is equivalent to        */
+/* disguising them in this way, and may have other advantages.  */
+typedef GC_word GC_hidden_pointer;
+#define GC_HIDE_POINTER(p) (~(GC_hidden_pointer)(p))
+/* Converting a hidden pointer to a real pointer requires verifying     */
+/* that the object still exists.  This involves acquiring the           */
+/* allocator lock to avoid a race with the collector.                   */
+#define GC_REVEAL_POINTER(p) ((void *)GC_HIDE_POINTER(p))
+
+#if defined(I_HIDE_POINTERS) || defined(GC_I_HIDE_POINTERS)
+  /* This exists only for compatibility (the GC-prefixed symbols are    */
+  /* preferred for new code).                                           */
+# define HIDE_POINTER(p) GC_HIDE_POINTER(p)
+# define REVEAL_POINTER(p) GC_REVEAL_POINTER(p)
+#endif
+
+typedef void * (GC_CALLBACK * GC_fn_type)(void * /* client_data */);
+GC_API void * GC_CALL GC_call_with_alloc_lock(GC_fn_type /* fn */,
+                                                void * /* client_data */);
+
+/* These routines are intended to explicitly notify the collector       */
+/* of new threads.  Often this is unnecessary because thread creation   */
+/* is implicitly intercepted by the collector, using header-file        */
+/* defines, or linker-based interception.  In the long run the intent   */
+/* is to always make redundant registration safe.  In the short run,    */
+/* this is being implemented a platform at a time.                      */
+/* The interface is complicated by the fact that we probably will not   */
+/* ever be able to automatically determine the stack base for thread    */
+/* stacks on all platforms.                                             */
+
+/* Structure representing the base of a thread stack.  On most          */
+/* platforms this contains just a single address.                       */
+struct GC_stack_base {
+  void * mem_base; /* Base of memory stack. */
+# if defined(__ia64) || defined(__ia64__) || defined(_M_IA64)
+    void * reg_base; /* Base of separate register stack. */
+# endif
+};
+
+typedef void * (GC_CALLBACK * GC_stack_base_func)(
+                struct GC_stack_base * /* sb */, void * /* arg */);
+
+/* Call a function with a stack base structure corresponding to         */
+/* somewhere in the GC_call_with_stack_base frame.  This often can      */
+/* be used to provide a sufficiently accurate stack base.  And we       */
+/* implement it everywhere.                                             */
+GC_API void * GC_CALL GC_call_with_stack_base(GC_stack_base_func /* fn */,
+                                              void * /* arg */);
+
+#define GC_SUCCESS 0
+#define GC_DUPLICATE 1          /* Was already registered.              */
+#define GC_NO_THREADS 2         /* No thread support in GC.             */
+        /* GC_NO_THREADS is not returned by any GC function anymore.    */
+#define GC_UNIMPLEMENTED 3 /* Not yet implemented on this platform.     */
+
+#if defined(GC_DARWIN_THREADS) || defined(GC_WIN32_THREADS)
+  /* Use implicit thread registration and processing (via Win32 DllMain */
+  /* or Darwin task_threads).  Deprecated.  Must be called before       */
+  /* GC_INIT() and other GC routines.  Should be avoided if             */
+  /* GC_pthread_create, GC_beginthreadex (or GC_CreateThread) could be  */
+  /* called instead.  Disables parallelized GC on Win32.                */
+  GC_API void GC_CALL GC_use_threads_discovery(void);
+#endif
+
+#ifdef GC_THREADS
+  /* Return the signal number (constant) used by the garbage collector  */
+  /* to suspend threads on POSIX systems.  Return -1 otherwise.         */
+  GC_API int GC_CALL GC_get_suspend_signal(void);
+
+  /* Explicitly enable GC_register_my_thread() invocation.              */
+  /* Done implicitly if a GC thread-creation function is called (or     */
+  /* implicit thread registration is activated).  Otherwise, it must    */
+  /* be called from the main (or any previously registered) thread      */
+  /* between the collector initialization and the first explicit        */
+  /* registering of a thread (it should be called as late as possible). */
+  GC_API void GC_CALL GC_allow_register_threads(void);
+
+  /* Register the current thread, with the indicated stack base, as     */
+  /* a new thread whose stack(s) should be traced by the GC.  If it     */
+  /* is not implicitly called by the GC, this must be called before a   */
+  /* thread can allocate garbage collected memory, or assign pointers   */
+  /* to the garbage collected heap.  Once registered, a thread will be  */
+  /* stopped during garbage collections.                                */
+  /* This call must be previously enabled (see above).                  */
+  /* This should never be called from the main thread, where it is      */
+  /* always done implicitly.  This is normally done implicitly if GC_   */
+  /* functions are called to create the thread, e.g. by including gc.h  */
+  /* (which redefines some system functions) before calling the system  */
+  /* thread creation function.  Nonetheless, thread cleanup routines    */
+  /* (eg., pthread key destructor) typically require manual thread      */
+  /* registering (and unregistering) if pointers to GC-allocated        */
+  /* objects are manipulated inside.                                    */
+  /* It is also always done implicitly on some platforms if             */
+  /* GC_use_threads_discovery() is called at start-up.  Except for the  */
+  /* latter case, the explicit call is normally required for threads    */
+  /* created by third-party libraries.                                  */
+  /* A manually registered thread requires manual unregistering.        */
+  GC_API int GC_CALL GC_register_my_thread(const struct GC_stack_base *);
+
+  /* Unregister the current thread.  Only an explicitly registered      */
+  /* thread (i.e. for which GC_register_my_thread() returns GC_SUCCESS) */
+  /* is allowed (and required) to call this function.  (As a special    */
+  /* exception, it is also allowed to once unregister the main thread.) */
+  /* The thread may no longer allocate garbage collected memory or      */
+  /* manipulate pointers to the garbage collected heap after making     */
+  /* this call.  Specifically, if it wants to return or otherwise       */
+  /* communicate a pointer to the garbage-collected heap to another     */
+  /* thread, it must do this before calling GC_unregister_my_thread,    */
+  /* most probably by saving it in a global data structure.  Must not   */
+  /* be called inside a GC callback function (except for                */
+  /* GC_call_with_stack_base() one).                                    */
+  GC_API int GC_CALL GC_unregister_my_thread(void);
+#endif /* GC_THREADS */
+
+/* Wrapper for functions that are likely to block (or, at least, do not */
+/* allocate garbage collected memory and/or manipulate pointers to the  */
+/* garbage collected heap) for an appreciable length of time.  While fn */
+/* is running, the collector is said to be in the "inactive" state for  */
+/* the current thread (this means that the thread is not suspended and  */
+/* the thread's stack frames "belonging" to the functions in the        */
+/* "inactive" state are not scanned during garbage collections).  It is */
+/* allowed for fn to call GC_call_with_gc_active() (even recursively),  */
+/* thus temporarily toggling the collector's state back to "active".    */
+GC_API void * GC_CALL GC_do_blocking(GC_fn_type /* fn */,
+                                     void * /* client_data */);
+
+/* Call a function switching to the "active" state of the collector for */
+/* the current thread (i.e. the user function is allowed to call any    */
+/* GC function and/or manipulate pointers to the garbage collected      */
+/* heap).  GC_call_with_gc_active() has the functionality opposite to   */
+/* GC_do_blocking() one.  It is assumed that the collector is already   */
+/* initialized and the current thread is registered.  fn may toggle     */
+/* the collector thread's state temporarily to "inactive" one by using  */
+/* GC_do_blocking.  GC_call_with_gc_active() often can be used to       */
+/* provide a sufficiently accurate stack base.                          */
+GC_API void * GC_CALL GC_call_with_gc_active(GC_fn_type /* fn */,
+                                             void * /* client_data */);
+
+/* Attempt to fill in the GC_stack_base structure with the stack base   */
+/* for this thread.  This appears to be required to implement anything  */
+/* like the JNI AttachCurrentThread in an environment in which new      */
+/* threads are not automatically registered with the collector.         */
+/* It is also unfortunately hard to implement well on many platforms.   */
+/* Returns GC_SUCCESS or GC_UNIMPLEMENTED.  This function acquires the  */
+/* GC lock on some platforms.                                           */
+GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *);
+
+/* The following routines are primarily intended for use with a         */
+/* preprocessor which inserts calls to check C pointer arithmetic.      */
+/* They indicate failure by invoking the corresponding _print_proc.     */
+
+/* Check that p and q point to the same object.                 */
+/* Fail conspicuously if they don't.                            */
+/* Returns the first argument.                                  */
+/* Succeeds if neither p nor q points to the heap.              */
+/* May succeed if both p and q point to between heap objects.   */
+GC_API void * GC_CALL GC_same_obj(void * /* p */, void * /* q */);
+
+/* Checked pointer pre- and post- increment operations.  Note that      */
+/* the second argument is in units of bytes, not multiples of the       */
+/* object size.  This should either be invoked from a macro, or the     */
+/* call should be automatically generated.                              */
+GC_API void * GC_CALL GC_pre_incr(void **, ptrdiff_t /* how_much */);
+GC_API void * GC_CALL GC_post_incr(void **, ptrdiff_t /* how_much */);
+
+/* Check that p is visible                                              */
+/* to the collector as a possibly pointer containing location.          */
+/* If it isn't fail conspicuously.                                      */
+/* Returns the argument in all cases.  May erroneously succeed          */
+/* in hard cases.  (This is intended for debugging use with             */
+/* untyped allocations.  The idea is that it should be possible, though */
+/* slow, to add such a call to all indirect pointer stores.)            */
+/* Currently useless for multi-threaded worlds.                         */
+GC_API void * GC_CALL GC_is_visible(void * /* p */);
+
+/* Check that if p is a pointer to a heap page, then it points to       */
+/* a valid displacement within a heap object.                           */
+/* Fail conspicuously if this property does not hold.                   */
+/* Uninteresting with GC_all_interior_pointers.                         */
+/* Always returns its argument.                                         */
+GC_API void * GC_CALL GC_is_valid_displacement(void * /* p */);
+
+/* Explicitly dump the GC state.  This is most often called from the    */
+/* debugger, or by setting the GC_DUMP_REGULARLY environment variable,  */
+/* but it may be useful to call it from client code during debugging.   */
+/* Defined only if the library has been compiled without NO_DEBUGGING.  */
+GC_API void GC_CALL GC_dump(void);
+
+/* Safer, but slow, pointer addition.  Probably useful mainly with      */
+/* a preprocessor.  Useful only for heap pointers.                      */
+/* Only the macros without trailing digits are meant to be used         */
+/* by clients.  These are designed to model the available C pointer     */
+/* arithmetic expressions.                                              */
+/* Even then, these are probably more useful as                         */
+/* documentation than as part of the API.                               */
+/* Note that GC_PTR_ADD evaluates the first argument more than once.    */
+#if defined(GC_DEBUG) && defined(__GNUC__)
+# define GC_PTR_ADD3(x, n, type_of_result) \
+        ((type_of_result)GC_same_obj((x)+(n), (x)))
+# define GC_PRE_INCR3(x, n, type_of_result) \
+        ((type_of_result)GC_pre_incr((void **)(&(x)), (n)*sizeof(*x)))
+# define GC_POST_INCR3(x, n, type_of_result) \
+        ((type_of_result)GC_post_incr((void **)(&(x)), (n)*sizeof(*x)))
+# define GC_PTR_ADD(x, n) GC_PTR_ADD3(x, n, typeof(x))
+# define GC_PRE_INCR(x, n) GC_PRE_INCR3(x, n, typeof(x))
+# define GC_POST_INCR(x) GC_POST_INCR3(x, 1, typeof(x))
+# define GC_POST_DECR(x) GC_POST_INCR3(x, -1, typeof(x))
+#else /* !GC_DEBUG || !__GNUC__ */
+  /* We can't do this right without typeof, which ANSI decided was not    */
+  /* sufficiently useful.  Without it we resort to the non-debug version. */
+  /* FIXME: This should eventually support C++0x decltype.                */
+# define GC_PTR_ADD(x, n) ((x)+(n))
+# define GC_PRE_INCR(x, n) ((x) += (n))
+# define GC_POST_INCR(x) ((x)++)
+# define GC_POST_DECR(x) ((x)--)
+#endif /* !GC_DEBUG || !__GNUC__ */
+
+/* Safer assignment of a pointer to a non-stack location.       */
+#ifdef GC_DEBUG
+# define GC_PTR_STORE(p, q) \
+        (*(void **)GC_is_visible(p) = GC_is_valid_displacement(q))
+#else
+# define GC_PTR_STORE(p, q) (*(p) = (q))
+#endif
+
+/* Functions called to report pointer checking errors */
+GC_API void (GC_CALLBACK * GC_same_obj_print_proc)(void * /* p */,
+                                                   void * /* q */);
+GC_API void (GC_CALLBACK * GC_is_valid_displacement_print_proc)(void *);
+GC_API void (GC_CALLBACK * GC_is_visible_print_proc)(void *);
+
+#ifdef GC_PTHREADS
+  /* For pthread support, we generally need to intercept a number of    */
+  /* thread library calls.  We do that here by macro defining them.     */
+# include "gc_pthread_redirects.h"
+#endif
+
+/* This returns a list of objects, linked through their first word.     */
+/* Its use can greatly reduce lock contention problems, since the       */
+/* allocation lock can be acquired and released many fewer times.       */
+GC_API void * GC_CALL GC_malloc_many(size_t /* lb */);
+#define GC_NEXT(p) (*(void * *)(p))     /* Retrieve the next element    */
+                                        /* in returned list.            */
+
+/* A filter function to control the scanning of dynamic libraries.      */
+/* If implemented, called by GC before registering a dynamic library    */
+/* (discovered by GC) section as a static data root (called only as     */
+/* a last reason not to register).  The filename of the library, the    */
+/* address and the length of the memory region (section) are passed.    */
+/* This routine should return nonzero if that region should be scanned. */
+/* Always called with the allocation lock held.  Depending on the       */
+/* platform, might be called with the "world" stopped.                  */
+typedef int (GC_CALLBACK * GC_has_static_roots_func)(
+                                        const char * /* dlpi_name */,
+                                        void * /* section_start */,
+                                        size_t /* section_size */);
+
+/* Register a new callback (a user-supplied filter) to control the      */
+/* scanning of dynamic libraries.  Replaces any previously registered   */
+/* callback.  May be 0 (means no filtering).  May be unused on some     */
+/* platforms (if the filtering is unimplemented or inappropriate).      */
+GC_API void GC_CALL GC_register_has_static_roots_callback(
+                                        GC_has_static_roots_func);
+
+#if defined(GC_WIN32_THREADS) && !defined(GC_PTHREADS)
+
+# if !defined(GC_NO_THREAD_DECLS) || defined(GC_BUILD)
+
+#   ifdef __cplusplus
+      } /* Including windows.h in an extern "C" context no longer works. */
+#   endif
+
+#   if !defined(_WIN32_WCE) && !defined(__CEGCC__)
+#     include <process.h> /* For _beginthreadex, _endthreadex */
+#   endif
+
+#   include <windows.h>
+
+#   ifdef __cplusplus
+      extern "C" {
+#   endif
+
+#   ifdef GC_UNDERSCORE_STDCALL
+      /* Explicitly prefix exported/imported WINAPI (__stdcall) symbols */
+      /* with '_' (underscore).  Might be useful if MinGW/x86 is used.  */
+#     define GC_CreateThread _GC_CreateThread
+#     define GC_ExitThread _GC_ExitThread
+#   endif
+
+#   if !defined(_UINTPTR_T) && !defined(_UINTPTR_T_DEFINED) \
+       && !defined(UINTPTR_MAX)
+      typedef GC_word GC_uintptr_t;
+#   else
+      typedef uintptr_t GC_uintptr_t;
+#   endif
+#   define GC_WIN32_SIZE_T GC_uintptr_t
+
+    /* All threads must be created using GC_CreateThread or             */
+    /* GC_beginthreadex, or must explicitly call GC_register_my_thread  */
+    /* (and call GC_unregister_my_thread before thread termination), so */
+    /* that they will be recorded in the thread table.  For backward    */
+    /* compatibility, it is possible to build the GC with GC_DLL        */
+    /* defined, and to call GC_use_threads_discovery.  This implicitly  */
+    /* registers all created threads, but appears to be less robust.    */
+    /* Currently the collector expects all threads to fall through and  */
+    /* terminate normally, or call GC_endthreadex() or GC_ExitThread,   */
+    /* so that the thread is properly unregistered.                     */
+    GC_API HANDLE WINAPI GC_CreateThread(
+                LPSECURITY_ATTRIBUTES /* lpThreadAttributes */,
+                GC_WIN32_SIZE_T /* dwStackSize */,
+                LPTHREAD_START_ROUTINE /* lpStartAddress */,
+                LPVOID /* lpParameter */, DWORD /* dwCreationFlags */,
+                LPDWORD /* lpThreadId */);
+
+#   ifndef DECLSPEC_NORETURN
+      /* Typically defined in winnt.h. */
+#     define DECLSPEC_NORETURN /* empty */
+#   endif
+
+    GC_API DECLSPEC_NORETURN void WINAPI GC_ExitThread(
+                                                DWORD /* dwExitCode */);
+
+#   if !defined(_WIN32_WCE) && !defined(__CEGCC__)
+      GC_API GC_uintptr_t GC_CALL GC_beginthreadex(
+                        void * /* security */, unsigned /* stack_size */,
+                        unsigned (__stdcall *)(void *),
+                        void * /* arglist */, unsigned /* initflag */,
+                        unsigned * /* thrdaddr */);
+
+      /* Note: _endthreadex() is not currently marked as no-return in   */
+      /* VC++ and MinGW headers, so we don't mark it neither.           */
+      GC_API void GC_CALL GC_endthreadex(unsigned /* retval */);
+#   endif /* !_WIN32_WCE */
+
+# endif /* !GC_NO_THREAD_DECLS */
+
+# ifdef GC_WINMAIN_REDIRECT
+    /* win32_threads.c implements the real WinMain(), which will start  */
+    /* a new thread to call GC_WinMain() after initializing the garbage */
+    /* collector.                                                       */
+#   define WinMain GC_WinMain
+# endif
+
+  /* For compatibility only. */
+# define GC_use_DllMain GC_use_threads_discovery
+
+# ifndef GC_NO_THREAD_REDIRECTS
+#   define CreateThread GC_CreateThread
+#   define ExitThread GC_ExitThread
+#   undef _beginthreadex
+#   define _beginthreadex GC_beginthreadex
+#   undef _endthreadex
+#   define _endthreadex GC_endthreadex
+/* #define _beginthread { > "Please use _beginthreadex instead of _beginthread" < } */
+# endif /* !GC_NO_THREAD_REDIRECTS */
+
+#endif /* GC_WIN32_THREADS */
+
+/* Public setter and getter for switching "unmap as much as possible"   */
+/* mode on(1) and off(0).  Has no effect unless unmapping is turned on. */
+/* Has no effect on implicitly-initiated garbage collections.  Initial  */
+/* value is controlled by GC_FORCE_UNMAP_ON_GCOLLECT.  The setter and   */
+/* getter are unsynchronized.                                           */
+GC_API void GC_CALL GC_set_force_unmap_on_gcollect(int);
+GC_API int GC_CALL GC_get_force_unmap_on_gcollect(void);
+
+/* Fully portable code should call GC_INIT() from the main program      */
+/* before making any other GC_ calls.  On most platforms this is a      */
+/* no-op and the collector self-initializes.  But a number of           */
+/* platforms make that too hard.                                        */
+/* A GC_INIT call is required if the collector is built with            */
+/* THREAD_LOCAL_ALLOC defined and the initial allocation call is not    */
+/* to GC_malloc() or GC_malloc_atomic().                                */
+
+#ifdef __CYGWIN32__
+  /* Similarly gnu-win32 DLLs need explicit initialization from the     */
+  /* main program, as does AIX.                                         */
+  extern int _data_start__[], _data_end__[], _bss_start__[], _bss_end__[];
+# define GC_DATASTART (_data_start__ < _bss_start__ ? \
+                       (void *)_data_start__ : (void *)_bss_start__)
+# define GC_DATAEND (_data_end__ > _bss_end__ ? \
+                     (void *)_data_end__ : (void *)_bss_end__)
+# define GC_INIT_CONF_ROOTS GC_add_roots(GC_DATASTART, GC_DATAEND); \
+                                 GC_gcollect() /* For blacklisting. */
+        /* Required at least if GC is in a DLL.  And doesn't hurt. */
+#elif defined(_AIX)
+  extern int _data[], _end[];
+# define GC_DATASTART ((void *)((ulong)_data))
+# define GC_DATAEND ((void *)((ulong)_end))
+# define GC_INIT_CONF_ROOTS GC_add_roots(GC_DATASTART, GC_DATAEND)
+#else
+# define GC_INIT_CONF_ROOTS /* empty */
+#endif
+
+#ifdef GC_DONT_EXPAND
+  /* Set GC_dont_expand to TRUE at start-up */
+# define GC_INIT_CONF_DONT_EXPAND GC_set_dont_expand(1)
+#else
+# define GC_INIT_CONF_DONT_EXPAND /* empty */
+#endif
+
+#ifdef GC_FORCE_UNMAP_ON_GCOLLECT
+  /* Turn on "unmap as much as possible on explicit GC" mode at start-up */
+# define GC_INIT_CONF_FORCE_UNMAP_ON_GCOLLECT \
+                GC_set_force_unmap_on_gcollect(1)
+#else
+# define GC_INIT_CONF_FORCE_UNMAP_ON_GCOLLECT /* empty */
+#endif
+
+#ifdef GC_MAX_RETRIES
+  /* Set GC_max_retries to the desired value at start-up */
+# define GC_INIT_CONF_MAX_RETRIES GC_set_max_retries(GC_MAX_RETRIES)
+#else
+# define GC_INIT_CONF_MAX_RETRIES /* empty */
+#endif
+
+#ifdef GC_FREE_SPACE_DIVISOR
+  /* Set GC_free_space_divisor to the desired value at start-up */
+# define GC_INIT_CONF_FREE_SPACE_DIVISOR \
+                GC_set_free_space_divisor(GC_FREE_SPACE_DIVISOR)
+#else
+# define GC_INIT_CONF_FREE_SPACE_DIVISOR /* empty */
+#endif
+
+#ifdef GC_FULL_FREQ
+  /* Set GC_full_freq to the desired value at start-up */
+# define GC_INIT_CONF_FULL_FREQ GC_set_full_freq(GC_FULL_FREQ)
+#else
+# define GC_INIT_CONF_FULL_FREQ /* empty */
+#endif
+
+#ifdef GC_TIME_LIMIT
+  /* Set GC_time_limit to the desired value at start-up */
+# define GC_INIT_CONF_TIME_LIMIT GC_set_time_limit(GC_TIME_LIMIT)
+#else
+# define GC_INIT_CONF_TIME_LIMIT /* empty */
+#endif
+
+#ifdef GC_MAXIMUM_HEAP_SIZE
+  /* Limit the heap size to the desired value (useful for debugging).   */
+  /* The limit could be overridden either at the program start-up by    */
+  /* the similar environment variable or anytime later by the           */
+  /* corresponding API function call.                                   */
+# define GC_INIT_CONF_MAXIMUM_HEAP_SIZE \
+                GC_set_max_heap_size(GC_MAXIMUM_HEAP_SIZE)
+#else
+# define GC_INIT_CONF_MAXIMUM_HEAP_SIZE /* empty */
+#endif
+
+#ifdef GC_IGNORE_WARN
+  /* Turn off all warnings at start-up (after GC initialization) */
+# define GC_INIT_CONF_IGNORE_WARN GC_set_warn_proc(GC_ignore_warn_proc)
+#else
+# define GC_INIT_CONF_IGNORE_WARN /* empty */
+#endif
+
+#ifdef GC_INITIAL_HEAP_SIZE
+  /* Set heap size to the desired value at start-up */
+# define GC_INIT_CONF_INITIAL_HEAP_SIZE \
+                { size_t heap_size = GC_get_heap_size(); \
+                  if (heap_size < (GC_INITIAL_HEAP_SIZE)) \
+                    (void)GC_expand_hp((GC_INITIAL_HEAP_SIZE) - heap_size); }
+#else
+# define GC_INIT_CONF_INITIAL_HEAP_SIZE /* empty */
+#endif
+
+/* Portable clients should call this at the program start-up.  More     */
+/* over, some platforms require this call to be done strictly from the  */
+/* primordial thread.                                                   */
+#define GC_INIT() { GC_INIT_CONF_DONT_EXPAND; /* pre-init */ \
+                    GC_INIT_CONF_FORCE_UNMAP_ON_GCOLLECT; \
+                    GC_INIT_CONF_MAX_RETRIES; \
+                    GC_INIT_CONF_FREE_SPACE_DIVISOR; \
+                    GC_INIT_CONF_FULL_FREQ; \
+                    GC_INIT_CONF_TIME_LIMIT; \
+                    GC_INIT_CONF_MAXIMUM_HEAP_SIZE; \
+                    GC_init(); /* real GC initialization */ \
+                    GC_INIT_CONF_ROOTS; /* post-init */ \
+                    GC_INIT_CONF_IGNORE_WARN; \
+                    GC_INIT_CONF_INITIAL_HEAP_SIZE; }
+
+/* win32S may not free all resources on process exit.   */
+/* This explicitly deallocates the heap.                */
+GC_API void GC_CALL GC_win32_free_heap(void);
+
+#if defined(_AMIGA) && !defined(GC_AMIGA_MAKINGLIB)
+  /* Allocation really goes through GC_amiga_allocwrapper_do    */
+# include "gc_amiga_redirects.h"
+#endif
+
+#ifdef __cplusplus
+  }  /* end of extern "C" */
+#endif
+
+#endif /* GC_H */
diff --git a/src/gc/bdwgc/include/gc_allocator.h b/src/gc/bdwgc/include/gc_allocator.h
new file mode 100644
index 0000000..367cfe2
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_allocator.h
@@ -0,0 +1,325 @@
+/*
+ * Copyright (c) 1996-1997
+ * Silicon Graphics Computer Systems, Inc.
+ *
+ * Permission to use, copy, modify, distribute and sell this software
+ * and its documentation for any purpose is hereby granted without fee,
+ * provided that the above copyright notice appear in all copies and
+ * that both that copyright notice and this permission notice appear
+ * in supporting documentation.  Silicon Graphics makes no
+ * representations about the suitability of this software for any
+ * purpose.  It is provided "as is" without express or implied warranty.
+ *
+ * Copyright (c) 2002
+ * Hewlett-Packard Company
+ *
+ * Permission to use, copy, modify, distribute and sell this software
+ * and its documentation for any purpose is hereby granted without fee,
+ * provided that the above copyright notice appear in all copies and
+ * that both that copyright notice and this permission notice appear
+ * in supporting documentation.  Hewlett-Packard Company makes no
+ * representations about the suitability of this software for any
+ * purpose.  It is provided "as is" without express or implied warranty.
+ */
+
+/*
+ * This implements standard-conforming allocators that interact with
+ * the garbage collector.  Gc_allocator<T> allocates garbage-collectible
+ * objects of type T.  Traceable_allocator<T> allocates objects that
+ * are not themselves garbage collected, but are scanned by the
+ * collector for pointers to collectible objects.  Traceable_alloc
+ * should be used for explicitly managed STL containers that may
+ * point to collectible objects.
+ *
+ * This code was derived from an earlier version of the GNU C++ standard
+ * library, which itself was derived from the SGI STL implementation.
+ *
+ * Ignore-off-page allocator: George T. Talbot
+ */
+
+#ifndef GC_ALLOCATOR_H
+
+#define GC_ALLOCATOR_H
+
+#include "gc.h"
+#include <new> // for placement new
+
+#if defined(__GNUC__)
+#  define GC_ATTR_UNUSED __attribute__((__unused__))
+#else
+#  define GC_ATTR_UNUSED
+#endif
+
+/* First some helpers to allow us to dispatch on whether or not a type
+ * is known to be pointer-free.
+ * These are private, except that the client may invoke the
+ * GC_DECLARE_PTRFREE macro.
+ */
+
+struct GC_true_type {};
+struct GC_false_type {};
+
+template <class GC_tp>
+struct GC_type_traits {
+  GC_false_type GC_is_ptr_free;
+};
+
+# define GC_DECLARE_PTRFREE(T) \
+template<> struct GC_type_traits<T> { GC_true_type GC_is_ptr_free; }
+
+GC_DECLARE_PTRFREE(char);
+GC_DECLARE_PTRFREE(signed char);
+GC_DECLARE_PTRFREE(unsigned char);
+GC_DECLARE_PTRFREE(signed short);
+GC_DECLARE_PTRFREE(unsigned short);
+GC_DECLARE_PTRFREE(signed int);
+GC_DECLARE_PTRFREE(unsigned int);
+GC_DECLARE_PTRFREE(signed long);
+GC_DECLARE_PTRFREE(unsigned long);
+GC_DECLARE_PTRFREE(float);
+GC_DECLARE_PTRFREE(double);
+GC_DECLARE_PTRFREE(long double);
+/* The client may want to add others.   */
+
+// In the following GC_Tp is GC_true_type if we are allocating a
+// pointer-free object.
+template <class GC_Tp>
+inline void * GC_selective_alloc(size_t n, GC_Tp, bool ignore_off_page) {
+    return ignore_off_page?GC_MALLOC_IGNORE_OFF_PAGE(n):GC_MALLOC(n);
+}
+
+template <>
+inline void * GC_selective_alloc<GC_true_type>(size_t n, GC_true_type,
+                                               bool ignore_off_page) {
+    return ignore_off_page? GC_MALLOC_ATOMIC_IGNORE_OFF_PAGE(n)
+                          : GC_MALLOC_ATOMIC(n);
+}
+
+/* Now the public gc_allocator<T> class:
+ */
+template <class GC_Tp>
+class gc_allocator {
+public:
+  typedef size_t     size_type;
+  typedef ptrdiff_t  difference_type;
+  typedef GC_Tp*       pointer;
+  typedef const GC_Tp* const_pointer;
+  typedef GC_Tp&       reference;
+  typedef const GC_Tp& const_reference;
+  typedef GC_Tp        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef gc_allocator<GC_Tp1> other;
+  };
+
+  gc_allocator()  {}
+    gc_allocator(const gc_allocator&) throw() {}
+# if !(GC_NO_MEMBER_TEMPLATES || 0 < _MSC_VER && _MSC_VER <= 1200)
+  // MSVC++ 6.0 do not support member templates
+  template <class GC_Tp1> gc_allocator(const gc_allocator<GC_Tp1>&) throw() {}
+# endif
+  ~gc_allocator() throw() {}
+
+  pointer address(reference GC_x) const { return &GC_x; }
+  const_pointer address(const_reference GC_x) const { return &GC_x; }
+
+  // GC_n is permitted to be 0.  The C++ standard says nothing about what
+  // the return value is when GC_n == 0.
+  GC_Tp* allocate(size_type GC_n, const void* = 0) {
+    GC_type_traits<GC_Tp> traits;
+    return static_cast<GC_Tp *>
+            (GC_selective_alloc(GC_n * sizeof(GC_Tp),
+                                traits.GC_is_ptr_free, false));
+  }
+
+  // __p is not permitted to be a null pointer.
+  void deallocate(pointer __p, size_type GC_ATTR_UNUSED GC_n)
+    { GC_FREE(__p); }
+
+  size_type max_size() const throw()
+    { return size_t(-1) / sizeof(GC_Tp); }
+
+  void construct(pointer __p, const GC_Tp& __val) { new(__p) GC_Tp(__val); }
+  void destroy(pointer __p) { __p->~GC_Tp(); }
+};
+
+template<>
+class gc_allocator<void> {
+  typedef size_t      size_type;
+  typedef ptrdiff_t   difference_type;
+  typedef void*       pointer;
+  typedef const void* const_pointer;
+  typedef void        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef gc_allocator<GC_Tp1> other;
+  };
+};
+
+
+template <class GC_T1, class GC_T2>
+inline bool operator==(const gc_allocator<GC_T1>&, const gc_allocator<GC_T2>&)
+{
+  return true;
+}
+
+template <class GC_T1, class GC_T2>
+inline bool operator!=(const gc_allocator<GC_T1>&, const gc_allocator<GC_T2>&)
+{
+  return false;
+}
+
+
+/* Now the public gc_allocator_ignore_off_page<T> class:
+ */
+template <class GC_Tp>
+class gc_allocator_ignore_off_page {
+public:
+  typedef size_t     size_type;
+  typedef ptrdiff_t  difference_type;
+  typedef GC_Tp*       pointer;
+  typedef const GC_Tp* const_pointer;
+  typedef GC_Tp&       reference;
+  typedef const GC_Tp& const_reference;
+  typedef GC_Tp        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef gc_allocator_ignore_off_page<GC_Tp1> other;
+  };
+
+  gc_allocator_ignore_off_page()  {}
+    gc_allocator_ignore_off_page(const gc_allocator_ignore_off_page&) throw() {}
+# if !(GC_NO_MEMBER_TEMPLATES || 0 < _MSC_VER && _MSC_VER <= 1200)
+  // MSVC++ 6.0 do not support member templates
+  template <class GC_Tp1>
+    gc_allocator_ignore_off_page(const gc_allocator_ignore_off_page<GC_Tp1>&)
+        throw() {}
+# endif
+  ~gc_allocator_ignore_off_page() throw() {}
+
+  pointer address(reference GC_x) const { return &GC_x; }
+  const_pointer address(const_reference GC_x) const { return &GC_x; }
+
+  // GC_n is permitted to be 0.  The C++ standard says nothing about what
+  // the return value is when GC_n == 0.
+  GC_Tp* allocate(size_type GC_n, const void* = 0) {
+    GC_type_traits<GC_Tp> traits;
+    return static_cast<GC_Tp *>
+            (GC_selective_alloc(GC_n * sizeof(GC_Tp),
+                                traits.GC_is_ptr_free, true));
+  }
+
+  // __p is not permitted to be a null pointer.
+  void deallocate(pointer __p, size_type GC_ATTR_UNUSED GC_n)
+    { GC_FREE(__p); }
+
+  size_type max_size() const throw()
+    { return size_t(-1) / sizeof(GC_Tp); }
+
+  void construct(pointer __p, const GC_Tp& __val) { new(__p) GC_Tp(__val); }
+  void destroy(pointer __p) { __p->~GC_Tp(); }
+};
+
+template<>
+class gc_allocator_ignore_off_page<void> {
+  typedef size_t      size_type;
+  typedef ptrdiff_t   difference_type;
+  typedef void*       pointer;
+  typedef const void* const_pointer;
+  typedef void        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef gc_allocator_ignore_off_page<GC_Tp1> other;
+  };
+};
+
+template <class GC_T1, class GC_T2>
+inline bool operator==(const gc_allocator_ignore_off_page<GC_T1>&, const gc_allocator_ignore_off_page<GC_T2>&)
+{
+  return true;
+}
+
+template <class GC_T1, class GC_T2>
+inline bool operator!=(const gc_allocator_ignore_off_page<GC_T1>&, const gc_allocator_ignore_off_page<GC_T2>&)
+{
+  return false;
+}
+
+/*
+ * And the public traceable_allocator class.
+ */
+
+// Note that we currently don't specialize the pointer-free case, since a
+// pointer-free traceable container doesn't make that much sense,
+// though it could become an issue due to abstraction boundaries.
+template <class GC_Tp>
+class traceable_allocator {
+public:
+  typedef size_t     size_type;
+  typedef ptrdiff_t  difference_type;
+  typedef GC_Tp*       pointer;
+  typedef const GC_Tp* const_pointer;
+  typedef GC_Tp&       reference;
+  typedef const GC_Tp& const_reference;
+  typedef GC_Tp        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef traceable_allocator<GC_Tp1> other;
+  };
+
+  traceable_allocator() throw() {}
+    traceable_allocator(const traceable_allocator&) throw() {}
+# if !(GC_NO_MEMBER_TEMPLATES || 0 < _MSC_VER && _MSC_VER <= 1200)
+  // MSVC++ 6.0 do not support member templates
+  template <class GC_Tp1> traceable_allocator
+          (const traceable_allocator<GC_Tp1>&) throw() {}
+# endif
+  ~traceable_allocator() throw() {}
+
+  pointer address(reference GC_x) const { return &GC_x; }
+  const_pointer address(const_reference GC_x) const { return &GC_x; }
+
+  // GC_n is permitted to be 0.  The C++ standard says nothing about what
+  // the return value is when GC_n == 0.
+  GC_Tp* allocate(size_type GC_n, const void* = 0) {
+    return static_cast<GC_Tp*>(GC_MALLOC_UNCOLLECTABLE(GC_n * sizeof(GC_Tp)));
+  }
+
+  // __p is not permitted to be a null pointer.
+  void deallocate(pointer __p, size_type GC_ATTR_UNUSED GC_n)
+    { GC_FREE(__p); }
+
+  size_type max_size() const throw()
+    { return size_t(-1) / sizeof(GC_Tp); }
+
+  void construct(pointer __p, const GC_Tp& __val) { new(__p) GC_Tp(__val); }
+  void destroy(pointer __p) { __p->~GC_Tp(); }
+};
+
+template<>
+class traceable_allocator<void> {
+  typedef size_t      size_type;
+  typedef ptrdiff_t   difference_type;
+  typedef void*       pointer;
+  typedef const void* const_pointer;
+  typedef void        value_type;
+
+  template <class GC_Tp1> struct rebind {
+    typedef traceable_allocator<GC_Tp1> other;
+  };
+};
+
+
+template <class GC_T1, class GC_T2>
+inline bool operator==(const traceable_allocator<GC_T1>&, const traceable_allocator<GC_T2>&)
+{
+  return true;
+}
+
+template <class GC_T1, class GC_T2>
+inline bool operator!=(const traceable_allocator<GC_T1>&, const traceable_allocator<GC_T2>&)
+{
+  return false;
+}
+
+#endif /* GC_ALLOCATOR_H */
diff --git a/src/gc/bdwgc/include/gc_amiga_redirects.h b/src/gc/bdwgc/include/gc_amiga_redirects.h
new file mode 100644
index 0000000..9e975c8
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_amiga_redirects.h
@@ -0,0 +1,30 @@
+#ifndef GC_AMIGA_REDIRECTS_H
+
+# define GC_AMIGA_REDIRECTS_H
+
+# if ( defined(_AMIGA) && !defined(GC_AMIGA_MAKINGLIB) )
+    extern void *GC_amiga_realloc(void *old_object,size_t new_size_in_bytes);
+#   define GC_realloc(a,b) GC_amiga_realloc(a,b)
+    extern void GC_amiga_set_toany(void (*func)(void));
+    extern int GC_amiga_free_space_divisor_inc;
+    extern void *(*GC_amiga_allocwrapper_do) \
+	(size_t size,void *(*AllocFunction)(size_t size2));
+#   define GC_malloc(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc)
+#   define GC_malloc_atomic(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_atomic)
+#   define GC_malloc_uncollectable(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_uncollectable)
+#   define GC_malloc_stubborn(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_stubborn)
+#   define GC_malloc_atomic_uncollectable(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_atomic_uncollectable)
+#   define GC_malloc_ignore_off_page(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_ignore_off_page)
+#   define GC_malloc_atomic_ignore_off_page(a) \
+	(*GC_amiga_allocwrapper_do)(a,GC_malloc_atomic_ignore_off_page)
+# endif /* _AMIGA && !GC_AMIGA_MAKINGLIB */
+
+#endif /* GC_AMIGA_REDIRECTS_H */
+
+
diff --git a/src/gc/bdwgc/include/gc_backptr.h b/src/gc/bdwgc/include/gc_backptr.h
new file mode 100644
index 0000000..6256f41
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_backptr.h
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/*
+ * This is a simple API to implement pointer back tracing, i.e.
+ * to answer questions such as "who is pointing to this" or
+ * "why is this object being retained by the collector"
+ *
+ * This API assumes that we have an ANSI C compiler.
+ *
+ * Most of these calls yield useful information on only after
+ * a garbage collection.  Usually the client will first force
+ * a full collection and then gather information, preferably
+ * before much intervening allocation.
+ *
+ * The implementation of the interface is only about 99.9999%
+ * correct.  It is intended to be good enough for profiling,
+ * but is not intended to be used with production code.
+ *
+ * Results are likely to be much more useful if all allocation is
+ * accomplished through the debugging allocators.
+ *
+ * The implementation idea is due to A. Demers.
+ */
+
+#ifndef GC_BACKPTR_H
+#define GC_BACKPTR_H
+
+#ifndef GC_H
+# include "gc.h"
+#endif
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+/* Store information about the object referencing dest in *base_p     */
+/* and *offset_p.                                                     */
+/* If multiple objects or roots point to dest, the one reported       */
+/* will be the last on used by the garbage collector to trace the     */
+/* object.                                                            */
+/*   source is root ==> *base_p = address, *offset_p = 0              */
+/*   source is heap object ==> *base_p != 0, *offset_p = offset       */
+/*   Returns 1 on success, 0 if source couldn't be determined.        */
+/* Dest can be any address within a heap object.                      */
+typedef enum {
+    GC_UNREFERENCED,    /* No reference info available.         */
+    GC_NO_SPACE,        /* Dest not allocated with debug alloc. */
+    GC_REFD_FROM_ROOT,  /* Referenced directly by root *base_p. */
+    GC_REFD_FROM_REG,   /* Referenced from a register, i.e.     */
+                        /* a root without an address.           */
+    GC_REFD_FROM_HEAP,  /* Referenced from another heap obj.    */
+    GC_FINALIZER_REFD   /* Finalizable and hence accessible.    */
+} GC_ref_kind;
+
+GC_API GC_ref_kind GC_CALL GC_get_back_ptr_info(void * /* dest */,
+                                                void ** /* base_p */,
+                                                size_t * /* offset_p */);
+
+/* Generate a random heap address.            */
+/* The resulting address is in the heap, but  */
+/* not necessarily inside a valid object.     */
+GC_API void * GC_CALL GC_generate_random_heap_address(void);
+
+/* Generate a random address inside a valid marked heap object. */
+GC_API void * GC_CALL GC_generate_random_valid_address(void);
+
+/* Force a garbage collection and generate a backtrace from a   */
+/* random heap address.                                         */
+/* This uses the GC logging mechanism (GC_printf) to produce    */
+/* output.  It can often be called from a debugger.  The        */
+/* source in dbg_mlc.c also serves as a sample client.          */
+GC_API void GC_CALL GC_generate_random_backtrace(void);
+
+/* Print a backtrace from a specific address.  Used by the      */
+/* above.  The client should call GC_gcollect() immediately     */
+/* before invocation.                                           */
+GC_API void GC_CALL GC_print_backtrace(void *);
+
+#ifdef __cplusplus
+  } /* end of extern "C" */
+#endif
+
+#endif /* GC_BACKPTR_H */
diff --git a/src/gc/bdwgc/include/gc_config_macros.h b/src/gc/bdwgc/include/gc_config_macros.h
new file mode 100644
index 0000000..52b7d9d
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_config_macros.h
@@ -0,0 +1,349 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* This should never be included directly; it is included only from gc.h. */
+/* We separate it only to make gc.h more suitable as documentation.       */
+#if defined(GC_H)
+
+/* Some tests for old macros.  These violate our namespace rules and    */
+/* will disappear shortly.  Use the GC_ names.                          */
+#if defined(SOLARIS_THREADS) || defined(_SOLARIS_THREADS) \
+    || defined(_SOLARIS_PTHREADS) || defined(GC_SOLARIS_PTHREADS)
+  /* We no longer support old style Solaris threads.            */
+  /* GC_SOLARIS_THREADS now means pthreads.                     */
+# ifndef GC_SOLARIS_THREADS
+#   define GC_SOLARIS_THREADS
+# endif
+#endif
+#if defined(IRIX_THREADS)
+# define GC_IRIX_THREADS
+#endif
+#if defined(DGUX_THREADS) && !defined(GC_DGUX386_THREADS)
+# define GC_DGUX386_THREADS
+#endif
+#if defined(AIX_THREADS)
+# define GC_AIX_THREADS
+#endif
+#if defined(HPUX_THREADS)
+# define GC_HPUX_THREADS
+#endif
+#if defined(OSF1_THREADS)
+# define GC_OSF1_THREADS
+#endif
+#if defined(LINUX_THREADS)
+# define GC_LINUX_THREADS
+#endif
+#if defined(WIN32_THREADS)
+# define GC_WIN32_THREADS
+#endif
+#if defined(RTEMS_THREADS)
+# define GC_RTEMS_PTHREADS
+#endif
+#if defined(USE_LD_WRAP)
+# define GC_USE_LD_WRAP
+#endif
+
+#if defined(GC_WIN32_PTHREADS) && !defined(GC_WIN32_THREADS)
+  /* Using pthreads-w32 library. */
+# define GC_WIN32_THREADS
+#endif
+
+#if defined(GC_AIX_THREADS) || defined(GC_DARWIN_THREADS) \
+    || defined(GC_DGUX386_THREADS) || defined(GC_FREEBSD_THREADS) \
+    || defined(GC_GNU_THREADS) || defined(GC_HPUX_THREADS) \
+    || defined(GC_IRIX_THREADS) || defined(GC_LINUX_THREADS) \
+    || defined(GC_NETBSD_THREADS) || defined(GC_OPENBSD_THREADS) \
+    || defined(GC_OSF1_THREADS) || defined(GC_SOLARIS_THREADS) \
+    || defined(GC_WIN32_THREADS) || defined(GC_RTEMS_PTHREADS)
+# ifndef GC_THREADS
+#   define GC_THREADS
+# endif
+#elif defined(GC_THREADS)
+# if defined(__linux__)
+#   define GC_LINUX_THREADS
+# endif
+# if !defined(__linux__) && (defined(_PA_RISC1_1) || defined(_PA_RISC2_0) \
+                             || defined(hppa) || defined(__HPPA)) \
+     || (defined(__ia64) && defined(_HPUX_SOURCE))
+#   define GC_HPUX_THREADS
+# endif
+# if !defined(__linux__) && (defined(__alpha) || defined(__alpha__))
+#   define GC_OSF1_THREADS
+# endif
+# if defined(__mips) && !defined(__linux__)
+#   define GC_IRIX_THREADS
+# endif
+# if defined(__sparc) && !defined(__linux__) \
+     || defined(sun) && (defined(i386) || defined(__i386__) \
+                         || defined(__amd64__))
+#   define GC_SOLARIS_THREADS
+# elif defined(__APPLE__) && defined(__MACH__)
+#   define GC_DARWIN_THREADS
+# elif defined(__OpenBSD__)
+#   define GC_OPENBSD_THREADS
+# elif !defined(GC_LINUX_THREADS) && !defined(GC_HPUX_THREADS) \
+       && !defined(GC_OSF1_THREADS) && !defined(GC_IRIX_THREADS)
+    /* FIXME: Should we really need for FreeBSD and NetBSD to check     */
+    /* that no other GC_xxx_THREADS macro is set?                       */
+#   if defined(__FreeBSD__) || defined(__DragonFly__)
+#     define GC_FREEBSD_THREADS
+#   elif defined(__NetBSD__)
+#     define GC_NETBSD_THREADS
+#   endif
+# endif
+# if defined(DGUX) && (defined(i386) || defined(__i386__))
+#   define GC_DGUX386_THREADS
+# endif
+# if defined(_AIX)
+#   define GC_AIX_THREADS
+# endif
+# if (defined(_WIN32) || defined(_MSC_VER) || defined(__BORLANDC__) \
+      || defined(__CYGWIN32__) || defined(__CYGWIN__) || defined(__CEGCC__) \
+      || defined(_WIN32_WCE) || defined(__MINGW32__)) \
+     && !defined(GC_WIN32_THREADS)
+    /* Either posix or native Win32 threads. */
+#   define GC_WIN32_THREADS
+# endif
+# if defined(__rtems__) && (defined(i386) || defined(__i386__))
+#   define GC_RTEMS_PTHREADS
+# endif
+#endif /* GC_THREADS */
+
+#undef GC_PTHREADS
+#if (!defined(GC_WIN32_THREADS) || defined(GC_WIN32_PTHREADS) \
+     || defined(GC_RTEMS_PTHREADS) || defined(__CYGWIN32__) \
+     || defined(__CYGWIN__)) && defined(GC_THREADS)
+  /* Posix threads. */
+# define GC_PTHREADS
+#endif
+
+#if !defined(_PTHREADS) && defined(GC_NETBSD_THREADS)
+# define _PTHREADS
+#endif
+
+#if defined(GC_DGUX386_THREADS) && !defined(_POSIX4A_DRAFT10_SOURCE)
+# define _POSIX4A_DRAFT10_SOURCE 1
+#endif
+
+#if !defined(_REENTRANT) && defined(GC_PTHREADS) && !defined(GC_WIN32_THREADS)
+  /* Better late than never.  This fails if system headers that depend  */
+  /* on this were previously included.                                  */
+# define _REENTRANT
+#endif
+
+#define __GC
+#if !defined(_WIN32_WCE) || defined(__GNUC__)
+# include <stddef.h>
+# if defined(__MINGW32__) && !defined(_WIN32_WCE)
+#   include <stdint.h>
+    /* We mention uintptr_t.                                            */
+    /* Perhaps this should be included in pure msft environments        */
+    /* as well?                                                         */
+# endif
+#else /* _WIN32_WCE */
+  /* Yet more kludges for WinCE.        */
+# include <stdlib.h> /* size_t is defined here */
+# ifndef _PTRDIFF_T_DEFINED
+    /* ptrdiff_t is not defined */
+#   define _PTRDIFF_T_DEFINED
+    typedef long ptrdiff_t;
+# endif
+#endif /* _WIN32_WCE */
+
+#if defined(_DLL) && !defined(GC_NOT_DLL) && !defined(GC_DLL) \
+        && !defined(__GNUC__)
+# define GC_DLL
+#endif
+
+#if defined(GC_DLL) && !defined(GC_API)
+
+# if defined(__MINGW32__) || defined(__CEGCC__)
+#   ifdef GC_BUILD
+#     define GC_API __declspec(dllexport)
+#   else
+#     define GC_API __declspec(dllimport)
+#   endif
+
+# elif defined(_MSC_VER) || defined(__DMC__) || defined(__BORLANDC__) \
+        || defined(__CYGWIN__)
+#   ifdef GC_BUILD
+#     define GC_API extern __declspec(dllexport)
+#   else
+#     define GC_API __declspec(dllimport)
+#   endif
+
+# elif defined(__WATCOMC__)
+#   ifdef GC_BUILD
+#     define GC_API extern __declspec(dllexport)
+#   else
+#     define GC_API extern __declspec(dllimport)
+#   endif
+
+# elif defined(__GNUC__)
+    /* Only matters if used in conjunction with -fvisibility=hidden option. */
+#   if __GNUC__ >= 4 && defined(GC_BUILD)
+#     define GC_API extern __attribute__((__visibility__("default")))
+#   endif
+# endif
+#endif /* GC_DLL */
+
+#ifndef GC_API
+# define GC_API extern
+#endif
+
+#ifndef GC_CALL
+# define GC_CALL
+#endif
+
+#ifndef GC_CALLBACK
+# define GC_CALLBACK GC_CALL
+#endif
+
+#ifndef GC_ATTR_MALLOC
+  /* 'malloc' attribute should be used for all malloc-like functions    */
+  /* (to tell the compiler that a function may be treated as if any     */
+  /* non-NULL pointer it returns cannot alias any other pointer valid   */
+  /* when the function returns).  If the client code violates this rule */
+  /* by using custom GC_oom_func then define GC_OOM_FUNC_RETURNS_ALIAS. */
+# if !defined(GC_OOM_FUNC_RETURNS_ALIAS) && defined(__GNUC__) \
+        && (__GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >= 1))
+#   define GC_ATTR_MALLOC __attribute__((__malloc__))
+# else
+#   define GC_ATTR_MALLOC
+# endif
+#endif
+
+#ifndef GC_ATTR_ALLOC_SIZE
+  /* 'alloc_size' attribute improves __builtin_object_size correctness. */
+  /* Only single-argument form of 'alloc_size' attribute is used.       */
+# if defined(__GNUC__) && (__GNUC__ > 4 \
+        || (__GNUC__ == 4 && __GNUC_MINOR__ >= 3 && !defined(__ICC)))
+#   define GC_ATTR_ALLOC_SIZE(argnum) __attribute__((__alloc_size__(argnum)))
+# else
+#   define GC_ATTR_ALLOC_SIZE(argnum)
+# endif
+#endif
+
+#if defined(__sgi) && !defined(__GNUC__) && _COMPILER_VERSION >= 720
+# define GC_ADD_CALLER
+# define GC_RETURN_ADDR (GC_word)__return_address
+#endif
+
+#if defined(__linux__) || defined(__GLIBC__)
+# if !defined(__native_client__)
+#   include <features.h>
+# endif
+# if (__GLIBC__ == 2 && __GLIBC_MINOR__ >= 1 || __GLIBC__ > 2) \
+        && !defined(__ia64__) && !defined(__UCLIBC__) \
+        && !defined(GC_HAVE_BUILTIN_BACKTRACE)
+#   define GC_HAVE_BUILTIN_BACKTRACE
+# endif
+# if defined(__i386__) || defined(__amd64__) || defined(__x86_64__)
+#   define GC_CAN_SAVE_CALL_STACKS
+# endif
+#endif /* GLIBC */
+
+#if defined(_MSC_VER) && _MSC_VER >= 1200 /* version 12.0+ (MSVC 6.0+) */ \
+        && !defined(_AMD64_) && !defined(_M_X64) && !defined(_WIN32_WCE) \
+        && !defined(GC_HAVE_NO_BUILTIN_BACKTRACE) \
+        && !defined(GC_HAVE_BUILTIN_BACKTRACE)
+# define GC_HAVE_BUILTIN_BACKTRACE
+#endif
+
+#if defined(GC_HAVE_BUILTIN_BACKTRACE) && !defined(GC_CAN_SAVE_CALL_STACKS)
+# define GC_CAN_SAVE_CALL_STACKS
+#endif
+
+#if defined(__sparc__)
+# define GC_CAN_SAVE_CALL_STACKS
+#endif
+
+/* If we're on a platform on which we can't save call stacks, but       */
+/* gcc is normally used, we go ahead and define GC_ADD_CALLER.          */
+/* We make this decision independent of whether gcc is actually being   */
+/* used, in order to keep the interface consistent, and allow mixing    */
+/* of compilers.                                                        */
+/* This may also be desirable if it is possible but expensive to        */
+/* retrieve the call chain.                                             */
+#if (defined(__linux__) || defined(__NetBSD__) || defined(__OpenBSD__) \
+     || defined(__FreeBSD__) || defined(__DragonFly__) \
+     || defined(PLATFORM_ANDROID)) && !defined(GC_CAN_SAVE_CALL_STACKS)
+# define GC_ADD_CALLER
+# if __GNUC__ >= 3 || (__GNUC__ == 2 && __GNUC_MINOR__ >= 95)
+    /* gcc knows how to retrieve return address, but we don't know      */
+    /* how to generate call stacks.                                     */
+#   define GC_RETURN_ADDR (GC_word)__builtin_return_address(0)
+#   if (__GNUC__ >= 4) && (defined(__i386__) || defined(__amd64__) \
+        || defined(__x86_64__) /* and probably others... */)
+#     define GC_RETURN_ADDR_PARENT \
+        (GC_word)__builtin_extract_return_addr(__builtin_return_address(1))
+#   endif
+# else
+    /* Just pass 0 for gcc compatibility.       */
+#   define GC_RETURN_ADDR 0
+# endif
+#endif /* !GC_CAN_SAVE_CALL_STACKS */
+
+#ifdef GC_PTHREADS
+
+# if (defined(GC_DARWIN_THREADS) || defined(GC_WIN32_PTHREADS) \
+      || defined(__native_client__) || defined(GC_RTEMS_PTHREADS)) \
+      && !defined(GC_NO_DLOPEN)
+    /* Either there is no dlopen() or we do not need to intercept it.   */
+#   define GC_NO_DLOPEN
+# endif
+
+# if (defined(GC_DARWIN_THREADS) || defined(GC_WIN32_PTHREADS) \
+      || defined(GC_OPENBSD_THREADS) || defined(__native_client__)) \
+     && !defined(GC_NO_PTHREAD_SIGMASK)
+    /* Either there is no pthread_sigmask() or no need to intercept it. */
+#   define GC_NO_PTHREAD_SIGMASK
+# endif
+
+# if defined(__native_client__)
+    /* At present, NaCl pthread_create() prototype does not have        */
+    /* "const" for its "attr" argument; also, NaCl pthread_exit() one   */
+    /* does not have "noreturn" attribute.                              */
+#   ifndef GC_PTHREAD_CREATE_CONST
+#     define GC_PTHREAD_CREATE_CONST /* empty */
+#   endif
+#   ifndef GC_PTHREAD_EXIT_ATTRIBUTE
+#     define GC_PTHREAD_EXIT_ATTRIBUTE /* empty */
+#   endif
+# endif
+
+# if !defined(GC_PTHREAD_EXIT_ATTRIBUTE) && !defined(PLATFORM_ANDROID) \
+     && (defined(GC_LINUX_THREADS) || defined(GC_SOLARIS_THREADS))
+    /* Intercept pthread_exit on Linux and Solaris.     */
+#   if defined(__GNUC__) /* since GCC v2.7 */
+#     define GC_PTHREAD_EXIT_ATTRIBUTE __attribute__((__noreturn__))
+#   elif defined(__NORETURN) /* used in Solaris */
+#     define GC_PTHREAD_EXIT_ATTRIBUTE __NORETURN
+#   else
+#     define GC_PTHREAD_EXIT_ATTRIBUTE /* empty */
+#   endif
+# endif
+
+# if (!defined(GC_PTHREAD_EXIT_ATTRIBUTE) || defined(__native_client__)) \
+     && !defined(GC_NO_PTHREAD_CANCEL)
+    /* Either there is no pthread_cancel() or no need to intercept it.  */
+#   define GC_NO_PTHREAD_CANCEL
+# endif
+
+#endif /* GC_PTHREADS */
+
+#endif
diff --git a/src/gc/bdwgc/include/gc_cpp.h b/src/gc/bdwgc/include/gc_cpp.h
new file mode 100644
index 0000000..e532ea4
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_cpp.h
@@ -0,0 +1,406 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program for any
+ * purpose, provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is
+ * granted, provided the above notices are retained, and a notice that
+ * the code was modified is included with the above copyright notice.
+ */
+
+#ifndef GC_CPP_H
+#define GC_CPP_H
+
+/****************************************************************************
+C++ Interface to the Boehm Collector
+
+    John R. Ellis and Jesse Hull
+
+This interface provides access to the Boehm collector.  It provides
+basic facilities similar to those described in "Safe, Efficient
+Garbage Collection for C++", by John R. Ellis and David L. Detlefs
+(ftp://ftp.parc.xerox.com/pub/ellis/gc).
+
+All heap-allocated objects are either "collectible" or
+"uncollectible".  Programs must explicitly delete uncollectible
+objects, whereas the garbage collector will automatically delete
+collectible objects when it discovers them to be inaccessible.
+Collectible objects may freely point at uncollectible objects and vice
+versa.
+
+Objects allocated with the built-in "::operator new" are uncollectible.
+
+Objects derived from class "gc" are collectible.  For example:
+
+    class A: public gc {...};
+    A* a = new A;       // a is collectible.
+
+Collectible instances of non-class types can be allocated using the GC
+(or UseGC) placement:
+
+    typedef int A[ 10 ];
+    A* a = new (GC) A;
+
+Uncollectible instances of classes derived from "gc" can be allocated
+using the NoGC placement:
+
+    class A: public gc {...};
+    A* a = new (NoGC) A;   // a is uncollectible.
+
+The new(PointerFreeGC) syntax allows the allocation of collectible
+objects that are not scanned by the collector.  This useful if you
+are allocating compressed data, bitmaps, or network packets.  (In
+the latter case, it may remove danger of unfriendly network packets
+intentionally containing values that cause spurious memory retention.)
+
+Both uncollectible and collectible objects can be explicitly deleted
+with "delete", which invokes an object's destructors and frees its
+storage immediately.
+
+A collectible object may have a clean-up function, which will be
+invoked when the collector discovers the object to be inaccessible.
+An object derived from "gc_cleanup" or containing a member derived
+from "gc_cleanup" has a default clean-up function that invokes the
+object's destructors.  Explicit clean-up functions may be specified as
+an additional placement argument:
+
+    A* a = ::new (GC, MyCleanup) A;
+
+An object is considered "accessible" by the collector if it can be
+reached by a path of pointers from static variables, automatic
+variables of active functions, or from some object with clean-up
+enabled; pointers from an object to itself are ignored.
+
+Thus, if objects A and B both have clean-up functions, and A points at
+B, B is considered accessible.  After A's clean-up is invoked and its
+storage released, B will then become inaccessible and will have its
+clean-up invoked.  If A points at B and B points to A, forming a
+cycle, then that's considered a storage leak, and neither will be
+collectible.  See the interface gc.h for low-level facilities for
+handling such cycles of objects with clean-up.
+
+The collector cannot guarantee that it will find all inaccessible
+objects.  In practice, it finds almost all of them.
+
+
+Cautions:
+
+1. Be sure the collector has been augmented with "make c++" or
+"--enable-cplusplus".
+
+2.  If your compiler supports the new "operator new[]" syntax, then
+add -DGC_OPERATOR_NEW_ARRAY to the Makefile.
+
+If your compiler doesn't support "operator new[]", beware that an
+array of type T, where T is derived from "gc", may or may not be
+allocated as a collectible object (it depends on the compiler).  Use
+the explicit GC placement to make the array collectible.  For example:
+
+    class A: public gc {...};
+    A* a1 = new A[ 10 ];        // collectible or uncollectible?
+    A* a2 = new (GC) A[ 10 ];   // collectible.
+
+3. The destructors of collectible arrays of objects derived from
+"gc_cleanup" will not be invoked properly.  For example:
+
+    class A: public gc_cleanup {...};
+    A* a = new (GC) A[ 10 ];    // destructors not invoked correctly
+
+Typically, only the destructor for the first element of the array will
+be invoked when the array is garbage-collected.  To get all the
+destructors of any array executed, you must supply an explicit
+clean-up function:
+
+    A* a = new (GC, MyCleanUp) A[ 10 ];
+
+(Implementing clean-up of arrays correctly, portably, and in a way
+that preserves the correct exception semantics requires a language
+extension, e.g. the "gc" keyword.)
+
+4. Compiler bugs (now hopefully history):
+
+* Solaris 2's CC (SC3.0) doesn't implement t->~T() correctly, so the
+destructors of classes derived from gc_cleanup won't be invoked.
+You'll have to explicitly register a clean-up function with
+new-placement syntax.
+
+* Evidently cfront 3.0 does not allow destructors to be explicitly
+invoked using the ANSI-conforming syntax t->~T().  If you're using
+cfront 3.0, you'll have to comment out the class gc_cleanup, which
+uses explicit invocation.
+
+5. GC name conflicts:
+
+Many other systems seem to use the identifier "GC" as an abbreviation
+for "Graphics Context".  Since version 5.0, GC placement has been replaced
+by UseGC.  GC is an alias for UseGC, unless GC_NAME_CONFLICT is defined.
+
+****************************************************************************/
+
+#include "gc.h"
+
+#ifndef THINK_CPLUS
+#  define GC_cdecl GC_CALLBACK
+#else
+#  define GC_cdecl _cdecl
+#endif
+
+#if ! defined( GC_NO_OPERATOR_NEW_ARRAY ) \
+    && !defined(_ENABLE_ARRAYNEW) /* Digimars */ \
+    && (defined(__BORLANDC__) && (__BORLANDC__ < 0x450) \
+        || (defined(__GNUC__) && \
+            (__GNUC__ < 2 || __GNUC__ == 2 && __GNUC_MINOR__ < 6)) \
+        || (defined(_MSC_VER) && _MSC_VER <= 1020) \
+        || (defined(__WATCOMC__) && __WATCOMC__ < 1050))
+#   define GC_NO_OPERATOR_NEW_ARRAY
+#endif
+
+#if !defined(GC_NO_OPERATOR_NEW_ARRAY) && !defined(GC_OPERATOR_NEW_ARRAY)
+#   define GC_OPERATOR_NEW_ARRAY
+#endif
+
+#if (!defined(__BORLANDC__) || __BORLANDC__ > 0x0620) \
+    && ! defined ( __sgi ) && ! defined( __WATCOMC__ ) \
+    && (!defined(_MSC_VER) || _MSC_VER > 1020)
+#  define GC_PLACEMENT_DELETE
+#endif
+
+enum GCPlacement {UseGC,
+#ifndef GC_NAME_CONFLICT
+                  GC=UseGC,
+#endif
+                  NoGC, PointerFreeGC};
+
+class gc {public:
+    inline void* operator new( size_t size );
+    inline void* operator new( size_t size, GCPlacement gcp );
+    inline void* operator new( size_t size, void *p );
+        /* Must be redefined here, since the other overloadings */
+        /* hide the global definition.                          */
+    inline void operator delete( void* obj );
+#   ifdef GC_PLACEMENT_DELETE
+      inline void operator delete( void*, GCPlacement );
+        /* called if construction fails.        */
+      inline void operator delete( void*, void* );
+#   endif
+
+#ifdef GC_OPERATOR_NEW_ARRAY
+    inline void* operator new[]( size_t size );
+    inline void* operator new[]( size_t size, GCPlacement gcp );
+    inline void* operator new[]( size_t size, void *p );
+    inline void operator delete[]( void* obj );
+#   ifdef GC_PLACEMENT_DELETE
+      inline void operator delete[]( void*, GCPlacement );
+      inline void operator delete[]( void*, void* );
+#   endif
+#endif /* GC_OPERATOR_NEW_ARRAY */
+    };
+    /*
+    Instances of classes derived from "gc" will be allocated in the
+    collected heap by default, unless an explicit NoGC placement is
+    specified. */
+
+class gc_cleanup: virtual public gc {public:
+    inline gc_cleanup();
+    inline virtual ~gc_cleanup();
+private:
+    inline static void GC_cdecl cleanup( void* obj, void* clientData );};
+    /*
+    Instances of classes derived from "gc_cleanup" will be allocated
+    in the collected heap by default.  When the collector discovers an
+    inaccessible object derived from "gc_cleanup" or containing a
+    member derived from "gc_cleanup", its destructors will be
+    invoked. */
+
+extern "C" {
+    typedef void (GC_CALLBACK * GCCleanUpFunc)( void* obj, void* clientData );
+}
+
+#ifdef _MSC_VER
+  // Disable warning that "no matching operator delete found; memory will
+  // not be freed if initialization throws an exception"
+# pragma warning(disable:4291)
+#endif
+
+inline void* operator new(
+    size_t size,
+    GCPlacement gcp,
+    GCCleanUpFunc cleanup = 0,
+    void* clientData = 0 );
+    /*
+    Allocates a collectible or uncollectible object, according to the
+    value of "gcp".
+
+    For collectible objects, if "cleanup" is non-null, then when the
+    allocated object "obj" becomes inaccessible, the collector will
+    invoke the function "cleanup( obj, clientData )" but will not
+    invoke the object's destructors.  It is an error to explicitly
+    delete an object allocated with a non-null "cleanup".
+
+    It is an error to specify a non-null "cleanup" with NoGC or for
+    classes derived from "gc_cleanup" or containing members derived
+    from "gc_cleanup". */
+
+#ifdef GC_PLACEMENT_DELETE
+  inline void operator delete( void*, GCPlacement, GCCleanUpFunc, void * );
+#endif
+
+#ifdef _MSC_VER
+ /** This ensures that the system default operator new[] doesn't get
+  *  undefined, which is what seems to happen on VC++ 6 for some reason
+  *  if we define a multi-argument operator new[].
+  *  There seems to be no way to redirect new in this environment without
+  *  including this everywhere.
+  */
+# if _MSC_VER > 1020
+    void *operator new[]( size_t size );
+
+    void operator delete[](void* obj);
+# endif
+
+  void* operator new(size_t size);
+
+  void operator delete(void* obj);
+
+  // This new operator is used by VC++ in case of Debug builds !
+  void* operator new(  size_t size,
+                      int ,//nBlockUse,
+                      const char * szFileName,
+                      int nLine );
+#endif /* _MSC_VER */
+
+#ifdef GC_OPERATOR_NEW_ARRAY
+
+  inline void* operator new[](
+    size_t size,
+    GCPlacement gcp,
+    GCCleanUpFunc cleanup = 0,
+    void* clientData = 0 );
+    /*
+    The operator new for arrays, identical to the above. */
+
+#endif /* GC_OPERATOR_NEW_ARRAY */
+
+/****************************************************************************
+
+Inline implementation
+
+****************************************************************************/
+
+inline void* gc::operator new( size_t size ) {
+    return GC_MALLOC( size );}
+
+inline void* gc::operator new( size_t size, GCPlacement gcp ) {
+    if (gcp == UseGC)
+        return GC_MALLOC( size );
+    else if (gcp == PointerFreeGC)
+        return GC_MALLOC_ATOMIC( size );
+    else
+        return GC_MALLOC_UNCOLLECTABLE( size );}
+
+inline void* gc::operator new( size_t size, void *p ) {
+    return p;}
+
+inline void gc::operator delete( void* obj ) {
+    GC_FREE( obj );}
+
+#ifdef GC_PLACEMENT_DELETE
+  inline void gc::operator delete( void*, void* ) {}
+
+  inline void gc::operator delete( void* p, GCPlacement gcp ) {
+    GC_FREE(p);
+  }
+#endif
+
+#ifdef GC_OPERATOR_NEW_ARRAY
+  inline void* gc::operator new[]( size_t size ) {
+    return gc::operator new( size );}
+
+  inline void* gc::operator new[]( size_t size, GCPlacement gcp ) {
+    return gc::operator new( size, gcp );}
+
+  inline void* gc::operator new[]( size_t size, void *p ) {
+    return p;}
+
+  inline void gc::operator delete[]( void* obj ) {
+    gc::operator delete( obj );}
+
+# ifdef GC_PLACEMENT_DELETE
+    inline void gc::operator delete[]( void*, void* ) {}
+
+    inline void gc::operator delete[]( void* p, GCPlacement gcp ) {
+      gc::operator delete(p); }
+# endif
+#endif /* GC_OPERATOR_NEW_ARRAY */
+
+inline gc_cleanup::~gc_cleanup() {
+    GC_register_finalizer_ignore_self( GC_base(this), 0, 0, 0, 0 );}
+
+inline void GC_CALLBACK gc_cleanup::cleanup( void* obj, void* displ ) {
+    ((gc_cleanup*) ((char*) obj + (ptrdiff_t) displ))->~gc_cleanup();}
+
+inline gc_cleanup::gc_cleanup() {
+    GC_finalization_proc oldProc;
+    void* oldData;
+    void* base = GC_base( (void *) this );
+    if (0 != base)  {
+      // Don't call the debug version, since this is a real base address.
+      GC_register_finalizer_ignore_self(
+        base, (GC_finalization_proc)cleanup, (void*)((char*)this - (char*)base),
+        &oldProc, &oldData );
+      if (0 != oldProc) {
+        GC_register_finalizer_ignore_self( base, oldProc, oldData, 0, 0 );}}}
+
+inline void* operator new(
+    size_t size,
+    GCPlacement gcp,
+    GCCleanUpFunc cleanup,
+    void* clientData )
+{
+    void* obj;
+
+    if (gcp == UseGC) {
+        obj = GC_MALLOC( size );
+        if (cleanup != 0)
+            GC_REGISTER_FINALIZER_IGNORE_SELF(
+                obj, cleanup, clientData, 0, 0 );}
+    else if (gcp == PointerFreeGC) {
+        obj = GC_MALLOC_ATOMIC( size );}
+    else {
+        obj = GC_MALLOC_UNCOLLECTABLE( size );};
+    return obj;}
+
+#ifdef GC_PLACEMENT_DELETE
+  inline void operator delete (
+    void *p,
+    GCPlacement gcp,
+    GCCleanUpFunc cleanup,
+    void* clientData )
+  {
+    GC_FREE(p);
+  }
+#endif /* GC_PLACEMENT_DELETE */
+
+#ifdef GC_OPERATOR_NEW_ARRAY
+  inline void* operator new[](
+    size_t size,
+    GCPlacement gcp,
+    GCCleanUpFunc cleanup,
+    void* clientData )
+  {
+    return ::operator new( size, gcp, cleanup, clientData );
+  }
+#endif /* GC_OPERATOR_NEW_ARRAY */
+
+#if defined(__CYGWIN__)
+# include <new> // for delete throw()
+  inline void operator delete(void *p)
+  {
+    GC_FREE(p);
+  }
+#endif
+
+#endif /* GC_CPP_H */
diff --git a/src/gc/bdwgc/include/gc_gcj.h b/src/gc/bdwgc/include/gc_gcj.h
new file mode 100644
index 0000000..7865878
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_gcj.h
@@ -0,0 +1,109 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright 1999 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* This file assumes the collector has been compiled with GC_GCJ_SUPPORT. */
+
+/*
+ * We allocate objects whose first word contains a pointer to a struct
+ * describing the object type.  This struct contains a garbage collector mark
+ * descriptor at offset MARK_DESCR_OFFSET.  Alternatively, the objects
+ * may be marked by the mark procedure passed to GC_init_gcj_malloc.
+ */
+
+#ifndef GC_GCJ_H
+#define GC_GCJ_H
+
+        /* Gcj keeps GC descriptor as second word of vtable.    This    */
+        /* probably needs to be adjusted for other clients.             */
+        /* We currently assume that this offset is such that:           */
+        /*      - all objects of this kind are large enough to have     */
+        /*        a value at that offset, and                           */
+        /*      - it is not zero.                                       */
+        /* These assumptions allow objects on the free list to be       */
+        /* marked normally.                                             */
+
+#ifndef GC_H
+# include "gc.h"
+#endif
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+/* The following allocators signal an out of memory condition with      */
+/* return GC_oom_fn(bytes);                                             */
+
+/* The following function must be called before the gcj allocators      */
+/* can be invoked.                                                      */
+/* mp_index and mp are the index and mark_proc (see gc_mark.h)          */
+/* respectively for the allocated objects.  Mark_proc will be           */
+/* used to build the descriptor for objects allocated through the       */
+/* debugging interface.  The mark_proc will be invoked on all such      */
+/* objects with an "environment" value of 1.  The client may choose     */
+/* to use the same mark_proc for some of its generated mark descriptors.*/
+/* In that case, it should use a different "environment" value to       */
+/* detect the presence or absence of the debug header.                  */
+/* Mp is really of type mark_proc, as defined in gc_mark.h.  We don't   */
+/* want to include that here for namespace pollution reasons.           */
+/* Passing in mp_index here instead of having GC_init_gcj_malloc()      */
+/* internally call GC_new_proc() is quite ugly, but in typical usage    */
+/* scenarios a compiler also has to know about mp_index, so             */
+/* generating it dynamically is not acceptable.  Mp_index will          */
+/* typically be an integer < RESERVED_MARK_PROCS, so that it doesn't    */
+/* collide with GC_new_proc allocated indices.  If the application      */
+/* needs no other reserved indices, zero                                */
+/* (GC_GCJ_RESERVED_MARK_PROC_INDEX in gc_mark.h) is an obvious choice. */
+GC_API void GC_CALL GC_init_gcj_malloc(int /* mp_index */,
+                                void * /* really mark_proc */ /* mp */);
+
+/* Allocate an object, clear it, and store the pointer to the   */
+/* type structure (vtable in gcj).                              */
+/* This adds a byte at the end of the object if GC_malloc would.*/
+GC_API void * GC_CALL GC_gcj_malloc(size_t /* lb */,
+                                void * /* ptr_to_struct_containing_descr */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+/* The debug versions allocate such that the specified mark_proc        */
+/* is always invoked.                                                   */
+GC_API void * GC_CALL GC_debug_gcj_malloc(size_t /* lb */,
+                                  void * /* ptr_to_struct_containing_descr */,
+                                  GC_EXTRA_PARAMS)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+
+/* Similar to GC_gcj_malloc, but assumes that a pointer to near the     */
+/* beginning of the resulting object is always maintained.              */
+GC_API void  * GC_CALL GC_gcj_malloc_ignore_off_page(size_t /* lb */,
+                                void * /* ptr_to_struct_containing_descr */)
+                        GC_ATTR_MALLOC GC_ATTR_ALLOC_SIZE(1);
+
+/* The kind numbers of normal and debug gcj objects.            */
+/* Useful only for debug support, we hope.                      */
+GC_API int GC_gcj_kind;
+
+GC_API int GC_gcj_debug_kind;
+
+#ifdef GC_DEBUG
+# define GC_GCJ_MALLOC(s,d) GC_debug_gcj_malloc(s,d,GC_EXTRAS)
+# define GC_GCJ_MALLOC_IGNORE_OFF_PAGE(s,d) GC_debug_gcj_malloc(s,d,GC_EXTRAS)
+#else
+# define GC_GCJ_MALLOC(s,d) GC_gcj_malloc(s,d)
+# define GC_GCJ_MALLOC_IGNORE_OFF_PAGE(s,d) GC_gcj_malloc_ignore_off_page(s,d)
+#endif
+
+#ifdef __cplusplus
+  } /* end of extern "C" */
+#endif
+
+#endif /* GC_GCJ_H */
diff --git a/src/gc/bdwgc/include/gc_inline.h b/src/gc/bdwgc/include/gc_inline.h
new file mode 100644
index 0000000..d6a0e2c
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_inline.h
@@ -0,0 +1,146 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_INLINE_H
+#define GC_INLINE_H
+
+/* WARNING:                                                             */
+/* Note that for these routines, it is the clients responsibility to    */
+/* add the extra byte at the end to deal with one-past-the-end pointers.*/
+/* In the standard collector configuration, the collector assumes that  */
+/* such a byte has been added, and hence does not trace the last word   */
+/* in the resulting object.                                             */
+/* This is not an issue if the collector is compiled with               */
+/* DONT_ADD_BYTE_AT_END, or if GC_all_interior_pointers is not set.     */
+/* This interface is most useful for compilers that generate C.         */
+/* It is also used internally for thread-local allocation.              */
+/* Manual use is hereby discouraged.                                    */
+
+#include "gc.h"
+#include "gc_tiny_fl.h"
+
+/* #if __GNUC__ >= 3 */
+/* # define GC_EXPECT(expr, outcome) __builtin_expect(expr,outcome) */
+/*   /\* Equivalent to (expr), but predict that usually (expr)==outcome. *\/ */
+/* #else */
+# define GC_EXPECT(expr, outcome) (expr)
+//#endif /* __GNUC__ */
+
+//#ifndef GC_ASSERT
+//# define GC_ASSERT(expr) /* empty */
+//#endif
+
+/* Store a pointer to a list of newly allocated objects of kind k and   */
+/* size lb in *result.  The caller must make sure that *result is       */
+/* traced even if objects are ptrfree.                                  */
+GC_API void GC_CALL GC_generic_malloc_many(size_t /* lb */, int /* k */,
+                                           void ** /* result */);
+
+/* The ultimately general inline allocation macro.  Allocate an object  */
+/* of size granules, putting the resulting pointer in result.  Tiny_fl  */
+/* is a "tiny" free list array, which will be used first, if the size   */
+/* is appropriate.  If granules is too large, we allocate with          */
+/* default_expr instead.  If we need to refill the free list, we use    */
+/* GC_generic_malloc_many with the indicated kind.                      */
+/* Tiny_fl should be an array of GC_TINY_FREELISTS void * pointers.     */
+/* If num_direct is nonzero, and the individual free list pointers      */
+/* are initialized to (void *)1, then we allocate numdirect granules    */
+/* directly using gmalloc before putting multiple objects into the      */
+/* tiny_fl entry.  If num_direct is zero, then the free lists may also  */
+/* be initialized to (void *)0.                                         */
+/* Note that we use the zeroth free list to hold objects 1 granule in   */
+/* size that are used to satisfy size 0 allocation requests.            */
+/* We rely on much of this hopefully getting optimized away in the      */
+/* num_direct = 0 case.                                                 */
+/* Particularly if granules is constant, this should generate a small   */
+/* amount of code.                                                      */
+# define GC_FAST_MALLOC_GRANS(result,granules,tiny_fl,num_direct,\
+                              kind,default_expr,init) \
+{ \
+    if (GC_EXPECT((granules) >= GC_TINY_FREELISTS,0)) { \
+        result = (default_expr); \
+    } else { \
+        void **my_fl = (tiny_fl) + (granules); \
+        void *my_entry=*my_fl; \
+        void *next; \
+ \
+        while (GC_EXPECT((GC_word)my_entry \
+                        <= (num_direct) + GC_TINY_FREELISTS + 1, 0)) { \
+            /* Entry contains counter or NULL */ \
+            if ((GC_word)my_entry - 1 < (num_direct)) { \
+                /* Small counter value, not NULL */ \
+                *my_fl = (char *)my_entry + (granules) + 1; \
+                result = (default_expr); \
+                goto out; \
+            } else { \
+                /* Large counter or NULL */ \
+                GC_generic_malloc_many(((granules) == 0? GC_GRANULE_BYTES : \
+                                        GC_RAW_BYTES_FROM_INDEX(granules)), \
+                                       kind, my_fl); \
+                my_entry = *my_fl; \
+                if (my_entry == 0) { \
+                    result = (*GC_get_oom_fn())((granules)*GC_GRANULE_BYTES); \
+                    goto out; \
+                } \
+            } \
+        } \
+        next = *(void **)(my_entry); \
+        result = (void *)my_entry; \
+        *my_fl = next; \
+        init; \
+        PREFETCH_FOR_WRITE(next); \
+        GC_ASSERT(GC_size(result) >= (granules)*GC_GRANULE_BYTES); \
+        GC_ASSERT((kind) == PTRFREE || ((GC_word *)result)[1] == 0); \
+      out: ; \
+   } \
+}
+
+# define GC_WORDS_TO_WHOLE_GRANULES(n) \
+        GC_WORDS_TO_GRANULES((n) + GC_GRANULE_WORDS - 1)
+
+/* Allocate n words (NOT BYTES).  X is made to point to the result.     */
+/* This should really only be used if GC_all_interior_pointers is       */
+/* not set, or DONT_ADD_BYTE_AT_END is set.  See above.                 */
+/* The semantics changed in version 7.0; we no longer lock, and         */
+/* the caller is responsible for supplying a cleared tiny_fl            */
+/* free list array.  For single-threaded applications, this may be      */
+/* a global array.                                                      */
+# define GC_MALLOC_WORDS(result,n,tiny_fl) \
+{ \
+    size_t grans = GC_WORDS_TO_WHOLE_GRANULES(n); \
+    GC_FAST_MALLOC_GRANS(result, grans, tiny_fl, 0, \
+                         NORMAL, GC_malloc(grans*GC_GRANULE_BYTES), \
+                         *(void **)(result) = 0); \
+}
+
+# define GC_MALLOC_ATOMIC_WORDS(result,n,tiny_fl) \
+{ \
+    size_t grans = GC_WORDS_TO_WHOLE_GRANULES(n); \
+    GC_FAST_MALLOC_GRANS(result, grans, tiny_fl, 0, \
+                         PTRFREE, GC_malloc_atomic(grans*GC_GRANULE_BYTES), \
+                         (void)0 /* no initialization */); \
+}
+
+/* And once more for two word initialized objects: */
+# define GC_CONS(result, first, second, tiny_fl) \
+{ \
+    size_t grans = GC_WORDS_TO_WHOLE_GRANULES(2); \
+    GC_FAST_MALLOC_GRANS(result, grans, tiny_fl, 0, \
+                         NORMAL, GC_malloc(grans*GC_GRANULE_BYTES), \
+                         *(void **)(result) = (void *)(first)); \
+    ((void **)(result))[1] = (void *)(second); \
+}
+
+#endif /* !GC_INLINE_H */
diff --git a/src/gc/bdwgc/include/gc_mark.h b/src/gc/bdwgc/include/gc_mark.h
new file mode 100644
index 0000000..f12998f
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_mark.h
@@ -0,0 +1,231 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 2001 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/*
+ * This contains interfaces to the GC marker that are likely to be useful to
+ * clients that provide detailed heap layout information to the collector.
+ * This interface should not be used by normal C or C++ clients.
+ * It will be useful to runtimes for other languages.
+ *
+ * This is an experts-only interface!  There are many ways to break the
+ * collector in subtle ways by using this functionality.
+ */
+#ifndef GC_MARK_H
+#define GC_MARK_H
+
+#ifndef GC_H
+# include "gc.h"
+#endif
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+/* A client supplied mark procedure.  Returns new mark stack pointer.   */
+/* Primary effect should be to push new entries on the mark stack.      */
+/* Mark stack pointer values are passed and returned explicitly.        */
+/* Global variables describing mark stack are not necessarily valid.    */
+/* (This usually saves a few cycles by keeping things in registers.)    */
+/* Assumed to scan about GC_PROC_BYTES on average.  If it needs to do   */
+/* much more work than that, it should do it in smaller pieces by       */
+/* pushing itself back on the mark stack.                               */
+/* Note that it should always do some work (defined as marking some     */
+/* objects) before pushing more than one entry on the mark stack.       */
+/* This is required to ensure termination in the event of mark stack    */
+/* overflows.                                                           */
+/* This procedure is always called with at least one empty entry on the */
+/* mark stack.                                                          */
+/* Currently we require that mark procedures look for pointers in a     */
+/* subset of the places the conservative marker would.  It must be safe */
+/* to invoke the normal mark procedure instead.                         */
+/* WARNING: Such a mark procedure may be invoked on an unused object    */
+/* residing on a free list.  Such objects are cleared, except for a     */
+/* free list link field in the first word.  Thus mark procedures may    */
+/* not count on the presence of a type descriptor, and must handle this */
+/* case correctly somehow.                                              */
+#define GC_PROC_BYTES 100
+struct GC_ms_entry;
+typedef struct GC_ms_entry * (*GC_mark_proc)(GC_word * /* addr */,
+                                struct GC_ms_entry * /* mark_stack_ptr */,
+                                struct GC_ms_entry * /* mark_stack_limit */,
+                                GC_word /* env */);
+
+#define GC_LOG_MAX_MARK_PROCS 6
+#define GC_MAX_MARK_PROCS (1 << GC_LOG_MAX_MARK_PROCS)
+
+/* In a few cases it's necessary to assign statically known indices to  */
+/* certain mark procs.  Thus we reserve a few for well known clients.   */
+/* (This is necessary if mark descriptors are compiler generated.)      */
+#define GC_RESERVED_MARK_PROCS 8
+#define GC_GCJ_RESERVED_MARK_PROC_INDEX 0
+
+/* Object descriptors on mark stack or in objects.  Low order two       */
+/* bits are tags distinguishing among the following 4 possibilities     */
+/* for the high order 30 bits.                                          */
+#define GC_DS_TAG_BITS 2
+#define GC_DS_TAGS   ((1 << GC_DS_TAG_BITS) - 1)
+#define GC_DS_LENGTH 0  /* The entire word is a length in bytes that    */
+                        /* must be a multiple of 4.                     */
+#define GC_DS_BITMAP 1  /* 30 (62) bits are a bitmap describing pointer */
+                        /* fields.  The msb is 1 if the first word      */
+                        /* is a pointer.                                */
+                        /* (This unconventional ordering sometimes      */
+                        /* makes the marker slightly faster.)           */
+                        /* Zeroes indicate definite nonpointers.  Ones  */
+                        /* indicate possible pointers.                  */
+                        /* Only usable if pointers are word aligned.    */
+#define GC_DS_PROC   2
+                        /* The objects referenced by this object can be */
+                        /* pushed on the mark stack by invoking         */
+                        /* PROC(descr).  ENV(descr) is passed as the    */
+                        /* last argument.                               */
+#define GC_MAKE_PROC(proc_index, env) \
+            (((((env) << GC_LOG_MAX_MARK_PROCS) \
+               | (proc_index)) << GC_DS_TAG_BITS) | GC_DS_PROC)
+#define GC_DS_PER_OBJECT 3  /* The real descriptor is at the            */
+                        /* byte displacement from the beginning of the  */
+                        /* object given by descr & ~DS_TAGS             */
+                        /* If the descriptor is negative, the real      */
+                        /* descriptor is at (*<object_start>) -         */
+                        /* (descr & ~DS_TAGS) - GC_INDIR_PER_OBJ_BIAS   */
+                        /* The latter alternative can be used if each   */
+                        /* object contains a type descriptor in the     */
+                        /* first word.                                  */
+                        /* Note that in the multi-threaded environments */
+                        /* per-object descriptors must be located in    */
+                        /* either the first two or last two words of    */
+                        /* the object, since only those are guaranteed  */
+                        /* to be cleared while the allocation lock is   */
+                        /* held.                                        */
+#define GC_INDIR_PER_OBJ_BIAS 0x10
+
+GC_API void * GC_least_plausible_heap_addr;
+GC_API void * GC_greatest_plausible_heap_addr;
+                        /* Bounds on the heap.  Guaranteed valid        */
+                        /* Likely to include future heap expansion.     */
+                        /* Hence usually includes not-yet-mapped        */
+                        /* memory.                                      */
+
+/* Handle nested references in a custom mark procedure.                 */
+/* Check if obj is a valid object. If so, ensure that it is marked.     */
+/* If it was not previously marked, push its contents onto the mark     */
+/* stack for future scanning.  The object will then be scanned using    */
+/* its mark descriptor.                                                 */
+/* Returns the new mark stack pointer.                                  */
+/* Handles mark stack overflows correctly.                              */
+/* Since this marks first, it makes progress even if there are mark     */
+/* stack overflows.                                                     */
+/* Src is the address of the pointer to obj, which is used only         */
+/* for back pointer-based heap debugging.                               */
+/* It is strongly recommended that most objects be handled without mark */
+/* procedures, e.g. with bitmap descriptors, and that mark procedures   */
+/* be reserved for exceptional cases.  That will ensure that            */
+/* performance of this call is not extremely performance critical.      */
+/* (Otherwise we would need to inline GC_mark_and_push completely,      */
+/* which would tie the client code to a fixed collector version.)       */
+/* Note that mark procedures should explicitly call FIXUP_POINTER()     */
+/* if required.                                                         */
+GC_API struct GC_ms_entry * GC_CALL GC_mark_and_push(void * /* obj */,
+                                struct GC_ms_entry * /* mark_stack_ptr */,
+                                struct GC_ms_entry * /* mark_stack_limit */,
+                                void ** /* src */);
+
+#define GC_MARK_AND_PUSH(obj, msp, lim, src) \
+          ((GC_word)(obj) >= (GC_word)GC_least_plausible_heap_addr && \
+           (GC_word)(obj) <= (GC_word)GC_greatest_plausible_heap_addr ? \
+           GC_mark_and_push(obj, msp, lim, src) : (msp))
+
+GC_API size_t GC_debug_header_size;
+       /* The size of the header added to objects allocated through    */
+       /* the GC_debug routines.                                       */
+       /* Defined as a variable so that client mark procedures don't   */
+       /* need to be recompiled for collector version changes.         */
+#define GC_USR_PTR_FROM_BASE(p) ((void *)((char *)(p) + GC_debug_header_size))
+
+/* And some routines to support creation of new "kinds", e.g. with      */
+/* custom mark procedures, by language runtimes.                        */
+/* The _inner versions assume the caller holds the allocation lock.     */
+
+/* Return a new free list array.        */
+GC_API void ** GC_CALL GC_new_free_list(void);
+GC_API void ** GC_CALL GC_new_free_list_inner(void);
+
+/* Return a new kind, as specified. */
+GC_API unsigned GC_CALL GC_new_kind(void ** /* free_list */,
+                                    GC_word /* mark_descriptor_template */,
+                                    int /* add_size_to_descriptor */,
+                                    int /* clear_new_objects */);
+                /* The last two parameters must be zero or one. */
+GC_API unsigned GC_CALL GC_new_kind_inner(void ** /* free_list */,
+                                    GC_word /* mark_descriptor_template */,
+                                    int /* add_size_to_descriptor */,
+                                    int /* clear_new_objects */);
+
+/* Return a new mark procedure identifier, suitable for use as  */
+/* the first argument in GC_MAKE_PROC.                          */
+GC_API unsigned GC_CALL GC_new_proc(GC_mark_proc);
+GC_API unsigned GC_CALL GC_new_proc_inner(GC_mark_proc);
+
+/* Allocate an object of a given kind.  Note that in the multi-threaded */
+/* contexts, this is usually unsafe for kinds that have the descriptor  */
+/* in the object itself, since there is otherwise a window in which     */
+/* the descriptor is not correct.  Even in the single-threaded case,    */
+/* we need to be sure that cleared objects on a free list don't         */
+/* cause a GC crash if they are accidentally traced.                    */
+GC_API void * GC_CALL GC_generic_malloc(size_t /* lb */, int /* k */);
+
+typedef void (GC_CALLBACK * GC_describe_type_fn)(void * /* p */,
+                                                 char * /* out_buf */);
+                                /* A procedure which                    */
+                                /* produces a human-readable            */
+                                /* description of the "type" of object  */
+                                /* p into the buffer out_buf of length  */
+                                /* GC_TYPE_DESCR_LEN.  This is used by  */
+                                /* the debug support when printing      */
+                                /* objects.                             */
+                                /* These functions should be as robust  */
+                                /* as possible, though we do avoid      */
+                                /* invoking them on objects on the      */
+                                /* global free list.                    */
+#define GC_TYPE_DESCR_LEN 40
+
+GC_API void GC_CALL GC_register_describe_type_fn(int /* kind */,
+                                                 GC_describe_type_fn);
+                                /* Register a describe_type function    */
+                                /* to be used when printing objects     */
+                                /* of a particular kind.                */
+
+/* Clear some of the inaccessible part of the stack.  Returns its       */
+/* argument, so it can be used in a tail call position, hence clearing  */
+/* another frame.  Argument may be NULL.                                */
+GC_API void * GC_CALL GC_clear_stack(void *);
+
+/* Set and get the client notifier on collections.  The client function */
+/* is called at the start of every full GC (called with the allocation  */
+/* lock held).  May be 0.  This is a really tricky interface to use     */
+/* correctly.  Unless you really understand the collector internals,    */
+/* the callback should not, directly or indirectly, make any GC_ or     */
+/* potentially blocking calls.  In particular, it is not safe to        */
+/* allocate memory using the garbage collector from within the callback */
+/* function.  Both the setter and getter acquire the GC lock.           */
+typedef void (GC_CALLBACK * GC_start_callback_proc)(void);
+GC_API void GC_CALL GC_set_start_callback(GC_start_callback_proc);
+GC_API GC_start_callback_proc GC_CALL GC_get_start_callback(void);
+
+#ifdef __cplusplus
+  } /* end of extern "C" */
+#endif
+
+#endif /* GC_MARK_H */
diff --git a/src/gc/bdwgc/include/gc_pthread_redirects.h b/src/gc/bdwgc/include/gc_pthread_redirects.h
new file mode 100644
index 0000000..f837787
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_pthread_redirects.h
@@ -0,0 +1,94 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2010 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* Our pthread support normally needs to intercept a number of thread   */
+/* calls.  We arrange to do that here, if appropriate.                  */
+
+/* Included from gc.h only.  Included only if GC_PTHREADS.              */
+#if defined(GC_H) && defined(GC_PTHREADS)
+
+/* We need to intercept calls to many of the threads primitives, so     */
+/* that we can locate thread stacks and stop the world.                 */
+/* Note also that the collector cannot always see thread specific data. */
+/* Thread specific data should generally consist of pointers to         */
+/* uncollectible objects (allocated with GC_malloc_uncollectable,       */
+/* not the system malloc), which are deallocated using the destructor   */
+/* facility in thr_keycreate.  Alternatively, keep a redundant pointer  */
+/* to thread specific data on the thread stack.                         */
+
+#include <pthread.h>
+
+#ifndef GC_NO_DLOPEN
+# include <dlfcn.h>
+  GC_API void *GC_dlopen(const char * /* path */, int /* mode */);
+#endif /* !GC_NO_DLOPEN */
+
+#ifndef GC_NO_PTHREAD_SIGMASK
+# include <signal.h>
+  GC_API int GC_pthread_sigmask(int /* how */, const sigset_t *,
+                                sigset_t * /* oset */);
+#endif /* !GC_NO_PTHREAD_SIGMASK */
+
+#ifndef GC_PTHREAD_CREATE_CONST
+  /* This is used for pthread_create() only.    */
+# define GC_PTHREAD_CREATE_CONST const
+#endif
+
+GC_API int GC_pthread_create(pthread_t *,
+                             GC_PTHREAD_CREATE_CONST pthread_attr_t *,
+                             void *(*)(void *), void * /* arg */);
+GC_API int GC_pthread_join(pthread_t, void ** /* retval */);
+GC_API int GC_pthread_detach(pthread_t);
+
+#ifndef GC_NO_PTHREAD_CANCEL
+  GC_API int GC_pthread_cancel(pthread_t);
+#endif
+
+#ifdef GC_PTHREAD_EXIT_ATTRIBUTE
+  GC_API void GC_pthread_exit(void *) GC_PTHREAD_EXIT_ATTRIBUTE;
+#endif
+
+#if !defined(GC_NO_THREAD_REDIRECTS) && !defined(GC_USE_LD_WRAP)
+  /* Unless the compiler supports #pragma extern_prefix, the Tru64      */
+  /* UNIX <pthread.h> redefines some POSIX thread functions to use      */
+  /* mangled names.  Anyway, it's safe to undef them before redefining. */
+# undef pthread_create
+# undef pthread_join
+# undef pthread_detach
+# define pthread_create GC_pthread_create
+# define pthread_join GC_pthread_join
+# define pthread_detach GC_pthread_detach
+
+# ifndef GC_NO_PTHREAD_SIGMASK
+#   undef pthread_sigmask
+#   define pthread_sigmask GC_pthread_sigmask
+# endif
+# ifndef GC_NO_DLOPEN
+#   undef dlopen
+#   define dlopen GC_dlopen
+# endif
+# ifndef GC_NO_PTHREAD_CANCEL
+#   undef pthread_cancel
+#   define pthread_cancel GC_pthread_cancel
+# endif
+# ifdef GC_PTHREAD_EXIT_ATTRIBUTE
+#   undef pthread_exit
+#   define pthread_exit GC_pthread_exit
+# endif
+#endif /* !GC_NO_THREAD_REDIRECTS */
+
+#endif /* GC_PTHREADS */
diff --git a/src/gc/bdwgc/include/gc_tiny_fl.h b/src/gc/bdwgc/include/gc_tiny_fl.h
new file mode 100644
index 0000000..7758151
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_tiny_fl.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (c) 1999-2005 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_TINY_FL_H
+#define GC_TINY_FL_H
+/*
+ * Constants and data structures for "tiny" free lists.
+ * These are used for thread-local allocation or in-lined allocators.
+ * Each global free list also essentially starts with one of these.
+ * However, global free lists are known to the GC.  "Tiny" free lists
+ * are basically private to the client.  Their contents are viewed as
+ * "in use" and marked accordingly by the core of the GC.
+ *
+ * Note that inlined code might know about the layout of these and the constants
+ * involved.  Thus any change here may invalidate clients, and such changes should
+ * be avoided.  Hence we keep this as simple as possible.
+ */
+
+/*
+ * We always set GC_GRANULE_BYTES to twice the length of a pointer.
+ * This means that all allocation requests are rounded up to the next
+ * multiple of 16 on 64-bit architectures or 8 on 32-bit architectures.
+ * This appears to be a reasonable compromise between fragmentation overhead
+ * and space usage for mark bits (usually mark bytes).
+ * On many 64-bit architectures some memory references require 16-byte
+ * alignment, making this necessary anyway.
+ * For a few 32-bit architecture (e.g. x86), we may also need 16-byte alignment
+ * for certain memory references.  But currently that does not seem to be the
+ * default for all conventional malloc implementations, so we ignore that
+ * problem.
+ * It would always be safe, and often useful, to be able to allocate very
+ * small objects with smaller alignment.  But that would cost us mark bit
+ * space, so we no longer do so.
+ */
+#ifndef GC_GRANULE_BYTES
+  /* GC_GRANULE_BYTES should not be overridden in any instances of the GC */
+  /* library that may be shared between applications, since it affects	  */
+  /* the binary interface to the library.				  */
+# if defined(__LP64__) || defined (_LP64) || defined(_WIN64) \
+        || defined(__s390x__) \
+        || (defined(__x86_64__) && !defined(__ILP32__)) \
+	|| defined(__alpha__) || defined(__powerpc64__) \
+	|| defined(__arch64__)
+#  define GC_GRANULE_BYTES 16
+#  define GC_GRANULE_WORDS 2
+# else
+#  define GC_GRANULE_BYTES 8
+#  define GC_GRANULE_WORDS 2
+# endif
+#endif /* !GC_GRANULE_BYTES */
+
+#if GC_GRANULE_WORDS == 2
+#  define GC_WORDS_TO_GRANULES(n) ((n)>>1)
+#else
+#  define GC_WORDS_TO_GRANULES(n) ((n)*sizeof(void *)/GC_GRANULE_BYTES)
+#endif
+
+/* A "tiny" free list header contains TINY_FREELISTS pointers to 	*/
+/* singly linked lists of objects of different sizes, the ith one	*/
+/* containing objects i granules in size.  Note that there is a list	*/
+/* of size zero objects.						*/
+#ifndef GC_TINY_FREELISTS
+# if GC_GRANULE_BYTES == 16
+#   define GC_TINY_FREELISTS 25
+# else
+#   define GC_TINY_FREELISTS 33	/* Up to and including 256 bytes */
+# endif
+#endif /* !GC_TINY_FREELISTS */
+
+/* The ith free list corresponds to size i*GC_GRANULE_BYTES	*/
+/* Internally to the collector, the index can be computed with	*/
+/* ROUNDED_UP_GRANULES.  Externally, we don't know whether	*/
+/* DONT_ADD_BYTE_AT_END is set, but the client should know.	*/
+
+/* Convert a free list index to the actual size of objects	*/
+/* on that list, including extra space we added.  Not an	*/
+/* inverse of the above.					*/
+#define GC_RAW_BYTES_FROM_INDEX(i) ((i) * GC_GRANULE_BYTES)
+
+#endif /* GC_TINY_FL_H */
diff --git a/src/gc/bdwgc/include/gc_typed.h b/src/gc/bdwgc/include/gc_typed.h
new file mode 100644
index 0000000..a794fbb
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_typed.h
@@ -0,0 +1,114 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright 1996 Silicon Graphics.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+/*
+ * Some simple primitives for allocation with explicit type information.
+ * Facilities for dynamic type inference may be added later.
+ * Should be used only for extremely performance critical applications,
+ * or if conservative collector leakage is otherwise a problem (unlikely).
+ * Note that this is implemented completely separately from the rest
+ * of the collector, and is not linked in unless referenced.
+ * This does not currently support GC_DEBUG in any interesting way.
+ */
+
+#ifndef GC_TYPED_H
+#define GC_TYPED_H
+
+#ifndef GC_H
+# include "gc.h"
+#endif
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+typedef GC_word * GC_bitmap;
+        /* The least significant bit of the first word is one if        */
+        /* the first word in the object may be a pointer.               */
+
+#define GC_WORDSZ (8 * sizeof(GC_word))
+#define GC_get_bit(bm, index) \
+            (((bm)[(index) / GC_WORDSZ] >> ((index) % GC_WORDSZ)) & 1)
+#define GC_set_bit(bm, index) \
+            ((bm)[(index) / GC_WORDSZ] |= (GC_word)1 << ((index) % GC_WORDSZ))
+#define GC_WORD_OFFSET(t, f) (offsetof(t,f) / sizeof(GC_word))
+#define GC_WORD_LEN(t) (sizeof(t) / sizeof(GC_word))
+#define GC_BITMAP_SIZE(t) ((GC_WORD_LEN(t) + GC_WORDSZ - 1) / GC_WORDSZ)
+
+typedef GC_word GC_descr;
+
+GC_API GC_descr GC_CALL GC_make_descriptor(GC_bitmap /* bm */,
+                                           size_t /* len */);
+                /* Return a type descriptor for the object whose layout */
+                /* is described by the argument.                        */
+                /* The least significant bit of the first word is one   */
+                /* if the first word in the object may be a pointer.    */
+                /* The second argument specifies the number of          */
+                /* meaningful bits in the bitmap.  The actual object    */
+                /* may be larger (but not smaller).  Any additional     */
+                /* words in the object are assumed not to contain       */
+                /* pointers.                                            */
+                /* Returns a conservative approximation in the          */
+                /* (unlikely) case of insufficient memory to build      */
+                /* the descriptor.  Calls to GC_make_descriptor         */
+                /* may consume some amount of a finite resource.  This  */
+                /* is intended to be called once per type, not once     */
+                /* per allocation.                                      */
+
+/* It is possible to generate a descriptor for a C type T with  */
+/* word aligned pointer fields f1, f2, ... as follows:                  */
+/*                                                                      */
+/* GC_descr T_descr;                                                    */
+/* GC_word T_bitmap[GC_BITMAP_SIZE(T)] = {0};                           */
+/* GC_set_bit(T_bitmap, GC_WORD_OFFSET(T,f1));                          */
+/* GC_set_bit(T_bitmap, GC_WORD_OFFSET(T,f2));                          */
+/* ...                                                                  */
+/* T_descr = GC_make_descriptor(T_bitmap, GC_WORD_LEN(T));              */
+
+GC_API void * GC_CALL GC_malloc_explicitly_typed(size_t /* size_in_bytes */,
+                                                 GC_descr /* d */);
+                /* Allocate an object whose layout is described by d.   */
+                /* The resulting object MAY NOT BE PASSED TO REALLOC.   */
+                /* The returned object is cleared.                      */
+
+GC_API void * GC_CALL GC_malloc_explicitly_typed_ignore_off_page(
+                                        size_t /* size_in_bytes */,
+                                        GC_descr /* d */);
+
+GC_API void * GC_CALL GC_calloc_explicitly_typed(size_t /* nelements */,
+                                        size_t /* element_size_in_bytes */,
+                                        GC_descr /* d */);
+        /* Allocate an array of nelements elements, each of the */
+        /* given size, and with the given descriptor.           */
+        /* The element size must be a multiple of the byte      */
+        /* alignment required for pointers.  E.g. on a 32-bit   */
+        /* machine with 16-bit aligned pointers, size_in_bytes  */
+        /* must be a multiple of 2.                             */
+        /* Returned object is cleared.                          */
+
+#ifdef GC_DEBUG
+# define GC_MALLOC_EXPLICITLY_TYPED(bytes, d) GC_MALLOC(bytes)
+# define GC_CALLOC_EXPLICITLY_TYPED(n, bytes, d) GC_MALLOC((n) * (bytes))
+#else
+# define GC_MALLOC_EXPLICITLY_TYPED(bytes, d) \
+                        GC_malloc_explicitly_typed(bytes, d)
+# define GC_CALLOC_EXPLICITLY_TYPED(n, bytes, d) \
+                        GC_calloc_explicitly_typed(n, bytes, d)
+#endif
+
+#ifdef __cplusplus
+  } /* matches extern "C" */
+#endif
+
+#endif /* GC_TYPED_H */
diff --git a/src/gc/bdwgc/include/gc_version.h b/src/gc/bdwgc/include/gc_version.h
new file mode 100644
index 0000000..3cf22ae
--- /dev/null
+++ b/src/gc/bdwgc/include/gc_version.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* This should never be included directly; it is included only from gc.h. */
+#if defined(GC_H)
+
+/* The version here should match that in configure/configure.ac */
+/* Eventually this one may become unnecessary.  For now we need */
+/* it to keep the old-style build process working.              */
+#define GC_TMP_VERSION_MAJOR 7
+#define GC_TMP_VERSION_MINOR 2
+#define GC_TMP_ALPHA_VERSION GC_NOT_ALPHA
+
+#ifndef GC_NOT_ALPHA
+# define GC_NOT_ALPHA 0xff
+#endif
+
+#ifdef GC_VERSION_MAJOR
+# if GC_TMP_VERSION_MAJOR != GC_VERSION_MAJOR \
+     || GC_TMP_VERSION_MINOR != GC_VERSION_MINOR \
+     || defined(GC_ALPHA_VERSION) != (GC_TMP_ALPHA_VERSION != GC_NOT_ALPHA) \
+     || (defined(GC_ALPHA_VERSION) && GC_TMP_ALPHA_VERSION != GC_ALPHA_VERSION)
+#   error Inconsistent version info.  Check doc/README, include/gc_version.h, and configure.ac.
+# endif
+#else
+# define GC_VERSION_MAJOR GC_TMP_VERSION_MAJOR
+# define GC_VERSION_MINOR GC_TMP_VERSION_MINOR
+# define GC_ALPHA_VERSION GC_TMP_ALPHA_VERSION
+#endif /* !GC_VERSION_MAJOR */
+
+#endif
diff --git a/src/gc/bdwgc/include/include.am b/src/gc/bdwgc/include/include.am
new file mode 100644
index 0000000..37e6d59
--- /dev/null
+++ b/src/gc/bdwgc/include/include.am
@@ -0,0 +1,59 @@
+# 
+# 
+# THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+# OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+# 
+# Permission is hereby granted to use or copy this program
+# for any purpose,  provided the above notices are retained on all copies.
+# Permission to modify the code and to distribute modified code is granted,
+# provided the above notices are retained, and a notice that the code was
+# modified is included with the above copyright notice.
+#
+# Modified by: Grzegorz Jakacki <jakacki at acm dot org>
+# Modified by: Petter Urkedal <petter.urkedal@nordita.dk>
+
+## Process this file with automake to produce part of Makefile.in.
+
+# installed headers
+#
+pkginclude_HEADERS += \
+	include/gc.h \
+	include/gc_typed.h \
+	include/gc_inline.h \
+	include/gc_mark.h \
+	include/weakpointer.h \
+	include/new_gc_alloc.h \
+	include/gc_allocator.h \
+	include/gc_backptr.h \
+	include/gc_gcj.h \
+	include/leak_detector.h \
+	include/gc_amiga_redirects.h \
+	include/gc_pthread_redirects.h \
+	include/gc_config_macros.h \
+	include/gc_tiny_fl.h \
+	include/gc_version.h
+
+# headers which are not installed
+#
+dist_noinst_HEADERS += \
+	include/private/gc_hdrs.h \
+	include/private/gc_priv.h \
+	include/private/gcconfig.h \
+	include/private/msvc_dbg.h \
+	include/private/gc_pmark.h \
+	include/private/gc_locks.h \
+	include/private/dbg_mlc.h \
+	include/private/specific.h \
+	include/private/cord_pos.h \
+	include/private/pthread_support.h \
+	include/private/pthread_stop_world.h \
+	include/private/darwin_semaphore.h \
+	include/private/darwin_stop_world.h \
+	include/private/thread_local_alloc.h \
+	include/cord.h \
+	include/ec.h \
+	include/javaxfc.h 
+
+# unprefixed header
+include_HEADERS += \
+        include/extra/gc.h
diff --git a/src/gc/bdwgc/include/javaxfc.h b/src/gc/bdwgc/include/javaxfc.h
new file mode 100644
index 0000000..1583a24
--- /dev/null
+++ b/src/gc/bdwgc/include/javaxfc.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_H
+# include "gc.h"
+#endif
+
+#ifdef __cplusplus
+  extern "C" {
+#endif
+
+/*
+ * Invoke all remaining finalizers that haven't yet been run.  (Since the
+ * notifier is not called, this should be called from a separate thread.)
+ * This function is needed for strict compliance with the Java standard,
+ * which can make the runtime guarantee that all finalizers are run.
+ * This is problematic for several reasons:
+ * 1) It means that finalizers, and all methods called by them,
+ *    must be prepared to deal with objects that have been finalized in
+ *    spite of the fact that they are still referenced by statically
+ *    allocated pointer variables.
+ * 2) It may mean that we get stuck in an infinite loop running
+ *    finalizers which create new finalizable objects, though that's
+ *    probably unlikely.
+ * Thus this is not recommended for general use.
+ */
+GC_API void GC_CALL GC_finalize_all(void);
+
+#ifdef __cplusplus
+  } /* end of extern "C" */
+#endif
diff --git a/src/gc/bdwgc/include/leak_detector.h b/src/gc/bdwgc/include/leak_detector.h
new file mode 100644
index 0000000..920a5fc
--- /dev/null
+++ b/src/gc/bdwgc/include/leak_detector.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (c) 2000-2011 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_LEAK_DETECTOR_H
+#define GC_LEAK_DETECTOR_H
+
+/* Include leak_detector.h (eg., via GCC --include directive)   */
+/* to turn BoehmGC into a Leak Detector.                        */
+
+#ifndef GC_DEBUG
+# define GC_DEBUG
+#endif
+#include "gc.h"
+
+#undef malloc
+#define malloc(n) GC_MALLOC(n)
+#undef calloc
+#define calloc(m,n) GC_MALLOC((m)*(n))
+#undef free
+#define free(p) GC_FREE(p)
+#undef realloc
+#define realloc(p,n) GC_REALLOC(p,n)
+
+#undef strdup
+#define strdup(s) GC_STRDUP(s)
+#undef strndup
+#define strndup(s,n) GC_STRNDUP(s,n)
+
+#undef memalign
+#define memalign(a,n) GC_memalign(a,n)
+#undef posix_memalign
+#define posix_memalign(p,a,n) GC_posix_memalign(p,a,n)
+
+#ifndef CHECK_LEAKS
+# define CHECK_LEAKS() GC_gcollect()
+  /* Note 1: CHECK_LEAKS does not have GC prefix (preserved for */
+  /* backward compatibility).                                   */
+  /* Note 2: GC_gcollect() is also called automatically in the  */
+  /* leak-finding mode at program exit.                         */
+#endif
+
+#endif /* GC_LEAK_DETECTOR_H */
diff --git a/src/gc/bdwgc/include/new_gc_alloc.h b/src/gc/bdwgc/include/new_gc_alloc.h
new file mode 100644
index 0000000..d429310
--- /dev/null
+++ b/src/gc/bdwgc/include/new_gc_alloc.h
@@ -0,0 +1,483 @@
+/*
+ * Copyright (c) 1996-1998 by Silicon Graphics.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+//
+// This is a revision of gc_alloc.h for SGI STL versions > 3.0
+// Unlike earlier versions, it supplements the standard "alloc.h"
+// instead of replacing it.
+//
+// This is sloppy about variable names used in header files.
+// It also doesn't yet understand the new header file names or
+// namespaces.
+//
+// This assumes the collector has been compiled with -DATOMIC_UNCOLLECTABLE.
+// The user should also consider -DREDIRECT_MALLOC=GC_uncollectable_malloc,
+// to ensure that object allocated through malloc are traced.
+//
+// Some of this could be faster in the explicit deallocation case.
+// In particular, we spend too much time clearing objects on the
+// free lists.  That could be avoided.
+//
+// This uses template classes with static members, and hence does not work
+// with g++ 2.7.2 and earlier.
+//
+// Unlike its predecessor, this one simply defines
+//      gc_alloc
+//      single_client_gc_alloc
+//      traceable_alloc
+//      single_client_traceable_alloc
+//
+// It does not redefine alloc.  Nor does it change the default allocator,
+// though the user may wish to do so.  (The argument against changing
+// the default allocator is that it may introduce subtle link compatibility
+// problems.  The argument for changing it is that the usual default
+// allocator is usually a very bad choice for a garbage collected environment.)
+//
+// This code assumes that the collector itself has been compiled with a
+// compiler that defines __STDC__ .
+//
+
+#ifndef GC_ALLOC_H
+
+#include "gc.h"
+
+#if (__GNUC__ < 3)
+# include <stack>  // A more portable way to get stl_alloc.h .
+#else
+# include <bits/stl_alloc.h>
+# ifndef __STL_BEGIN_NAMESPACE
+# define __STL_BEGIN_NAMESPACE namespace std {
+# define __STL_END_NAMESPACE };
+# endif
+#ifndef __STL_USE_STD_ALLOCATORS
+#define __STL_USE_STD_ALLOCATORS
+#endif
+#endif
+
+/* A hack to deal with gcc 3.1.  If you are using gcc3.1 and later,     */
+/* you should probably really use gc_allocator.h instead.               */
+#if defined (__GNUC__) && \
+    (__GNUC__ > 3 || (__GNUC__ == 3 && (__GNUC_MINOR__ >= 1)))
+# define simple_alloc __simple_alloc
+#endif
+
+#define GC_ALLOC_H
+
+#include <stddef.h>
+#include <string.h>
+
+// The following need to match collector data structures.
+// We can't include gc_priv.h, since that pulls in way too much stuff.
+// This should eventually be factored out into another include file.
+
+extern "C" {
+    GC_API void ** const GC_objfreelist_ptr;
+    GC_API void ** const GC_aobjfreelist_ptr;
+    GC_API void ** const GC_uobjfreelist_ptr;
+    GC_API void ** const GC_auobjfreelist_ptr;
+
+    GC_API void GC_CALL GC_incr_bytes_allocd(size_t bytes);
+    GC_API void GC_CALL GC_incr_bytes_freed(size_t bytes);
+
+    GC_API char * GC_CALL GC_generic_malloc_words_small(size_t word, int kind);
+                /* FIXME: Doesn't exist anymore.        */
+}
+
+// Object kinds; must match PTRFREE, NORMAL, UNCOLLECTABLE, and
+// AUNCOLLECTABLE in gc_priv.h.
+
+enum { GC_PTRFREE = 0, GC_NORMAL = 1, GC_UNCOLLECTABLE = 2,
+       GC_AUNCOLLECTABLE = 3 };
+
+enum { GC_max_fast_bytes = 255 };
+
+enum { GC_bytes_per_word = sizeof(char *) };
+
+enum { GC_byte_alignment = 8 };
+
+enum { GC_word_alignment = GC_byte_alignment/GC_bytes_per_word };
+
+inline void * &GC_obj_link(void * p)
+{   return *reinterpret_cast<void **>(p);  }
+
+// Compute a number of words >= n+1 bytes.
+// The +1 allows for pointers one past the end.
+inline size_t GC_round_up(size_t n)
+{
+    return ((n + GC_byte_alignment)/GC_byte_alignment)*GC_word_alignment;
+}
+
+// The same but don't allow for extra byte.
+inline size_t GC_round_up_uncollectable(size_t n)
+{
+    return ((n + GC_byte_alignment - 1)/GC_byte_alignment)*GC_word_alignment;
+}
+
+template <int dummy>
+class GC_aux_template {
+public:
+  // File local count of allocated words.  Occasionally this is
+  // added into the global count.  A separate count is necessary since the
+  // real one must be updated with a procedure call.
+  static size_t GC_bytes_recently_allocd;
+
+  // Same for uncollectible memory.  Not yet reflected in either
+  // GC_bytes_recently_allocd or GC_non_gc_bytes.
+  static size_t GC_uncollectable_bytes_recently_allocd;
+
+  // Similar counter for explicitly deallocated memory.
+  static size_t GC_bytes_recently_freed;
+
+  // Again for uncollectible memory.
+  static size_t GC_uncollectable_bytes_recently_freed;
+
+  static void * GC_out_of_line_malloc(size_t nwords, int kind);
+};
+
+template <int dummy>
+size_t GC_aux_template<dummy>::GC_bytes_recently_allocd = 0;
+
+template <int dummy>
+size_t GC_aux_template<dummy>::GC_uncollectable_bytes_recently_allocd = 0;
+
+template <int dummy>
+size_t GC_aux_template<dummy>::GC_bytes_recently_freed = 0;
+
+template <int dummy>
+size_t GC_aux_template<dummy>::GC_uncollectable_bytes_recently_freed = 0;
+
+template <int dummy>
+void * GC_aux_template<dummy>::GC_out_of_line_malloc(size_t nwords, int kind)
+{
+    GC_bytes_recently_allocd += GC_uncollectable_bytes_recently_allocd;
+    GC_non_gc_bytes +=
+                GC_uncollectable_bytes_recently_allocd;
+    GC_uncollectable_bytes_recently_allocd = 0;
+
+    GC_bytes_recently_freed += GC_uncollectable_bytes_recently_freed;
+    GC_non_gc_bytes -= GC_uncollectable_bytes_recently_freed;
+    GC_uncollectable_bytes_recently_freed = 0;
+
+    GC_incr_bytes_allocd(GC_bytes_recently_allocd);
+    GC_bytes_recently_allocd = 0;
+
+    GC_incr_bytes_freed(GC_bytes_recently_freed);
+    GC_bytes_recently_freed = 0;
+
+    return GC_generic_malloc_words_small(nwords, kind);
+}
+
+typedef GC_aux_template<0> GC_aux;
+
+// A fast, single-threaded, garbage-collected allocator
+// We assume the first word will be immediately overwritten.
+// In this version, deallocation is not a no-op, and explicit
+// deallocation is likely to help performance.
+template <int dummy>
+class single_client_gc_alloc_template {
+    public:
+        static void * allocate(size_t n)
+        {
+            size_t nwords = GC_round_up(n);
+            void ** flh;
+            void * op;
+
+            if (n > GC_max_fast_bytes) return GC_malloc(n);
+            flh = GC_objfreelist_ptr + nwords;
+            if (0 == (op = *flh)) {
+                return GC_aux::GC_out_of_line_malloc(nwords, GC_NORMAL);
+            }
+            *flh = GC_obj_link(op);
+            GC_aux::GC_bytes_recently_allocd += nwords * GC_bytes_per_word;
+            return op;
+        }
+        static void * ptr_free_allocate(size_t n)
+        {
+            size_t nwords = GC_round_up(n);
+            void ** flh;
+            void * op;
+
+            if (n > GC_max_fast_bytes) return GC_malloc_atomic(n);
+            flh = GC_aobjfreelist_ptr + nwords;
+            if (0 == (op = *flh)) {
+                return GC_aux::GC_out_of_line_malloc(nwords, GC_PTRFREE);
+            }
+            *flh = GC_obj_link(op);
+            GC_aux::GC_bytes_recently_allocd += nwords * GC_bytes_per_word;
+            return op;
+        }
+        static void deallocate(void *p, size_t n)
+        {
+            size_t nwords = GC_round_up(n);
+            void ** flh;
+
+            if (n > GC_max_fast_bytes)  {
+                GC_free(p);
+            } else {
+                flh = GC_objfreelist_ptr + nwords;
+                GC_obj_link(p) = *flh;
+                memset(reinterpret_cast<char *>(p) + GC_bytes_per_word, 0,
+                       GC_bytes_per_word * (nwords - 1));
+                *flh = p;
+                GC_aux::GC_bytes_recently_freed += nwords * GC_bytes_per_word;
+            }
+        }
+        static void ptr_free_deallocate(void *p, size_t n)
+        {
+            size_t nwords = GC_round_up(n);
+            void ** flh;
+
+            if (n > GC_max_fast_bytes) {
+                GC_free(p);
+            } else {
+                flh = GC_aobjfreelist_ptr + nwords;
+                GC_obj_link(p) = *flh;
+                *flh = p;
+                GC_aux::GC_bytes_recently_freed += nwords * GC_bytes_per_word;
+            }
+        }
+};
+
+typedef single_client_gc_alloc_template<0> single_client_gc_alloc;
+
+// Once more, for uncollectible objects.
+template <int dummy>
+class single_client_traceable_alloc_template {
+    public:
+        static void * allocate(size_t n)
+        {
+            size_t nwords = GC_round_up_uncollectable(n);
+            void ** flh;
+            void * op;
+
+            if (n > GC_max_fast_bytes) return GC_malloc_uncollectable(n);
+            flh = GC_uobjfreelist_ptr + nwords;
+            if (0 == (op = *flh)) {
+                return GC_aux::GC_out_of_line_malloc(nwords, GC_UNCOLLECTABLE);
+            }
+            *flh = GC_obj_link(op);
+            GC_aux::GC_uncollectable_bytes_recently_allocd +=
+                                        nwords * GC_bytes_per_word;
+            return op;
+        }
+        static void * ptr_free_allocate(size_t n)
+        {
+            size_t nwords = GC_round_up_uncollectable(n);
+            void ** flh;
+            void * op;
+
+            if (n > GC_max_fast_bytes) return GC_malloc_atomic_uncollectable(n);
+            flh = GC_auobjfreelist_ptr + nwords;
+            if (0 == (op = *flh)) {
+                return GC_aux::GC_out_of_line_malloc(nwords, GC_AUNCOLLECTABLE);
+            }
+            *flh = GC_obj_link(op);
+            GC_aux::GC_uncollectable_bytes_recently_allocd +=
+                                        nwords * GC_bytes_per_word;
+            return op;
+        }
+        static void deallocate(void *p, size_t n)
+        {
+            size_t nwords = GC_round_up_uncollectable(n);
+            void ** flh;
+
+            if (n > GC_max_fast_bytes)  {
+                GC_free(p);
+            } else {
+                flh = GC_uobjfreelist_ptr + nwords;
+                GC_obj_link(p) = *flh;
+                *flh = p;
+                GC_aux::GC_uncollectable_bytes_recently_freed +=
+                                nwords * GC_bytes_per_word;
+            }
+        }
+        static void ptr_free_deallocate(void *p, size_t n)
+        {
+            size_t nwords = GC_round_up_uncollectable(n);
+            void ** flh;
+
+            if (n > GC_max_fast_bytes) {
+                GC_free(p);
+            } else {
+                flh = GC_auobjfreelist_ptr + nwords;
+                GC_obj_link(p) = *flh;
+                *flh = p;
+                GC_aux::GC_uncollectable_bytes_recently_freed +=
+                                nwords * GC_bytes_per_word;
+            }
+        }
+};
+
+typedef single_client_traceable_alloc_template<0> single_client_traceable_alloc;
+
+template < int dummy >
+class gc_alloc_template {
+    public:
+        static void * allocate(size_t n) { return GC_malloc(n); }
+        static void * ptr_free_allocate(size_t n)
+                { return GC_malloc_atomic(n); }
+        static void deallocate(void *, size_t) { }
+        static void ptr_free_deallocate(void *, size_t) { }
+};
+
+typedef gc_alloc_template < 0 > gc_alloc;
+
+template < int dummy >
+class traceable_alloc_template {
+    public:
+        static void * allocate(size_t n) { return GC_malloc_uncollectable(n); }
+        static void * ptr_free_allocate(size_t n)
+                { return GC_malloc_atomic_uncollectable(n); }
+        static void deallocate(void *p, size_t) { GC_free(p); }
+        static void ptr_free_deallocate(void *p, size_t) { GC_free(p); }
+};
+
+typedef traceable_alloc_template < 0 > traceable_alloc;
+
+// We want to specialize simple_alloc so that it does the right thing
+// for all pointer-free types.  At the moment there is no portable way to
+// even approximate that.  The following approximation should work for
+// SGI compilers, and recent versions of g++.
+
+// GC_SPECIALIZE() is used internally.
+#define GC_SPECIALIZE(T,alloc) \
+  class simple_alloc<T, alloc> { \
+  public: \
+    static T *allocate(size_t n) \
+        { return 0 == n? 0 : \
+            reinterpret_cast<T*>(alloc::ptr_free_allocate(n * sizeof(T))); } \
+    static T *allocate(void) \
+        { return reinterpret_cast<T*>(alloc::ptr_free_allocate(sizeof(T))); } \
+    static void deallocate(T *p, size_t n) \
+        { if (0 != n) alloc::ptr_free_deallocate(p, n * sizeof(T)); } \
+    static void deallocate(T *p) \
+        { alloc::ptr_free_deallocate(p, sizeof(T)); } \
+  };
+
+__STL_BEGIN_NAMESPACE
+
+GC_SPECIALIZE(char, gc_alloc)
+GC_SPECIALIZE(int, gc_alloc)
+GC_SPECIALIZE(unsigned, gc_alloc)
+GC_SPECIALIZE(float, gc_alloc)
+GC_SPECIALIZE(double, gc_alloc)
+
+GC_SPECIALIZE(char, traceable_alloc)
+GC_SPECIALIZE(int, traceable_alloc)
+GC_SPECIALIZE(unsigned, traceable_alloc)
+GC_SPECIALIZE(float, traceable_alloc)
+GC_SPECIALIZE(double, traceable_alloc)
+
+GC_SPECIALIZE(char, single_client_gc_alloc)
+GC_SPECIALIZE(int, single_client_gc_alloc)
+GC_SPECIALIZE(unsigned, single_client_gc_alloc)
+GC_SPECIALIZE(float, single_client_gc_alloc)
+GC_SPECIALIZE(double, single_client_gc_alloc)
+
+GC_SPECIALIZE(char, single_client_traceable_alloc)
+GC_SPECIALIZE(int, single_client_traceable_alloc)
+GC_SPECIALIZE(unsigned, single_client_traceable_alloc)
+GC_SPECIALIZE(float, single_client_traceable_alloc)
+GC_SPECIALIZE(double, single_client_traceable_alloc)
+
+__STL_END_NAMESPACE
+
+#ifdef __STL_USE_STD_ALLOCATORS
+
+__STL_BEGIN_NAMESPACE
+
+template <class _Tp>
+struct _Alloc_traits<_Tp, gc_alloc >
+{
+  static const bool _S_instanceless = true;
+  typedef simple_alloc<_Tp, gc_alloc > _Alloc_type;
+  typedef __allocator<_Tp, gc_alloc > allocator_type;
+};
+
+inline bool operator==(const gc_alloc&,
+                       const gc_alloc&)
+{
+  return true;
+}
+
+inline bool operator!=(const gc_alloc&,
+                       const gc_alloc&)
+{
+  return false;
+}
+
+template <class _Tp>
+struct _Alloc_traits<_Tp, single_client_gc_alloc >
+{
+  static const bool _S_instanceless = true;
+  typedef simple_alloc<_Tp, single_client_gc_alloc > _Alloc_type;
+  typedef __allocator<_Tp, single_client_gc_alloc > allocator_type;
+};
+
+inline bool operator==(const single_client_gc_alloc&,
+                       const single_client_gc_alloc&)
+{
+  return true;
+}
+
+inline bool operator!=(const single_client_gc_alloc&,
+                       const single_client_gc_alloc&)
+{
+  return false;
+}
+
+template <class _Tp>
+struct _Alloc_traits<_Tp, traceable_alloc >
+{
+  static const bool _S_instanceless = true;
+  typedef simple_alloc<_Tp, traceable_alloc > _Alloc_type;
+  typedef __allocator<_Tp, traceable_alloc > allocator_type;
+};
+
+inline bool operator==(const traceable_alloc&,
+                       const traceable_alloc&)
+{
+  return true;
+}
+
+inline bool operator!=(const traceable_alloc&,
+                       const traceable_alloc&)
+{
+  return false;
+}
+
+template <class _Tp>
+struct _Alloc_traits<_Tp, single_client_traceable_alloc >
+{
+  static const bool _S_instanceless = true;
+  typedef simple_alloc<_Tp, single_client_traceable_alloc > _Alloc_type;
+  typedef __allocator<_Tp, single_client_traceable_alloc > allocator_type;
+};
+
+inline bool operator==(const single_client_traceable_alloc&,
+                       const single_client_traceable_alloc&)
+{
+  return true;
+}
+
+inline bool operator!=(const single_client_traceable_alloc&,
+                       const single_client_traceable_alloc&)
+{
+  return false;
+}
+
+__STL_END_NAMESPACE
+
+#endif /* __STL_USE_STD_ALLOCATORS */
+
+#endif /* GC_ALLOC_H */
diff --git a/src/gc/bdwgc/include/private/config.h.in b/src/gc/bdwgc/include/private/config.h.in
new file mode 100644
index 0000000..f09756a
--- /dev/null
+++ b/src/gc/bdwgc/include/private/config.h.in
@@ -0,0 +1,237 @@
+/* include/private/config.h.in.  Generated from configure.ac by autoheader.  */
+
+/* Define to recognise all pointers to the interior of objects. */
+#undef ALL_INTERIOR_POINTERS
+
+/* Define to enable atomic uncollectible allocation. */
+#undef ATOMIC_UNCOLLECTABLE
+
+/* See doc/README.macros. */
+#undef DARWIN_DONT_PARSE_STACK
+
+/* Define to force debug headers on all objects. */
+#undef DBG_HDRS_ALL
+
+/* Define to enable support for DB/UX threads. */
+#undef DGUX_THREADS
+
+/* Define to enable eCos target support. */
+#undef ECOS
+
+/* Wine getenv may not return NULL for missing entry. */
+#undef EMPTY_GETENV_RESULTS
+
+/* Define to support IBM AIX threads. */
+#undef GC_AIX_THREADS
+
+/* The alpha version number, if applicable. */
+#undef GC_ALPHA_VERSION
+
+/* Define to enable internal debug assertions. */
+#undef GC_ASSERTIONS
+
+/* Define to support Darwin pthreads. */
+#undef GC_DARWIN_THREADS
+
+/* Define to enable support for DB/UX threads on i386. */
+#undef GC_DGUX386_THREADS
+
+/* Define to build dynamic libraries with only API symbols exposed. */
+#undef GC_DLL
+
+/* Define to support FreeBSD pthreads. */
+#undef GC_FREEBSD_THREADS
+
+/* Define to include support for gcj. */
+#undef GC_GCJ_SUPPORT
+
+/* Define to support GNU pthreads. */
+#undef GC_GNU_THREADS
+
+/* Define if backtrace information is supported. */
+#undef GC_HAVE_BUILTIN_BACKTRACE
+
+/* Define to support HP/UX 11 pthreads. */
+#undef GC_HPUX_THREADS
+
+/* Define to support Irix pthreads. */
+#undef GC_IRIX_THREADS
+
+/* Define to support pthreads on Linux. */
+#undef GC_LINUX_THREADS
+
+/* Define to support NetBSD pthreads. */
+#undef GC_NETBSD_THREADS
+
+/* Define to support OpenBSD pthreads. */
+#undef GC_OPENBSD_THREADS
+
+/* Define to support Tru64 pthreads. */
+#undef GC_OSF1_THREADS
+
+/* Define to support rtems-pthreads. */
+#undef GC_RTEMS_PTHREADS
+
+/* Define to support Solaris pthreads. */
+#undef GC_SOLARIS_THREADS
+
+/* Define to support platform-specific threads. */
+#undef GC_THREADS
+
+/* See doc/README.macros. */
+#undef GC_USE_DLOPEN_WRAP
+
+/* The major version number of this GC release. */
+#undef GC_VERSION_MAJOR
+
+/* The minor version number of this GC release. */
+#undef GC_VERSION_MINOR
+
+/* Define to support win32-pthreads. */
+#undef GC_WIN32_PTHREADS
+
+/* Define to support Win32 threads. */
+#undef GC_WIN32_THREADS
+
+/* Define to install pthread_atfork() handlers by default. */
+#undef HANDLE_FORK
+
+/* Define to 1 if you have the <dlfcn.h> header file. */
+#undef HAVE_DLFCN_H
+
+/* Define to 1 if you have the <inttypes.h> header file. */
+#undef HAVE_INTTYPES_H
+
+/* Define to 1 if you have the <memory.h> header file. */
+#undef HAVE_MEMORY_H
+
+/* Define to 1 if you have the <stdint.h> header file. */
+#undef HAVE_STDINT_H
+
+/* Define to 1 if you have the <stdlib.h> header file. */
+#undef HAVE_STDLIB_H
+
+/* Define to 1 if you have the <strings.h> header file. */
+#undef HAVE_STRINGS_H
+
+/* Define to 1 if you have the <string.h> header file. */
+#undef HAVE_STRING_H
+
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#undef HAVE_SYS_STAT_H
+
+/* Define to 1 if you have the <sys/types.h> header file. */
+#undef HAVE_SYS_TYPES_H
+
+/* Define to 1 if you have the <unistd.h> header file. */
+#undef HAVE_UNISTD_H
+
+/* See doc/README.macros. */
+#undef JAVA_FINALIZATION
+
+/* Define to save back-pointers in debugging headers. */
+#undef KEEP_BACK_PTRS
+
+/* Define to optimize for large heaps or root sets. */
+#undef LARGE_CONFIG
+
+/* Define to the sub-directory in which libtool stores uninstalled libraries.
+   */
+#undef LT_OBJDIR
+
+/* See doc/README.macros. */
+#undef MAKE_BACK_GRAPH
+
+/* Number of GC cycles to wait before unmapping an unused block. */
+#undef MUNMAP_THRESHOLD
+
+/* Define to not use system clock (cross compiling). */
+#undef NO_CLOCK
+
+/* Disable debugging, like GC_dump and its callees. */
+#undef NO_DEBUGGING
+
+/* Define to make the collector not allocate executable memory by default. */
+#undef NO_EXECUTE_PERMISSION
+
+/* Prohibit installation of pthread_atfork() handlers. */
+#undef NO_HANDLE_FORK
+
+/* Name of package */
+#undef PACKAGE
+
+/* Define to the address where bug reports for this package should be sent. */
+#undef PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#undef PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#undef PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#undef PACKAGE_TARNAME
+
+/* Define to the home page for this package. */
+#undef PACKAGE_URL
+
+/* Define to the version of this package. */
+#undef PACKAGE_VERSION
+
+/* Define to enable parallel marking. */
+#undef PARALLEL_MARK
+
+/* If defined, redirect free to this function. */
+#undef REDIRECT_FREE
+
+/* If defined, redirect malloc to this function. */
+#undef REDIRECT_MALLOC
+
+/* If defined, redirect GC_realloc to this function. */
+#undef REDIRECT_REALLOC
+
+/* The number of caller frames saved when allocating with the debugging API.
+   */
+#undef SAVE_CALL_COUNT
+
+/* Define to tune the collector for small heap sizes. */
+#undef SMALL_CONFIG
+
+/* See the comment in gcconfig.h. */
+#undef SOLARIS25_PROC_VDB_BUG_FIXED
+
+/* Define to 1 if you have the ANSI C header files. */
+#undef STDC_HEADERS
+
+/* Define to work around a Solaris 5.3 bug (see dyn_load.c). */
+#undef SUNOS53_SHARED_LIB
+
+/* Define to enable thread-local allocation optimization. */
+#undef THREAD_LOCAL_ALLOC
+
+/* Define to use of compiler-support for thread-local variables. */
+#undef USE_COMPILER_TLS
+
+/* Define to use mmap instead of sbrk to expand the heap. */
+#undef USE_MMAP
+
+/* Define to return memory to OS with munmap calls (see doc/README.macros). */
+#undef USE_MUNMAP
+
+/* Version number of package */
+#undef VERSION
+
+/* The POSIX feature macro. */
+#undef _POSIX_C_SOURCE
+
+/* Indicates the use of pthreads (NetBSD). */
+#undef _PTHREADS
+
+/* Required define if using POSIX threads. */
+#undef _REENTRANT
+
+/* Define to `__inline__' or `__inline' if that's what the C compiler
+   calls it, or to nothing if 'inline' is not supported under any name.  */
+#ifndef __cplusplus
+#undef inline
+#endif
diff --git a/src/gc/bdwgc/include/private/cord_pos.h b/src/gc/bdwgc/include/private/cord_pos.h
new file mode 100644
index 0000000..d2b24bb
--- /dev/null
+++ b/src/gc/bdwgc/include/private/cord_pos.h
@@ -0,0 +1,118 @@
+/* 
+ * Copyright (c) 1993-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+/* Boehm, May 19, 1994 2:23 pm PDT */
+# ifndef CORD_POSITION_H
+
+/* The representation of CORD_position.  This is private to the	*/
+/* implementation, but the size is known to clients.  Also	*/
+/* the implementation of some exported macros relies on it.	*/
+/* Don't use anything defined here and not in cord.h.		*/
+
+# define MAX_DEPTH 48
+	/* The maximum depth of a balanced cord + 1.		*/
+	/* We don't let cords get deeper than MAX_DEPTH.	*/
+
+struct CORD_pe {
+    CORD pe_cord;
+    size_t pe_start_pos;
+};
+
+/* A structure describing an entry on the path from the root 	*/
+/* to current position.						*/
+typedef struct CORD_Pos {
+    size_t cur_pos;
+    int path_len;
+#	define CORD_POS_INVALID (0x55555555)
+		/* path_len == INVALID <==> position invalid */
+    const char *cur_leaf;	/* Current leaf, if it is a string.	*/
+    				/* If the current leaf is a function,	*/
+    				/* then this may point to function_buf	*/
+    				/* containing the next few characters.	*/
+    				/* Always points to a valid string	*/
+    				/* containing the current character 	*/
+    				/* unless cur_end is 0.			*/
+    size_t cur_start;	/* Start position of cur_leaf	*/
+    size_t cur_end;	/* Ending position of cur_leaf	*/
+    			/* 0 if cur_leaf is invalid.	*/
+    struct CORD_pe path[MAX_DEPTH + 1];
+    	/* path[path_len] is the leaf corresponding to cur_pos	*/
+    	/* path[0].pe_cord is the cord we point to.		*/
+#   define FUNCTION_BUF_SZ 8
+    char function_buf[FUNCTION_BUF_SZ];	/* Space for next few chars	*/
+    					/* from function node.		*/
+} CORD_pos[1];
+
+/* Extract the cord from a position:	*/
+CORD CORD_pos_to_cord(CORD_pos p);
+	
+/* Extract the current index from a position:	*/
+size_t CORD_pos_to_index(CORD_pos p);
+	
+/* Fetch the character located at the given position:	*/
+char CORD_pos_fetch(CORD_pos p);
+	
+/* Initialize the position to refer to the give cord and index.	*/
+/* Note that this is the most expensive function on positions:	*/
+void CORD_set_pos(CORD_pos p, CORD x, size_t i);
+	
+/* Advance the position to the next character.	*/
+/* P must be initialized and valid.		*/
+/* Invalidates p if past end:			*/
+void CORD_next(CORD_pos p);
+
+/* Move the position to the preceding character.	*/
+/* P must be initialized and valid.			*/
+/* Invalidates p if past beginning:			*/
+void CORD_prev(CORD_pos p);
+	
+/* Is the position valid, i.e. inside the cord?		*/
+int CORD_pos_valid(CORD_pos p);
+
+char CORD__pos_fetch(CORD_pos);
+void CORD__next(CORD_pos);
+void CORD__prev(CORD_pos);
+
+#define CORD_pos_fetch(p)	\
+    (((p)[0].cur_end != 0)? \
+     	(p)[0].cur_leaf[(p)[0].cur_pos - (p)[0].cur_start] \
+     	: CORD__pos_fetch(p))
+
+#define CORD_next(p)	\
+    (((p)[0].cur_pos + 1 < (p)[0].cur_end)? \
+    	(p)[0].cur_pos++ \
+    	: (CORD__next(p), 0))
+
+#define CORD_prev(p)	\
+    (((p)[0].cur_end != 0 && (p)[0].cur_pos > (p)[0].cur_start)? \
+    	(p)[0].cur_pos-- \
+    	: (CORD__prev(p), 0))
+
+#define CORD_pos_to_index(p) ((p)[0].cur_pos)
+
+#define CORD_pos_to_cord(p) ((p)[0].path[0].pe_cord)
+
+#define CORD_pos_valid(p) ((p)[0].path_len != CORD_POS_INVALID)
+
+/* Some grubby stuff for performance-critical friends:	*/
+#define CORD_pos_chars_left(p) ((long)((p)[0].cur_end) - (long)((p)[0].cur_pos))
+	/* Number of characters in cache.  <= 0 ==> none	*/
+
+#define CORD_pos_advance(p,n) ((p)[0].cur_pos += (n) - 1, CORD_next(p))
+	/* Advance position by n characters	*/
+	/* 0 < n < CORD_pos_chars_left(p)	*/
+
+#define CORD_pos_cur_char_addr(p) \
+	(p)[0].cur_leaf + ((p)[0].cur_pos - (p)[0].cur_start)
+	/* address of current character in cache.	*/
+
+#endif
diff --git a/src/gc/bdwgc/include/private/darwin_semaphore.h b/src/gc/bdwgc/include/private/darwin_semaphore.h
new file mode 100644
index 0000000..379a7f7
--- /dev/null
+++ b/src/gc/bdwgc/include/private/darwin_semaphore.h
@@ -0,0 +1,85 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_DARWIN_SEMAPHORE_H
+#define GC_DARWIN_SEMAPHORE_H
+
+#if !defined(GC_DARWIN_THREADS)
+#error darwin_semaphore.h included with GC_DARWIN_THREADS not defined
+#endif
+
+/*
+   This is a very simple semaphore implementation for darwin. It
+   is implemented in terms of pthreads calls so it isn't async signal
+   safe. This isn't a problem because signals aren't used to
+   suspend threads on darwin.
+*/
+
+typedef struct {
+    pthread_mutex_t mutex;
+    pthread_cond_t cond;
+    int value;
+} sem_t;
+
+static int sem_init(sem_t *sem, int pshared, int value) {
+    int ret;
+    if(pshared)
+        ABORT("sem_init with pshared set");
+    sem->value = value;
+
+    ret = pthread_mutex_init(&sem->mutex,NULL);
+    if(ret < 0) return -1;
+    ret = pthread_cond_init(&sem->cond,NULL);
+    if(ret < 0) return -1;
+    return 0;
+}
+
+static int sem_post(sem_t *sem) {
+    if(pthread_mutex_lock(&sem->mutex) < 0)
+        return -1;
+    sem->value++;
+    if(pthread_cond_signal(&sem->cond) < 0) {
+        pthread_mutex_unlock(&sem->mutex);
+        return -1;
+    }
+    if(pthread_mutex_unlock(&sem->mutex) < 0)
+        return -1;
+    return 0;
+}
+
+static int sem_wait(sem_t *sem) {
+    if(pthread_mutex_lock(&sem->mutex) < 0)
+        return -1;
+    while(sem->value == 0) {
+        pthread_cond_wait(&sem->cond,&sem->mutex);
+    }
+    sem->value--;
+    if(pthread_mutex_unlock(&sem->mutex) < 0)
+        return -1;
+    return 0;
+}
+
+static int sem_destroy(sem_t *sem) {
+    int ret;
+    ret = pthread_cond_destroy(&sem->cond);
+    if(ret < 0) return -1;
+    ret = pthread_mutex_destroy(&sem->mutex);
+    if(ret < 0) return -1;
+    return 0;
+}
+
+#endif
diff --git a/src/gc/bdwgc/include/private/darwin_stop_world.h b/src/gc/bdwgc/include/private/darwin_stop_world.h
new file mode 100644
index 0000000..399304e
--- /dev/null
+++ b/src/gc/bdwgc/include/private/darwin_stop_world.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_DARWIN_STOP_WORLD_H
+#define GC_DARWIN_STOP_WORLD_H
+
+#if !defined(GC_DARWIN_THREADS)
+# error darwin_stop_world.h included without GC_DARWIN_THREADS defined
+#endif
+
+#include <mach/mach.h>
+#include <mach/thread_act.h>
+
+struct thread_stop_info {
+  mach_port_t mach_thread;
+  ptr_t stack_ptr; /* Valid only when thread is in a "blocked" state.   */
+};
+
+#ifndef DARWIN_DONT_PARSE_STACK
+  GC_INNER ptr_t GC_FindTopOfStack(unsigned long);
+#endif
+
+#ifdef MPROTECT_VDB
+  GC_INNER void GC_mprotect_stop(void);
+  GC_INNER void GC_mprotect_resume(void);
+#endif
+
+#if defined(PARALLEL_MARK) && !defined(GC_NO_THREADS_DISCOVERY)
+  GC_INNER GC_bool GC_is_mach_marker(thread_act_t);
+#endif
+
+#endif
diff --git a/src/gc/bdwgc/include/private/dbg_mlc.h b/src/gc/bdwgc/include/private/dbg_mlc.h
new file mode 100644
index 0000000..deda69c
--- /dev/null
+++ b/src/gc/bdwgc/include/private/dbg_mlc.h
@@ -0,0 +1,166 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1997 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/*
+ * This is mostly an internal header file.  Typical clients should
+ * not use it.  Clients that define their own object kinds with
+ * debugging allocators will probably want to include this, however.
+ * No attempt is made to keep the namespace clean.  This should not be
+ * included from header files that are frequently included by clients.
+ */
+
+#ifndef _DBG_MLC_H
+#define _DBG_MLC_H
+
+#include "gc_priv.h"
+#ifdef KEEP_BACK_PTRS
+# include "gc_backptr.h"
+#endif
+
+#if CPP_WORDSZ == 32
+# define START_FLAG (word)0xfedcedcb
+# define END_FLAG (word)0xbcdecdef
+#else
+# define START_FLAG GC_WORD_C(0xFEDCEDCBfedcedcb)
+# define END_FLAG GC_WORD_C(0xBCDECDEFbcdecdef)
+#endif
+        /* Stored both one past the end of user object, and one before  */
+        /* the end of the object as seen by the allocator.              */
+
+#if defined(KEEP_BACK_PTRS) || defined(PRINT_BLACK_LIST) \
+    || defined(MAKE_BACK_GRAPH)
+  /* Pointer "source"s that aren't real locations.      */
+  /* Used in oh_back_ptr fields and as "source"         */
+  /* argument to some marking functions.                */
+# define NOT_MARKED (ptr_t)0
+# define MARKED_FOR_FINALIZATION ((ptr_t)(word)2)
+                /* Object was marked because it is finalizable. */
+# define MARKED_FROM_REGISTER ((ptr_t)(word)4)
+                /* Object was marked from a register.  Hence the        */
+                /* source of the reference doesn't have an address.     */
+#endif /* KEEP_BACK_PTRS || PRINT_BLACK_LIST */
+
+/* Object header */
+typedef struct {
+# if defined(KEEP_BACK_PTRS) || defined(MAKE_BACK_GRAPH)
+    /* We potentially keep two different kinds of back          */
+    /* pointers.  KEEP_BACK_PTRS stores a single back           */
+    /* pointer in each reachable object to allow reporting      */
+    /* of why an object was retained.  MAKE_BACK_GRAPH          */
+    /* builds a graph containing the inverse of all             */
+    /* "points-to" edges including those involving              */
+    /* objects that have just become unreachable. This          */
+    /* allows detection of growing chains of unreachable        */
+    /* objects.  It may be possible to eventually combine       */
+    /* both, but for now we keep them separate.  Both           */
+    /* kinds of back pointers are hidden using the              */
+    /* following macros.  In both cases, the plain version      */
+    /* is constrained to have an least significant bit of 1,    */
+    /* to allow it to be distinguished from a free list         */
+    /* link.  This means the plain version must have an         */
+    /* lsb of 0.                                                */
+    /* Note that blocks dropped by black-listing will           */
+    /* also have the lsb clear once debugging has               */
+    /* started.                                                 */
+    /* We're careful never to overwrite a value with lsb 0.     */
+#   if ALIGNMENT == 1
+      /* Fudge back pointer to be even. */
+#     define HIDE_BACK_PTR(p) GC_HIDE_POINTER(~1 & (GC_word)(p))
+#   else
+#     define HIDE_BACK_PTR(p) GC_HIDE_POINTER(p)
+#   endif
+#   ifdef KEEP_BACK_PTRS
+      GC_hidden_pointer oh_back_ptr;
+#   endif
+#   ifdef MAKE_BACK_GRAPH
+      GC_hidden_pointer oh_bg_ptr;
+#   endif
+#   if defined(KEEP_BACK_PTRS) != defined(MAKE_BACK_GRAPH)
+      /* Keep double-pointer-sized alignment.   */
+      word oh_dummy;
+#   endif
+# endif
+  const char * oh_string;       /* object descriptor string     */
+  word oh_int;                  /* object descriptor integers   */
+# ifdef NEED_CALLINFO
+    struct callinfo oh_ci[NFRAMES];
+# endif
+# ifndef SHORT_DBG_HDRS
+    word oh_sz;                 /* Original malloc arg.         */
+    word oh_sf;                 /* start flag */
+# endif /* SHORT_DBG_HDRS */
+} oh;
+/* The size of the above structure is assumed not to de-align things,   */
+/* and to be a multiple of the word length.                             */
+
+#ifdef SHORT_DBG_HDRS
+# define DEBUG_BYTES (sizeof (oh))
+# define UNCOLLECTABLE_DEBUG_BYTES DEBUG_BYTES
+#else
+  /* Add space for END_FLAG, but use any extra space that was already   */
+  /* added to catch off-the-end pointers.                               */
+  /* For uncollectible objects, the extra byte is not added.            */
+# define UNCOLLECTABLE_DEBUG_BYTES (sizeof (oh) + sizeof (word))
+# define DEBUG_BYTES (UNCOLLECTABLE_DEBUG_BYTES - EXTRA_BYTES)
+#endif
+
+/* Round bytes to words without adding extra byte at end.       */
+#define SIMPLE_ROUNDED_UP_WORDS(n) BYTES_TO_WORDS((n) + WORDS_TO_BYTES(1) - 1)
+
+/* ADD_CALL_CHAIN stores a (partial) call chain into an object  */
+/* header.  It may be called with or without the allocation     */
+/* lock.                                                        */
+/* PRINT_CALL_CHAIN prints the call chain stored in an object   */
+/* to stderr.  It requires that we do not hold the lock.        */
+#if defined(SAVE_CALL_CHAIN)
+  struct callinfo;
+  GC_INNER void GC_save_callers(struct callinfo info[NFRAMES]);
+  GC_INNER void GC_print_callers(struct callinfo info[NFRAMES]);
+# define ADD_CALL_CHAIN(base, ra) GC_save_callers(((oh *)(base)) -> oh_ci)
+# define PRINT_CALL_CHAIN(base) GC_print_callers(((oh *)(base)) -> oh_ci)
+#elif defined(GC_ADD_CALLER)
+  struct callinfo;
+  GC_INNER void GC_print_callers(struct callinfo info[NFRAMES]);
+# define ADD_CALL_CHAIN(base, ra) ((oh *)(base)) -> oh_ci[0].ci_pc = (ra)
+# define PRINT_CALL_CHAIN(base) GC_print_callers(((oh *)(base)) -> oh_ci)
+#else
+# define ADD_CALL_CHAIN(base, ra)
+# define PRINT_CALL_CHAIN(base)
+#endif
+
+#ifdef GC_ADD_CALLER
+# define OPT_RA ra,
+#else
+# define OPT_RA
+#endif
+
+/* Check whether object with base pointer p has debugging info  */
+/* p is assumed to point to a legitimate object in our part     */
+/* of the heap.                                                 */
+#ifdef SHORT_DBG_HDRS
+# define GC_has_other_debug_info(p) 1
+#else
+  GC_INNER int GC_has_other_debug_info(ptr_t p);
+#endif
+
+#if defined(KEEP_BACK_PTRS) || defined(MAKE_BACK_GRAPH)
+# define GC_HAS_DEBUG_INFO(p) \
+        ((*((word *)p) & 1) && GC_has_other_debug_info(p) > 0)
+#else
+# define GC_HAS_DEBUG_INFO(p) (GC_has_other_debug_info(p) > 0)
+#endif
+
+#endif /* _DBG_MLC_H */
diff --git a/src/gc/bdwgc/include/private/gc_hdrs.h b/src/gc/bdwgc/include/private/gc_hdrs.h
new file mode 100644
index 0000000..0360adb
--- /dev/null
+++ b/src/gc/bdwgc/include/private/gc_hdrs.h
@@ -0,0 +1,204 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+/* Boehm, July 11, 1995 11:54 am PDT */
+#ifndef GC_HEADERS_H
+#define GC_HEADERS_H
+
+typedef struct hblkhdr hdr;
+
+#if CPP_WORDSZ != 32 && CPP_WORDSZ < 36
+        --> Get a real machine.
+#endif
+
+/*
+ * The 2 level tree data structure that is used to find block headers.
+ * If there are more than 32 bits in a pointer, the top level is a hash
+ * table.
+ *
+ * This defines HDR, GET_HDR, and SET_HDR, the main macros used to
+ * retrieve and set object headers.
+ *
+ * We take advantage of a header lookup
+ * cache.  This is a locally declared direct mapped cache, used inside
+ * the marker.  The HC_GET_HDR macro uses and maintains this
+ * cache.  Assuming we get reasonable hit rates, this shaves a few
+ * memory references from each pointer validation.
+ */
+
+#if CPP_WORDSZ > 32
+# define HASH_TL
+#endif
+
+/* Define appropriate out-degrees for each of the two tree levels       */
+#if defined(LARGE_CONFIG) || !defined(SMALL_CONFIG)
+# define LOG_BOTTOM_SZ 10
+#else
+# define LOG_BOTTOM_SZ 11
+        /* Keep top index size reasonable with smaller blocks.  */
+#endif
+#define BOTTOM_SZ (1 << LOG_BOTTOM_SZ)
+
+#ifndef HASH_TL
+# define LOG_TOP_SZ (WORDSZ - LOG_BOTTOM_SZ - LOG_HBLKSIZE)
+#else
+# define LOG_TOP_SZ 11
+#endif
+#define TOP_SZ (1 << LOG_TOP_SZ)
+
+/* #define COUNT_HDR_CACHE_HITS  */
+
+#ifdef COUNT_HDR_CACHE_HITS
+  extern word GC_hdr_cache_hits; /* used for debugging/profiling */
+  extern word GC_hdr_cache_misses;
+# define HC_HIT() ++GC_hdr_cache_hits
+# define HC_MISS() ++GC_hdr_cache_misses
+#else
+# define HC_HIT()
+# define HC_MISS()
+#endif
+
+typedef struct hce {
+  word block_addr;    /* right shifted by LOG_HBLKSIZE */
+  hdr * hce_hdr;
+} hdr_cache_entry;
+
+#define HDR_CACHE_SIZE 8  /* power of 2 */
+
+#define DECLARE_HDR_CACHE \
+        hdr_cache_entry hdr_cache[HDR_CACHE_SIZE]
+
+#define INIT_HDR_CACHE BZERO(hdr_cache, sizeof(hdr_cache))
+
+#define HCE(h) hdr_cache + (((word)(h) >> LOG_HBLKSIZE) & (HDR_CACHE_SIZE-1))
+
+#define HCE_VALID_FOR(hce,h) ((hce) -> block_addr == \
+                                ((word)(h) >> LOG_HBLKSIZE))
+
+#define HCE_HDR(h) ((hce) -> hce_hdr)
+
+#ifdef PRINT_BLACK_LIST
+  GC_INNER hdr * GC_header_cache_miss(ptr_t p, hdr_cache_entry *hce,
+                                      ptr_t source);
+# define HEADER_CACHE_MISS(p, hce, source) \
+          GC_header_cache_miss(p, hce, source)
+#else
+  GC_INNER hdr * GC_header_cache_miss(ptr_t p, hdr_cache_entry *hce);
+# define HEADER_CACHE_MISS(p, hce, source) GC_header_cache_miss(p, hce)
+#endif
+
+/* Set hhdr to the header for p.  Analogous to GET_HDR below,           */
+/* except that in the case of large objects, it                         */
+/* gets the header for the object beginning, if GC_all_interior_ptrs    */
+/* is set.                                                              */
+/* Returns zero if p points to somewhere other than the first page      */
+/* of an object, and it is not a valid pointer to the object.           */
+#define HC_GET_HDR(p, hhdr, source, exit_label) \
+        { \
+          hdr_cache_entry * hce = HCE(p); \
+          if (EXPECT(HCE_VALID_FOR(hce, p), TRUE)) { \
+            HC_HIT(); \
+            hhdr = hce -> hce_hdr; \
+          } else { \
+            hhdr = HEADER_CACHE_MISS(p, hce, source); \
+            if (0 == hhdr) goto exit_label; \
+          } \
+        }
+
+typedef struct bi {
+    hdr * index[BOTTOM_SZ];
+        /*
+         * The bottom level index contains one of three kinds of values:
+         * 0 means we're not responsible for this block,
+         *   or this is a block other than the first one in a free block.
+         * 1 < (long)X <= MAX_JUMP means the block starts at least
+         *        X * HBLKSIZE bytes before the current address.
+         * A valid pointer points to a hdr structure. (The above can't be
+         * valid pointers due to the GET_MEM return convention.)
+         */
+    struct bi * asc_link;       /* All indices are linked in    */
+                                /* ascending order...           */
+    struct bi * desc_link;      /* ... and in descending order. */
+    word key;                   /* high order address bits.     */
+# ifdef HASH_TL
+    struct bi * hash_link;      /* Hash chain link.             */
+# endif
+} bottom_index;
+
+/* bottom_index GC_all_nils; - really part of GC_arrays */
+
+/* extern bottom_index * GC_top_index []; - really part of GC_arrays */
+                                /* Each entry points to a bottom_index. */
+                                /* On a 32 bit machine, it points to    */
+                                /* the index for a set of high order    */
+                                /* bits equal to the index.  For longer */
+                                /* addresses, we hash the high order    */
+                                /* bits to compute the index in         */
+                                /* GC_top_index, and each entry points  */
+                                /* to a hash chain.                     */
+                                /* The last entry in each chain is      */
+                                /* GC_all_nils.                         */
+
+
+#define MAX_JUMP (HBLKSIZE - 1)
+
+#define HDR_FROM_BI(bi, p) \
+                ((bi)->index[((word)(p) >> LOG_HBLKSIZE) & (BOTTOM_SZ - 1)])
+#ifndef HASH_TL
+# define BI(p) (GC_top_index \
+              [(word)(p) >> (LOG_BOTTOM_SZ + LOG_HBLKSIZE)])
+# define HDR_INNER(p) HDR_FROM_BI(BI(p),p)
+# ifdef SMALL_CONFIG
+#     define HDR(p) GC_find_header((ptr_t)(p))
+# else
+#     define HDR(p) HDR_INNER(p)
+# endif
+# define GET_BI(p, bottom_indx) (bottom_indx) = BI(p)
+# define GET_HDR(p, hhdr) (hhdr) = HDR(p)
+# define SET_HDR(p, hhdr) HDR_INNER(p) = (hhdr)
+# define GET_HDR_ADDR(p, ha) (ha) = &(HDR_INNER(p))
+#else /* hash */
+  /* Hash function for tree top level */
+# define TL_HASH(hi) ((hi) & (TOP_SZ - 1))
+  /* Set bottom_indx to point to the bottom index for address p */
+# define GET_BI(p, bottom_indx) \
+      { \
+          register word hi = \
+              (word)(p) >> (LOG_BOTTOM_SZ + LOG_HBLKSIZE); \
+          register bottom_index * _bi = GC_top_index[TL_HASH(hi)]; \
+          while (_bi -> key != hi && _bi != GC_all_nils) \
+              _bi = _bi -> hash_link; \
+          (bottom_indx) = _bi; \
+      }
+# define GET_HDR_ADDR(p, ha) \
+      { \
+          register bottom_index * bi; \
+          GET_BI(p, bi); \
+          (ha) = &(HDR_FROM_BI(bi, p)); \
+      }
+# define GET_HDR(p, hhdr) { register hdr ** _ha; GET_HDR_ADDR(p, _ha); \
+                            (hhdr) = *_ha; }
+# define SET_HDR(p, hhdr) { register hdr ** _ha; GET_HDR_ADDR(p, _ha); \
+                            *_ha = (hhdr); }
+# define HDR(p) GC_find_header((ptr_t)(p))
+#endif
+
+/* Is the result a forwarding address to someplace closer to the        */
+/* beginning of the block or NULL?                                      */
+#define IS_FORWARDING_ADDR_OR_NIL(hhdr) ((size_t) (hhdr) <= MAX_JUMP)
+
+/* Get an HBLKSIZE aligned address closer to the beginning of the block */
+/* h.  Assumes hhdr == HDR(h) and IS_FORWARDING_ADDR(hhdr).             */
+#define FORWARDED_ADDR(h, hhdr) ((struct hblk *)(h) - (size_t)(hhdr))
+
+#endif /*  GC_HEADERS_H */
diff --git a/src/gc/bdwgc/include/private/gc_locks.h b/src/gc/bdwgc/include/private/gc_locks.h
new file mode 100644
index 0000000..daa730f
--- /dev/null
+++ b/src/gc/bdwgc/include/private/gc_locks.h
@@ -0,0 +1,247 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_LOCKS_H
+#define GC_LOCKS_H
+
+/*
+ * Mutual exclusion between allocator/collector routines.
+ * Needed if there is more than one allocator thread.
+ * DCL_LOCK_STATE declares any local variables needed by LOCK and UNLOCK.
+ *
+ * Note that I_HOLD_LOCK and I_DONT_HOLD_LOCK are used only positively
+ * in assertions, and may return TRUE in the "don't know" case.
+ */
+# ifdef THREADS
+
+# ifdef NAUT_THREADS
+
+#include "private/pthread_support.h"
+
+GC_EXTERN GC_bool GC_need_to_lock;
+GC_EXTERN unsigned long GC_lock_holder;
+GC_EXTERN NK_LOCK_T GC_allocate_ml;
+
+# define GetCurrentThreadId() NUMERIC_THREAD_ID(get_cur_thread())
+
+#   define NO_THREAD ((unsigned long)(-1l))
+
+#   define LOCK() NK_LOCK(&GC_allocate_ml)
+#   define UNLOCK() NK_UNLOCK(&GC_allocate_ml)
+#   define SET_LOCK_HOLDER() GC_lock_holder = NUMERIC_THREAD_ID(get_cur_thread())
+#   define UNSET_LOCK_HOLDER() GC_lock_holder = NO_THREAD
+#   define NUMERIC_THREAD_ID(id) ((unsigned long)(id))
+#   define THREAD_EQUAL(id1, id2) ((id1) == (id2))
+
+// For assertions
+#   define I_HOLD_LOCK() (!GC_need_to_lock                             \
+                           || GC_lock_holder == GetCurrentThreadId())
+#   define I_DONT_HOLD_LOCK() (!GC_need_to_lock                        \
+                                || GC_lock_holder != GetCurrentThreadId())     
+# else
+
+#  if defined(GC_PTHREADS) && !defined(GC_WIN32_THREADS)
+#    include "atomic_ops.h"
+#  endif
+
+   GC_API void GC_CALL GC_noop1(word);
+#  ifdef PCR
+#    include <base/PCR_Base.h>
+#    include <th/PCR_Th.h>
+     GC_EXTERN PCR_Th_ML GC_allocate_ml;
+#    define DCL_LOCK_STATE \
+         PCR_ERes GC_fastLockRes; PCR_sigset_t GC_old_sig_mask
+#    define UNCOND_LOCK() PCR_Th_ML_Acquire(&GC_allocate_ml)
+#    define UNCOND_UNLOCK() PCR_Th_ML_Release(&GC_allocate_ml)
+#  endif
+
+#  if !defined(AO_HAVE_test_and_set_acquire) && defined(GC_PTHREADS)
+#    define USE_PTHREAD_LOCKS
+#  endif
+
+#  if defined(GC_WIN32_THREADS) && defined(GC_PTHREADS)
+#    define USE_PTHREAD_LOCKS
+#  endif
+
+#  if defined(GC_RTEMS_PTHREADS)
+#    define USE_PTHREAD_LOCKS
+#  endif
+
+#  if defined(GC_WIN32_THREADS) && !defined(USE_PTHREAD_LOCKS)
+#    ifndef WIN32_LEAN_AND_MEAN
+#      define WIN32_LEAN_AND_MEAN 1
+#    endif
+#    define NOSERVICE
+#    include <windows.h>
+#    define NO_THREAD (DWORD)(-1)
+     GC_EXTERN DWORD GC_lock_holder;
+     GC_EXTERN CRITICAL_SECTION GC_allocate_ml;
+#    ifdef GC_ASSERTIONS
+#        define UNCOND_LOCK() \
+                { EnterCriticalSection(&GC_allocate_ml); \
+                  SET_LOCK_HOLDER(); }
+#        define UNCOND_UNLOCK() \
+                { GC_ASSERT(I_HOLD_LOCK()); UNSET_LOCK_HOLDER(); \
+                  LeaveCriticalSection(&GC_allocate_ml); }
+#    else
+#      define UNCOND_LOCK() EnterCriticalSection(&GC_allocate_ml)
+#      define UNCOND_UNLOCK() LeaveCriticalSection(&GC_allocate_ml)
+#    endif /* !GC_ASSERTIONS */
+#    define SET_LOCK_HOLDER() GC_lock_holder = GetCurrentThreadId()
+#    define UNSET_LOCK_HOLDER() GC_lock_holder = NO_THREAD
+#    define I_HOLD_LOCK() (!GC_need_to_lock \
+                           || GC_lock_holder == GetCurrentThreadId())
+#    define I_DONT_HOLD_LOCK() (!GC_need_to_lock \
+                           || GC_lock_holder != GetCurrentThreadId())
+#  elif defined(SN_TARGET_PS3)
+#    include <pthread.h>
+     GC_EXTERN pthread_mutex_t GC_allocate_ml;
+#    define LOCK() pthread_mutex_lock(&GC_allocate_ml)
+#    define UNLOCK() pthread_mutex_unlock(&GC_allocate_ml)
+#  elif defined(GC_PTHREADS)
+#    include <pthread.h>
+
+     /* Posix allows pthread_t to be a struct, though it rarely is.     */
+     /* Unfortunately, we need to use a pthread_t to index a data       */
+     /* structure.  It also helps if comparisons don't involve a        */
+     /* function call.  Hence we introduce platform-dependent macros    */
+     /* to compare pthread_t ids and to map them to integers.           */
+     /* the mapping to integers does not need to result in different    */
+     /* integers for each thread, though that should be true as much    */
+     /* as possible.                                                    */
+     /* Refine to exclude platforms on which pthread_t is struct */
+#    if !defined(GC_WIN32_PTHREADS)
+#      define NUMERIC_THREAD_ID(id) ((unsigned long)(id))
+#      define THREAD_EQUAL(id1, id2) ((id1) == (id2))
+#      define NUMERIC_THREAD_ID_UNIQUE
+#    else
+#      define NUMERIC_THREAD_ID(id) ((unsigned long)(id.p))
+       /* Using documented internal details of win32-pthread library.   */
+       /* Faster than pthread_equal(). Should not change with           */
+       /* future versions of win32-pthread library.                     */
+#      define THREAD_EQUAL(id1, id2) ((id1.p == id2.p) && (id1.x == id2.x))
+#      undef NUMERIC_THREAD_ID_UNIQUE
+       /* Generic definitions based on pthread_equal() always work but  */
+       /* will result in poor performance (as NUMERIC_THREAD_ID is      */
+       /* defined to just a constant) and weak assertion checking.      */
+#    endif
+#    define NO_THREAD ((unsigned long)(-1l))
+                /* != NUMERIC_THREAD_ID(pthread_self()) for any thread */
+
+#    if !defined(THREAD_LOCAL_ALLOC) && !defined(USE_PTHREAD_LOCKS)
+      /* In the THREAD_LOCAL_ALLOC case, the allocation lock tends to   */
+      /* be held for long periods, if it is held at all.  Thus spinning */
+      /* and sleeping for fixed periods are likely to result in         */
+      /* significant wasted time.  We thus rely mostly on queued locks. */
+#     define USE_SPIN_LOCK
+      GC_EXTERN volatile AO_TS_t GC_allocate_lock;
+      GC_INNER void GC_lock(void);
+        /* Allocation lock holder.  Only set if acquired by client through */
+        /* GC_call_with_alloc_lock.                                        */
+#     ifdef GC_ASSERTIONS
+#        define UNCOND_LOCK() \
+              { if (AO_test_and_set_acquire(&GC_allocate_lock) == AO_TS_SET) \
+                  GC_lock(); \
+                SET_LOCK_HOLDER(); }
+#        define UNCOND_UNLOCK() \
+              { GC_ASSERT(I_HOLD_LOCK()); UNSET_LOCK_HOLDER(); \
+                AO_CLEAR(&GC_allocate_lock); }
+#     else
+#        define UNCOND_LOCK() \
+              { if (AO_test_and_set_acquire(&GC_allocate_lock) == AO_TS_SET) \
+                  GC_lock(); }
+#        define UNCOND_UNLOCK() AO_CLEAR(&GC_allocate_lock)
+#     endif /* !GC_ASSERTIONS */
+#    else /* THREAD_LOCAL_ALLOC  || USE_PTHREAD_LOCKS */
+#      ifndef USE_PTHREAD_LOCKS
+#        define USE_PTHREAD_LOCKS
+#      endif
+#    endif /* THREAD_LOCAL_ALLOC || USE_PTHREAD_LOCK */
+#    ifdef USE_PTHREAD_LOCKS
+#      include <pthread.h>
+       GC_EXTERN pthread_mutex_t GC_allocate_ml;
+#      ifdef GC_ASSERTIONS
+#        define UNCOND_LOCK() { GC_lock(); SET_LOCK_HOLDER(); }
+#        define UNCOND_UNLOCK() \
+                { GC_ASSERT(I_HOLD_LOCK()); UNSET_LOCK_HOLDER(); \
+                  pthread_mutex_unlock(&GC_allocate_ml); }
+#      else /* !GC_ASSERTIONS */
+#        if defined(NO_PTHREAD_TRYLOCK)
+#          define UNCOND_LOCK() GC_lock()
+#        else /* !defined(NO_PTHREAD_TRYLOCK) */
+#        define UNCOND_LOCK() \
+           { if (0 != pthread_mutex_trylock(&GC_allocate_ml)) \
+               GC_lock(); }
+#        endif
+#        define UNCOND_UNLOCK() pthread_mutex_unlock(&GC_allocate_ml)
+#      endif /* !GC_ASSERTIONS */
+#    endif /* USE_PTHREAD_LOCKS */
+#    define SET_LOCK_HOLDER() \
+                GC_lock_holder = NUMERIC_THREAD_ID(pthread_self())
+#    define UNSET_LOCK_HOLDER() GC_lock_holder = NO_THREAD
+#    define I_HOLD_LOCK() \
+                (!GC_need_to_lock || \
+                 GC_lock_holder == NUMERIC_THREAD_ID(pthread_self()))
+#    ifndef NUMERIC_THREAD_ID_UNIQUE
+#      define I_DONT_HOLD_LOCK() 1  /* Conservatively say yes */
+#    else
+#      define I_DONT_HOLD_LOCK() \
+                (!GC_need_to_lock \
+                 || GC_lock_holder != NUMERIC_THREAD_ID(pthread_self()))
+#    endif
+     GC_EXTERN volatile GC_bool GC_collecting;
+#    define ENTER_GC() GC_collecting = 1;
+#    define EXIT_GC() GC_collecting = 0;
+     GC_INNER void GC_lock(void);
+     GC_EXTERN unsigned long GC_lock_holder;
+#    if defined(GC_ASSERTIONS) && defined(PARALLEL_MARK)
+       GC_EXTERN unsigned long GC_mark_lock_holder;
+#    endif
+#  endif /* GC_PTHREADS with linux_threads.c implementation */
+   GC_EXTERN GC_bool GC_need_to_lock;
+
+# endif //! NAUT
+
+# else /* !THREADS */
+#   define LOCK()
+#   define UNLOCK()
+#   define SET_LOCK_HOLDER()
+#   define UNSET_LOCK_HOLDER()
+#   define I_HOLD_LOCK() TRUE
+#   define I_DONT_HOLD_LOCK() TRUE
+                /* Used only in positive assertions or to test whether  */
+                /* we still need to acquire the lock.  TRUE works in    */
+                /* either case.                                         */
+
+# endif /* !THREADS */
+
+#if defined(UNCOND_LOCK) && !defined(LOCK)
+                /* At least two thread running; need to lock.   */
+#    define LOCK() { if (GC_need_to_lock) UNCOND_LOCK(); }
+#    define UNLOCK() { if (GC_need_to_lock) UNCOND_UNLOCK(); }
+#endif
+
+# ifndef ENTER_GC
+#   define ENTER_GC()
+#   define EXIT_GC()
+# endif
+
+# ifndef DCL_LOCK_STATE
+#   define DCL_LOCK_STATE
+# endif
+
+#endif /* GC_LOCKS_H */
diff --git a/src/gc/bdwgc/include/private/gc_pmark.h b/src/gc/bdwgc/include/private/gc_pmark.h
new file mode 100644
index 0000000..7e302d2
--- /dev/null
+++ b/src/gc/bdwgc/include/private/gc_pmark.h
@@ -0,0 +1,468 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 2001 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/* Private declarations of GC marker data structures and macros */
+
+/*
+ * Declarations of mark stack.  Needed by marker and client supplied mark
+ * routines.  Transitively include gc_priv.h.
+ */
+#ifndef GC_PMARK_H
+#define GC_PMARK_H
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+#ifndef GC_BUILD
+# define GC_BUILD
+#endif
+
+#if defined(KEEP_BACK_PTRS) || defined(PRINT_BLACK_LIST)
+# include "dbg_mlc.h"
+#endif
+
+#ifndef GC_MARK_H
+# include "../gc_mark.h"
+#endif
+
+#ifndef GC_PRIVATE_H
+# include "gc_priv.h"
+#endif
+
+/* The real declarations of the following is in gc_priv.h, so that      */
+/* we can avoid scanning the following table.                           */
+/*
+mark_proc GC_mark_procs[MAX_MARK_PROCS];
+*/
+
+#ifndef MARK_DESCR_OFFSET
+# define MARK_DESCR_OFFSET sizeof(word)
+#endif
+
+/*
+ * Mark descriptor stuff that should remain private for now, mostly
+ * because it's hard to export WORDSZ without including gcconfig.h.
+ */
+#define BITMAP_BITS (WORDSZ - GC_DS_TAG_BITS)
+#define PROC(descr) \
+      (GC_mark_procs[((descr) >> GC_DS_TAG_BITS) & (GC_MAX_MARK_PROCS-1)])
+#define ENV(descr) \
+      ((descr) >> (GC_DS_TAG_BITS + GC_LOG_MAX_MARK_PROCS))
+#define MAX_ENV \
+      (((word)1 << (WORDSZ - GC_DS_TAG_BITS - GC_LOG_MAX_MARK_PROCS)) - 1)
+
+GC_EXTERN unsigned GC_n_mark_procs;
+
+/* Number of mark stack entries to discard on overflow. */
+#define GC_MARK_STACK_DISCARDS (INITIAL_MARK_STACK_SIZE/8)
+
+GC_EXTERN size_t GC_mark_stack_size;
+
+#ifdef PARALLEL_MARK
+    /*
+     * Allow multiple threads to participate in the marking process.
+     * This works roughly as follows:
+     *  The main mark stack never shrinks, but it can grow.
+     *
+     *  The initiating threads holds the GC lock, and sets GC_help_wanted.
+     *
+     *  Other threads:
+     *     1) update helper_count (while holding mark_lock.)
+     *     2) allocate a local mark stack
+     *     repeatedly:
+     *          3) Steal a global mark stack entry by atomically replacing
+     *             its descriptor with 0.
+     *          4) Copy it to the local stack.
+     *          5) Mark on the local stack until it is empty, or
+     *             it may be profitable to copy it back.
+     *          6) If necessary, copy local stack to global one,
+     *             holding mark lock.
+     *    7) Stop when the global mark stack is empty.
+     *    8) decrement helper_count (holding mark_lock).
+     *
+     * This is an experiment to see if we can do something along the lines
+     * of the University of Tokyo SGC in a less intrusive, though probably
+     * also less performant, way.
+     */
+
+    /* GC_mark_stack_top is protected by mark lock.     */
+
+    /*
+     * GC_notify_all_marker() is used when GC_help_wanted is first set,
+     * when the last helper becomes inactive,
+     * when something is added to the global mark stack, and just after
+     * GC_mark_no is incremented.
+     * This could be split into multiple CVs (and probably should be to
+     * scale to really large numbers of processors.)
+     */
+#endif /* PARALLEL_MARK */
+
+GC_INNER mse * GC_signal_mark_stack_overflow(mse *msp);
+
+/* Push the object obj with corresponding heap block header hhdr onto   */
+/* the mark stack.                                                      */
+#define PUSH_OBJ(obj, hhdr, mark_stack_top, mark_stack_limit) \
+{ \
+    register word _descr = (hhdr) -> hb_descr; \
+    GC_ASSERT(!HBLK_IS_FREE(hhdr)); \
+    if (_descr != 0) { \
+        mark_stack_top++; \
+        if (mark_stack_top >= mark_stack_limit) { \
+          mark_stack_top = GC_signal_mark_stack_overflow(mark_stack_top); \
+        } \
+        mark_stack_top -> mse_start = (obj); \
+        mark_stack_top -> mse_descr = _descr; \
+    } \
+}
+
+/* Push the contents of current onto the mark stack if it is a valid    */
+/* ptr to a currently unmarked object.  Mark it.                        */
+/* If we assumed a standard-conforming compiler, we could probably      */
+/* generate the exit_label transparently.                               */
+#define PUSH_CONTENTS(current, mark_stack_top, mark_stack_limit, \
+                      source, exit_label) \
+{ \
+    hdr * my_hhdr; \
+    HC_GET_HDR(current, my_hhdr, source, exit_label); \
+    PUSH_CONTENTS_HDR(current, mark_stack_top, mark_stack_limit, \
+                  source, exit_label, my_hhdr, TRUE); \
+exit_label: ; \
+}
+
+/* Set mark bit, exit if it was already set.    */
+#ifdef USE_MARK_BYTES
+  /* There is a race here, and we may set                               */
+  /* the bit twice in the concurrent case.  This can result in the      */
+  /* object being pushed twice.  But that's only a performance issue.   */
+# define SET_MARK_BIT_EXIT_IF_SET(hhdr,bit_no,exit_label) \
+    { \
+        char * mark_byte_addr = (char *)hhdr -> hb_marks + (bit_no); \
+        if (*mark_byte_addr) goto exit_label; \
+        *mark_byte_addr = 1; \
+    }
+#else
+# ifdef PARALLEL_MARK
+    /* This is used only if we explicitly set USE_MARK_BITS.            */
+    /* The following may fail to exit even if the bit was already set.  */
+    /* For our uses, that's benign:                                     */
+#   define OR_WORD_EXIT_IF_SET(addr, bits, exit_label) \
+        { \
+          if (!(*(addr) & (bits))) { \
+            AO_or((AO_t *)(addr), (bits)); \
+          } else { \
+            goto exit_label; \
+          } \
+        }
+# else
+#   define OR_WORD_EXIT_IF_SET(addr, bits, exit_label) \
+        { \
+           word old = *(addr); \
+           word my_bits = (bits); \
+           if (old & my_bits) goto exit_label; \
+           *(addr) = (old | my_bits); \
+         }
+# endif /* !PARALLEL_MARK */
+# define SET_MARK_BIT_EXIT_IF_SET(hhdr,bit_no,exit_label) \
+    { \
+        word * mark_word_addr = hhdr -> hb_marks + divWORDSZ(bit_no); \
+        OR_WORD_EXIT_IF_SET(mark_word_addr, (word)1 << modWORDSZ(bit_no), \
+                            exit_label); \
+    }
+#endif /* !USE_MARK_BYTES */
+
+#ifdef PARALLEL_MARK
+# define INCR_MARKS(hhdr) \
+                AO_store(&hhdr->hb_n_marks, AO_load(&hhdr->hb_n_marks) + 1)
+#else
+# define INCR_MARKS(hhdr) (void)(++hhdr->hb_n_marks)
+#endif
+
+#ifdef ENABLE_TRACE
+# define TRACE(source, cmd) \
+        if (GC_trace_addr != 0 && (ptr_t)(source) == GC_trace_addr) cmd
+# define TRACE_TARGET(target, cmd) \
+        if (GC_trace_addr != 0 && (target) == *(ptr_t *)GC_trace_addr) cmd
+#else
+# define TRACE(source, cmd)
+# define TRACE_TARGET(source, cmd)
+#endif
+
+#if defined(I386) && defined(__GNUC__)
+# define LONG_MULT(hprod, lprod, x, y) { \
+        __asm__ __volatile__("mull %2" : "=a"(lprod), "=d"(hprod) \
+                             : "g"(y), "0"(x)); \
+  }
+#else
+# define LONG_MULT(hprod, lprod, x, y) { \
+        unsigned long long prod = (unsigned long long)(x) \
+                                  * (unsigned long long)(y); \
+        hprod = prod >> 32; \
+        lprod = (unsigned32)prod; \
+  }
+#endif /* !I386 */
+
+/* If the mark bit corresponding to current is not set, set it, and     */
+/* push the contents of the object on the mark stack.  Current points   */
+/* to the beginning of the object.  We rely on the fact that the        */
+/* preceding header calculation will succeed for a pointer past the     */
+/* first page of an object, only if it is in fact a valid pointer       */
+/* to the object.  Thus we can omit the otherwise necessary tests       */
+/* here.  Note in particular that the "displ" value is the displacement */
+/* from the beginning of the heap block, which may itself be in the     */
+/* interior of a large object.                                          */
+#ifdef MARK_BIT_PER_GRANULE
+# define PUSH_CONTENTS_HDR(current, mark_stack_top, mark_stack_limit, \
+                           source, exit_label, hhdr, do_offset_check) \
+{ \
+    size_t displ = HBLKDISPL(current); /* Displacement in block; in bytes. */\
+    /* displ is always within range.  If current doesn't point to       */ \
+    /* first block, then we are in the all_interior_pointers case, and  */ \
+    /* it is safe to use any displacement value.                        */ \
+    size_t gran_displ = BYTES_TO_GRANULES(displ); \
+    size_t gran_offset = hhdr -> hb_map[gran_displ]; \
+    size_t byte_offset = displ & (GRANULE_BYTES - 1); \
+    ptr_t base = current; \
+    /* The following always fails for large block references. */ \
+    if (EXPECT((gran_offset | byte_offset) != 0, FALSE))  { \
+        if (hhdr -> hb_large_block) { \
+          /* gran_offset is bogus.      */ \
+          size_t obj_displ; \
+          base = (ptr_t)(hhdr -> hb_block); \
+          obj_displ = (ptr_t)(current) - base; \
+          if (obj_displ != displ) { \
+            GC_ASSERT(obj_displ < hhdr -> hb_sz); \
+            /* Must be in all_interior_pointer case, not first block */ \
+            /* already did validity check on cache miss.             */ \
+          } else { \
+            if (do_offset_check && !GC_valid_offsets[obj_displ]) { \
+              GC_ADD_TO_BLACK_LIST_NORMAL(current, source); \
+              goto exit_label; \
+            } \
+          } \
+          gran_displ = 0; \
+          GC_ASSERT(hhdr -> hb_sz > HBLKSIZE || \
+                    hhdr -> hb_block == HBLKPTR(current)); \
+          GC_ASSERT((ptr_t)(hhdr -> hb_block) <= (ptr_t) current); \
+        } else { \
+          size_t obj_displ = GRANULES_TO_BYTES(gran_offset) \
+                             + byte_offset; \
+          if (do_offset_check && !GC_valid_offsets[obj_displ]) { \
+            GC_ADD_TO_BLACK_LIST_NORMAL(current, source); \
+            goto exit_label; \
+          } \
+          gran_displ -= gran_offset; \
+          base -= obj_displ; \
+        } \
+    } \
+    GC_ASSERT(hhdr == GC_find_header(base)); \
+    GC_ASSERT(gran_displ % BYTES_TO_GRANULES(hhdr -> hb_sz) == 0); \
+    TRACE(source, GC_log_printf("GC:%u: passed validity tests\n", \
+                                (unsigned)GC_gc_no)); \
+    SET_MARK_BIT_EXIT_IF_SET(hhdr, gran_displ, exit_label); \
+    TRACE(source, GC_log_printf("GC:%u: previously unmarked\n", \
+                                (unsigned)GC_gc_no)); \
+    TRACE_TARGET(base, \
+        GC_log_printf("GC:%u: marking %p from %p instead\n", \
+                      (unsigned)GC_gc_no, base, source)); \
+    INCR_MARKS(hhdr); \
+    GC_STORE_BACK_PTR((ptr_t)source, base); \
+    PUSH_OBJ(base, hhdr, mark_stack_top, mark_stack_limit); \
+}
+#endif /* MARK_BIT_PER_GRANULE */
+
+#ifdef MARK_BIT_PER_OBJ
+# define PUSH_CONTENTS_HDR(current, mark_stack_top, mark_stack_limit, \
+                           source, exit_label, hhdr, do_offset_check) \
+{ \
+    size_t displ = HBLKDISPL(current); /* Displacement in block; in bytes. */\
+    unsigned32 low_prod, high_prod; \
+    unsigned32 inv_sz = hhdr -> hb_inv_sz; \
+    ptr_t base = current; \
+    LONG_MULT(high_prod, low_prod, displ, inv_sz); \
+    /* product is > and within sz_in_bytes of displ * sz_in_bytes * 2**32 */ \
+    if (EXPECT(low_prod >> 16 != 0, FALSE))  { \
+            FIXME: fails if offset is a multiple of HBLKSIZE which becomes 0 \
+        if (inv_sz == LARGE_INV_SZ) { \
+          size_t obj_displ; \
+          base = (ptr_t)(hhdr -> hb_block); \
+          obj_displ = (ptr_t)(current) - base; \
+          if (obj_displ != displ) { \
+            GC_ASSERT(obj_displ < hhdr -> hb_sz); \
+            /* Must be in all_interior_pointer case, not first block */ \
+            /* already did validity check on cache miss.             */ \
+          } else { \
+            if (do_offset_check && !GC_valid_offsets[obj_displ]) { \
+              GC_ADD_TO_BLACK_LIST_NORMAL(current, source); \
+              goto exit_label; \
+            } \
+          } \
+          GC_ASSERT(hhdr -> hb_sz > HBLKSIZE || \
+                    hhdr -> hb_block == HBLKPTR(current)); \
+          GC_ASSERT((ptr_t)(hhdr -> hb_block) < (ptr_t) current); \
+        } else { \
+          /* Accurate enough if HBLKSIZE <= 2**15.      */ \
+          GC_STATIC_ASSERT(HBLKSIZE <= (1 << 15)); \
+          size_t obj_displ = (((low_prod >> 16) + 1) * (hhdr->hb_sz)) >> 16; \
+          if (do_offset_check && !GC_valid_offsets[obj_displ]) { \
+            GC_ADD_TO_BLACK_LIST_NORMAL(current, source); \
+            goto exit_label; \
+          } \
+          base -= obj_displ; \
+        } \
+    } \
+    /* May get here for pointer to start of block not at        */ \
+    /* beginning of object.  If so, it's valid, and we're fine. */ \
+    GC_ASSERT(high_prod >= 0 && high_prod <= HBLK_OBJS(hhdr -> hb_sz)); \
+    TRACE(source, GC_log_printf("GC:%u: passed validity tests\n", \
+                                (unsigned)GC_gc_no)); \
+    SET_MARK_BIT_EXIT_IF_SET(hhdr, high_prod, exit_label); \
+    TRACE(source, GC_log_printf("GC:%u: previously unmarked\n", \
+                                (unsigned)GC_gc_no)); \
+    TRACE_TARGET(base, \
+        GC_log_printf("GC:%u: marking %p from %p instead\n", \
+                      (unsigned)GC_gc_no, base, source)); \
+    INCR_MARKS(hhdr); \
+    GC_STORE_BACK_PTR((ptr_t)source, base); \
+    PUSH_OBJ(base, hhdr, mark_stack_top, mark_stack_limit); \
+}
+#endif /* MARK_BIT_PER_OBJ */
+
+#if defined(PRINT_BLACK_LIST) || defined(KEEP_BACK_PTRS)
+# define PUSH_ONE_CHECKED_STACK(p, source) \
+        GC_mark_and_push_stack((ptr_t)(p), (ptr_t)(source))
+#else
+# define PUSH_ONE_CHECKED_STACK(p, source) \
+        GC_mark_and_push_stack((ptr_t)(p))
+#endif
+
+/*
+ * Push a single value onto mark stack. Mark from the object pointed to by p.
+ * Invoke FIXUP_POINTER(p) before any further processing.
+ * P is considered valid even if it is an interior pointer.
+ * Previously marked objects are not pushed.  Hence we make progress even
+ * if the mark stack overflows.
+ */
+
+#if NEED_FIXUP_POINTER
+    /* Try both the raw version and the fixed up one.   */
+# define GC_PUSH_ONE_STACK(p, source) \
+      if ((ptr_t)(p) >= (ptr_t)GC_least_plausible_heap_addr \
+          && (ptr_t)(p) < (ptr_t)GC_greatest_plausible_heap_addr) { \
+         PUSH_ONE_CHECKED_STACK(p, source); \
+      } \
+      FIXUP_POINTER(p); \
+      if ((ptr_t)(p) >= (ptr_t)GC_least_plausible_heap_addr \
+          && (ptr_t)(p) < (ptr_t)GC_greatest_plausible_heap_addr) { \
+         PUSH_ONE_CHECKED_STACK(p, source); \
+      }
+#else /* !NEED_FIXUP_POINTER */
+# define GC_PUSH_ONE_STACK(p, source) \
+      if ((ptr_t)(p) >= (ptr_t)GC_least_plausible_heap_addr \
+          && (ptr_t)(p) < (ptr_t)GC_greatest_plausible_heap_addr) { \
+         PUSH_ONE_CHECKED_STACK(p, source); \
+      }
+#endif
+
+
+/*
+ * As above, but interior pointer recognition as for
+ * normal heap pointers.
+ */
+#define GC_PUSH_ONE_HEAP(p,source) \
+    FIXUP_POINTER(p); \
+    if ((ptr_t)(p) >= (ptr_t)GC_least_plausible_heap_addr \
+         && (ptr_t)(p) < (ptr_t)GC_greatest_plausible_heap_addr) { \
+      GC_mark_stack_top = GC_mark_and_push( \
+                            (void *)(p), GC_mark_stack_top, \
+                            GC_mark_stack_limit, (void * *)(source)); \
+    }
+
+/* Mark starting at mark stack entry top (incl.) down to        */
+/* mark stack entry bottom (incl.).  Stop after performing      */
+/* about one page worth of work.  Return the new mark stack     */
+/* top entry.                                                   */
+GC_INNER mse * GC_mark_from(mse * top, mse * bottom, mse *limit);
+
+#define MARK_FROM_MARK_STACK() \
+        GC_mark_stack_top = GC_mark_from(GC_mark_stack_top, \
+                                         GC_mark_stack, \
+                                         GC_mark_stack + GC_mark_stack_size);
+
+/*
+ * Mark from one finalizable object using the specified
+ * mark proc. May not mark the object pointed to by
+ * real_ptr. That is the job of the caller, if appropriate.
+ * Note that this is called with the mutator running, but
+ * with us holding the allocation lock.  This is safe only if the
+ * mutator needs the allocation lock to reveal hidden pointers.
+ * FIXME: Why do we need the GC_mark_state test below?
+ */
+#define GC_MARK_FO(real_ptr, mark_proc) \
+{ \
+    (*(mark_proc))(real_ptr); \
+    while (!GC_mark_stack_empty()) MARK_FROM_MARK_STACK(); \
+    if (GC_mark_state != MS_NONE) { \
+        GC_set_mark_bit(real_ptr); \
+        while (!GC_mark_some((ptr_t)0)) {} \
+    } \
+}
+
+GC_EXTERN GC_bool GC_mark_stack_too_small;
+                                /* We need a larger mark stack.  May be */
+                                /* set by client supplied mark routines.*/
+
+typedef int mark_state_t;       /* Current state of marking, as follows:*/
+                                /* Used to remember where we are during */
+                                /* concurrent marking.                  */
+
+                                /* We say something is dirty if it was  */
+                                /* written since the last time we       */
+                                /* retrieved dirty bits.  We say it's   */
+                                /* grungy if it was marked dirty in the */
+                                /* last set of bits we retrieved.       */
+
+                                /* Invariant I: all roots and marked    */
+                                /* objects p are either dirty, or point */
+                                /* to objects q that are either marked  */
+                                /* or a pointer to q appears in a range */
+                                /* on the mark stack.                   */
+
+#define MS_NONE 0               /* No marking in progress. I holds.     */
+                                /* Mark stack is empty.                 */
+
+#define MS_PUSH_RESCUERS 1      /* Rescuing objects are currently       */
+                                /* being pushed.  I holds, except       */
+                                /* that grungy roots may point to       */
+                                /* unmarked objects, as may marked      */
+                                /* grungy objects above scan_ptr.       */
+
+#define MS_PUSH_UNCOLLECTABLE 2 /* I holds, except that marked          */
+                                /* uncollectible objects above scan_ptr */
+                                /* may point to unmarked objects.       */
+                                /* Roots may point to unmarked objects  */
+
+#define MS_ROOTS_PUSHED 3       /* I holds, mark stack may be nonempty  */
+
+#define MS_PARTIALLY_INVALID 4  /* I may not hold, e.g. because of M.S. */
+                                /* overflow.  However marked heap       */
+                                /* objects below scan_ptr point to      */
+                                /* marked or stacked objects.           */
+
+#define MS_INVALID 5            /* I may not hold.                      */
+
+GC_EXTERN mark_state_t GC_mark_state;
+
+#endif  /* GC_PMARK_H */
diff --git a/src/gc/bdwgc/include/private/gc_priv.h b/src/gc/bdwgc/include/private/gc_priv.h
new file mode 100644
index 0000000..6b3cb11
--- /dev/null
+++ b/src/gc/bdwgc/include/private/gc_priv.h
@@ -0,0 +1,2415 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_PRIVATE_H
+#define GC_PRIVATE_H
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+
+#ifndef GC_BUILD
+# define GC_BUILD
+#endif
+
+#if (defined(__linux__) || defined(__GLIBC__) || defined(__GNU__)) \
+    && !defined(_GNU_SOURCE)
+  /* Can't test LINUX, since this must be defined before other includes. */
+# define _GNU_SOURCE 1
+#endif
+
+#if (defined(DGUX) && defined(GC_THREADS) || defined(DGUX386_THREADS) \
+     || defined(GC_DGUX386_THREADS)) && !defined(_USING_POSIX4A_DRAFT10)
+# define _USING_POSIX4A_DRAFT10 1
+#endif
+
+# if defined(NO_DEBUGGING) && !defined(GC_ASSERTIONS) && !defined(NDEBUG)
+    /* To turn off assertion checking (in atomic_ops.h). */
+#   define NDEBUG 1
+# endif
+
+#ifndef GC_H
+# include "../gc.h"
+#endif
+
+#if !defined(NAUT)
+# include <stdlib.h>
+#endif 
+#if !defined(sony_news)
+# include <stddef.h>
+#endif
+
+#undef DGUX
+
+#ifdef DGUX
+# include <sys/types.h>
+# include <sys/time.h>
+# include <sys/resource.h>
+#endif /* DGUX */
+
+#ifdef BSD_TIME
+# include <sys/types.h>
+# include <sys/time.h>
+# include <sys/resource.h>
+#endif /* BSD_TIME */
+
+#ifdef PARALLEL_MARK
+# define AO_REQUIRE_CAS
+# if !defined(__GNUC__) && !defined(AO_ASSUME_WINDOWS98)
+#   define AO_ASSUME_WINDOWS98
+# endif
+#endif
+
+#ifndef GC_TINY_FL_H
+# include "../gc_tiny_fl.h"
+#endif
+
+#ifndef GC_MARK_H
+# include "../gc_mark.h"
+#endif
+
+typedef GC_word word;
+typedef GC_signed_word signed_word;
+typedef unsigned int unsigned32;
+
+typedef int GC_bool;
+#define TRUE 1
+#define FALSE 0
+
+typedef char * ptr_t;   /* A generic pointer to which we can add        */
+                        /* byte displacements and which can be used     */
+                        /* for address comparisons.                     */
+
+#ifndef GCCONFIG_H
+# include "gcconfig.h"
+#endif
+
+#ifndef GC_INNER
+  /* This tagging macro must be used at the start of every variable     */
+  /* definition which is declared with GC_EXTERN.  Should be also used  */
+  /* for the GC-scope function definitions and prototypes.  Must not be */
+  /* used in gcconfig.h.  Shouldn't be used for the debugging-only      */
+  /* functions.  Currently, not used for the functions declared in or   */
+  /* called from the "dated" source files (pcr_interface.c, specific.c  */
+  /* and in the "extra" folder).                                        */
+# if defined(GC_DLL) && defined(__GNUC__) && !defined(MSWIN32) \
+        && !defined(MSWINCE) && !defined(CYGWIN32)
+#   if __GNUC__ >= 4
+      /* See the corresponding GC_API definition. */
+#     define GC_INNER __attribute__((__visibility__("hidden")))
+#   else
+      /* The attribute is unsupported. */
+#     define GC_INNER /* empty */
+#   endif
+# else
+#   define GC_INNER /* empty */
+# endif
+
+# define GC_EXTERN extern GC_INNER
+  /* Used only for the GC-scope variables (prefixed with "GC_")         */
+  /* declared in the header files.  Must not be used for thread-local   */
+  /* variables.  Must not be used in gcconfig.h.  Shouldn't be used for */
+  /* the debugging-only or profiling-only variables.  Currently, not    */
+  /* used for the variables accessed from the "dated" source files      */
+  /* (pcr_interface.c, specific.c/h, and in the "extra" folder).        */
+  /* The corresponding variable definition must start with GC_INNER.    */
+#endif /* !GC_INNER */
+
+#ifndef HEADERS_H
+# include "gc_hdrs.h"
+#endif
+
+#if __GNUC__ >= 3 && !defined(LINT2)
+# define EXPECT(expr, outcome) __builtin_expect(expr,outcome)
+  /* Equivalent to (expr), but predict that usually (expr)==outcome. */
+#else
+# define EXPECT(expr, outcome) (expr)
+#endif /* __GNUC__ */
+
+#ifdef HAVE_CONFIG_H
+  /* The `inline' keyword as determined by Autoconf's `AC_C_INLINE'.    */
+# define GC_INLINE static inline
+#elif defined(_MSC_VER) || defined(__INTEL_COMPILER) || defined(__DMC__) \
+        || defined(__WATCOMC__)
+# define GC_INLINE static __inline
+#elif (__GNUC__ >= 3) || defined(__sun)
+# define GC_INLINE static inline
+#else
+# define GC_INLINE static
+#endif
+
+#ifndef GC_API_OSCALL
+  /* This is used to identify GC routines called by name from OS.       */
+# if defined(__GNUC__)
+#   if __GNUC__ >= 4
+      /* Same as GC_API if GC_DLL.      */
+#     define GC_API_OSCALL extern __attribute__((__visibility__("default")))
+#   else
+      /* The attribute is unsupported.  */
+#     define GC_API_OSCALL extern
+#   endif
+# else
+#   define GC_API_OSCALL GC_API
+# endif
+#endif
+
+#ifndef GC_API_PRIV
+# define GC_API_PRIV GC_API
+#endif
+
+#ifndef GC_LOCKS_H
+# include "gc_locks.h"
+#endif
+
+#define ONES ((word)(signed_word)(-1))
+
+# ifdef STACK_GROWS_DOWN
+#   define COOLER_THAN >
+#   define HOTTER_THAN <
+#   define MAKE_COOLER(x,y) if ((x)+(y) > (x)) {(x) += (y);} \
+                            else {(x) = (ptr_t)ONES;}
+#   define MAKE_HOTTER(x,y) (x) -= (y)
+# else
+#   define COOLER_THAN <
+#   define HOTTER_THAN >
+#   define MAKE_COOLER(x,y) if ((x)-(y) < (x)) {(x) -= (y);} else {(x) = 0;}
+#   define MAKE_HOTTER(x,y) (x) += (y)
+# endif
+
+#if defined(AMIGA) && defined(__SASC)
+#   define GC_FAR __far
+#else
+#   define GC_FAR
+#endif
+
+
+/*********************************/
+/*                               */
+/* Definitions for conservative  */
+/* collector                     */
+/*                               */
+/*********************************/
+
+/*********************************/
+/*                               */
+/* Easily changeable parameters  */
+/*                               */
+/*********************************/
+
+/* #define STUBBORN_ALLOC */
+                    /* Enable stubborn allocation, and thus a limited   */
+                    /* form of incremental collection w/o dirty bits.   */
+
+/* #define ALL_INTERIOR_POINTERS */
+                    /* Forces all pointers into the interior of an      */
+                    /* object to be considered valid.  Also causes the  */
+                    /* sizes of all objects to be inflated by at least  */
+                    /* one byte.  This should suffice to guarantee      */
+                    /* that in the presence of a compiler that does     */
+                    /* not perform garbage-collector-unsafe             */
+                    /* optimizations, all portable, strictly ANSI       */
+                    /* conforming C programs should be safely usable    */
+                    /* with malloc replaced by GC_malloc and free       */
+                    /* calls removed.  There are several disadvantages: */
+                    /* 1. There are probably no interesting, portable,  */
+                    /*    strictly ANSI conforming C programs.          */
+                    /* 2. This option makes it hard for the collector   */
+                    /*    to allocate space that is not ``pointed to''  */
+                    /*    by integers, etc.  Under SunOS 4.X with a     */
+                    /*    statically linked libc, we empirically        */
+                    /*    observed that it would be difficult to        */
+                    /*    allocate individual objects larger than 100K. */
+                    /*    Even if only smaller objects are allocated,   */
+                    /*    more swap space is likely to be needed.       */
+                    /*    Fortunately, much of this will never be       */
+                    /*    touched.                                      */
+                    /* If you can easily avoid using this option, do.   */
+                    /* If not, try to keep individual objects small.    */
+                    /* This is now really controlled at startup,        */
+                    /* through GC_all_interior_pointers.                */
+
+
+#define GC_INVOKE_FINALIZERS() GC_notify_or_invoke_finalizers()
+
+#if !defined(DONT_ADD_BYTE_AT_END)
+# ifdef LINT2
+    /* Explicitly instruct the code analysis tool that                  */
+    /* GC_all_interior_pointers is assumed to have only 0 or 1 value.   */
+#   define EXTRA_BYTES (GC_all_interior_pointers? 1 : 0)
+# else
+#   define EXTRA_BYTES GC_all_interior_pointers
+# endif
+# define MAX_EXTRA_BYTES 1
+#else
+# define EXTRA_BYTES 0
+# define MAX_EXTRA_BYTES 0
+#endif
+
+
+# ifndef LARGE_CONFIG
+#   define MINHINCR 16   /* Minimum heap increment, in blocks of HBLKSIZE  */
+                         /* Must be multiple of largest page size.         */
+#   define MAXHINCR 2048 /* Maximum heap increment, in blocks              */
+# else
+#   define MINHINCR 64
+#   define MAXHINCR 4096
+# endif
+
+# define BL_LIMIT GC_black_list_spacing
+                           /* If we need a block of N bytes, and we have */
+                           /* a block of N + BL_LIMIT bytes available,   */
+                           /* and N > BL_LIMIT,                          */
+                           /* but all possible positions in it are       */
+                           /* blacklisted, we just use it anyway (and    */
+                           /* print a warning, if warnings are enabled). */
+                           /* This risks subsequently leaking the block  */
+                           /* due to a false reference.  But not using   */
+                           /* the block risks unreasonable immediate     */
+                           /* heap growth.                               */
+
+/*********************************/
+/*                               */
+/* Stack saving for debugging    */
+/*                               */
+/*********************************/
+
+#ifdef NEED_CALLINFO
+    struct callinfo {
+        word ci_pc;     /* Caller, not callee, pc       */
+#       if NARGS > 0
+            word ci_arg[NARGS]; /* bit-wise complement to avoid retention */
+#       endif
+#       if (NFRAMES * (NARGS + 1)) % 2 == 1
+            /* Likely alignment problem. */
+            word ci_dummy;
+#       endif
+    };
+#endif
+
+#ifdef SAVE_CALL_CHAIN
+  /* Fill in the pc and argument information for up to NFRAMES of my    */
+  /* callers.  Ignore my frame and my callers frame.                    */
+  GC_INNER void GC_save_callers(struct callinfo info[NFRAMES]);
+  GC_INNER void GC_print_callers(struct callinfo info[NFRAMES]);
+#endif
+
+
+/*********************************/
+/*                               */
+/* OS interface routines         */
+/*                               */
+/*********************************/
+
+#ifdef BSD_TIME
+# undef CLOCK_TYPE
+# undef GET_TIME
+# undef MS_TIME_DIFF
+# define CLOCK_TYPE struct timeval
+# define GET_TIME(x) { struct rusage rusage; \
+                       getrusage (RUSAGE_SELF,  &rusage); \
+                       x = rusage.ru_utime; }
+# define MS_TIME_DIFF(a,b) ((unsigned long)(a.tv_sec - b.tv_sec) * 1000 \
+                            + (unsigned long)(a.tv_usec - b.tv_usec) / 1000)
+#elif defined(MSWIN32) || defined(MSWINCE)
+# ifndef WIN32_LEAN_AND_MEAN
+#   define WIN32_LEAN_AND_MEAN 1
+# endif
+# define NOSERVICE
+# include <windows.h>
+# include <winbase.h>
+# define CLOCK_TYPE DWORD
+# define GET_TIME(x) x = GetTickCount()
+# define MS_TIME_DIFF(a,b) ((long)((a)-(b)))
+# elif NAUT
+
+// Stubs -> do nothing
+# define CLOCK_TYPE int
+# define GET_TIME(x) x = 0
+# define MS_TIME_DIFF(a,b) 0
+
+# else /* !MSWIN32, !MSWINCE, !BSD_TIME */
+# include <time.h>
+# if !defined(__STDC__) && defined(SPARC) && defined(SUNOS4)
+    clock_t clock(void);        /* Not in time.h, where it belongs      */
+# endif
+# if defined(FREEBSD) && !defined(CLOCKS_PER_SEC)
+#   include <machine/limits.h>
+#   define CLOCKS_PER_SEC CLK_TCK
+# endif
+# if !defined(CLOCKS_PER_SEC)
+#   define CLOCKS_PER_SEC 1000000
+    /* This is technically a bug in the implementation.                 */
+    /* ANSI requires that CLOCKS_PER_SEC be defined.  But at least      */
+    /* under SunOS 4.1.1, it isn't.  Also note that the combination of  */
+    /* ANSI C and POSIX is incredibly gross here.  The type clock_t     */
+    /* is used by both clock() and times().  But on some machines       */
+    /* these use different notions of a clock tick, CLOCKS_PER_SEC      */
+    /* seems to apply only to clock.  Hence we use it here.  On many    */
+    /* machines, including SunOS, clock actually uses units of          */
+    /* microseconds (which are not really clock ticks).                 */
+# endif
+# define CLOCK_TYPE clock_t
+# define GET_TIME(x) x = clock()
+# define MS_TIME_DIFF(a,b) (CLOCKS_PER_SEC % 1000 == 0 ? \
+        (unsigned long)((a) - (b)) / (unsigned long)(CLOCKS_PER_SEC / 1000) \
+        : ((unsigned long)((a) - (b)) * 1000) / (unsigned long)CLOCKS_PER_SEC)
+  /* Avoid using double type since some targets (like ARM) might        */
+  /* require -lm option for double-to-long conversion.                  */
+#endif /* !BSD_TIME && !MSWIN32 */
+
+/* We use bzero and bcopy internally.  They may not be available.       */
+# if defined(SPARC) && defined(SUNOS4)
+#   define BCOPY_EXISTS
+# endif
+# if defined(M68K) && defined(AMIGA)
+#   define BCOPY_EXISTS
+# endif
+# if defined(M68K) && defined(NEXT)
+#   define BCOPY_EXISTS
+# endif
+# if defined(VAX)
+#   define BCOPY_EXISTS
+# endif
+# if defined(AMIGA)
+#   include <string.h>
+#   define BCOPY_EXISTS
+# endif
+# if defined(DARWIN)
+#   include <string.h>
+#   define BCOPY_EXISTS
+# endif
+
+# ifndef BCOPY_EXISTS
+#   ifndef NAUT
+#       include <string.h>
+#   endif
+#   define BCOPY(x,y,n) memcpy(y, x, (size_t)(n))
+#   define BZERO(x,n)  memset(x, 0, (size_t)(n))
+# else
+#   define BCOPY(x,y,n) bcopy((void *)(x),(void *)(y),(size_t)(n))
+#   define BZERO(x,n) bzero((void *)(x),(size_t)(n))
+# endif
+
+/*
+ * Stop and restart mutator threads.
+ */
+# ifdef PCR
+#     include "th/PCR_ThCtl.h"
+#     define STOP_WORLD() \
+        PCR_ThCtl_SetExclusiveMode(PCR_ThCtl_ExclusiveMode_stopNormal, \
+                                   PCR_allSigsBlocked, \
+                                   PCR_waitForever)
+#     define START_WORLD() \
+        PCR_ThCtl_SetExclusiveMode(PCR_ThCtl_ExclusiveMode_null, \
+                                   PCR_allSigsBlocked, \
+                                   PCR_waitForever)
+# else
+#   if defined(GC_WIN32_THREADS) || defined(GC_PTHREADS) || defined(NAUT_THREADS)
+      GC_INNER void GC_stop_world(void);
+      GC_INNER void GC_start_world(void);
+#     define STOP_WORLD() GC_stop_world()
+#     define START_WORLD() GC_start_world()
+#   else
+        /* Just do a sanity check: we are not inside GC_do_blocking().  */
+#     define STOP_WORLD() GC_ASSERT(GC_blocked_sp == NULL)
+#     define START_WORLD()
+#   endif
+# endif
+
+/* Abandon ship */
+# ifdef PCR
+#   define ABORT(s) PCR_Base_Panic(s)
+# else
+#   if defined(MSWINCE) && !defined(DebugBreak) \
+       && (!defined(UNDER_CE) || (defined(__MINGW32CE__) && !defined(ARM32)))
+      /* This simplifies linking for WinCE (and, probably, doesn't      */
+      /* hurt debugging much); use -DDebugBreak=DebugBreak to override  */
+      /* this behavior if really needed.  This is also a workaround for */
+      /* x86mingw32ce toolchain (if it is still declaring DebugBreak()  */
+      /* instead of defining it as a macro).                            */
+#     define DebugBreak() _exit(-1) /* there is no abort() in WinCE */
+#   endif
+#   ifdef SMALL_CONFIG
+#       if (defined(MSWIN32) && !defined(LINT2)) || defined(MSWINCE)
+#           define ABORT(msg) DebugBreak()
+#       else
+#           define ABORT(msg) abort()
+#       endif
+#   else
+        GC_API_PRIV void GC_abort(const char * msg);
+#       define ABORT(msg) GC_abort(msg)
+#   endif
+# endif
+
+/* Exit abnormally, but without making a mess (e.g. out of memory) */
+# ifdef PCR
+#   define EXIT() PCR_Base_Exit(1,PCR_waitForever)
+# else
+#   define EXIT() (void)exit(1)
+# endif
+
+/* Print warning message, e.g. almost out of memory.    */
+#define WARN(msg, arg) (*GC_current_warn_proc)("GC Warning: " msg, \
+                                               (GC_word)(arg))
+GC_EXTERN GC_warn_proc GC_current_warn_proc;
+
+/* Print format type macro for signed_word.  Currently used for WARN()  */
+/* only.  This could be of use on Win64 but commented out since Win64   */
+/* is only a little-endian architecture (for now) and the WARN format   */
+/* string is, possibly, processed on the client side, so non-standard   */
+/* print type modifiers should be avoided (if possible).                */
+#if defined(_MSC_VER) && defined(_WIN64) && !defined(GC_PRIdPTR)
+/* #define GC_PRIdPTR "I64d" */
+#endif
+
+#if !defined(GC_PRIdPTR) && (defined(_LLP64) || defined(__LLP64__) \
+        || defined(_WIN64))
+/* #include <inttypes.h> */
+/* #define GC_PRIdPTR PRIdPTR */
+#endif
+
+#ifndef GC_PRIdPTR
+  /* Assume sizeof(void *) == sizeof(long) (or a little-endian machine) */
+# define GC_PRIdPTR "ld"
+#endif
+
+/* Get environment entry */
+#ifdef GC_READ_ENV_FILE
+  GC_INNER char * GC_envfile_getenv(const char *name);
+# define GETENV(name) GC_envfile_getenv(name)
+#elif defined(NO_GETENV)
+# define GETENV(name) NULL
+#elif defined(EMPTY_GETENV_RESULTS)
+  /* Workaround for a reputed Wine bug.   */
+  GC_INLINE char * fixed_getenv(const char *name)
+  {
+    char *value = getenv(name);
+    return value != NULL && *value != '\0' ? value : NULL;
+  }
+# define GETENV(name) fixed_getenv(name)
+#else
+# define GETENV(name) getenv(name)
+#endif
+
+#if defined(DARWIN)
+# if defined(POWERPC)
+#   if CPP_WORDSZ == 32
+#     define GC_THREAD_STATE_T          ppc_thread_state_t
+#     define GC_MACH_THREAD_STATE       PPC_THREAD_STATE
+#     define GC_MACH_THREAD_STATE_COUNT PPC_THREAD_STATE_COUNT
+#   else
+#     define GC_THREAD_STATE_T          ppc_thread_state64_t
+#     define GC_MACH_THREAD_STATE       PPC_THREAD_STATE64
+#     define GC_MACH_THREAD_STATE_COUNT PPC_THREAD_STATE64_COUNT
+#   endif
+# elif defined(I386) || defined(X86_64)
+#   if CPP_WORDSZ == 32
+#     define GC_THREAD_STATE_T          x86_thread_state32_t
+#     define GC_MACH_THREAD_STATE       x86_THREAD_STATE32
+#     define GC_MACH_THREAD_STATE_COUNT x86_THREAD_STATE32_COUNT
+#   else
+#     define GC_THREAD_STATE_T          x86_thread_state64_t
+#     define GC_MACH_THREAD_STATE       x86_THREAD_STATE64
+#     define GC_MACH_THREAD_STATE_COUNT x86_THREAD_STATE64_COUNT
+#   endif
+# elif defined(ARM32)
+#   define GC_THREAD_STATE_T            arm_thread_state_t
+#   define GC_MACH_THREAD_STATE         ARM_THREAD_STATE
+#   define GC_MACH_THREAD_STATE_COUNT   ARM_THREAD_STATE_COUNT
+# else
+#   error define GC_THREAD_STATE_T
+# endif
+# ifndef GC_MACH_THREAD_STATE
+#   define GC_MACH_THREAD_STATE         MACHINE_THREAD_STATE
+#   define GC_MACH_THREAD_STATE_COUNT   MACHINE_THREAD_STATE_COUNT
+# endif
+
+# if CPP_WORDSZ == 32
+#   define GC_MACH_HEADER   mach_header
+#   define GC_MACH_SECTION  section
+#   define GC_GETSECTBYNAME getsectbynamefromheader
+# else
+#   define GC_MACH_HEADER   mach_header_64
+#   define GC_MACH_SECTION  section_64
+#   define GC_GETSECTBYNAME getsectbynamefromheader_64
+# endif
+
+  /* Try to work out the right way to access thread state structure     */
+  /* members.  The structure has changed its definition in different    */
+  /* Darwin versions.  This now defaults to the (older) names           */
+  /* without __, thus hopefully, not breaking any existing              */
+  /* Makefile.direct builds.                                            */
+# if __DARWIN_UNIX03
+#   define THREAD_FLD(x) __ ## x
+# else
+#   define THREAD_FLD(x) x
+# endif
+#endif /* DARWIN */
+
+/*********************************/
+/*                               */
+/* Word-size-dependent defines   */
+/*                               */
+/*********************************/
+
+#if CPP_WORDSZ == 32
+# define WORDS_TO_BYTES(x) ((x)<<2)
+# define BYTES_TO_WORDS(x) ((x)>>2)
+# define LOGWL             ((word)5) /* log[2] of CPP_WORDSZ    */
+# define modWORDSZ(n) ((n) & 0x1f) /* n mod size of word        */
+# if ALIGNMENT != 4
+#   define UNALIGNED_PTRS
+# endif
+#endif
+
+#if CPP_WORDSZ == 64
+#  define WORDS_TO_BYTES(x)   ((x)<<3)
+#  define BYTES_TO_WORDS(x)   ((x)>>3)
+#  define LOGWL               ((word)6)    /* log[2] of CPP_WORDSZ */
+#  define modWORDSZ(n) ((n) & 0x3f)        /* n mod size of word            */
+#  if ALIGNMENT != 8
+#       define UNALIGNED_PTRS
+#  endif
+#endif
+
+/* The first TINY_FREELISTS free lists correspond to the first  */
+/* TINY_FREELISTS multiples of GRANULE_BYTES, i.e. we keep      */
+/* separate free lists for each multiple of GRANULE_BYTES       */
+/* up to (TINY_FREELISTS-1) * GRANULE_BYTES.  After that they   */
+/* may be spread out further.                                   */
+#include "../gc_tiny_fl.h"
+#define GRANULE_BYTES GC_GRANULE_BYTES
+
+#ifndef TINY_FREELISTS
+#define TINY_FREELISTS GC_TINY_FREELISTS
+#endif
+
+#define WORDSZ ((word)CPP_WORDSZ)
+#define SIGNB  ((word)1 << (WORDSZ-1))
+#define BYTES_PER_WORD      ((word)(sizeof (word)))
+#define divWORDSZ(n) ((n) >> LOGWL)     /* divide n by size of word */
+
+#if GRANULE_BYTES == 8
+# define BYTES_TO_GRANULES(n) ((n)>>3)
+# define GRANULES_TO_BYTES(n) ((n)<<3)
+# if CPP_WORDSZ == 64
+#   define GRANULES_TO_WORDS(n) (n)
+# elif CPP_WORDSZ == 32
+#   define GRANULES_TO_WORDS(n) ((n)<<1)
+# else
+#   define GRANULES_TO_WORDS(n) BYTES_TO_WORDS(GRANULES_TO_BYTES(n))
+# endif
+#elif GRANULE_BYTES == 16
+# define BYTES_TO_GRANULES(n) ((n)>>4)
+# define GRANULES_TO_BYTES(n) ((n)<<4)
+# if CPP_WORDSZ == 64
+#   define GRANULES_TO_WORDS(n) ((n)<<1)
+# elif CPP_WORDSZ == 32
+#   define GRANULES_TO_WORDS(n) ((n)<<2)
+# else
+#   define GRANULES_TO_WORDS(n) BYTES_TO_WORDS(GRANULES_TO_BYTES(n))
+# endif
+#else
+# error Bad GRANULE_BYTES value
+#endif
+
+/*********************/
+/*                   */
+/*  Size Parameters  */
+/*                   */
+/*********************/
+
+/* Heap block size, bytes. Should be power of 2.                */
+/* Incremental GC with MPROTECT_VDB currently requires the      */
+/* page size to be a multiple of HBLKSIZE.  Since most modern   */
+/* architectures support variable page sizes down to 4K, and    */
+/* X86 is generally 4K, we now default to 4K, except for        */
+/*   Alpha: Seems to be used with 8K pages.                     */
+/*   SMALL_CONFIG: Want less block-level fragmentation.         */
+#ifndef HBLKSIZE
+# if defined(LARGE_CONFIG) || !defined(SMALL_CONFIG)
+#   ifdef ALPHA
+#     define CPP_LOG_HBLKSIZE 13
+#   else
+#     define CPP_LOG_HBLKSIZE 12
+#   endif
+# else
+#   define CPP_LOG_HBLKSIZE 10
+# endif
+#else
+# if HBLKSIZE == 512
+#   define CPP_LOG_HBLKSIZE 9
+# elif HBLKSIZE == 1024
+#   define CPP_LOG_HBLKSIZE 10
+# elif HBLKSIZE == 2048
+#   define CPP_LOG_HBLKSIZE 11
+# elif HBLKSIZE == 4096
+#   define CPP_LOG_HBLKSIZE 12
+# elif HBLKSIZE == 8192
+#   define CPP_LOG_HBLKSIZE 13
+# elif HBLKSIZE == 16384
+#   define CPP_LOG_HBLKSIZE 14
+# else
+    --> fix HBLKSIZE
+# endif
+# undef HBLKSIZE
+#endif
+
+# define CPP_HBLKSIZE (1 << CPP_LOG_HBLKSIZE)
+# define LOG_HBLKSIZE   ((size_t)CPP_LOG_HBLKSIZE)
+# define HBLKSIZE ((size_t)CPP_HBLKSIZE)
+
+
+/*  Max size objects supported by freelist (larger objects are  */
+/*  allocated directly with allchblk(), by rounding to the next */
+/*  multiple of HBLKSIZE).                                      */
+#define CPP_MAXOBJBYTES (CPP_HBLKSIZE/2)
+#define MAXOBJBYTES ((size_t)CPP_MAXOBJBYTES)
+#define CPP_MAXOBJWORDS BYTES_TO_WORDS(CPP_MAXOBJBYTES)
+#define MAXOBJWORDS ((size_t)CPP_MAXOBJWORDS)
+#define CPP_MAXOBJGRANULES BYTES_TO_GRANULES(CPP_MAXOBJBYTES)
+#define MAXOBJGRANULES ((size_t)CPP_MAXOBJGRANULES)
+
+# define divHBLKSZ(n) ((n) >> LOG_HBLKSIZE)
+
+# define HBLK_PTR_DIFF(p,q) divHBLKSZ((ptr_t)p - (ptr_t)q)
+        /* Equivalent to subtracting 2 hblk pointers.   */
+        /* We do it this way because a compiler should  */
+        /* find it hard to use an integer division      */
+        /* instead of a shift.  The bundled SunOS 4.1   */
+        /* o.w. sometimes pessimizes the subtraction to */
+        /* involve a call to .div.                      */
+
+# define modHBLKSZ(n) ((n) & (HBLKSIZE-1))
+
+# define HBLKPTR(objptr) ((struct hblk *)(((word) (objptr)) & ~(HBLKSIZE-1)))
+
+# define HBLKDISPL(objptr) (((size_t) (objptr)) & (HBLKSIZE-1))
+
+/* Round up byte allocation requests to integral number of words, etc. */
+# define ROUNDED_UP_GRANULES(n) \
+        BYTES_TO_GRANULES((n) + (GRANULE_BYTES - 1 + EXTRA_BYTES))
+# if MAX_EXTRA_BYTES == 0
+#  define SMALL_OBJ(bytes) EXPECT((bytes) <= (MAXOBJBYTES), TRUE)
+# else
+#  define SMALL_OBJ(bytes) \
+            (EXPECT((bytes) <= (MAXOBJBYTES - MAX_EXTRA_BYTES), TRUE) \
+             || (bytes) <= MAXOBJBYTES - EXTRA_BYTES)
+        /* This really just tests bytes <= MAXOBJBYTES - EXTRA_BYTES.   */
+        /* But we try to avoid looking up EXTRA_BYTES.                  */
+# endif
+# define ADD_SLOP(bytes) ((bytes) + EXTRA_BYTES)
+# ifndef MIN_WORDS
+#  define MIN_WORDS 2   /* FIXME: obsolete */
+# endif
+
+/*
+ * Hash table representation of sets of pages.
+ * Implements a map from aligned HBLKSIZE chunks of the address space to one
+ * bit each.
+ * This assumes it is OK to spuriously set bits, e.g. because multiple
+ * addresses are represented by a single location.
+ * Used by black-listing code, and perhaps by dirty bit maintenance code.
+ */
+
+# ifdef LARGE_CONFIG
+#   if CPP_WORDSZ == 32
+#     define LOG_PHT_ENTRIES 20 /* Collisions likely at 1M blocks,      */
+                                /* which is >= 4GB.  Each table takes   */
+                                /* 128KB, some of which may never be    */
+                                /* touched.                             */
+#   else
+#     define LOG_PHT_ENTRIES 21 /* Collisions likely at 2M blocks,      */
+                                /* which is >= 8GB.  Each table takes   */
+                                /* 256KB, some of which may never be    */
+                                /* touched.                             */
+#   endif
+# elif !defined(SMALL_CONFIG)
+#   define LOG_PHT_ENTRIES  18   /* Collisions are likely if heap grows */
+                                 /* to more than 256K hblks >= 1GB.     */
+                                 /* Each hash table occupies 32K bytes. */
+                                 /* Even for somewhat smaller heaps,    */
+                                 /* say half that, collisions may be an */
+                                 /* issue because we blacklist          */
+                                 /* addresses outside the heap.         */
+# else
+#   define LOG_PHT_ENTRIES  15   /* Collisions are likely if heap grows */
+                                 /* to more than 32K hblks = 128MB.     */
+                                 /* Each hash table occupies 4K bytes.  */
+# endif
+# define PHT_ENTRIES ((word)1 << LOG_PHT_ENTRIES)
+# define PHT_SIZE (PHT_ENTRIES >> LOGWL)
+typedef word page_hash_table[PHT_SIZE];
+
+# define PHT_HASH(addr) ((((word)(addr)) >> LOG_HBLKSIZE) & (PHT_ENTRIES - 1))
+
+# define get_pht_entry_from_index(bl, index) \
+                (((bl)[divWORDSZ(index)] >> modWORDSZ(index)) & 1)
+# define set_pht_entry_from_index(bl, index) \
+                (bl)[divWORDSZ(index)] |= (word)1 << modWORDSZ(index)
+# define clear_pht_entry_from_index(bl, index) \
+                (bl)[divWORDSZ(index)] &= ~((word)1 << modWORDSZ(index))
+/* And a dumb but thread-safe version of set_pht_entry_from_index.      */
+/* This sets (many) extra bits.                                         */
+# define set_pht_entry_from_index_safe(bl, index) \
+                (bl)[divWORDSZ(index)] = ONES
+
+
+/********************************************/
+/*                                          */
+/*    H e a p   B l o c k s                 */
+/*                                          */
+/********************************************/
+
+/*  heap block header */
+#define HBLKMASK   (HBLKSIZE-1)
+
+#define MARK_BITS_PER_HBLK (HBLKSIZE/GRANULE_BYTES)
+           /* upper bound                                    */
+           /* We allocate 1 bit per allocation granule.      */
+           /* If MARK_BIT_PER_GRANULE is defined, we use     */
+           /* every nth bit, where n is the number of        */
+           /* allocation granules per object.  If            */
+           /* MARK_BIT_PER_OBJ is defined, we only use the   */
+           /* initial group of mark bits, and it is safe     */
+           /* to allocate smaller header for large objects.  */
+
+#ifdef PARALLEL_MARK
+# include "atomic_ops.h"
+  typedef AO_t counter_t;
+#else
+  typedef size_t counter_t;
+# if defined(THREADS) && defined(MPROTECT_VDB)
+#   include "atomic_ops.h"
+# endif
+#endif /* !PARALLEL_MARK */
+
+/* We maintain layout maps for heap blocks containing objects of a given */
+/* size.  Each entry in this map describes a byte offset and has the     */
+/* following type.                                                       */
+struct hblkhdr {
+    struct hblk * hb_next;      /* Link field for hblk free list         */
+                                /* and for lists of chunks waiting to be */
+                                /* reclaimed.                            */
+    struct hblk * hb_prev;      /* Backwards link for free list.        */
+    struct hblk * hb_block;     /* The corresponding block.             */
+    unsigned char hb_obj_kind;
+                         /* Kind of objects in the block.  Each kind    */
+                         /* identifies a mark procedure and a set of    */
+                         /* list headers.  Sometimes called regions.    */
+    unsigned char hb_flags;
+#       define IGNORE_OFF_PAGE  1       /* Ignore pointers that do not  */
+                                        /* point to the first page of   */
+                                        /* this object.                 */
+#       define WAS_UNMAPPED 2   /* This is a free block, which has      */
+                                /* been unmapped from the address       */
+                                /* space.                               */
+                                /* GC_remap must be invoked on it       */
+                                /* before it can be reallocated.        */
+                                /* Only set with USE_MUNMAP.            */
+#       define FREE_BLK 4       /* Block is free, i.e. not in use.      */
+    unsigned short hb_last_reclaimed;
+                                /* Value of GC_gc_no when block was     */
+                                /* last allocated or swept. May wrap.   */
+                                /* For a free block, this is maintained */
+                                /* only for USE_MUNMAP, and indicates   */
+                                /* when the header was allocated, or    */
+                                /* when the size of the block last      */
+                                /* changed.                             */
+    size_t hb_sz;  /* If in use, size in bytes, of objects in the block. */
+                   /* if free, the size in bytes of the whole block      */
+                   /* We assume that this is convertible to signed_word  */
+                   /* without generating a negative result.  We avoid    */
+                   /* generating free blocks larger than that.           */
+    word hb_descr;              /* object descriptor for marking.  See  */
+                                /* mark.h.                              */
+#   ifdef MARK_BIT_PER_OBJ
+      unsigned32 hb_inv_sz;     /* A good upper bound for 2**32/hb_sz.  */
+                                /* For large objects, we use            */
+                                /* LARGE_INV_SZ.                        */
+#     define LARGE_INV_SZ (1 << 16)
+#   else
+      unsigned char hb_large_block;
+      short * hb_map;           /* Essentially a table of remainders    */
+                                /* mod BYTES_TO_GRANULES(hb_sz), except */
+                                /* for large blocks.  See GC_obj_map.   */
+#   endif
+    counter_t hb_n_marks;       /* Number of set mark bits, excluding   */
+                                /* the one always set at the end.       */
+                                /* Currently it is concurrently         */
+                                /* updated and hence only approximate.  */
+                                /* But a zero value does guarantee that */
+                                /* the block contains no marked         */
+                                /* objects.                             */
+                                /* Ensuring this property means that we */
+                                /* never decrement it to zero during a  */
+                                /* collection, and hence the count may  */
+                                /* be one too high.  Due to concurrent  */
+                                /* updates, an arbitrary number of      */
+                                /* increments, but not all of them (!)  */
+                                /* may be lost, hence it may in theory  */
+                                /* be much too low.                     */
+                                /* The count may also be too high if    */
+                                /* multiple mark threads mark the       */
+                                /* same object due to a race.           */
+                                /* Without parallel marking, the count  */
+                                /* is accurate.                         */
+#   ifdef USE_MARK_BYTES
+#     define MARK_BITS_SZ (MARK_BITS_PER_HBLK + 1)
+        /* Unlike the other case, this is in units of bytes.            */
+        /* Since we force double-word alignment, we need at most one    */
+        /* mark bit per 2 words.  But we do allocate and set one        */
+        /* extra mark bit to avoid an explicit check for the            */
+        /* partial object at the end of each block.                     */
+      union {
+        char _hb_marks[MARK_BITS_SZ];
+                            /* The i'th byte is 1 if the object         */
+                            /* starting at granule i or object i is     */
+                            /* marked, 0 o.w.                           */
+                            /* The mark bit for the "one past the       */
+                            /* end" object is always set to avoid a     */
+                            /* special case test in the marker.         */
+        word dummy;     /* Force word alignment of mark bytes. */
+      } _mark_byte_union;
+#     define hb_marks _mark_byte_union._hb_marks
+#   else
+#     define MARK_BITS_SZ (MARK_BITS_PER_HBLK/CPP_WORDSZ + 1)
+      word hb_marks[MARK_BITS_SZ];
+#   endif /* !USE_MARK_BYTES */
+};
+
+# define ANY_INDEX 23   /* "Random" mark bit index for assertions */
+
+/*  heap block body */
+
+# define HBLK_WORDS (HBLKSIZE/sizeof(word))
+# define HBLK_GRANULES (HBLKSIZE/GRANULE_BYTES)
+
+/* The number of objects in a block dedicated to a certain size.        */
+/* may erroneously yield zero (instead of one) for large objects.       */
+# define HBLK_OBJS(sz_in_bytes) (HBLKSIZE/(sz_in_bytes))
+
+struct hblk {
+    char hb_body[HBLKSIZE];
+};
+
+# define HBLK_IS_FREE(hdr) (((hdr) -> hb_flags & FREE_BLK) != 0)
+
+# define OBJ_SZ_TO_BLOCKS(sz) divHBLKSZ((sz) + HBLKSIZE-1)
+    /* Size of block (in units of HBLKSIZE) needed to hold objects of   */
+    /* given sz (in bytes).                                             */
+
+/* Object free list link */
+# define obj_link(p) (*(void  **)(p))
+
+# define LOG_MAX_MARK_PROCS 6
+# define MAX_MARK_PROCS (1 << LOG_MAX_MARK_PROCS)
+
+/* Root sets.  Logically private to mark_rts.c.  But we don't want the  */
+/* tables scanned, so we put them here.                                 */
+/* MAX_ROOT_SETS is the maximum number of ranges that can be    */
+/* registered as static roots.                                  */
+# ifdef LARGE_CONFIG
+#   define MAX_ROOT_SETS 8192
+# elif !defined(SMALL_CONFIG)
+#   define MAX_ROOT_SETS 2048
+# else
+#   define MAX_ROOT_SETS 512
+# endif
+
+# define MAX_EXCLUSIONS (MAX_ROOT_SETS/4)
+/* Maximum number of segments that can be excluded from root sets.      */
+
+/*
+ * Data structure for excluded static roots.
+ */
+struct exclusion {
+    ptr_t e_start;
+    ptr_t e_end;
+};
+
+/* Data structure for list of root sets.                                */
+/* We keep a hash table, so that we can filter out duplicate additions. */
+/* Under Win32, we need to do a better job of filtering overlaps, so    */
+/* we resort to sequential search, and pay the price.                   */
+struct roots {
+        ptr_t r_start;/* multiple of word size */
+        ptr_t r_end;  /* multiple of word size and greater than r_start */
+#       if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+          struct roots * r_next;
+#       endif
+        GC_bool r_tmp;
+                /* Delete before registering new dynamic libraries */
+};
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+    /* Size of hash table index to roots.       */
+#   define LOG_RT_SIZE 6
+#   define RT_SIZE (1 << LOG_RT_SIZE) /* Power of 2, may be != MAX_ROOT_SETS */
+#endif
+
+#ifndef MAX_HEAP_SECTS
+# ifdef LARGE_CONFIG
+#   if CPP_WORDSZ > 32
+#     define MAX_HEAP_SECTS 8192        /* overflows at roughly 128 GB  */
+#   else
+#     define MAX_HEAP_SECTS 768         /* Separately added heap sections. */
+#   endif
+# elif defined(SMALL_CONFIG) && !defined(USE_PROC_FOR_LIBRARIES)
+#   if defined(PARALLEL_MARK) && (defined(MSWIN32) || defined(CYGWIN32))
+#     define MAX_HEAP_SECTS 384
+#   else
+#     define MAX_HEAP_SECTS 128         /* Roughly 256MB (128*2048*1K)  */
+#   endif
+# elif CPP_WORDSZ > 32
+#   define MAX_HEAP_SECTS 1024          /* Roughly 8GB                  */
+# else
+#   define MAX_HEAP_SECTS 512           /* Roughly 4GB                  */
+# endif
+#endif /* !MAX_HEAP_SECTS */
+
+typedef struct GC_ms_entry {
+    ptr_t mse_start;    /* First word of object, word aligned.  */
+    word mse_descr;     /* Descriptor; low order two bits are tags,     */
+                        /* as described in gc_mark.h.                   */
+} mse;
+
+/* Lists of all heap blocks and free lists      */
+/* as well as other random data structures      */
+/* that should not be scanned by the            */
+/* collector.                                   */
+/* These are grouped together in a struct       */
+/* so that they can be easily skipped by the    */
+/* GC_mark routine.                             */
+/* The ordering is weird to make GC_malloc      */
+/* faster by keeping the important fields       */
+/* sufficiently close together that a           */
+/* single load of a base register will do.      */
+/* Scalars that could easily appear to          */
+/* be pointers are also put here.               */
+/* The main fields should precede any           */
+/* conditionally included fields, so that       */
+/* gc_inl.h will work even if a different set   */
+/* of macros is defined when the client is      */
+/* compiled.                                    */
+
+struct _GC_arrays {
+  word _heapsize;               /* Heap size in bytes.                  */
+  word _max_heapsize;
+  word _requested_heapsize;     /* Heap size due to explicit expansion. */
+  ptr_t _last_heap_addr;
+  ptr_t _prev_heap_addr;
+  word _large_free_bytes;
+        /* Total bytes contained in blocks on large object free */
+        /* list.                                                */
+  word _large_allocd_bytes;
+        /* Total number of bytes in allocated large objects blocks.     */
+        /* For the purposes of this counter and the next one only, a    */
+        /* large object is one that occupies a block of at least        */
+        /* 2*HBLKSIZE.                                                  */
+  word _max_large_allocd_bytes;
+        /* Maximum number of bytes that were ever allocated in          */
+        /* large object blocks.  This is used to help decide when it    */
+        /* is safe to split up a large block.                           */
+  word _bytes_allocd_before_gc;
+                /* Number of bytes allocated before this        */
+                /* collection cycle.                            */
+# ifndef SEPARATE_GLOBALS
+#   define GC_bytes_allocd GC_arrays._bytes_allocd
+    word _bytes_allocd;
+        /* Number of bytes allocated during this collection cycle.      */
+# endif
+  word _bytes_dropped;
+        /* Number of black-listed bytes dropped during GC cycle */
+        /* as a result of repeated scanning during allocation   */
+        /* attempts.  These are treated largely as allocated,   */
+        /* even though they are not useful to the client.       */
+  word _bytes_finalized;
+        /* Approximate number of bytes in objects (and headers) */
+        /* that became ready for finalization in the last       */
+        /* collection.                                          */
+  word _non_gc_bytes_at_gc;
+        /* Number of explicitly managed bytes of storage        */
+        /* at last collection.                                  */
+  word _bytes_freed;
+        /* Number of explicitly deallocated bytes of memory     */
+        /* since last collection.                               */
+  word _finalizer_bytes_freed;
+        /* Bytes of memory explicitly deallocated while         */
+        /* finalizers were running.  Used to approximate mem.   */
+        /* explicitly deallocated by finalizers.                */
+  ptr_t _scratch_end_ptr;
+  ptr_t _scratch_last_end_ptr;
+        /* Used by headers.c, and can easily appear to point to */
+        /* heap.                                                */
+  mse *_mark_stack;
+        /* Limits of stack for GC_mark routine.  All ranges     */
+        /* between GC_mark_stack (incl.) and GC_mark_stack_top  */
+        /* (incl.) still need to be marked from.                */
+  mse *_mark_stack_limit;
+# ifdef PARALLEL_MARK
+    mse *volatile _mark_stack_top;
+        /* Updated only with mark lock held, but read asynchronously.   */
+# else
+    mse *_mark_stack_top;
+# endif
+  GC_mark_proc _mark_procs[MAX_MARK_PROCS];
+        /* Table of user-defined mark procedures.  There is     */
+        /* a small number of these, which can be referenced     */
+        /* by DS_PROC mark descriptors.  See gc_mark.h.         */
+# ifndef SEPARATE_GLOBALS
+#   define GC_objfreelist GC_arrays._objfreelist
+    void *_objfreelist[MAXOBJGRANULES+1];
+                          /* free list for objects */
+#   define GC_aobjfreelist GC_arrays._aobjfreelist
+    void *_aobjfreelist[MAXOBJGRANULES+1];
+                          /* free list for atomic objs  */
+# endif
+  void *_uobjfreelist[MAXOBJGRANULES+1];
+                          /* Uncollectible but traced objs      */
+                          /* objects on this and auobjfreelist  */
+                          /* are always marked, except during   */
+                          /* garbage collections.               */
+# ifdef ATOMIC_UNCOLLECTABLE
+#   define GC_auobjfreelist GC_arrays._auobjfreelist
+    void *_auobjfreelist[MAXOBJGRANULES+1];
+                        /* Atomic uncollectible but traced objs */
+# endif
+  word _composite_in_use; /* Number of bytes in the accessible  */
+                          /* composite objects.                 */
+  word _atomic_in_use;    /* Number of bytes in the accessible  */
+                          /* atomic objects.                    */
+# ifdef USE_MUNMAP
+#   define GC_unmapped_bytes GC_arrays._unmapped_bytes
+    word _unmapped_bytes;
+# else
+#   define GC_unmapped_bytes 0
+# endif
+  size_t _size_map[MAXOBJBYTES+1];
+        /* Number of granules to allocate when asked for a certain      */
+        /* number of bytes.                                             */
+
+# ifdef STUBBORN_ALLOC
+#   define GC_sobjfreelist GC_arrays._sobjfreelist
+    ptr_t _sobjfreelist[MAXOBJGRANULES+1];
+# endif
+                          /* free list for immutable objects    */
+# ifdef MARK_BIT_PER_GRANULE
+#   define GC_obj_map GC_arrays._obj_map
+    short * _obj_map[MAXOBJGRANULES+1];
+                       /* If not NULL, then a pointer to a map of valid */
+                       /* object addresses.                             */
+                       /* _obj_map[sz_in_granules][i] is                */
+                       /* i % sz_in_granules.                           */
+                       /* This is now used purely to replace a          */
+                       /* division in the marker by a table lookup.     */
+                       /* _obj_map[0] is used for large objects and     */
+                       /* contains all nonzero entries.  This gets us   */
+                       /* out of the marker fast path without an extra  */
+                       /* test.                                         */
+#   define MAP_LEN BYTES_TO_GRANULES(HBLKSIZE)
+# endif
+# define VALID_OFFSET_SZ HBLKSIZE
+  char _valid_offsets[VALID_OFFSET_SZ];
+                                /* GC_valid_offsets[i] == TRUE ==> i    */
+                                /* is registered as a displacement.     */
+  char _modws_valid_offsets[sizeof(word)];
+                                /* GC_valid_offsets[i] ==>                */
+                                /* GC_modws_valid_offsets[i%sizeof(word)] */
+# ifdef STUBBORN_ALLOC
+#   define GC_changed_pages GC_arrays._changed_pages
+    page_hash_table _changed_pages;
+        /* Stubborn object pages that were changes since last call to   */
+        /* GC_read_changed.                                             */
+#   define GC_prev_changed_pages GC_arrays._prev_changed_pages
+    page_hash_table _prev_changed_pages;
+        /* Stubborn object pages that were changes before last call to  */
+        /* GC_read_changed.                                             */
+# endif
+# if defined(PROC_VDB) || defined(MPROTECT_VDB) \
+     || defined(GWW_VDB) || defined(MANUAL_VDB)
+#   define GC_grungy_pages GC_arrays._grungy_pages
+    page_hash_table _grungy_pages; /* Pages that were dirty at last     */
+                                   /* GC_read_dirty.                    */
+# endif
+# if defined(MPROTECT_VDB) || defined(MANUAL_VDB)
+#   define GC_dirty_pages GC_arrays._dirty_pages
+    volatile page_hash_table _dirty_pages;
+                        /* Pages dirtied since last GC_read_dirty. */
+# endif
+# if defined(PROC_VDB) || defined(GWW_VDB)
+#   define GC_written_pages GC_arrays._written_pages
+    page_hash_table _written_pages;     /* Pages ever dirtied   */
+# endif
+# define GC_heap_sects GC_arrays._heap_sects
+  struct HeapSect {
+    ptr_t hs_start;
+    size_t hs_bytes;
+  } _heap_sects[MAX_HEAP_SECTS];        /* Heap segments potentially    */
+                                        /* client objects.              */
+# if defined(USE_PROC_FOR_LIBRARIES)
+#   define GC_our_memory GC_arrays._our_memory
+    struct HeapSect _our_memory[MAX_HEAP_SECTS];
+                                        /* All GET_MEM allocated        */
+                                        /* memory.  Includes block      */
+                                        /* headers and the like.        */
+# endif
+# if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+#   define GC_heap_bases GC_arrays._heap_bases
+    ptr_t _heap_bases[MAX_HEAP_SECTS];
+                /* Start address of memory regions obtained from kernel. */
+# endif
+# ifdef MSWINCE
+#   define GC_heap_lengths GC_arrays._heap_lengths
+    word _heap_lengths[MAX_HEAP_SECTS];
+                /* Committed lengths of memory regions obtained from kernel. */
+# endif
+  struct roots _static_roots[MAX_ROOT_SETS];
+# if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+#   define GC_root_index GC_arrays._root_index
+    struct roots * _root_index[RT_SIZE];
+# endif
+  struct exclusion _excl_table[MAX_EXCLUSIONS];
+  /* Block header index; see gc_headers.h */
+  bottom_index * _all_nils;
+  bottom_index * _top_index [TOP_SZ];
+# ifdef ENABLE_TRACE
+#   define GC_trace_addr GC_arrays._trace_addr
+    ptr_t _trace_addr;
+# endif
+# ifdef SAVE_CALL_CHAIN
+#   define GC_last_stack GC_arrays._last_stack
+    struct callinfo _last_stack[NFRAMES];
+                /* Stack at last garbage collection.  Useful for        */
+                /* debugging mysterious object disappearances.  In the  */
+                /* multi-threaded case, we currently only save the      */
+                /* calling stack.                                       */
+# endif
+};
+
+GC_API_PRIV GC_FAR struct _GC_arrays GC_arrays;
+
+#define GC_all_nils GC_arrays._all_nils
+#define GC_atomic_in_use GC_arrays._atomic_in_use
+#define GC_bytes_allocd_before_gc GC_arrays._bytes_allocd_before_gc
+#define GC_bytes_dropped GC_arrays._bytes_dropped
+#define GC_bytes_finalized GC_arrays._bytes_finalized
+#define GC_bytes_freed GC_arrays._bytes_freed
+#define GC_composite_in_use GC_arrays._composite_in_use
+#define GC_excl_table GC_arrays._excl_table
+#define GC_finalizer_bytes_freed GC_arrays._finalizer_bytes_freed
+#define GC_heapsize GC_arrays._heapsize
+#define GC_large_allocd_bytes GC_arrays._large_allocd_bytes
+#define GC_large_free_bytes GC_arrays._large_free_bytes
+#define GC_last_heap_addr GC_arrays._last_heap_addr
+#define GC_mark_stack GC_arrays._mark_stack
+#define GC_mark_stack_limit GC_arrays._mark_stack_limit
+#define GC_mark_stack_top GC_arrays._mark_stack_top
+#define GC_mark_procs GC_arrays._mark_procs
+#define GC_max_heapsize GC_arrays._max_heapsize
+#define GC_max_large_allocd_bytes GC_arrays._max_large_allocd_bytes
+#define GC_modws_valid_offsets GC_arrays._modws_valid_offsets
+#define GC_non_gc_bytes_at_gc GC_arrays._non_gc_bytes_at_gc
+#define GC_prev_heap_addr GC_arrays._prev_heap_addr
+#define GC_requested_heapsize GC_arrays._requested_heapsize
+#define GC_scratch_end_ptr GC_arrays._scratch_end_ptr
+#define GC_scratch_last_end_ptr GC_arrays._scratch_last_end_ptr
+#define GC_size_map GC_arrays._size_map
+#define GC_static_roots GC_arrays._static_roots
+#define GC_top_index GC_arrays._top_index
+#define GC_uobjfreelist GC_arrays._uobjfreelist
+#define GC_valid_offsets GC_arrays._valid_offsets
+
+#define beginGC_arrays ((ptr_t)(&GC_arrays))
+#define endGC_arrays (((ptr_t)(&GC_arrays)) + (sizeof GC_arrays))
+#define USED_HEAP_SIZE (GC_heapsize - GC_large_free_bytes)
+
+/* Object kinds: */
+#define MAXOBJKINDS 16
+
+GC_EXTERN struct obj_kind {
+   void **ok_freelist;  /* Array of free list headers for this kind of  */
+                        /* object.  Point either to GC_arrays or to     */
+                        /* storage allocated with GC_scratch_alloc.     */
+   struct hblk **ok_reclaim_list;
+                        /* List headers for lists of blocks waiting to  */
+                        /* be swept.  Indexed by object size in         */
+                        /* granules.                                    */
+   word ok_descriptor;  /* Descriptor template for objects in this      */
+                        /* block.                                       */
+   GC_bool ok_relocate_descr;
+                        /* Add object size in bytes to descriptor       */
+                        /* template to obtain descriptor.  Otherwise    */
+                        /* template is used as is.                      */
+   GC_bool ok_init;   /* Clear objects before putting them on the free list. */
+} GC_obj_kinds[MAXOBJKINDS];
+
+#define beginGC_obj_kinds ((ptr_t)(&GC_obj_kinds))
+#define endGC_obj_kinds (beginGC_obj_kinds + (sizeof GC_obj_kinds))
+
+/* Variables that used to be in GC_arrays, but need to be accessed by   */
+/* inline allocation code.  If they were in GC_arrays, the inlined      */
+/* allocation code would include GC_arrays offsets (as it did), which   */
+/* introduce maintenance problems.                                      */
+
+#ifdef SEPARATE_GLOBALS
+  extern word GC_bytes_allocd;
+        /* Number of bytes allocated during this collection cycle.      */
+  extern ptr_t GC_objfreelist[MAXOBJGRANULES+1];
+                          /* free list for NORMAL objects */
+# define beginGC_objfreelist ((ptr_t)(&GC_objfreelist))
+# define endGC_objfreelist (beginGC_objfreelist + sizeof(GC_objfreelist))
+
+  extern ptr_t GC_aobjfreelist[MAXOBJGRANULES+1];
+                          /* free list for atomic (PTRFREE) objs        */
+# define beginGC_aobjfreelist ((ptr_t)(&GC_aobjfreelist))
+# define endGC_aobjfreelist (beginGC_aobjfreelist + sizeof(GC_aobjfreelist))
+#endif /* SEPARATE_GLOBALS */
+
+/* Predefined kinds: */
+#define PTRFREE 0
+#define NORMAL  1
+#define UNCOLLECTABLE 2
+#ifdef ATOMIC_UNCOLLECTABLE
+# define AUNCOLLECTABLE 3
+# define STUBBORN 4
+# define IS_UNCOLLECTABLE(k) (((k) & ~1) == UNCOLLECTABLE)
+#else
+# define STUBBORN 3
+# define IS_UNCOLLECTABLE(k) ((k) == UNCOLLECTABLE)
+#endif
+
+GC_EXTERN unsigned GC_n_kinds;
+
+GC_EXTERN word GC_n_heap_sects; /* Number of separately added heap      */
+                                /* sections.                            */
+
+#ifdef USE_PROC_FOR_LIBRARIES
+  GC_EXTERN word GC_n_memory;   /* Number of GET_MEM allocated memory   */
+                                /* sections.                            */
+#endif
+
+GC_EXTERN word GC_page_size;
+
+/* Round up allocation size to a multiple of a page size.       */
+/* GC_setpagesize() is assumed to be already invoked.           */
+#define ROUNDUP_PAGESIZE(bytes) \
+                (((bytes) + GC_page_size - 1) & ~(GC_page_size - 1))
+
+/* Same as above but used to make GET_MEM() argument safe.      */
+#ifdef MMAP_SUPPORTED
+# define ROUNDUP_PAGESIZE_IF_MMAP(bytes) ROUNDUP_PAGESIZE(bytes)
+#else
+# define ROUNDUP_PAGESIZE_IF_MMAP(bytes) (bytes)
+#endif
+
+#if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+  struct _SYSTEM_INFO;
+  GC_EXTERN struct _SYSTEM_INFO GC_sysinfo;
+  GC_INNER GC_bool GC_is_heap_base(ptr_t p);
+#endif
+
+
+GC_EXTERN word GC_black_list_spacing;
+                        /* Average number of bytes between blacklisted  */
+                        /* blocks. Approximate.                         */
+                        /* Counts only blocks that are                  */
+                        /* "stack-blacklisted", i.e. that are           */
+                        /* problematic in the interior of an object.    */
+
+#ifdef GC_GCJ_SUPPORT
+  extern struct hblk * GC_hblkfreelist[];
+                                        /* Remains visible to GNU GCJ. */
+#endif
+
+#ifdef GC_DISABLE_INCREMENTAL
+# define GC_incremental FALSE
+                        /* Hopefully allow optimizer to remove some code. */
+# define TRUE_INCREMENTAL FALSE
+#else
+  GC_EXTERN GC_bool GC_incremental;
+                        /* Using incremental/generational collection. */
+# define TRUE_INCREMENTAL \
+        (GC_incremental && GC_time_limit != GC_TIME_UNLIMITED)
+        /* True incremental, not just generational, mode */
+#endif /* !GC_DISABLE_INCREMENTAL */
+
+GC_EXTERN word GC_root_size; /* Total size of registered root sections. */
+
+GC_EXTERN GC_bool GC_debugging_started;
+                                /* GC_debug_malloc has been called.     */
+
+/* This is used by GC_do_blocking[_inner]().            */
+struct blocking_data {
+    GC_fn_type fn;
+    void * client_data; /* and result */
+};
+
+/* This is used by GC_call_with_gc_active(), GC_push_all_stack_sections(). */
+#ifndef NAUT
+struct GC_traced_stack_sect_s {
+  ptr_t saved_stack_ptr;
+# ifdef IA64
+    ptr_t saved_backing_store_ptr;
+    ptr_t backing_store_end;
+# endif
+  struct GC_traced_stack_sect_s *prev;
+};
+#endif
+
+#ifdef THREADS
+  /* Process all "traced stack sections" - scan entire stack except for */
+  /* frames belonging to the user functions invoked by GC_do_blocking.  */
+  GC_INNER void GC_push_all_stack_sections(ptr_t lo, ptr_t hi,
+                        struct GC_traced_stack_sect_s *traced_stack_sect);
+  GC_EXTERN word GC_total_stacksize; /* updated on every push_all_stacks */
+#else
+  GC_EXTERN ptr_t GC_blocked_sp;
+  GC_EXTERN struct GC_traced_stack_sect_s *GC_traced_stack_sect;
+                        /* Points to the "frame" data held in stack by  */
+                        /* the innermost GC_call_with_gc_active().      */
+                        /* NULL if no such "frame" active.              */
+#endif /* !THREADS */
+
+#ifdef IA64
+  /* Similar to GC_push_all_stack_sections() but for IA-64 registers store. */
+  GC_INNER void GC_push_all_register_sections(ptr_t bs_lo, ptr_t bs_hi,
+                  int eager, struct GC_traced_stack_sect_s *traced_stack_sect);
+#endif
+
+/*  Marks are in a reserved area in                          */
+/*  each heap block.  Each word has one mark bit associated  */
+/*  with it. Only those corresponding to the beginning of an */
+/*  object are used.                                         */
+
+/* Mark bit operations */
+
+/*
+ * Retrieve, set, clear the nth mark bit in a given heap block.
+ *
+ * (Recall that bit n corresponds to nth object or allocation granule
+ * relative to the beginning of the block, including unused words)
+ */
+
+#ifdef USE_MARK_BYTES
+# define mark_bit_from_hdr(hhdr,n) ((hhdr)->hb_marks[n])
+# define set_mark_bit_from_hdr(hhdr,n) ((hhdr)->hb_marks[n] = 1)
+# define clear_mark_bit_from_hdr(hhdr,n) ((hhdr)->hb_marks[n] = 0)
+#else
+/* Set mark bit correctly, even if mark bits may be concurrently        */
+/* accessed.                                                            */
+# ifdef PARALLEL_MARK
+    /* This is used only if we explicitly set USE_MARK_BITS.    */
+#   define OR_WORD(addr, bits) AO_or((volatile AO_t *)(addr), (AO_t)(bits))
+# else
+#   define OR_WORD(addr, bits) (void)(*(addr) |= (bits))
+# endif
+# define mark_bit_from_hdr(hhdr,n) \
+              (((hhdr)->hb_marks[divWORDSZ(n)] >> modWORDSZ(n)) & (word)1)
+# define set_mark_bit_from_hdr(hhdr,n) \
+              OR_WORD((hhdr)->hb_marks+divWORDSZ(n), (word)1 << modWORDSZ(n))
+# define clear_mark_bit_from_hdr(hhdr,n) \
+              ((hhdr)->hb_marks[divWORDSZ(n)] &= ~((word)1 << modWORDSZ(n)))
+#endif /* !USE_MARK_BYTES */
+
+#ifdef MARK_BIT_PER_OBJ
+#  define MARK_BIT_NO(offset, sz) (((unsigned)(offset))/(sz))
+        /* Get the mark bit index corresponding to the given byte       */
+        /* offset and size (in bytes).                                  */
+#  define MARK_BIT_OFFSET(sz) 1
+        /* Spacing between useful mark bits.                            */
+#  define IF_PER_OBJ(x) x
+#  define FINAL_MARK_BIT(sz) ((sz) > MAXOBJBYTES? 1 : HBLK_OBJS(sz))
+        /* Position of final, always set, mark bit.                     */
+#else /* MARK_BIT_PER_GRANULE */
+#  define MARK_BIT_NO(offset, sz) BYTES_TO_GRANULES((unsigned)(offset))
+#  define MARK_BIT_OFFSET(sz) BYTES_TO_GRANULES(sz)
+#  define IF_PER_OBJ(x)
+#  define FINAL_MARK_BIT(sz) \
+                ((sz) > MAXOBJBYTES ? MARK_BITS_PER_HBLK \
+                                : BYTES_TO_GRANULES((sz) * HBLK_OBJS(sz)))
+#endif
+
+/* Important internal collector routines */
+
+GC_INNER ptr_t GC_approx_sp(void);
+
+GC_INNER GC_bool GC_should_collect(void);
+
+void GC_apply_to_all_blocks(void (*fn)(struct hblk *h, word client_data),
+                            word client_data);
+                        /* Invoke fn(hbp, client_data) for each         */
+                        /* allocated heap block.                        */
+GC_INNER struct hblk * GC_next_used_block(struct hblk * h);
+                        /* Return first in-use block >= h       */
+GC_INNER struct hblk * GC_prev_block(struct hblk * h);
+                        /* Return last block <= h.  Returned block      */
+                        /* is managed by GC, but may or may not be in   */
+                        /* use.                                         */
+GC_INNER void GC_mark_init(void);
+GC_INNER void GC_clear_marks(void);
+                        /* Clear mark bits for all heap objects.        */
+GC_INNER void GC_invalidate_mark_state(void);
+                                /* Tell the marker that marked          */
+                                /* objects may point to unmarked        */
+                                /* ones, and roots may point to         */
+                                /* unmarked objects.  Reset mark stack. */
+GC_INNER GC_bool GC_mark_stack_empty(void);
+GC_INNER GC_bool GC_mark_some(ptr_t cold_gc_frame);
+                        /* Perform about one pages worth of marking     */
+                        /* work of whatever kind is needed.  Returns    */
+                        /* quickly if no collection is in progress.     */
+                        /* Return TRUE if mark phase finished.          */
+GC_INNER void GC_initiate_gc(void);
+                                /* initiate collection.                 */
+                                /* If the mark state is invalid, this   */
+                                /* becomes full collection.  Otherwise  */
+                                /* it's partial.                        */
+
+GC_INNER GC_bool GC_collection_in_progress(void);
+                        /* Collection is in progress, or was abandoned. */
+
+GC_API_PRIV void GC_push_all(ptr_t bottom, ptr_t top);
+                                /* Push everything in a range           */
+                                /* onto mark stack.                     */
+#ifndef GC_DISABLE_INCREMENTAL
+  GC_API_PRIV void GC_push_conditional(ptr_t b, ptr_t t, GC_bool all);
+#else
+# define GC_push_conditional(b, t, all) GC_push_all(b, t)
+#endif
+                                /* Do either of the above, depending    */
+                                /* on the third arg.                    */
+GC_INNER void GC_push_all_stack(ptr_t b, ptr_t t);
+                                    /* As above, but consider           */
+                                    /*  interior pointers as valid      */
+GC_INNER void GC_push_all_eager(ptr_t b, ptr_t t);
+                                    /* Same as GC_push_all_stack, but   */
+                                    /* ensures that stack is scanned    */
+                                    /* immediately, not just scheduled  */
+                                    /* for scanning.                    */
+
+  /* In the threads case, we push part of the current thread stack      */
+  /* with GC_push_all_eager when we push the registers.  This gets the  */
+  /* callee-save registers that may disappear.  The remainder of the    */
+  /* stacks are scheduled for scanning in *GC_push_other_roots, which   */
+  /* is thread-package-specific.                                        */
+
+GC_INNER void GC_push_roots(GC_bool all, ptr_t cold_gc_frame);
+                                        /* Push all or dirty roots.     */
+
+GC_API_PRIV void (*GC_push_other_roots)(void);
+                        /* Push system or application specific roots    */
+                        /* onto the mark stack.  In some environments   */
+                        /* (e.g. threads environments) this is          */
+                        /* predefined to be non-zero.  A client         */
+                        /* supplied replacement should also call the    */
+                        /* original function.  Remains externally       */
+                        /* visible as used by some well-known 3rd-party */
+                        /* software (e.g., ECL) currently.              */
+
+GC_INNER void GC_push_finalizer_structures(void);
+#ifdef THREADS
+  void GC_push_thread_structures(void);
+#endif
+GC_EXTERN void (*GC_push_typed_structures)(void);
+                        /* A pointer such that we can avoid linking in  */
+                        /* the typed allocation support if unused.      */
+
+GC_INNER void GC_with_callee_saves_pushed(void (*fn)(ptr_t, void *),
+                                          ptr_t arg);
+
+#if defined(SPARC) || defined(IA64)
+  /* Cause all stacked registers to be saved in memory.  Return a       */
+  /* pointer to the top of the corresponding memory stack.              */
+  ptr_t GC_save_regs_in_stack(void);
+#endif
+                        /* Push register contents onto mark stack.      */
+
+#if defined(MSWIN32) || defined(MSWINCE)
+  void __cdecl GC_push_one(word p);
+#else
+  void GC_push_one(word p);
+                              /* If p points to an object, mark it    */
+                              /* and push contents on the mark stack  */
+                              /* Pointer recognition test always      */
+                              /* accepts interior pointers, i.e. this */
+                              /* is appropriate for pointers found on */
+                              /* stack.                               */
+#endif
+
+#if defined(PRINT_BLACK_LIST) || defined(KEEP_BACK_PTRS)
+  GC_INNER void GC_mark_and_push_stack(ptr_t p, ptr_t source);
+                                /* Ditto, omits plausibility test       */
+#else
+  GC_INNER void GC_mark_and_push_stack(ptr_t p);
+#endif
+
+GC_INNER void GC_clear_hdr_marks(hdr * hhdr);
+                                    /* Clear the mark bits in a header */
+GC_INNER void GC_set_hdr_marks(hdr * hhdr);
+                                    /* Set the mark bits in a header */
+GC_INNER void GC_set_fl_marks(ptr_t p);
+                                    /* Set all mark bits associated with */
+                                    /* a free list.                      */
+#if defined(GC_ASSERTIONS) && defined(THREADS) && defined(THREAD_LOCAL_ALLOC)
+  void GC_check_fl_marks(void **);
+                                    /* Check that all mark bits         */
+                                    /* associated with a free list are  */
+                                    /* set.  Abort if not.              */
+#endif
+void GC_add_roots_inner(ptr_t b, ptr_t e, GC_bool tmp);
+GC_INNER void GC_exclude_static_roots_inner(void *start, void *finish);
+#if defined(DYNAMIC_LOADING) || defined(MSWIN32) || defined(MSWINCE) \
+    || defined(CYGWIN32) || defined(PCR)
+  GC_INNER void GC_register_dynamic_libraries(void);
+                /* Add dynamic library data sections to the root set. */
+#endif
+GC_INNER void GC_cond_register_dynamic_libraries(void);
+                /* Remove and reregister dynamic libraries if we're     */
+                /* configured to do that at each GC.                    */
+
+/* Machine dependent startup routines */
+ptr_t GC_get_main_stack_base(void);     /* Cold end of stack.           */
+#ifdef IA64
+  GC_INNER ptr_t GC_get_register_stack_base(void);
+                                        /* Cold end of register stack.  */
+#endif
+void GC_register_data_segments(void);
+
+#ifdef THREADS
+  GC_INNER void GC_thr_init(void);
+  GC_INNER void GC_init_parallel(void);
+#else
+  GC_INNER GC_bool GC_is_static_root(ptr_t p);
+                /* Is the address p in one of the registered static     */
+                /* root sections?                                       */
+#endif
+
+/* Black listing: */
+#ifdef PRINT_BLACK_LIST
+  GC_INNER void GC_add_to_black_list_normal(word p, ptr_t source);
+                        /* Register bits as a possible future false     */
+                        /* reference from the heap or static data       */
+# define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \
+                if (GC_all_interior_pointers) { \
+                  GC_add_to_black_list_stack((word)(bits), (source)); \
+                } else { \
+                  GC_add_to_black_list_normal((word)(bits), (source)); \
+                }
+  GC_INNER void GC_add_to_black_list_stack(word p, ptr_t source);
+# define GC_ADD_TO_BLACK_LIST_STACK(bits, source) \
+            GC_add_to_black_list_stack((word)(bits), (source))
+#else
+  GC_INNER void GC_add_to_black_list_normal(word p);
+# define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \
+                if (GC_all_interior_pointers) { \
+                  GC_add_to_black_list_stack((word)(bits)); \
+                } else { \
+                  GC_add_to_black_list_normal((word)(bits)); \
+                }
+  GC_INNER void GC_add_to_black_list_stack(word p);
+# define GC_ADD_TO_BLACK_LIST_STACK(bits, source) \
+            GC_add_to_black_list_stack((word)(bits))
+#endif /* PRINT_BLACK_LIST */
+
+struct hblk * GC_is_black_listed(struct hblk * h, word len);
+                        /* If there are likely to be false references   */
+                        /* to a block starting at h of the indicated    */
+                        /* length, then return the next plausible       */
+                        /* starting location for h that might avoid     */
+                        /* these false references.  Remains externally  */
+                        /* visible as used by GNU GCJ currently.        */
+
+GC_INNER void GC_promote_black_lists(void);
+                        /* Declare an end to a black listing phase.     */
+GC_INNER void GC_unpromote_black_lists(void);
+                        /* Approximately undo the effect of the above.  */
+                        /* This actually loses some information, but    */
+                        /* only in a reasonably safe way.               */
+
+GC_INNER ptr_t GC_scratch_alloc(size_t bytes);
+                                /* GC internal memory allocation for    */
+                                /* small objects.  Deallocation is not  */
+                                /* possible.  May return NULL.          */
+
+/* Heap block layout maps: */
+GC_INNER GC_bool GC_add_map_entry(size_t sz);
+                                /* Add a heap block map for objects of  */
+                                /* size sz to obj_map.                  */
+                                /* Return FALSE on failure.             */
+GC_INNER void GC_register_displacement_inner(size_t offset);
+                                /* Version of GC_register_displacement  */
+                                /* that assumes lock is already held.   */
+
+/*  hblk allocation: */
+GC_INNER void GC_new_hblk(size_t size_in_granules, int kind);
+                                /* Allocate a new heap block, and build */
+                                /* a free list in it.                   */
+
+GC_INNER ptr_t GC_build_fl(struct hblk *h, size_t words, GC_bool clear,
+                           ptr_t list);
+                                /* Build a free list for objects of     */
+                                /* size sz in block h.  Append list to  */
+                                /* end of the free lists.  Possibly     */
+                                /* clear objects on the list.  Normally */
+                                /* called by GC_new_hblk, but also      */
+                                /* called explicitly without GC lock.   */
+
+GC_INNER struct hblk * GC_allochblk(size_t size_in_bytes, int kind,
+                                    unsigned flags);
+                                /* Allocate a heap block, inform        */
+                                /* the marker that block is valid       */
+                                /* for objects of indicated size.       */
+
+GC_INNER ptr_t GC_alloc_large(size_t lb, int k, unsigned flags);
+                        /* Allocate a large block of size lb bytes.     */
+                        /* The block is not cleared.                    */
+                        /* Flags is 0 or IGNORE_OFF_PAGE.               */
+                        /* Calls GC_allchblk to do the actual           */
+                        /* allocation, but also triggers GC and/or      */
+                        /* heap expansion as appropriate.               */
+                        /* Does not update GC_bytes_allocd, but does    */
+                        /* other accounting.                            */
+
+GC_INNER void GC_freehblk(struct hblk * p);
+                                /* Deallocate a heap block and mark it  */
+                                /* as invalid.                          */
+
+/*  Misc GC: */
+GC_INNER GC_bool GC_expand_hp_inner(word n);
+GC_INNER void GC_start_reclaim(GC_bool abort_if_found);
+                                /* Restore unmarked objects to free     */
+                                /* lists, or (if abort_if_found is      */
+                                /* TRUE) report them.                   */
+                                /* Sweeping of small object pages is    */
+                                /* largely deferred.                    */
+GC_INNER void GC_continue_reclaim(size_t sz, int kind);
+                                /* Sweep pages of the given size and    */
+                                /* kind, as long as possible, and       */
+                                /* as long as the corr. free list is    */
+                                /* empty.  Sz is in granules.           */
+
+GC_INNER GC_bool GC_reclaim_all(GC_stop_func stop_func, GC_bool ignore_old);
+                                /* Reclaim all blocks.  Abort (in a     */
+                                /* consistent state) if f returns TRUE. */
+GC_INNER ptr_t GC_reclaim_generic(struct hblk * hbp, hdr *hhdr, size_t sz,
+                                  GC_bool init, ptr_t list,
+                                  signed_word *count);
+                                /* Rebuild free list in hbp with        */
+                                /* header hhdr, with objects of size sz */
+                                /* bytes.  Add list to the end of the   */
+                                /* free list.  Add the number of        */
+                                /* reclaimed bytes to *count.           */
+GC_INNER GC_bool GC_block_empty(hdr * hhdr);
+                                /* Block completely unmarked?   */
+GC_INNER int GC_CALLBACK GC_never_stop_func(void);
+                                /* Always returns 0 (FALSE).            */
+GC_INNER GC_bool GC_try_to_collect_inner(GC_stop_func f);
+
+                                /* Collect; caller must have acquired   */
+                                /* lock.  Collection is aborted if f    */
+                                /* returns TRUE.  Returns TRUE if it    */
+                                /* completes successfully.              */
+#define GC_gcollect_inner() \
+                (void)GC_try_to_collect_inner(GC_never_stop_func)
+
+GC_EXTERN GC_bool GC_is_initialized; /* GC_init() has been run. */
+
+#if defined(MSWIN32) || defined(MSWINCE)
+  void GC_deinit(void);
+                                /* Free any resources allocated by      */
+                                /* GC_init                              */
+#endif
+
+GC_INNER void GC_collect_a_little_inner(int n);
+                                /* Do n units worth of garbage          */
+                                /* collection work, if appropriate.     */
+                                /* A unit is an amount appropriate for  */
+                                /* HBLKSIZE bytes of allocation.        */
+/* void * GC_generic_malloc(size_t lb, int k); */
+                                /* Allocate an object of the given      */
+                                /* kind.  By default, there are only    */
+                                /* a few kinds: composite(pointerfree), */
+                                /* atomic, uncollectable, etc.          */
+                                /* We claim it's possible for clever    */
+                                /* client code that understands GC      */
+                                /* internals to add more, e.g. to       */
+                                /* communicate object layout info       */
+                                /* to the collector.                    */
+                                /* The actual decl is in gc_mark.h.     */
+GC_INNER void * GC_generic_malloc_ignore_off_page(size_t b, int k);
+                                /* As above, but pointers past the      */
+                                /* first page of the resulting object   */
+                                /* are ignored.                         */
+GC_INNER void * GC_generic_malloc_inner(size_t lb, int k);
+                                /* Ditto, but I already hold lock, etc. */
+GC_INNER void * GC_generic_malloc_inner_ignore_off_page(size_t lb, int k);
+                                /* Allocate an object, where            */
+                                /* the client guarantees that there     */
+                                /* will always be a pointer to the      */
+                                /* beginning of the object while the    */
+                                /* object is live.                      */
+
+GC_INNER ptr_t GC_allocobj(size_t sz, int kind);
+                                /* Make the indicated                   */
+                                /* free list nonempty, and return its   */
+                                /* head.  Sz is in granules.            */
+
+#ifdef GC_ADD_CALLER
+  /* GC_DBG_EXTRAS is used by GC debug API functions (unlike GC_EXTRAS  */
+  /* used by GC debug API macros) thus GC_RETURN_ADDR_PARENT (pointing  */
+  /* to client caller) should be used if possible.                      */
+# ifdef GC_RETURN_ADDR_PARENT
+#  define GC_DBG_EXTRAS GC_RETURN_ADDR_PARENT, NULL, 0
+# else
+#  define GC_DBG_EXTRAS GC_RETURN_ADDR, NULL, 0
+# endif
+#else
+# define GC_DBG_EXTRAS "unknown", 0
+#endif
+
+/* We make the GC_clear_stack() call a tail one, hoping to get more of  */
+/* the stack.                                                           */
+
+#ifdef NAUT
+# define GENERAL_MALLOC(lb,k) \
+     GC_generic_malloc(lb, k)
+
+#else
+# define GENERAL_MALLOC(lb,k) \
+     GC_clear_stack(GC_generic_malloc(lb, k))
+#endif
+
+#define GENERAL_MALLOC_IOP(lb,k) \
+    GC_clear_stack(GC_generic_malloc_ignore_off_page(lb, k))
+
+/* Allocation routines that bypass the thread local cache.      */
+#ifdef THREAD_LOCAL_ALLOC
+  GC_INNER void * GC_core_malloc(size_t);
+  GC_INNER void * GC_core_malloc_atomic(size_t);
+# ifdef GC_GCJ_SUPPORT
+    GC_INNER void * GC_core_gcj_malloc(size_t, void *);
+# endif
+#endif /* THREAD_LOCAL_ALLOC */
+
+GC_INNER void GC_init_headers(void);
+GC_INNER struct hblkhdr * GC_install_header(struct hblk *h);
+                                /* Install a header for block h.        */
+                                /* Return 0 on failure, or the header   */
+                                /* otherwise.                           */
+GC_INNER GC_bool GC_install_counts(struct hblk * h, size_t sz);
+                                /* Set up forwarding counts for block   */
+                                /* h of size sz.                        */
+                                /* Return FALSE on failure.             */
+GC_INNER void GC_remove_header(struct hblk * h);
+                                /* Remove the header for block h.       */
+GC_INNER void GC_remove_counts(struct hblk * h, size_t sz);
+                                /* Remove forwarding counts for h.      */
+GC_INNER hdr * GC_find_header(ptr_t h);
+
+GC_INNER void GC_finalize(void);
+                        /* Perform all indicated finalization actions   */
+                        /* on unmarked objects.                         */
+                        /* Unreachable finalizable objects are enqueued */
+                        /* for processing by GC_invoke_finalizers.      */
+                        /* Invoked with lock.                           */
+
+GC_INNER void GC_notify_or_invoke_finalizers(void);
+                        /* If GC_finalize_on_demand is not set, invoke  */
+                        /* eligible finalizers. Otherwise:              */
+                        /* Call *GC_finalizer_notifier if there are     */
+                        /* finalizers to be run, and we haven't called  */
+                        /* this procedure yet this GC cycle.            */
+
+GC_INNER void GC_add_to_heap(struct hblk *p, size_t bytes);
+                        /* Add a HBLKSIZE aligned chunk to the heap.    */
+
+#ifdef USE_PROC_FOR_LIBRARIES
+  GC_INNER void GC_add_to_our_memory(ptr_t p, size_t bytes);
+                        /* Add a chunk to GC_our_memory.        */
+                        /* If p == 0, do nothing.               */
+#else
+# define GC_add_to_our_memory(p, bytes)
+#endif
+
+GC_INNER void GC_print_all_errors(void);
+                        /* Print smashed and leaked objects, if any.    */
+                        /* Clear the lists of such objects.             */
+
+GC_EXTERN void (*GC_check_heap)(void);
+                        /* Check that all objects in the heap with      */
+                        /* debugging info are intact.                   */
+                        /* Add any that are not to GC_smashed list.     */
+GC_EXTERN void (*GC_print_all_smashed)(void);
+                        /* Print GC_smashed if it's not empty.          */
+                        /* Clear GC_smashed list.                       */
+GC_EXTERN void (*GC_print_heap_obj)(ptr_t p);
+                        /* If possible print s followed by a more       */
+                        /* detailed description of the object           */
+                        /* referred to by p.                            */
+
+#if defined(LINUX) && defined(__ELF__) && !defined(SMALL_CONFIG)
+  void GC_print_address_map(void);
+                        /* Print an address map of the process.         */
+#endif
+
+#ifndef SHORT_DBG_HDRS
+  GC_EXTERN GC_bool GC_findleak_delay_free;
+                        /* Do not immediately deallocate object on      */
+                        /* free() in the leak-finding mode, just mark   */
+                        /* it as freed (and deallocate it after GC).    */
+  GC_INNER GC_bool GC_check_leaked(ptr_t base); /* from dbg_mlc.c */
+#endif
+
+GC_EXTERN GC_bool GC_have_errors; /* We saw a smashed or leaked object. */
+                                  /* Call error printing routine        */
+                                  /* occasionally.  It is ok to read it */
+                                  /* without acquiring the lock.        */
+
+#ifndef SMALL_CONFIG
+  /* GC_print_stats should be visible to extra/MacOS.c. */
+  extern int GC_print_stats;    /* Nonzero generates basic GC log.      */
+                                /* VERBOSE generates add'l messages.    */
+#else
+# define GC_print_stats 0
+  /* Will this remove the message character strings from the executable? */
+  /* With a particular level of optimizations, it should...              */
+#endif
+#define VERBOSE 2
+
+#ifndef NO_DEBUGGING
+  GC_EXTERN GC_bool GC_dump_regularly;
+                                /* Generate regular debugging dumps.    */
+# define COND_DUMP if (GC_dump_regularly) GC_dump()
+#else
+# define COND_DUMP /* empty */
+#endif
+
+#ifdef KEEP_BACK_PTRS
+  GC_EXTERN long GC_backtraces;
+  GC_INNER void GC_generate_random_backtrace_no_gc(void);
+#endif
+
+GC_EXTERN GC_bool GC_print_back_height;
+
+#ifdef MAKE_BACK_GRAPH
+  void GC_print_back_graph_stats(void);
+#endif
+
+#ifdef THREADS
+  GC_INNER void GC_free_inner(void * p);
+#endif
+
+/* Macros used for collector internal allocation.       */
+/* These assume the collector lock is held.             */
+#ifdef DBG_HDRS_ALL
+  GC_INNER void * GC_debug_generic_malloc_inner(size_t lb, int k);
+  GC_INNER void * GC_debug_generic_malloc_inner_ignore_off_page(size_t lb,
+                                                                int k);
+# define GC_INTERNAL_MALLOC GC_debug_generic_malloc_inner
+# define GC_INTERNAL_MALLOC_IGNORE_OFF_PAGE \
+               GC_debug_generic_malloc_inner_ignore_off_page
+# ifdef THREADS
+    GC_INNER void GC_debug_free_inner(void * p);
+#   define GC_INTERNAL_FREE GC_debug_free_inner
+# else
+#   define GC_INTERNAL_FREE GC_debug_free
+# endif
+#else
+# define GC_INTERNAL_MALLOC GC_generic_malloc_inner
+# define GC_INTERNAL_MALLOC_IGNORE_OFF_PAGE \
+               GC_generic_malloc_inner_ignore_off_page
+# ifdef THREADS
+#   define GC_INTERNAL_FREE GC_free_inner
+# else
+#   define GC_INTERNAL_FREE GC_free
+# endif
+#endif /* !DBG_HDRS_ALL */
+
+#ifdef USE_MUNMAP
+  /* Memory unmapping: */
+  GC_INNER void GC_unmap_old(void);
+  GC_INNER void GC_merge_unmapped(void);
+  GC_INNER void GC_unmap(ptr_t start, size_t bytes);
+  GC_INNER void GC_remap(ptr_t start, size_t bytes);
+  GC_INNER void GC_unmap_gap(ptr_t start1, size_t bytes1, ptr_t start2,
+                             size_t bytes2);
+#endif
+
+#ifdef CAN_HANDLE_FORK
+  GC_EXTERN GC_bool GC_handle_fork;
+#endif
+
+#ifndef GC_DISABLE_INCREMENTAL
+  GC_EXTERN GC_bool GC_dirty_maintained;
+                                /* Dirty bits are being maintained,     */
+                                /* either for incremental collection,   */
+                                /* or to limit the root set.            */
+
+  /* Virtual dirty bit implementation:            */
+  /* Each implementation exports the following:   */
+  GC_INNER void GC_read_dirty(void);
+                        /* Retrieve dirty bits. */
+  GC_INNER GC_bool GC_page_was_dirty(struct hblk *h);
+                        /* Read retrieved dirty bits.   */
+  GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                   GC_bool pointerfree);
+                        /* h is about to be written or allocated.  Ensure   */
+                        /* that it's not write protected by the virtual     */
+                        /* dirty bit implementation.                        */
+
+  GC_INNER void GC_dirty_init(void);
+#endif /* !GC_DISABLE_INCREMENTAL */
+
+/* Slow/general mark bit manipulation: */
+GC_API_PRIV GC_bool GC_is_marked(ptr_t p);
+GC_API_PRIV void GC_clear_mark_bit(ptr_t p);
+GC_API_PRIV void GC_set_mark_bit(ptr_t p);
+
+/* Stubborn objects: */
+void GC_read_changed(void); /* Analogous to GC_read_dirty */
+GC_bool GC_page_was_changed(struct hblk * h);
+                                /* Analogous to GC_page_was_dirty */
+void GC_clean_changing_list(void);
+                                /* Collect obsolete changing list entries */
+void GC_stubborn_init(void);
+
+/* Debugging print routines: */
+void GC_print_block_list(void);
+void GC_print_hblkfreelist(void);
+void GC_print_heap_sects(void);
+void GC_print_static_roots(void);
+#ifndef SMALL_CONFIG
+  GC_INNER void GC_print_finalization_stats(void);
+#endif
+/* void GC_dump(void); - declared in gc.h */
+
+#ifdef KEEP_BACK_PTRS
+   GC_INNER void GC_store_back_pointer(ptr_t source, ptr_t dest);
+   GC_INNER void GC_marked_for_finalization(ptr_t dest);
+#  define GC_STORE_BACK_PTR(source, dest) GC_store_back_pointer(source, dest)
+#  define GC_MARKED_FOR_FINALIZATION(dest) GC_marked_for_finalization(dest)
+#else
+#  define GC_STORE_BACK_PTR(source, dest)
+#  define GC_MARKED_FOR_FINALIZATION(dest)
+#endif
+
+/* Make arguments appear live to compiler */
+#if defined(__BORLANDC__) || defined(__WATCOMC__) || defined(__CC_ARM)
+  void GC_noop(void*, ...);
+#else
+# ifdef __DMC__
+    void GC_noop(...);
+# else
+    void GC_noop();
+# endif
+#endif
+
+GC_API void GC_CALL GC_noop1(word);
+
+/*
+#ifndef GC_ATTR_FORMAT_PRINTF
+# if defined(__GNUC__) && __GNUC__ >= 3
+#   define GC_ATTR_FORMAT_PRINTF(spec_argnum, first_checked) \
+        __attribute__((__format__(__printf__, spec_argnum, first_checked)))
+# else
+#   define GC_ATTR_FORMAT_PRINTF(spec_argnum, first_checked)
+# endif
+#endif
+*/
+#ifdef NAUT // Use normal print functionality
+//# include<nautilus/printk.h>
+# define GC_printf(...) printk(__VA_ARGS__)
+//#define GC_printf(...) DEBUG_PRINT("Thread: " fmt, ##args)
+# define GC_err_printf(...) GC_printf(__VA_ARGS__)
+# define GC_log_printf(...) GC_printf(__VA_ARGS__)
+#else 
+/* Logging and diagnostic output:       */
+GC_API_PRIV void GC_printf(const char * format, ...)
+  GC_ATTR_FORMAT_PRINTF(1, 2);
+/* A version of printf that doesn't allocate,   */
+/* 1K total output length.                      */
+/* (We use sprintf.  Hopefully that doesn't     */
+/* allocate for long arguments.)                */
+GC_API_PRIV void GC_err_printf(const char * format, ...)
+  GC_ATTR_FORMAT_PRINTF(1, 2);
+GC_API_PRIV void GC_log_printf(const char * format, ...)
+  GC_ATTR_FORMAT_PRINTF(1, 2);
+#endif
+
+
+void GC_err_puts(const char *s);
+                        /* Write s to stderr, don't buffer, don't add   */
+                        /* newlines, don't ...                          */
+
+GC_EXTERN unsigned GC_fail_count;
+                        /* How many consecutive GC/expansion failures?  */
+                        /* Reset by GC_allochblk(); defined in alloc.c. */
+
+GC_EXTERN long GC_large_alloc_warn_interval; /* defined in misc.c */
+
+GC_EXTERN signed_word GC_bytes_found;
+                /* Number of reclaimed bytes after garbage collection;  */
+                /* protected by GC lock; defined in reclaim.c.          */
+
+#ifdef USE_MUNMAP
+  GC_EXTERN int GC_unmap_threshold; /* defined in allchblk.c */
+  GC_EXTERN GC_bool GC_force_unmap_on_gcollect; /* defined in misc.c */
+#endif
+
+#ifdef MSWIN32
+  GC_EXTERN GC_bool GC_no_win32_dlls; /* defined in os_dep.c */
+  GC_EXTERN GC_bool GC_wnt;     /* Is Windows NT derivative;    */
+                                /* defined and set in os_dep.c. */
+#endif
+
+#ifdef THREADS
+# if defined(MSWIN32) || defined(MSWINCE)
+    GC_EXTERN CRITICAL_SECTION GC_write_cs; /* defined in misc.c */
+#   ifdef GC_ASSERTIONS
+      GC_EXTERN GC_bool GC_write_disabled;
+                                /* defined in win32_threads.c;  */
+                                /* protected by GC_write_cs.    */
+
+#   endif
+# endif
+# ifdef MPROTECT_VDB
+    GC_EXTERN volatile AO_TS_t GC_fault_handler_lock;
+                                        /* defined in os_dep.c */
+# endif
+# ifdef MSWINCE
+    GC_EXTERN GC_bool GC_dont_query_stack_min;
+                                /* Defined and set in os_dep.c. */
+# endif
+#elif defined(IA64)
+  GC_EXTERN ptr_t GC_save_regs_ret_val; /* defined in mach_dep.c. */
+                        /* Previously set to backing store pointer.     */
+#endif /* !THREADS */
+
+#ifdef THREAD_LOCAL_ALLOC
+  GC_EXTERN GC_bool GC_world_stopped; /* defined in alloc.c */
+  GC_INNER void GC_mark_thread_local_free_lists(void);
+#endif
+
+#ifdef GC_GCJ_SUPPORT
+# ifdef GC_ASSERTIONS
+    GC_EXTERN GC_bool GC_gcj_malloc_initialized; /* defined in gcj_mlc.c */
+# endif
+  GC_EXTERN ptr_t * GC_gcjobjfreelist;
+#endif
+
+#if defined(GWW_VDB) && defined(MPROTECT_VDB)
+  GC_INNER GC_bool GC_gww_dirty_init(void);
+  /* Defined in os_dep.c.  Returns TRUE if GetWriteWatch is available.  */
+  /* May be called repeatedly.                                          */
+#endif
+
+#if defined(CHECKSUMS) || defined(PROC_VDB)
+  GC_INNER GC_bool GC_page_was_ever_dirty(struct hblk * h);
+                        /* Could the page contain valid heap pointers?  */
+#endif
+
+GC_INNER void GC_default_print_heap_obj_proc(ptr_t p);
+
+GC_INNER void GC_extend_size_map(size_t); /* in misc.c */
+
+GC_INNER void GC_setpagesize(void);
+
+GC_INNER void GC_initialize_offsets(void);      /* defined in obj_map.c */
+
+GC_INNER void GC_bl_init(void);
+GC_INNER void GC_bl_init_no_interiors(void);    /* defined in blacklst.c */
+
+GC_INNER void GC_start_debugging(void); /* defined in dbg_mlc.c */
+
+/* Store debugging info into p.  Return displaced pointer.      */
+/* Assumes we don't hold allocation lock.                       */
+GC_INNER ptr_t GC_store_debug_info(ptr_t p, word sz, const char *str,
+                                   int linenum);
+
+#ifdef REDIRECT_MALLOC
+# ifdef GC_LINUX_THREADS
+    GC_INNER GC_bool GC_text_mapping(char *nm, ptr_t *startp, ptr_t *endp);
+                                                /* from os_dep.c */
+# endif
+#elif defined(MSWIN32) || defined(MSWINCE)
+  GC_INNER void GC_add_current_malloc_heap(void);
+#endif /* !REDIRECT_MALLOC */
+
+#ifdef MAKE_BACK_GRAPH
+  GC_INNER void GC_build_back_graph(void);
+  GC_INNER void GC_traverse_back_graph(void);
+#endif
+
+#ifdef MSWIN32
+  GC_INNER void GC_init_win32(void);
+#endif
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+  GC_INNER void * GC_roots_present(ptr_t);
+        /* The type is a lie, since the real type doesn't make sense here, */
+        /* and we only test for NULL.                                      */
+#endif
+
+#ifdef GC_WIN32_THREADS
+  GC_INNER void GC_get_next_stack(char *start, char * limit, char **lo,
+                                  char **hi);
+# ifdef MPROTECT_VDB
+    GC_INNER void GC_set_write_fault_handler(void);
+# endif
+#endif /* GC_WIN32_THREADS */
+
+#ifdef THREADS
+  GC_INNER void GC_reset_finalizer_nested(void);
+  GC_INNER unsigned char *GC_check_finalizer_nested(void);
+  GC_INNER void GC_do_blocking_inner(ptr_t data, void * context);
+  GC_INNER void GC_push_all_stacks(void);
+# ifdef USE_PROC_FOR_LIBRARIES
+    GC_INNER GC_bool GC_segment_is_thread_stack(ptr_t lo, ptr_t hi);
+# endif
+# ifdef IA64
+    GC_INNER ptr_t GC_greatest_stack_base_below(ptr_t bound);
+# endif
+#endif /* THREADS */
+
+#ifdef DYNAMIC_LOADING
+  GC_INNER GC_bool GC_register_main_static_data(void);
+# ifdef DARWIN
+    GC_INNER void GC_init_dyld(void);
+# endif
+#endif /* DYNAMIC_LOADING */
+
+#ifdef SEARCH_FOR_DATA_START
+  GC_INNER void GC_init_linux_data_start(void);
+#endif
+
+#if defined(NETBSD) && defined(__ELF__)
+  GC_INNER void GC_init_netbsd_elf(void);
+#endif
+
+#ifdef UNIX_LIKE
+  GC_INNER void GC_set_and_save_fault_handler(void (*handler)(int));
+#endif
+
+#ifdef NEED_PROC_MAPS
+  GC_INNER char *GC_parse_map_entry(char *buf_ptr, ptr_t *start, ptr_t *end,
+                                    char **prot, unsigned int *maj_dev,
+                                    char **mapping_name);
+  GC_INNER char *GC_get_maps(void); /* from os_dep.c */
+#endif
+
+#ifdef GC_ASSERTIONS
+#  define GC_ASSERT(expr) \
+                if (!(expr)) { \
+                  BDWGC_DEBUG("Assertion failure: %s:%d for thread %p \n", \
+                              __FILE__, __LINE__, get_cur_thread());   \
+                  panic("assertion failure"); \
+                }
+#else
+#  define GC_ASSERT(expr)
+#endif
+
+/* Check a compile time assertion at compile time.  The error   */
+/* message for failure is a bit baroque, but ...                */
+#if defined(mips) && !defined(__GNUC__)
+/* DOB: MIPSPro C gets an internal error taking the sizeof an array type.
+   This code works correctly (ugliness is to avoid "unused var" warnings) */
+# define GC_STATIC_ASSERT(expr) \
+    do { if (0) { char j[(expr)? 1 : -1]; j[0]='\0'; j[0]=j[0]; } } while(0)
+#else
+# define GC_STATIC_ASSERT(expr) (void)sizeof(char[(expr)? 1 : -1])
+#endif
+
+#if defined(PARALLEL_MARK)
+  /* We need additional synchronization facilities from the thread      */
+  /* support.  We believe these are less performance critical           */
+  /* than the main garbage collector lock; standard pthreads-based      */
+  /* implementations should be sufficient.                              */
+
+  GC_EXTERN long GC_markers;  /* Number of mark threads we would like   */
+                              /* to have.  Includes the initiating      */
+                              /* thread.  Defined in mark.c.            */
+
+  /* The mark lock and condition variable.  If the GC lock is also      */
+  /* acquired, the GC lock must be acquired first.  The mark lock is    */
+  /* used to both protect some variables used by the parallel           */
+  /* marker, and to protect GC_fl_builder_count, below.                 */
+  /* GC_notify_all_marker() is called when                              */
+  /* the state of the parallel marker changes                           */
+  /* in some significant way (see gc_mark.h for details).  The          */
+  /* latter set of events includes incrementing GC_mark_no.             */
+  /* GC_notify_all_builder() is called when GC_fl_builder_count         */
+  /* reaches 0.                                                         */
+
+  GC_INNER void GC_wait_for_markers_init(void);
+  GC_INNER void GC_acquire_mark_lock(void);
+  GC_INNER void GC_release_mark_lock(void);
+  GC_INNER void GC_notify_all_builder(void);
+  GC_INNER void GC_wait_for_reclaim(void);
+
+  GC_EXTERN word GC_fl_builder_count;   /* Protected by mark lock.      */
+
+  GC_INNER void GC_notify_all_marker(void);
+  GC_INNER void GC_wait_marker(void);
+  GC_EXTERN word GC_mark_no;            /* Protected by mark lock.      */
+
+  GC_INNER void GC_help_marker(word my_mark_no);
+              /* Try to help out parallel marker for mark cycle         */
+              /* my_mark_no.  Returns if the mark cycle finishes or     */
+              /* was already done, or there was nothing to do for       */
+              /* some other reason.                                     */
+#endif /* PARALLEL_MARK */
+
+#if defined(GC_PTHREADS) && !defined(GC_WIN32_THREADS) && !defined(NACL) \
+    && !defined(SIG_SUSPEND)
+  /* We define the thread suspension signal here, so that we can refer  */
+  /* to it in the dirty bit implementation, if necessary.  Ideally we   */
+  /* would allocate a (real-time?) signal using the standard mechanism. */
+  /* unfortunately, there is no standard mechanism.  (There is one      */
+  /* in Linux glibc, but it's not exported.)  Thus we continue to use   */
+  /* the same hard-coded signals we've always used.                     */
+# if defined(GC_LINUX_THREADS) || defined(GC_DGUX386_THREADS)
+#   if defined(SPARC) && !defined(SIGPWR)
+      /* SPARC/Linux doesn't properly define SIGPWR in <signal.h>.      */
+      /* It is aliased to SIGLOST in asm/signal.h, though.              */
+#     define SIG_SUSPEND SIGLOST
+#   else
+      /* Linuxthreads itself uses SIGUSR1 and SIGUSR2.                  */
+#     define SIG_SUSPEND SIGPWR
+#   endif
+# elif !defined(GC_OPENBSD_THREADS) && !defined(GC_DARWIN_THREADS)
+#   if defined(_SIGRTMIN)
+#     define SIG_SUSPEND _SIGRTMIN + 6
+#   else
+#     define SIG_SUSPEND SIGRTMIN + 6
+#   endif
+# endif
+#endif /* GC_PTHREADS && !SIG_SUSPEND */
+
+#if defined(GC_PTHREADS) && !defined(GC_SEM_INIT_PSHARED)
+# define GC_SEM_INIT_PSHARED 0
+#endif
+
+/* Some macros for setjmp that works across signal handlers     */
+/* were possible, and a couple of routines to facilitate        */
+/* catching accesses to bad addresses when that's               */
+/* possible/needed.                                             */
+#if defined(NAUT)
+# include <nautilus/setjmp.h>
+# define SETJMP(env) setjmp(env)
+# define LONGJMP(env, val) longjmp(env, val)
+# define JMP_BUF jmp_buf
+  extern JMP_BUF GC_jmp_buf;
+
+#elif defined(UNIX_LIKE) || (defined(NEED_FIND_LIMIT) && defined(CYGWIN32))
+# include <setjmp.h>
+# if defined(SUNOS5SIGS) && !defined(FREEBSD) && !defined(LINUX)
+#  include <sys/siginfo.h>
+# endif
+  /* Define SETJMP and friends to be the version that restores  */
+  /* the signal mask.                                           */
+# define SETJMP(env) sigsetjmp(env, 1)
+# define LONGJMP(env, val) siglongjmp(env, val)
+# define JMP_BUF sigjmp_buf
+
+
+#else
+# ifdef ECOS
+#   define SETJMP(env)  hal_setjmp(env)
+# else
+#   define SETJMP(env) setjmp(env)
+# endif
+# define LONGJMP(env, val) longjmp(env, val)
+# define JMP_BUF jmp_buf
+#endif /* !UNIX_LIKE */
+extern JMP_BUF GC_jmp_buf;
+/* Do we need the GC_find_limit machinery to find the end of a  */
+/* data segment.                                                */
+#if defined(HEURISTIC2) || defined(SEARCH_FOR_DATA_START)
+# define NEED_FIND_LIMIT
+#endif
+
+#if !defined(STACKBOTTOM) && defined(HEURISTIC2)
+# define NEED_FIND_LIMIT
+#endif
+
+#if (defined(SVR4) || defined(AUX) || defined(DGUX) \
+    || (defined(LINUX) && defined(SPARC))) && !defined(PCR)
+# define NEED_FIND_LIMIT
+#endif
+
+#if defined(FREEBSD) && (defined(I386) || defined(X86_64) \
+                        || defined(powerpc) || defined(__powerpc__))
+# include <machine/trap.h>
+# if !defined(PCR)
+#   define NEED_FIND_LIMIT
+# endif
+#endif /* FREEBSD */
+
+#if (defined(NETBSD) || defined(OPENBSD)) && defined(__ELF__) \
+    && !defined(NEED_FIND_LIMIT)
+  /* Used by GC_init_netbsd_elf() in os_dep.c. */
+# define NEED_FIND_LIMIT
+#endif
+
+#if defined(IA64) && !defined(NEED_FIND_LIMIT)
+# define NEED_FIND_LIMIT
+     /* May be needed for register backing store base. */
+#endif
+
+#if defined(NEED_FIND_LIMIT) \
+     || (defined(USE_PROC_FOR_LIBRARIES) && defined(THREADS))
+  extern JMP_BUF GC_jmp_buf;
+
+  /* Set up a handler for address faults which will longjmp to  */
+  /* GC_jmp_buf;                                                */
+  GC_INNER void GC_setup_temporary_fault_handler(void);
+  /* Undo the effect of GC_setup_temporary_fault_handler.       */
+  GC_INNER void GC_reset_fault_handler(void);
+#endif /* NEED_FIND_LIMIT || USE_PROC_FOR_LIBRARIES */
+
+/* Some convenience macros for cancellation support. */
+#if defined(CANCEL_SAFE)
+# if defined(GC_ASSERTIONS) && (defined(USE_COMPILER_TLS) \
+     || (defined(LINUX) && !defined(ARM32) \
+              && (__GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >= 3)) \
+     || defined(HPUX) /* and probably others ... */))
+    extern __thread unsigned char GC_cancel_disable_count;
+#   define NEED_CANCEL_DISABLE_COUNT
+#   define INCR_CANCEL_DISABLE() ++GC_cancel_disable_count
+#   define DECR_CANCEL_DISABLE() --GC_cancel_disable_count
+#   define ASSERT_CANCEL_DISABLED() GC_ASSERT(GC_cancel_disable_count > 0)
+# else
+#   define INCR_CANCEL_DISABLE()
+#   define DECR_CANCEL_DISABLE()
+#   define ASSERT_CANCEL_DISABLED()
+# endif /* GC_ASSERTIONS & ... */
+# define DISABLE_CANCEL(state) \
+        { pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, &state); \
+          INCR_CANCEL_DISABLE(); }
+# define RESTORE_CANCEL(state) \
+        { ASSERT_CANCEL_DISABLED(); \
+          pthread_setcancelstate(state, NULL); \
+          DECR_CANCEL_DISABLE(); }
+#else /* !CANCEL_SAFE */
+# define DISABLE_CANCEL(state)
+# define RESTORE_CANCEL(state)
+# define ASSERT_CANCEL_DISABLED()
+#endif /* !CANCEL_SAFE */
+
+#endif /* GC_PRIVATE_H */
diff --git a/src/gc/bdwgc/include/private/gcconfig.h b/src/gc/bdwgc/include/private/gcconfig.h
new file mode 100644
index 0000000..46043a4
--- /dev/null
+++ b/src/gc/bdwgc/include/private/gcconfig.h
@@ -0,0 +1,2892 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 2000-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/*
+ * This header is private to the gc.  It is almost always included from
+ * gc_priv.h.  However it is possible to include it by itself if just the
+ * configuration macros are needed.  In that
+ * case, a few declarations relying on types declared in gc_priv.h will be
+ * omitted.
+ */
+
+#ifndef GCCONFIG_H
+#define GCCONFIG_H
+
+# ifndef GC_PRIVATE_H
+    /* Fake ptr_t declaration, just to avoid compilation errors.        */
+    /* This avoids many instances if "ifndef GC_PRIVATE_H" below.       */
+    typedef struct GC_undefined_struct * ptr_t;
+#   include <stddef.h>  /* For size_t etc. */
+# endif
+
+/* Machine dependent parameters.  Some tuning parameters can be found   */
+/* near the top of gc_private.h.                                        */
+
+/* Machine specific parts contributed by various people.  See README file. */
+
+/* First a unified test for Linux: */
+# if (defined(linux) || defined(__linux__) || defined(PLATFORM_ANDROID)) \
+     && !defined(LINUX) && !defined(__native_client__)
+#   define LINUX
+# endif
+
+/* And one for NetBSD: */
+# if defined(__NetBSD__)
+#    define NETBSD
+# endif
+
+/* And one for OpenBSD: */
+# if defined(__OpenBSD__)
+#    define OPENBSD
+# endif
+
+/* And one for FreeBSD: */
+# if (defined(__FreeBSD__) || defined(__DragonFly__) \
+      || defined(__FreeBSD_kernel__)) && !defined(FREEBSD)
+#    define FREEBSD
+# endif
+
+/* And one for Darwin: */
+# if defined(macosx) || (defined(__APPLE__) && defined(__MACH__))
+#   define DARWIN
+# endif
+
+/* Determine the machine type: */
+# if defined(__native_client__)
+#    define NACL
+#    define I386
+#    define mach_type_known
+# endif
+# if defined(__arm) || defined(__arm__) || defined(__thumb__)
+#    define ARM32
+#    if !defined(LINUX) && !defined(NETBSD) && !defined(FREEBSD) \
+        && !defined(OPENBSD) && !defined(DARWIN) \
+        && !defined(_WIN32) && !defined(__CEGCC__)
+#      define NOSYS
+#      define mach_type_known
+#    endif
+# endif
+# if defined(sun) && defined(mc68000)
+#    error SUNOS4 no longer supported
+# endif
+# if defined(hp9000s300)
+#    error M68K based HP machines no longer supported.
+# endif
+# if defined(OPENBSD) && defined(m68k)
+     /* FIXME: Should we remove this case? */
+#    define M68K
+#    define mach_type_known
+# endif
+# if defined(OPENBSD) && defined(__sparc__)
+#    define SPARC
+#    define mach_type_known
+# endif
+# if defined(OPENBSD) && defined(__arm__)
+#    define ARM32
+#    define mach_type_known
+# endif
+# if defined(OPENBSD) && defined(__sh__)
+#    define SH
+#    define mach_type_known
+# endif
+# if defined(NETBSD) && (defined(m68k) || defined(__m68k__))
+#    define M68K
+#    define mach_type_known
+# endif
+# if defined(NETBSD) && defined(__powerpc__)
+#    define POWERPC
+#    define mach_type_known
+# endif
+# if defined(NETBSD) && (defined(__arm32__) || defined(__arm__))
+#    define ARM32
+#    define mach_type_known
+# endif
+# if defined(NETBSD) && defined(__sh__)
+#    define SH
+#    define mach_type_known
+# endif
+# if defined(vax) || defined(__vax__)
+#    define VAX
+#    ifdef ultrix
+#       define ULTRIX
+#    else
+#       define BSD
+#    endif
+#    define mach_type_known
+# endif
+# if defined(__NetBSD__) && defined(__vax__)
+#    define VAX
+#    define mach_type_known
+# endif
+# if defined(mips) || defined(__mips) || defined(_mips)
+#    define MIPS
+#    if defined(nec_ews) || defined(_nec_ews)
+#      define EWS4800
+#    endif
+#    if !defined(LINUX) && !defined(EWS4800) && !defined(NETBSD) \
+        && !defined(OPENBSD)
+#      if defined(ultrix) || defined(__ultrix)
+#        define ULTRIX
+#      else
+#        define IRIX5   /* or IRIX 6.X */
+#      endif
+#    endif /* !LINUX */
+#    if defined(__NetBSD__) && defined(__MIPSEL__)
+#      undef ULTRIX
+#    endif
+#    define mach_type_known
+# endif
+# if defined(DGUX) && (defined(i386) || defined(__i386__))
+#    define I386
+#    ifndef _USING_DGUX
+#    define _USING_DGUX
+#    endif
+#    define mach_type_known
+# endif
+# if defined(sequent) && (defined(i386) || defined(__i386__))
+#    define I386
+#    define SEQUENT
+#    define mach_type_known
+# endif
+# if defined(sun) && (defined(i386) || defined(__i386__))
+#    define I386
+#    define SOLARIS
+#    define mach_type_known
+# endif
+# if defined(sun) && defined(__amd64)
+#    define X86_64
+#    define SOLARIS
+#    define mach_type_known
+# endif
+# if (defined(__OS2__) || defined(__EMX__)) && defined(__32BIT__)
+#    define I386
+#    define OS2
+#    define mach_type_known
+# endif
+# if defined(ibm032)
+#   error IBM PC/RT no longer supported.
+# endif
+# if defined(sun) && (defined(sparc) || defined(__sparc))
+#   define SPARC
+    /* Test for SunOS 5.x */
+#     include <errno.h>
+#     define SOLARIS
+#   define mach_type_known
+# endif
+# if defined(sparc) && defined(unix) && !defined(sun) && !defined(linux) \
+     && !defined(__OpenBSD__) && !defined(__NetBSD__) \
+     && !defined(__FreeBSD__) && !defined(__DragonFly__)
+#   define SPARC
+#   define DRSNX
+#   define mach_type_known
+# endif
+# if defined(_IBMR2)
+#   define POWERPC
+#   define AIX
+#   define mach_type_known
+# endif
+# if defined(__NetBSD__) && defined(__sparc__)
+#   define SPARC
+#   define mach_type_known
+# endif
+# if defined(_M_XENIX) && defined(_M_SYSV) && defined(_M_I386)
+        /* The above test may need refinement   */
+#   define I386
+#   if defined(_SCO_ELF)
+#     define SCO_ELF
+#   else
+#     define SCO
+#   endif
+#   define mach_type_known
+# endif
+# if defined(_AUX_SOURCE)
+#   error A/UX no longer supported
+# endif
+# if defined(_PA_RISC1_0) || defined(_PA_RISC1_1) || defined(_PA_RISC2_0) \
+     || defined(hppa) || defined(__hppa__)
+#   define HP_PA
+#   if !defined(LINUX) && !defined(HPUX) && !defined(OPENBSD)
+#     define HPUX
+#   endif
+#   define mach_type_known
+# endif
+# if defined(__ia64) && (defined(_HPUX_SOURCE) || defined(__HP_aCC))
+#   define IA64
+#   ifndef HPUX
+#     define HPUX
+#   endif
+#   define mach_type_known
+# endif
+# if defined(__BEOS__) && defined(_X86_)
+#    define I386
+#    define BEOS
+#    define mach_type_known
+# endif
+# if defined(OPENBSD) && defined(__amd64__)
+#    define X86_64
+#    define mach_type_known
+# endif
+# if defined(LINUX) && (defined(i386) || defined(__i386__))
+#    define I386
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__x86_64__)
+#    define X86_64
+#    define mach_type_known
+# endif
+# if defined(LINUX) && (defined(__ia64__) || defined(__ia64))
+#    define IA64
+#    define mach_type_known
+# endif
+# if defined(LINUX) && (defined(__arm) || defined(__arm__))
+#    define ARM32
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__cris__)
+#    ifndef CRIS
+#       define CRIS
+#    endif
+#    define mach_type_known
+# endif
+# if defined(LINUX) && (defined(powerpc) || defined(__powerpc__) \
+                        || defined(powerpc64) || defined(__powerpc64__))
+#    define POWERPC
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__mc68000__)
+#    define M68K
+#    define mach_type_known
+# endif
+# if defined(LINUX) && (defined(sparc) || defined(__sparc__))
+#    define SPARC
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__sh__)
+#    define SH
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__avr32__)
+#    define AVR32
+#    define mach_type_known
+# endif
+# if defined(LINUX) && defined(__m32r__)
+#    define M32R
+#    define mach_type_known
+# endif
+# if defined(__alpha) || defined(__alpha__)
+#   define ALPHA
+#   if !defined(LINUX) && !defined(NETBSD) && !defined(OPENBSD) \
+       && !defined(FREEBSD)
+#     define OSF1       /* a.k.a Digital Unix */
+#   endif
+#   define mach_type_known
+# endif
+# if defined(_AMIGA) && !defined(AMIGA)
+#   define AMIGA
+# endif
+# ifdef AMIGA
+#   define M68K
+#   define mach_type_known
+# endif
+# if defined(THINK_C) || defined(__MWERKS__) && !defined(__powerc)
+#   define M68K
+#   define MACOS
+#   define mach_type_known
+# endif
+# if defined(__MWERKS__) && defined(__powerc) && !defined(__MACH__)
+#   define POWERPC
+#   define MACOS
+#   define mach_type_known
+# endif
+# if defined(__OpenBSD__) && defined(__powerpc__)
+#   define POWERPC
+#   define OPENBSD
+#   define mach_type_known
+# endif
+# if defined(DARWIN)
+#   if defined(__ppc__)  || defined(__ppc64__)
+#    define POWERPC
+#    define mach_type_known
+#   elif defined(__x86_64__) || defined(__x86_64)
+#    define X86_64
+#    define mach_type_known
+#   elif defined(__i386__)
+#    define I386
+#    define mach_type_known
+#   elif defined(__arm__)
+#    define ARM32
+#    define mach_type_known
+#    define DARWIN_DONT_PARSE_STACK
+#   endif
+# endif
+# if defined(__rtems__) && (defined(i386) || defined(__i386__))
+#   define I386
+#   define RTEMS
+#   define mach_type_known
+# endif
+# if defined(NeXT) && defined(mc68000)
+#   define M68K
+#   define NEXT
+#   define mach_type_known
+# endif
+# if defined(NeXT) && (defined(i386) || defined(__i386__))
+#   define I386
+#   define NEXT
+#   define mach_type_known
+# endif
+# if defined(__OpenBSD__) && (defined(i386) || defined(__i386__))
+#   define I386
+#   define OPENBSD
+#   define mach_type_known
+# endif
+# if defined(FREEBSD) && (defined(powerpc) || defined(__powerpc__))
+#    define POWERPC
+#    define mach_type_known
+# endif
+# if defined(FREEBSD) && (defined(i386) || defined(__i386__))
+#   define I386
+#   define mach_type_known
+# endif
+# if defined(FREEBSD) && (defined(__amd64__) || defined(__x86_64__))
+#   define X86_64
+#   define mach_type_known
+# endif
+# if defined(FREEBSD) && defined(__sparc__)
+#    define SPARC
+#    define mach_type_known
+# endif
+# if defined(FREEBSD) && defined(__ia64__)
+#   define IA64
+#   define mach_type_known
+# endif
+# if defined(FREEBSD) && defined(__arm__)
+#   define ARM32
+#   define mach_type_known
+# endif
+# if defined(__NetBSD__) && (defined(i386) || defined(__i386__))
+#   define I386
+#   define mach_type_known
+# endif
+# if defined(__NetBSD__) && defined(__x86_64__)
+#    define X86_64
+#    define mach_type_known
+# endif
+# if defined(bsdi) && (defined(i386) || defined(__i386__))
+#    define I386
+#    define BSDI
+#    define mach_type_known
+# endif
+# if !defined(mach_type_known) && defined(__386BSD__)
+#   define I386
+#   define THREE86BSD
+#   define mach_type_known
+# endif
+# if defined(_CX_UX) && defined(_M88K)
+#   define M88K
+#   define CX_UX
+#   define mach_type_known
+# endif
+# if defined(DGUX) && defined(m88k)
+#   define M88K
+    /* DGUX defined */
+#   define mach_type_known
+# endif
+# if defined(_WIN32_WCE) || defined(__CEGCC__) || defined(__MINGW32CE__)
+    /* SH3, SH4, MIPS already defined for corresponding architectures */
+#   if defined(SH3) || defined(SH4)
+#     define SH
+#   endif
+#   if defined(x86) || defined(__i386__)
+#     define I386
+#   endif
+#   if defined(_M_ARM) || defined(ARM) || defined(_ARM_)
+#     define ARM32
+#   endif
+#   define MSWINCE
+#   define mach_type_known
+# else
+#   if (defined(_MSDOS) || defined(_MSC_VER)) && (_M_IX86 >= 300) \
+        || defined(_WIN32) && !defined(__CYGWIN32__) && !defined(__CYGWIN__)
+#     if defined(__LP64__) || defined(_WIN64)
+#       define X86_64
+#     else
+#       define I386
+#     endif
+#     define MSWIN32    /* or Win64 */
+#     define mach_type_known
+#   endif
+#   if defined(_MSC_VER) && defined(_M_IA64)
+#     define IA64
+#     define MSWIN32    /* Really win64, but we don't treat 64-bit      */
+                        /* variants as a different platform.            */
+#   endif
+# endif
+# if defined(__DJGPP__)
+#   define I386
+#   ifndef DJGPP
+#     define DJGPP  /* MSDOS running the DJGPP port of GCC */
+#   endif
+#   define mach_type_known
+# endif
+# if defined(__CYGWIN32__) || defined(__CYGWIN__)
+#   define I386
+#   define CYGWIN32
+#   define mach_type_known
+# endif
+# if defined(__MINGW32__) && !defined(mach_type_known)
+#   define I386
+#   define MSWIN32
+#   define mach_type_known
+# endif
+# if defined(__BORLANDC__)
+#   define I386
+#   define MSWIN32
+#   define mach_type_known
+# endif
+# if defined(_UTS) && !defined(mach_type_known)
+#   define S370
+#   define UTS4
+#   define mach_type_known
+# endif
+# if defined(__pj__)
+#   error PicoJava no longer supported
+    /* The implementation had problems, and I haven't heard of users    */
+    /* in ages.  If you want it resurrected, let me know.               */
+# endif
+# if defined(__embedded__) && defined(PPC)
+#   define POWERPC
+#   define NOSYS
+#   define mach_type_known
+# endif
+/* Ivan Demakov */
+# if defined(__WATCOMC__) && defined(__386__)
+#   define I386
+#   if !defined(OS2) && !defined(MSWIN32) && !defined(DOS4GW)
+#     if defined(__OS2__)
+#       define OS2
+#     else
+#       if defined(__WINDOWS_386__) || defined(__NT__)
+#         define MSWIN32
+#       else
+#         define DOS4GW
+#       endif
+#     endif
+#   endif
+#   define mach_type_known
+# endif
+# if defined(__s390__) && defined(LINUX)
+#    define S390
+#    define mach_type_known
+# endif
+# if defined(__GNU__)
+#   if defined(__i386__)
+/* The Debian Hurd running on generic PC */
+#     define  HURD
+#     define  I386
+#     define  mach_type_known
+#    endif
+# endif
+# if defined(__TANDEM)
+    /* Nonstop S-series */
+    /* FIXME: Should recognize Integrity series? */
+#   define MIPS
+#   define NONSTOP
+#   define mach_type_known
+# endif
+# if defined(__hexagon__) && defined(LINUX)
+#    define HEXAGON
+#    define mach_type_known
+# endif
+
+# if defined(NAUT)
+#    define X86_64
+#    define mach_type_known
+# endif
+
+/* Feel free to add more clauses here */
+
+/* Or manually define the machine type here.  A machine type is         */
+/* characterized by the architecture.  Some                             */
+/* machine types are further subdivided by OS.                          */
+/* Macros such as LINUX, FREEBSD, etc. distinguish them.                */
+/* SYSV on an M68K actually means A/UX.                                 */
+/* The distinction in these cases is usually the stack starting address */
+# ifndef mach_type_known
+#   error "The collector has not been ported to this machine/OS combination."
+# endif
+                    /* Mapping is: M68K       ==> Motorola 680X0        */
+                    /*             (NEXT, and SYSV (A/UX),              */
+                    /*             MACOS and AMIGA variants)            */
+                    /*             I386       ==> Intel 386             */
+                    /*              (SEQUENT, OS2, SCO, LINUX, NETBSD,  */
+                    /*               FREEBSD, THREE86BSD, MSWIN32,      */
+                    /*               BSDI,SOLARIS, NEXT, other variants)        */
+                    /*             NS32K      ==> Encore Multimax       */
+                    /*             MIPS       ==> R2000 through R14K    */
+                    /*                  (many variants)                 */
+                    /*             VAX        ==> DEC VAX               */
+                    /*                  (BSD, ULTRIX variants)          */
+                    /*             HP_PA      ==> HP9000/700 & /800     */
+                    /*                            HP/UX, LINUX          */
+                    /*             SPARC      ==> SPARC v7/v8/v9        */
+                    /*                  (SOLARIS, LINUX, DRSNX variants)        */
+                    /*             ALPHA      ==> DEC Alpha             */
+                    /*                  (OSF1 and LINUX variants)       */
+                    /*             M88K       ==> Motorola 88XX0        */
+                    /*                  (CX_UX and DGUX)                */
+                    /*             S370       ==> 370-like machine      */
+                    /*                  running Amdahl UTS4             */
+                    /*             S390       ==> 390-like machine      */
+                    /*                  running LINUX                   */
+                    /*             ARM32      ==> Intel StrongARM       */
+                    /*             IA64       ==> Intel IPF             */
+                    /*                            (e.g. Itanium)        */
+                    /*                  (LINUX and HPUX)                */
+                    /*             SH         ==> Hitachi SuperH        */
+                    /*                  (LINUX & MSWINCE)               */
+                    /*             X86_64     ==> AMD x86-64            */
+                    /*             POWERPC    ==> IBM/Apple PowerPC     */
+                    /*                  (MACOS(<=9),DARWIN(incl.MACOSX),*/
+                    /*                   LINUX, NETBSD, AIX, NOSYS      */
+                    /*                   variants)                      */
+                    /*                  Handles 32 and 64-bit variants. */
+                    /*             CRIS       ==> Axis Etrax            */
+                    /*             M32R       ==> Renesas M32R          */
+                    /*             HEXAGON    ==> Qualcomm Hexagon      */
+
+
+/*
+ * For each architecture and OS, the following need to be defined:
+ *
+ * CPP_WORDSZ is a simple integer constant representing the word size.
+ * in bits.  We assume byte addressability, where a byte has 8 bits.
+ * We also assume CPP_WORDSZ is either 32 or 64.
+ * (We care about the length of pointers, not hardware
+ * bus widths.  Thus a 64 bit processor with a C compiler that uses
+ * 32 bit pointers should use CPP_WORDSZ of 32, not 64. Default is 32.)
+ *
+ * MACH_TYPE is a string representation of the machine type.
+ * OS_TYPE is analogous for the OS.
+ *
+ * ALIGNMENT is the largest N, such that
+ * all pointer are guaranteed to be aligned on N byte boundaries.
+ * defining it to be 1 will always work, but perform poorly.
+ *
+ * DATASTART is the beginning of the data segment.
+ * On some platforms SEARCH_FOR_DATA_START is defined.
+ * SEARCH_FOR_DATASTART will cause GC_data_start to
+ * be set to an address determined by accessing data backwards from _end
+ * until an unmapped page is found.  DATASTART will be defined to be
+ * GC_data_start.
+ * On UNIX-like systems, the collector will scan the area between DATASTART
+ * and DATAEND for root pointers.
+ *
+ * DATAEND, if not `end' where `end' is defined as ``extern int end[];''.
+ * RTH suggests gaining access to linker script synth'd values with
+ * this idiom instead of `&end' where `end' is defined as ``extern int end;'' .
+ * Otherwise, ``GCC will assume these are in .sdata/.sbss'' and it will, e.g.,
+ * cause failures on alpha*-*-* with ``-msmall-data or -fpic'' or mips-*-*
+ * without any special options.
+ *
+ * STACKBOTTOM is the cool end of the stack, which is usually the
+ * highest address in the stack.
+ * Under PCR or OS/2, we have other ways of finding thread stacks.
+ * For each machine, the following should:
+ * 1) define STACK_GROWS_UP if the stack grows toward higher addresses, and
+ * 2) define exactly one of
+ *      STACKBOTTOM (should be defined to be an expression)
+ *      LINUX_STACKBOTTOM
+ *      HEURISTIC1
+ *      HEURISTIC2
+ * If STACKBOTTOM is defined, then it's value will be used directly as the
+ * stack base.  If LINUX_STACKBOTTOM is defined, then it will be determined
+ * with a method appropriate for most Linux systems.  Currently we look
+ * first for __libc_stack_end (currently only if USE_LIBC_PRIVATES is
+ * defined), and if that fails read it from /proc.  (If USE_LIBC_PRIVATES
+ * is not defined and NO_PROC_STAT is defined, we revert to HEURISTIC2.)
+ * If either of the last two macros are defined, then STACKBOTTOM is computed
+ * during collector startup using one of the following two heuristics:
+ * HEURISTIC1:  Take an address inside GC_init's frame, and round it up to
+ *              the next multiple of STACK_GRAN.
+ * HEURISTIC2:  Take an address inside GC_init's frame, increment it repeatedly
+ *              in small steps (decrement if STACK_GROWS_UP), and read the value
+ *              at each location.  Remember the value when the first
+ *              Segmentation violation or Bus error is signaled.  Round that
+ *              to the nearest plausible page boundary, and use that instead
+ *              of STACKBOTTOM.
+ *
+ * Gustavo Rodriguez-Rivera points out that on most (all?) Unix machines,
+ * the value of environ is a pointer that can serve as STACKBOTTOM.
+ * I expect that HEURISTIC2 can be replaced by this approach, which
+ * interferes far less with debugging.  However it has the disadvantage
+ * that it's confused by a putenv call before the collector is initialized.
+ * This could be dealt with by intercepting putenv ...
+ *
+ * If no expression for STACKBOTTOM can be found, and neither of the above
+ * heuristics are usable, the collector can still be used with all of the above
+ * undefined, provided one of the following is done:
+ * 1) GC_mark_roots can be changed to somehow mark from the correct stack(s)
+ *    without reference to STACKBOTTOM.  This is appropriate for use in
+ *    conjunction with thread packages, since there will be multiple stacks.
+ *    (Allocating thread stacks in the heap, and treating them as ordinary
+ *    heap data objects is also possible as a last resort.  However, this is
+ *    likely to introduce significant amounts of excess storage retention
+ *    unless the dead parts of the thread stacks are periodically cleared.)
+ * 2) Client code may set GC_stackbottom before calling any GC_ routines.
+ *    If the author of the client code controls the main program, this is
+ *    easily accomplished by introducing a new main program, setting
+ *    GC_stackbottom to the address of a local variable, and then calling
+ *    the original main program.  The new main program would read something
+ *    like (provided real_main() is not inlined by the compiler):
+ *
+ *              # include "gc_private.h"
+ *
+ *              main(argc, argv, envp)
+ *              int argc;
+ *              char **argv, **envp;
+ *              {
+ *                  int dummy;
+ *
+ *                  GC_stackbottom = (ptr_t)(&dummy);
+ *                  return(real_main(argc, argv, envp));
+ *              }
+ *
+ *
+ * Each architecture may also define the style of virtual dirty bit
+ * implementation to be used:
+ *   MPROTECT_VDB: Write protect the heap and catch faults.
+ *   GWW_VDB: Use win32 GetWriteWatch primitive.
+ *   PROC_VDB: Use the SVR4 /proc primitives to read dirty bits.
+ *
+ * The first and second one may be combined, in which case a runtime
+ * selection will be made, based on GetWriteWatch availability.
+ *
+ * An architecture may define DYNAMIC_LOADING if dyn_load.c
+ * defined GC_register_dynamic_libraries() for the architecture.
+ *
+ * An architecture may define PREFETCH(x) to preload the cache with *x.
+ * This defaults to a no-op.
+ *
+ * PREFETCH_FOR_WRITE(x) is used if *x is about to be written.
+ *
+ * An architecture may also define CLEAR_DOUBLE(x) to be a fast way to
+ * clear the two words at GC_malloc-aligned address x.  By default,
+ * word stores of 0 are used instead.
+ *
+ * HEAP_START may be defined as the initial address hint for mmap-based
+ * allocation.
+ */
+
+/* If we are using a recent version of gcc, we can use                    */
+/* __builtin_unwind_init() to push the relevant registers onto the stack. */
+# if defined(__GNUC__) && ((__GNUC__ >= 3) \
+                           || (__GNUC__ == 2 && __GNUC_MINOR__ >= 8)) \
+     && !defined(__INTEL_COMPILER) && !defined(__PATHCC__) \
+     && !defined(__clang__) /* since no-op in clang (3.0) */
+#   define HAVE_BUILTIN_UNWIND_INIT
+# endif
+
+# define STACK_GRAN 0x1000000
+# ifdef M68K
+#   define MACH_TYPE "M68K"
+#   define ALIGNMENT 2
+#   ifdef OPENBSD
+        /* FIXME: Should we remove this case? */
+#       define OS_TYPE "OPENBSD"
+#       define HEURISTIC2
+#       ifdef __ELF__
+          extern ptr_t GC_data_start;
+#         define DATASTART GC_data_start
+#         define DYNAMIC_LOADING
+#       else
+          extern char etext[];
+#         define DATASTART ((ptr_t)(etext))
+#       endif
+#   endif
+#   ifdef NETBSD
+#       define OS_TYPE "NETBSD"
+#       define HEURISTIC2
+#       ifdef __ELF__
+          extern ptr_t GC_data_start;
+#         define DATASTART GC_data_start
+#         define DYNAMIC_LOADING
+#       else
+          extern char etext[];
+#         define DATASTART ((ptr_t)(etext))
+#       endif
+#   endif
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       define MPROTECT_VDB
+#       ifdef __ELF__
+#            define DYNAMIC_LOADING
+#            include <features.h>
+#            if defined(__GLIBC__) && __GLIBC__ >= 2
+#              define SEARCH_FOR_DATA_START
+#            else /* !GLIBC2 */
+#              ifdef PLATFORM_ANDROID
+#                define __environ environ
+#              endif
+               extern char **__environ;
+#              define DATASTART ((ptr_t)(&__environ))
+                             /* hideous kludge: __environ is the first */
+                             /* word in crt0.o, and delimits the start */
+                             /* of the data segment, no matter which   */
+                             /* ld options were passed through.        */
+                             /* We could use _etext instead, but that  */
+                             /* would include .rodata, which may       */
+                             /* contain large read-only data tables    */
+                             /* that we'd rather not scan.             */
+#            endif /* !GLIBC2 */
+             extern int _end[];
+#            define DATAEND (ptr_t)(_end)
+#       else
+             extern int etext[];
+#            define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#       endif
+#   endif
+#   ifdef AMIGA
+#       define OS_TYPE "AMIGA"
+                /* STACKBOTTOM and DATASTART handled specially  */
+                /* in os_dep.c                                  */
+#       define DATAEND  /* not needed */
+#       define GETPAGESIZE() 4096
+#   endif
+#   ifdef MACOS
+#     ifndef __LOWMEM__
+#     include <LowMem.h>
+#     endif
+#     define OS_TYPE "MACOS"
+                /* see os_dep.c for details of global data segments. */
+#     define STACKBOTTOM ((ptr_t) LMGetCurStackBase())
+#     define DATAEND    /* not needed */
+#     define GETPAGESIZE() 4096
+#   endif
+#   ifdef NEXT
+#       define OS_TYPE "NEXT"
+#       define DATASTART ((ptr_t) get_etext())
+#       define DATASTART_IS_FUNC
+#       define STACKBOTTOM ((ptr_t) 0x4000000)
+#       define DATAEND  /* not needed */
+#   endif
+# endif
+
+# if defined(POWERPC)
+#   define MACH_TYPE "POWERPC"
+#   ifdef MACOS
+#     define ALIGNMENT 2  /* Still necessary?  Could it be 4?   */
+#     ifndef __LOWMEM__
+#     include <LowMem.h>
+#     endif
+#     define OS_TYPE "MACOS"
+                        /* see os_dep.c for details of global data segments. */
+#     define STACKBOTTOM ((ptr_t) LMGetCurStackBase())
+#     define DATAEND  /* not needed */
+#   endif
+#   ifdef LINUX
+#     if defined(__powerpc64__)
+#       define ALIGNMENT 8
+#       define CPP_WORDSZ 64
+#       ifndef HBLKSIZE
+#         define HBLKSIZE 4096
+#       endif
+#     else
+#       define ALIGNMENT 4
+#     endif
+#     define OS_TYPE "LINUX"
+      /* HEURISTIC1 has been reliably reported to fail for a 32-bit     */
+      /* executable on a 64 bit kernel.                                 */
+#     define LINUX_STACKBOTTOM
+#     define DYNAMIC_LOADING
+#     define SEARCH_FOR_DATA_START
+      extern int _end[];
+#     define DATAEND (ptr_t)(_end)
+#   endif
+#   ifdef DARWIN
+#     define OS_TYPE "DARWIN"
+#     define DYNAMIC_LOADING
+#     if defined(__ppc64__)
+#       define ALIGNMENT 8
+#       define CPP_WORDSZ 64
+#       define STACKBOTTOM ((ptr_t) 0x7fff5fc00000)
+#       define CACHE_LINE_SIZE 64
+#       ifndef HBLKSIZE
+#         define HBLKSIZE 4096
+#       endif
+#     else
+#       define ALIGNMENT 4
+#       define STACKBOTTOM ((ptr_t) 0xc0000000)
+#     endif
+      /* XXX: see get_end(3), get_etext() and get_end() should not be used. */
+      /* These aren't used when dyld support is enabled (it is by default). */
+#     define DATASTART ((ptr_t) get_etext())
+#     define DATAEND   ((ptr_t) get_end())
+#     ifndef USE_MMAP
+#       define USE_MMAP
+#     endif
+#     define USE_MMAP_ANON
+#     define MPROTECT_VDB
+#     include <unistd.h>
+#     define GETPAGESIZE() getpagesize()
+#     if defined(USE_PPC_PREFETCH) && defined(__GNUC__)
+        /* The performance impact of prefetches is untested */
+#       define PREFETCH(x) \
+          __asm__ __volatile__ ("dcbt 0,%0" : : "r" ((const void *) (x)))
+#       define PREFETCH_FOR_WRITE(x) \
+          __asm__ __volatile__ ("dcbtst 0,%0" : : "r" ((const void *) (x)))
+#     endif
+      /* There seems to be some issues with trylock hanging on darwin.  */
+      /* This should be looked into some more.                          */
+#     define NO_PTHREAD_TRYLOCK
+#   endif
+#   ifdef OPENBSD
+#     define OS_TYPE "OPENBSD"
+#     define ALIGNMENT 4
+#     ifdef GC_OPENBSD_THREADS
+#      define UTHREAD_SP_OFFSET 268
+#     else
+#       include <sys/param.h>
+#       include <uvm/uvm_extern.h>
+#       define STACKBOTTOM USRSTACK
+#     endif
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern char _end[];
+#     define DATAEND ((ptr_t)(&_end))
+#     define DYNAMIC_LOADING
+#   endif
+#   ifdef FREEBSD
+#       if defined(__powerpc64__)
+#           define ALIGNMENT 8
+#           define CPP_WORDSZ 64
+#           ifndef HBLKSIZE
+#               define HBLKSIZE 4096
+#           endif
+#       else
+#           define ALIGNMENT 4
+#       endif
+#       define OS_TYPE "FREEBSD"
+#       ifndef GC_FREEBSD_THREADS
+#           define MPROTECT_VDB
+#       endif
+#       define SIG_SUSPEND SIGUSR1
+#       define SIG_THR_RESTART SIGUSR2
+#       define FREEBSD_STACKBOTTOM
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+        extern char etext[];
+        ptr_t GC_FreeBSDGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_FreeBSDGetDataStart(0x1000, (ptr_t)etext)
+#       define DATASTART_IS_FUNC
+#   endif
+#   ifdef NETBSD
+#     define ALIGNMENT 4
+#     define OS_TYPE "NETBSD"
+#     define HEURISTIC2
+      extern ptr_t GC_data_start;
+#     define DATASTART GC_data_start
+#     define DYNAMIC_LOADING
+#   endif
+#   ifdef SN_TARGET_PS3
+#     define NO_GETENV
+#     define CPP_WORDSZ 32
+#     define ALIGNMENT 4
+      extern int _end [];
+      extern int __bss_start;
+#     define DATAEND (ptr_t)(_end)
+#     define DATASTART (ptr_t)(__bss_start)
+#     define STACKBOTTOM ((ptr_t)ps3_get_stack_bottom())
+#   endif
+#   ifdef AIX
+#     define OS_TYPE "AIX"
+#     undef ALIGNMENT /* in case it's defined   */
+#     undef IA64
+      /* DOB: some AIX installs stupidly define IA64 in */
+      /* /usr/include/sys/systemcfg.h                   */
+#     ifdef __64BIT__
+#       define ALIGNMENT 8
+#       define CPP_WORDSZ 64
+#       define STACKBOTTOM ((ptr_t)0x1000000000000000)
+#     else
+#       define ALIGNMENT 4
+#       define CPP_WORDSZ 32
+#       define STACKBOTTOM ((ptr_t)((ulong)&errno))
+#     endif
+#     ifndef USE_MMAP
+#       define USE_MMAP
+#     endif
+#     define USE_MMAP_ANON
+        /* From AIX linker man page:
+        _text Specifies the first location of the program.
+        _etext Specifies the first location after the program.
+        _data Specifies the first location of the data.
+        _edata Specifies the first location after the initialized data
+        _end or end Specifies the first location after all data.
+        */
+      extern int _data[], _end[];
+#     define DATASTART ((ptr_t)((ulong)_data))
+#     define DATAEND ((ptr_t)((ulong)_end))
+      extern int errno;
+#     define DYNAMIC_LOADING
+        /* For really old versions of AIX, this may have to be removed. */
+#   endif
+
+#   ifdef NOSYS
+#     define ALIGNMENT 4
+#     define OS_TYPE "NOSYS"
+      extern void __end[], __dso_handle[];
+#     define DATASTART (__dso_handle)  /* OK, that's ugly.  */
+#     define DATAEND (ptr_t)(__end)
+        /* Stack starts at 0xE0000000 for the simulator.  */
+#     undef STACK_GRAN
+#     define STACK_GRAN 0x10000000
+#     define HEURISTIC1
+#   endif
+# endif
+
+# ifdef VAX
+#   define MACH_TYPE "VAX"
+#   define ALIGNMENT 4  /* Pointers are longword aligned by 4.2 C compiler */
+    extern char etext[];
+#   define DATASTART ((ptr_t)(etext))
+#   ifdef BSD
+#       define OS_TYPE "BSD"
+#       define HEURISTIC1
+                        /* HEURISTIC2 may be OK, but it's hard to test. */
+#   endif
+#   ifdef ULTRIX
+#       define OS_TYPE "ULTRIX"
+#       define STACKBOTTOM ((ptr_t) 0x7fffc800)
+#   endif
+# endif
+
+# ifdef SPARC
+#   define MACH_TYPE "SPARC"
+#   if defined(__arch64__) || defined(__sparcv9)
+#     define ALIGNMENT 8
+#     define CPP_WORDSZ 64
+#     define ELF_CLASS ELFCLASS64
+#   else
+#     define ALIGNMENT 4        /* Required by hardware */
+#     define CPP_WORDSZ 32
+#   endif
+    /* Don't define USE_ASM_PUSH_REGS.  We do use an asm helper, but    */
+    /* not to push the registers on the mark stack.                     */
+#   ifdef SOLARIS
+#       define OS_TYPE "SOLARIS"
+        extern int _etext[];
+        extern int _end[];
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x10000, (ptr_t)_etext)
+#       define DATASTART_IS_FUNC
+#       define DATAEND (ptr_t)(_end)
+#       if !defined(USE_MMAP) && defined(REDIRECT_MALLOC)
+#         define USE_MMAP
+            /* Otherwise we now use calloc.  Mmap may result in the     */
+            /* heap interleaved with thread stacks, which can result in */
+            /* excessive blacklisting.  Sbrk is unusable since it       */
+            /* doesn't interact correctly with the system malloc.       */
+#       endif
+#       ifdef USE_MMAP
+#         define HEAP_START (ptr_t)0x40000000
+#       else
+#         define HEAP_START DATAEND
+#       endif
+#       define PROC_VDB
+/*      HEURISTIC1 reportedly no longer works under 2.7.                */
+/*      HEURISTIC2 probably works, but this appears to be preferable.   */
+/*      Apparently USRSTACK is defined to be USERLIMIT, but in some     */
+/*      installations that's undefined.  We work around this with a     */
+/*      gross hack:                                                     */
+#       include <sys/vmparam.h>
+#       ifdef USERLIMIT
+          /* This should work everywhere, but doesn't.  */
+#         define STACKBOTTOM ((ptr_t) USRSTACK)
+#       else
+#         define HEURISTIC2
+#       endif
+#       include <unistd.h>
+#       define GETPAGESIZE()  sysconf(_SC_PAGESIZE)
+                /* getpagesize() appeared to be missing from at least one */
+                /* Solaris 5.4 installation.  Weird.                      */
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef DRSNX
+#       define OS_TYPE "DRSNX"
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+        extern int etext[];
+#       define DATASTART GC_SysVGetDataStart(0x10000, (ptr_t)etext)
+#       define DATASTART_IS_FUNC
+#       define MPROTECT_VDB
+#       define STACKBOTTOM ((ptr_t) 0xdfff0000)
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     ifdef __ELF__
+#       define DYNAMIC_LOADING
+#     else
+          Linux Sparc/a.out not supported
+#     endif
+      extern int _end[];
+      extern int _etext[];
+#     define DATAEND (ptr_t)(_end)
+#     define SVR4
+      ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#     ifdef __arch64__
+#       define DATASTART GC_SysVGetDataStart(0x100000, (ptr_t)_etext)
+#     else
+#       define DATASTART GC_SysVGetDataStart(0x10000, (ptr_t)_etext)
+#     endif
+#     define DATASTART_IS_FUNC
+#     define LINUX_STACKBOTTOM
+#   endif
+#   ifdef OPENBSD
+#     define OS_TYPE "OPENBSD"
+#     ifdef GC_OPENBSD_THREADS
+#      define UTHREAD_SP_OFFSET 232
+#     else
+#       include <sys/param.h>
+#       include <uvm/uvm_extern.h>
+#       define STACKBOTTOM USRSTACK
+#     endif
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern char _end[];
+#     define DATAEND ((ptr_t)(&_end))
+#     define DYNAMIC_LOADING
+#   endif
+#   ifdef NETBSD
+#     define OS_TYPE "NETBSD"
+#     define HEURISTIC2
+#     ifdef __ELF__
+        extern ptr_t GC_data_start;
+#       define DATASTART GC_data_start
+#       define DYNAMIC_LOADING
+#     else
+        extern char etext[];
+#       define DATASTART ((ptr_t)(etext))
+#     endif
+#   endif
+#   ifdef FREEBSD
+#       define OS_TYPE "FREEBSD"
+#       define SIG_SUSPEND SIGUSR1
+#       define SIG_THR_RESTART SIGUSR2
+#       define FREEBSD_STACKBOTTOM
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+        extern char etext[];
+        extern char edata[];
+        extern char end[];
+#       define NEED_FIND_LIMIT
+#       define DATASTART ((ptr_t)(&etext))
+        ptr_t GC_find_limit(ptr_t, GC_bool);
+#       define DATAEND (GC_find_limit (DATASTART, TRUE))
+#       define DATAEND_IS_FUNC
+#       define DATASTART2 ((ptr_t)(&edata))
+#       define DATAEND2 ((ptr_t)(&end))
+#   endif
+# endif
+
+# ifdef I386
+#   define MACH_TYPE "I386"
+#   if defined(__LP64__) || defined(_WIN64)
+#     error This should be handled as X86_64
+#   else
+#     define CPP_WORDSZ 32
+#     define ALIGNMENT 4
+                        /* Appears to hold for all "32 bit" compilers   */
+                        /* except Borland.  The -a4 option fixes        */
+                        /* Borland.                                     */
+                        /* Ivan Demakov: For Watcom the option is -zp4. */
+#   endif
+#   ifdef SEQUENT
+#       define OS_TYPE "SEQUENT"
+        extern int etext[];
+#       define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#       define STACKBOTTOM ((ptr_t) 0x3ffff000)
+#   endif
+#   ifdef BEOS
+#     define OS_TYPE "BEOS"
+#     include <OS.h>
+#     define GETPAGESIZE() B_PAGE_SIZE
+      extern int etext[];
+#     define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#   endif
+#   ifdef SOLARIS
+#       define OS_TYPE "SOLARIS"
+        extern int _etext[], _end[];
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x1000, (ptr_t)_etext)
+#       define DATASTART_IS_FUNC
+#       define DATAEND (ptr_t)(_end)
+/*      # define STACKBOTTOM ((ptr_t)(_start)) worked through 2.7,      */
+/*      but reportedly breaks under 2.8.  It appears that the stack     */
+/*      base is a property of the executable, so this should not break  */
+/*      old executables.                                                */
+/*      HEURISTIC2 probably works, but this appears to be preferable.   */
+#       include <sys/vm.h>
+#       define STACKBOTTOM ((ptr_t) USRSTACK)
+/* At least in Solaris 2.5, PROC_VDB gives wrong values for dirty bits. */
+/* It appears to be fixed in 2.8 and 2.9.                               */
+#       ifdef SOLARIS25_PROC_VDB_BUG_FIXED
+#         define PROC_VDB
+#       endif
+#       ifndef GC_THREADS
+#         define MPROTECT_VDB
+#       endif
+#       define DYNAMIC_LOADING
+#       if !defined(USE_MMAP) && defined(REDIRECT_MALLOC)
+#         define USE_MMAP
+            /* Otherwise we now use calloc.  Mmap may result in the     */
+            /* heap interleaved with thread stacks, which can result in */
+            /* excessive blacklisting.  Sbrk is unusable since it       */
+            /* doesn't interact correctly with the system malloc.       */
+#       endif
+#       ifdef USE_MMAP
+#         define HEAP_START (ptr_t)0x40000000
+#       else
+#         define HEAP_START DATAEND
+#       endif
+#   endif
+#   ifdef SCO
+#       define OS_TYPE "SCO"
+        extern int etext[];
+#       define DATASTART ((ptr_t)((((word) (etext)) + 0x3fffff) \
+                                  & ~0x3fffff) \
+                                 +((word)etext & 0xfff))
+#       define STACKBOTTOM ((ptr_t) 0x7ffffffc)
+#   endif
+#   ifdef SCO_ELF
+#       define OS_TYPE "SCO_ELF"
+        extern int etext[];
+#       define DATASTART ((ptr_t)(etext))
+#       define STACKBOTTOM ((ptr_t) 0x08048000)
+#       define DYNAMIC_LOADING
+#       define ELF_CLASS ELFCLASS32
+#   endif
+#   ifdef DGUX
+#       define OS_TYPE "DGUX"
+        extern int _etext, _end;
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x1000, (ptr_t)(&_etext))
+#       define DATASTART_IS_FUNC
+#       define DATAEND (ptr_t)(&_end)
+#       define STACK_GROWS_DOWN
+#       define HEURISTIC2
+#       include <unistd.h>
+#       define GETPAGESIZE()  sysconf(_SC_PAGESIZE)
+#       define DYNAMIC_LOADING
+#       ifndef USE_MMAP
+#         define USE_MMAP
+#       endif
+#       define MAP_FAILED (void *) ((word)-1)
+#       ifdef USE_MMAP
+#         define HEAP_START (ptr_t)0x40000000
+#       else
+#         define HEAP_START DATAEND
+#       endif
+#   endif /* DGUX */
+
+#   ifdef NACL
+#      define OS_TYPE "NACL"
+       extern int etext[];
+#      define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+       extern int _end[];
+#      define DATAEND (_end)
+#      undef STACK_GRAN
+#      define STACK_GRAN 0x10000
+#      define HEURISTIC1
+#      define GETPAGESIZE() 65536
+#      ifndef MAX_NACL_GC_THREADS
+#        define MAX_NACL_GC_THREADS 1024
+#      endif
+#   endif /* NACL */
+
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       if 0
+#         define HEURISTIC1
+#         undef STACK_GRAN
+#         define STACK_GRAN 0x10000000
+          /* STACKBOTTOM is usually 0xc0000000, but this changes with   */
+          /* different kernel configurations.  In particular, systems   */
+          /* with 2GB physical memory will usually move the user        */
+          /* address space limit, and hence initial SP to 0x80000000.   */
+#       endif
+#       if !defined(GC_LINUX_THREADS) || !defined(REDIRECT_MALLOC)
+#           define MPROTECT_VDB
+#       else
+            /* We seem to get random errors in incremental mode,        */
+            /* possibly because Linux threads is itself a malloc client */
+            /* and can't deal with the signals.                         */
+#       endif
+#       define HEAP_START (ptr_t)0x1000
+                /* This encourages mmap to give us low addresses,       */
+                /* thus allowing the heap to grow to ~3GB               */
+#       ifdef __ELF__
+#            define DYNAMIC_LOADING
+#            ifdef UNDEFINED    /* includes ro data */
+               extern int _etext[];
+#              define DATASTART ((ptr_t)((((word) (_etext)) + 0xfff) & ~0xfff))
+#            endif
+#            include <features.h>
+#            if defined(__GLIBC__) && __GLIBC__ >= 2 \
+                || defined(PLATFORM_ANDROID)
+#                define SEARCH_FOR_DATA_START
+#            else
+                 extern char **__environ;
+#                define DATASTART ((ptr_t)(&__environ))
+                              /* hideous kludge: __environ is the first */
+                              /* word in crt0.o, and delimits the start */
+                              /* of the data segment, no matter which   */
+                              /* ld options were passed through.        */
+                              /* We could use _etext instead, but that  */
+                              /* would include .rodata, which may       */
+                              /* contain large read-only data tables    */
+                              /* that we'd rather not scan.             */
+#            endif
+             extern int _end[];
+#            define DATAEND (ptr_t)(_end)
+#       else
+             extern int etext[];
+#            define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#       endif
+#       ifdef USE_I686_PREFETCH
+          /* FIXME: Thus should use __builtin_prefetch, but we'll leave that */
+          /* for the next rtelease.                                          */
+#         define PREFETCH(x) \
+            __asm__ __volatile__ ("prefetchnta %0" : : "m"(*(char *)(x)))
+            /* Empirically prefetcht0 is much more effective at reducing     */
+            /* cache miss stalls for the targeted load instructions.  But it */
+            /* seems to interfere enough with other cache traffic that the   */
+            /* net result is worse than prefetchnta.                         */
+#         ifdef FORCE_WRITE_PREFETCH
+            /* Using prefetches for write seems to have a slight negative    */
+            /* impact on performance, at least for a PIII/500.               */
+#           define PREFETCH_FOR_WRITE(x) \
+              __asm__ __volatile__ ("prefetcht0 %0" : : "m"(*(char *)(x)))
+#         endif
+#       endif
+#       ifdef USE_3DNOW_PREFETCH
+#         define PREFETCH(x) \
+            __asm__ __volatile__ ("prefetch %0" : : "m"(*(char *)(x)))
+#         define PREFETCH_FOR_WRITE(x) \
+            __asm__ __volatile__ ("prefetchw %0" : : "m"(*(char *)(x)))
+#       endif
+#   endif
+#   ifdef CYGWIN32
+#       define OS_TYPE "CYGWIN32"
+#       define DATASTART ((ptr_t)GC_DATASTART)  /* From gc.h */
+#       define DATAEND   ((ptr_t)GC_DATAEND)
+#       undef STACK_GRAN
+#       define STACK_GRAN 0x10000
+#       ifdef USE_MMAP
+#         define NEED_FIND_LIMIT
+#         define USE_MMAP_ANON
+#       endif
+#   endif
+#   ifdef OS2
+#       define OS_TYPE "OS2"
+                /* STACKBOTTOM and DATASTART are handled specially in   */
+                /* os_dep.c. OS2 actually has the right                 */
+                /* system call!                                         */
+#       define DATAEND  /* not needed */
+#   endif
+#   ifdef MSWIN32
+#       define OS_TYPE "MSWIN32"
+                /* STACKBOTTOM and DATASTART are handled specially in   */
+                /* os_dep.c.                                            */
+#       define MPROTECT_VDB
+#       define GWW_VDB
+#       define DATAEND  /* not needed */
+#   endif
+#   ifdef MSWINCE
+#       define OS_TYPE "MSWINCE"
+#       define DATAEND  /* not needed */
+#   endif
+#   ifdef DJGPP
+#       define OS_TYPE "DJGPP"
+#       include "stubinfo.h"
+        extern int etext[];
+        extern int _stklen;
+        extern int __djgpp_stack_limit;
+#       define DATASTART ((ptr_t)((((word) (etext)) + 0x1ff) & ~0x1ff))
+/* #define STACKBOTTOM ((ptr_t)((word)_stubinfo+_stubinfo->size+_stklen)) */
+#       define STACKBOTTOM ((ptr_t)((word) __djgpp_stack_limit + _stklen))
+                /* This may not be right.  */
+#   endif
+#   ifdef OPENBSD
+#       define OS_TYPE "OPENBSD"
+#       ifdef GC_OPENBSD_THREADS
+#         define UTHREAD_SP_OFFSET 176
+#       else
+#         include <sys/param.h>
+#         include <uvm/uvm_extern.h>
+#         define STACKBOTTOM USRSTACK
+#       endif
+        extern int __data_start[];
+#       define DATASTART ((ptr_t)__data_start)
+        extern char _end[];
+#       define DATAEND ((ptr_t)(&_end))
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef FREEBSD
+#       define OS_TYPE "FREEBSD"
+#       ifndef GC_FREEBSD_THREADS
+#           define MPROTECT_VDB
+#       endif
+#       ifdef __GLIBC__
+#           define SIG_SUSPEND          (32+6)
+#           define SIG_THR_RESTART      (32+5)
+            extern int _end[];
+#           define DATAEND (ptr_t)(_end)
+#       else
+#           define SIG_SUSPEND SIGUSR1
+#           define SIG_THR_RESTART SIGUSR2
+#       endif
+#       define FREEBSD_STACKBOTTOM
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+        extern char etext[];
+        char * GC_FreeBSDGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_FreeBSDGetDataStart(0x1000, (ptr_t)etext)
+#       define DATASTART_IS_FUNC
+#   endif
+#   ifdef NETBSD
+#       define OS_TYPE "NETBSD"
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+#   endif
+#   ifdef THREE86BSD
+#       define OS_TYPE "THREE86BSD"
+#   endif
+#   ifdef BSDI
+#       define OS_TYPE "BSDI"
+#   endif
+#   if defined(NETBSD) || defined(THREE86BSD) || defined(BSDI)
+#       define HEURISTIC2
+        extern char etext[];
+#       define DATASTART ((ptr_t)(etext))
+#   endif
+#   ifdef NEXT
+#       define OS_TYPE "NEXT"
+#       define DATASTART ((ptr_t) get_etext())
+#       define DATASTART_IS_FUNC
+#       define STACKBOTTOM ((ptr_t)0xc0000000)
+#       define DATAEND  /* not needed */
+#   endif
+#   ifdef RTEMS
+#       define OS_TYPE "RTEMS"
+#       include <sys/unistd.h>
+        extern int etext[];
+        extern int end[];
+        void *rtems_get_stack_bottom(void);
+#       define InitStackBottom rtems_get_stack_bottom()
+#       define DATASTART ((ptr_t)etext)
+#       define DATAEND ((ptr_t)end)
+#       define STACKBOTTOM ((ptr_t)InitStackBottom)
+#       define SIG_SUSPEND SIGUSR1
+#       define SIG_THR_RESTART SIGUSR2
+#   endif
+#   ifdef DOS4GW
+#     define OS_TYPE "DOS4GW"
+      extern long __nullarea;
+      extern char _end;
+      extern char *_STACKTOP;
+      /* Depending on calling conventions Watcom C either precedes      */
+      /* or does not precedes with underscore names of C-variables.     */
+      /* Make sure startup code variables always have the same names.   */
+      #pragma aux __nullarea "*";
+      #pragma aux _end "*";
+#     define STACKBOTTOM ((ptr_t) _STACKTOP)
+                         /* confused? me too. */
+#     define DATASTART ((ptr_t) &__nullarea)
+#     define DATAEND ((ptr_t) &_end)
+#   endif
+#   ifdef HURD
+#     define OS_TYPE "HURD"
+#     define STACK_GROWS_DOWN
+#     define HEURISTIC2
+#     define SIG_SUSPEND SIGUSR1
+#     define SIG_THR_RESTART SIGUSR2
+#     define SEARCH_FOR_DATA_START
+      extern int _end[];
+#     define DATAEND ((ptr_t) (_end))
+/* #     define MPROTECT_VDB  Not quite working yet? */
+#     define DYNAMIC_LOADING
+#   endif
+#   ifdef DARWIN
+#     define OS_TYPE "DARWIN"
+#     define DARWIN_DONT_PARSE_STACK
+#     define DYNAMIC_LOADING
+      /* XXX: see get_end(3), get_etext() and get_end() should not be used. */
+      /* These aren't used when dyld support is enabled (it is by default). */
+#     define DATASTART ((ptr_t) get_etext())
+#     define DATAEND   ((ptr_t) get_end())
+#     define STACKBOTTOM ((ptr_t) 0xc0000000)
+#     ifndef USE_MMAP
+#       define USE_MMAP
+#     endif
+#     define USE_MMAP_ANON
+#     define MPROTECT_VDB
+#     include <unistd.h>
+#     define GETPAGESIZE() getpagesize()
+      /* There seems to be some issues with trylock hanging on darwin.  */
+      /* This should be looked into some more.                          */
+#     define NO_PTHREAD_TRYLOCK
+#   endif /* DARWIN */
+# endif
+
+# ifdef NS32K
+#   define MACH_TYPE "NS32K"
+#   define ALIGNMENT 4
+    extern char **environ;
+#   define DATASTART ((ptr_t)(&environ))
+                              /* hideous kludge: environ is the first   */
+                              /* word in crt0.o, and delimits the start */
+                              /* of the data segment, no matter which   */
+                              /* ld options were passed through.        */
+#   define STACKBOTTOM ((ptr_t) 0xfffff000) /* for Encore */
+# endif
+
+# ifdef MIPS
+#   define MACH_TYPE "MIPS"
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     define DYNAMIC_LOADING
+      extern int _end[];
+#     define DATAEND (ptr_t)(_end)
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)(__data_start))
+#     ifdef _MIPS_SZPTR
+#       define CPP_WORDSZ _MIPS_SZPTR
+#       define ALIGNMENT (_MIPS_SZPTR/8)
+#     else
+#       define ALIGNMENT 4
+#     endif
+#     ifndef HBLKSIZE
+#       define HBLKSIZE 4096
+#     endif
+#     if __GLIBC__ == 2 && __GLIBC_MINOR__ >= 2 || __GLIBC__ > 2
+#       define LINUX_STACKBOTTOM
+#     else
+#       define STACKBOTTOM ((ptr_t)0x7fff8000)
+#     endif
+#   endif /* Linux */
+#   ifdef EWS4800
+#      define HEURISTIC2
+#      if defined(_MIPS_SZPTR) && (_MIPS_SZPTR == 64)
+         extern int _fdata[], _end[];
+#        define DATASTART ((ptr_t)_fdata)
+#        define DATAEND ((ptr_t)_end)
+#        define CPP_WORDSZ _MIPS_SZPTR
+#        define ALIGNMENT (_MIPS_SZPTR/8)
+#      else
+         extern int etext[], edata[], end[];
+         extern int _DYNAMIC_LINKING[], _gp[];
+#        define DATASTART ((ptr_t)((((word)etext + 0x3ffff) & ~0x3ffff) \
+               + ((word)etext & 0xffff)))
+#        define DATAEND (ptr_t)(edata)
+#        define DATASTART2 (_DYNAMIC_LINKING \
+               ? (ptr_t)(((word)_gp + 0x8000 + 0x3ffff) & ~0x3ffff) \
+               : (ptr_t)edata)
+#        define DATAEND2 (ptr_t)(end)
+#        define ALIGNMENT 4
+#      endif
+#      define OS_TYPE "EWS4800"
+#   endif
+#   ifdef ULTRIX
+#       define HEURISTIC2
+#       define DATASTART (ptr_t)0x10000000
+                              /* Could probably be slightly higher since */
+                              /* startup code allocates lots of stuff.   */
+#       define OS_TYPE "ULTRIX"
+#       define ALIGNMENT 4
+#   endif
+#   ifdef IRIX5
+#       define HEURISTIC2
+        extern int _fdata[];
+#       define DATASTART ((ptr_t)(_fdata))
+#       ifdef USE_MMAP
+#         define HEAP_START (ptr_t)0x30000000
+#       else
+#         define HEAP_START DATASTART
+#       endif
+                              /* Lowest plausible heap address.         */
+                              /* In the MMAP case, we map there.        */
+                              /* In either case it is used to identify  */
+                              /* heap sections so they're not           */
+                              /* considered as roots.                   */
+#       define OS_TYPE "IRIX5"
+/*#       define MPROTECT_VDB DOB: this should work, but there is evidence */
+/*              of recent breakage.                                        */
+#       ifdef _MIPS_SZPTR
+#         define CPP_WORDSZ _MIPS_SZPTR
+#         define ALIGNMENT (_MIPS_SZPTR/8)
+#       else
+#         define ALIGNMENT 4
+#       endif
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef MSWINCE
+#       define OS_TYPE "MSWINCE"
+#       define ALIGNMENT 4
+#       define DATAEND /* not needed */
+#   endif
+#   if defined(NETBSD)
+#     define OS_TYPE "NETBSD"
+#     define ALIGNMENT 4
+#     define HEURISTIC2
+#     ifdef __ELF__
+        extern ptr_t GC_data_start;
+#       define DATASTART GC_data_start
+#       define NEED_FIND_LIMIT
+#       define DYNAMIC_LOADING
+#     else
+#       define DATASTART ((ptr_t) 0x10000000)
+#       define STACKBOTTOM ((ptr_t) 0x7ffff000)
+#     endif /* _ELF_ */
+#  endif
+#  ifdef OPENBSD
+#    define OS_TYPE "OPENBSD"
+#    define ALIGNMENT 4
+#    ifdef GC_OPENBSD_THREADS
+#      define UTHREAD_SP_OFFSET 808
+#    else
+#      include <sys/param.h>
+#      include <uvm/uvm_extern.h>
+#      define STACKBOTTOM USRSTACK
+#    endif
+     extern int _fdata[];
+#    define DATASTART ((ptr_t)_fdata)
+     extern char _end[];
+#    define DATAEND ((ptr_t)(&_end))
+#    define DYNAMIC_LOADING
+#  endif
+#  if defined(NONSTOP)
+#    define CPP_WORDSZ 32
+#    define OS_TYPE "NONSTOP"
+#    define ALIGNMENT 4
+#    define DATASTART ((ptr_t) 0x08000000)
+     extern char **environ;
+#    define DATAEND ((ptr_t)(environ - 0x10))
+#    define STACKBOTTOM ((ptr_t) 0x4fffffff)
+#   endif
+# endif
+
+# ifdef HP_PA
+#   define MACH_TYPE "HP_PA"
+#   ifdef __LP64__
+#     define CPP_WORDSZ 64
+#     define ALIGNMENT 8
+#   else
+#     define CPP_WORDSZ 32
+#     define ALIGNMENT 4
+#   endif
+#   if !defined(GC_HPUX_THREADS) && !defined(GC_LINUX_THREADS) \
+       && !defined(OPENBSD) && !defined(LINUX) /* For now. */
+#     define MPROTECT_VDB
+#   endif
+#   define STACK_GROWS_UP
+#   ifdef HPUX
+#     define OS_TYPE "HPUX"
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)(__data_start))
+#     ifdef USE_HPUX_FIXED_STACKBOTTOM
+        /* The following appears to work for 7xx systems running HP/UX  */
+        /* 9.xx Furthermore, it might result in much faster             */
+        /* collections than HEURISTIC2, which may involve scanning      */
+        /* segments that directly precede the stack.  It is not the     */
+        /* default, since it may not work on older machine/OS           */
+        /* combinations. (Thanks to Raymond X.T. Nijssen for uncovering */
+        /* this.)                                                       */
+#       define STACKBOTTOM ((ptr_t) 0x7b033000)  /* from /etc/conf/h/param.h */
+#     else
+        /* Gustavo Rodriguez-Rivera suggested changing HEURISTIC2       */
+        /* to this.  Note that the GC must be initialized before the    */
+        /* first putenv call.                                           */
+        extern char ** environ;
+#       define STACKBOTTOM ((ptr_t)environ)
+#     endif
+#     define DYNAMIC_LOADING
+#     include <unistd.h>
+#     define GETPAGESIZE() sysconf(_SC_PAGE_SIZE)
+#     ifndef __GNUC__
+#       define PREFETCH(x)  { \
+                              register long addr = (long)(x); \
+                              (void) _asm ("LDW", 0, 0, addr, 0); \
+                            }
+#     endif
+#   endif /* HPUX */
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     define LINUX_STACKBOTTOM
+#     define DYNAMIC_LOADING
+#     define SEARCH_FOR_DATA_START
+      extern int _end[];
+#     define DATAEND (ptr_t)(&_end)
+#   endif /* LINUX */
+#  ifdef OPENBSD
+#     define OS_TYPE "OPENBSD"
+#     ifdef GC_OPENBSD_THREADS
+#       define UTHREAD_SP_OFFSET 520
+#     else
+#       include <sys/param.h>
+#       include <uvm/uvm_extern.h>
+#       define STACKBOTTOM USRSTACK
+#     endif
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern char _end[];
+#     define DATAEND ((ptr_t)(&_end))
+#     define DYNAMIC_LOADING
+#  endif
+# endif /* HP_PA */
+
+# ifdef ALPHA
+#   define MACH_TYPE "ALPHA"
+#   define ALIGNMENT 8
+#   define CPP_WORDSZ 64
+#   ifdef NETBSD
+#       define OS_TYPE "NETBSD"
+#       define HEURISTIC2
+        extern ptr_t GC_data_start;
+#       define DATASTART GC_data_start
+#       define ELFCLASS32 32
+#       define ELFCLASS64 64
+#       define ELF_CLASS ELFCLASS64
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef OPENBSD
+#       define OS_TYPE "OPENBSD"
+#       define ELF_CLASS ELFCLASS64
+#       ifdef GC_OPENBSD_THREADS
+#         define UTHREAD_SP_OFFSET 816
+#       else
+#         include <sys/param.h>
+#         include <uvm/uvm_extern.h>
+#         define STACKBOTTOM USRSTACK
+#       endif
+        extern int __data_start[];
+#       define DATASTART ((ptr_t)__data_start)
+        extern char _end[];
+#       define DATAEND ((ptr_t)(&_end))
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef FREEBSD
+#       define OS_TYPE "FREEBSD"
+/* MPROTECT_VDB is not yet supported at all on FreeBSD/alpha. */
+#       define SIG_SUSPEND SIGUSR1
+#       define SIG_THR_RESTART SIGUSR2
+#       define FREEBSD_STACKBOTTOM
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+/* Handle unmapped hole alpha*-*-freebsd[45]* puts between etext and edata. */
+        extern char etext[];
+        extern char edata[];
+        extern char end[];
+#       define NEED_FIND_LIMIT
+#       define DATASTART ((ptr_t)(&etext))
+        ptr_t GC_find_limit(ptr_t, GC_bool);
+#       define DATAEND (GC_find_limit (DATASTART, TRUE))
+#       define DATAEND_IS_FUNC
+#       define DATASTART2 ((ptr_t)(&edata))
+#       define DATAEND2 ((ptr_t)(&end))
+#   endif
+#   ifdef OSF1
+#       define OS_TYPE "OSF1"
+#       define DATASTART ((ptr_t) 0x140000000)
+        extern int _end[];
+#       define DATAEND ((ptr_t) &_end)
+        extern char ** environ;
+        /* round up from the value of environ to the nearest page boundary */
+        /* Probably breaks if putenv is called before collector            */
+        /* initialization.                                                 */
+#       define STACKBOTTOM ((ptr_t)(((word)(environ) | (getpagesize()-1))+1))
+/* #    define HEURISTIC2 */
+        /* Normally HEURISTIC2 is too conservative, since               */
+        /* the text segment immediately follows the stack.              */
+        /* Hence we give an upper pound.                                */
+        /* This is currently unused, since we disabled HEURISTIC2       */
+        extern int __start[];
+#       define HEURISTIC2_LIMIT ((ptr_t)((word)(__start) & ~(getpagesize()-1)))
+#       ifndef GC_OSF1_THREADS
+          /* Unresolved signal issues with threads.     */
+#         define MPROTECT_VDB
+#       endif
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       ifdef __ELF__
+#         define SEARCH_FOR_DATA_START
+#         define DYNAMIC_LOADING
+#       else
+#           define DATASTART ((ptr_t) 0x140000000)
+#       endif
+        extern int _end[];
+#       define DATAEND (ptr_t)(_end)
+#       define MPROTECT_VDB
+                /* Has only been superficially tested.  May not */
+                /* work on all versions.                        */
+#   endif
+# endif
+
+# ifdef IA64
+#   define MACH_TYPE "IA64"
+#   ifdef HPUX
+#       ifdef _ILP32
+#         define CPP_WORDSZ 32
+            /* Requires 8 byte alignment for malloc */
+#         define ALIGNMENT 4
+#       else
+#         ifndef _LP64
+#           error --> unknown ABI
+#         endif
+#         define CPP_WORDSZ 64
+            /* Requires 16 byte alignment for malloc */
+#         define ALIGNMENT 8
+#       endif
+#       define OS_TYPE "HPUX"
+        extern int __data_start[];
+#       define DATASTART ((ptr_t)(__data_start))
+        /* Gustavo Rodriguez-Rivera suggested changing HEURISTIC2       */
+        /* to this.  Note that the GC must be initialized before the    */
+        /* first putenv call.                                           */
+        extern char ** environ;
+#       define STACKBOTTOM ((ptr_t)environ)
+#       define HPUX_STACKBOTTOM
+#       define DYNAMIC_LOADING
+#       include <unistd.h>
+#       define GETPAGESIZE() sysconf(_SC_PAGE_SIZE)
+        /* The following was empirically determined, and is probably    */
+        /* not very robust.                                             */
+        /* Note that the backing store base seems to be at a nice       */
+        /* address minus one page.                                      */
+#       define BACKING_STORE_DISPLACEMENT 0x1000000
+#       define BACKING_STORE_ALIGNMENT 0x1000
+        extern ptr_t GC_register_stackbottom;
+#       define BACKING_STORE_BASE GC_register_stackbottom
+        /* Known to be wrong for recent HP/UX versions!!!       */
+#   endif
+#   ifdef LINUX
+#       define CPP_WORDSZ 64
+#       define ALIGNMENT 8
+#       define OS_TYPE "LINUX"
+        /* The following works on NUE and older kernels:        */
+/* #       define STACKBOTTOM ((ptr_t) 0xa000000000000000l)     */
+        /* This does not work on NUE:                           */
+#       define LINUX_STACKBOTTOM
+        /* We also need the base address of the register stack  */
+        /* backing store.  This is computed in                  */
+        /* GC_linux_register_stack_base based on the following  */
+        /* constants:                                           */
+#       define BACKING_STORE_ALIGNMENT 0x100000
+#       define BACKING_STORE_DISPLACEMENT 0x80000000
+        extern ptr_t GC_register_stackbottom;
+#       define BACKING_STORE_BASE GC_register_stackbottom
+#       define SEARCH_FOR_DATA_START
+#       ifdef __GNUC__
+#         define DYNAMIC_LOADING
+#       else
+          /* In the Intel compiler environment, we seem to end up with  */
+          /* statically linked executables and an undefined reference   */
+          /* to _DYNAMIC                                                */
+#       endif
+#       define MPROTECT_VDB
+                /* Requires Linux 2.3.47 or later.      */
+        extern int _end[];
+#       define DATAEND (ptr_t)(_end)
+#       ifdef __GNUC__
+#         ifndef __INTEL_COMPILER
+#           define PREFETCH(x) \
+              __asm__ ("        lfetch  [%0]": : "r"(x))
+#           define PREFETCH_FOR_WRITE(x) \
+              __asm__ ("        lfetch.excl     [%0]": : "r"(x))
+#           define CLEAR_DOUBLE(x) \
+              __asm__ ("        stf.spill       [%0]=f0": : "r"((void *)(x)))
+#         else
+#           include <ia64intrin.h>
+#           define PREFETCH(x) \
+              __lfetch(__lfhint_none, (x))
+#           define PREFETCH_FOR_WRITE(x) \
+              __lfetch(__lfhint_nta,  (x))
+#           define CLEAR_DOUBLE(x) \
+              __stf_spill((void *)(x), 0)
+#         endif /* __INTEL_COMPILER */
+#       endif
+#   endif
+#   ifdef MSWIN32
+      /* FIXME: This is a very partial guess.  There is no port, yet.   */
+#     define OS_TYPE "MSWIN32"
+                /* STACKBOTTOM and DATASTART are handled specially in   */
+                /* os_dep.c.                                            */
+#     define DATAEND  /* not needed */
+#     if defined(_WIN64)
+#       define CPP_WORDSZ 64
+#     else
+#       define CPP_WORDSZ 32   /* Is this possible?     */
+#     endif
+#     define ALIGNMENT 8
+#   endif
+# endif
+
+# ifdef M88K
+#   define MACH_TYPE "M88K"
+#   define ALIGNMENT 4
+    extern int etext[];
+#   ifdef CX_UX
+#       define OS_TYPE "CX_UX"
+#       define DATASTART ((((word)etext + 0x3fffff) & ~0x3fffff) + 0x10000)
+#   endif
+#   ifdef  DGUX
+#       define OS_TYPE "DGUX"
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x10000, (ptr_t)etext)
+#       define DATASTART_IS_FUNC
+#   endif
+#   define STACKBOTTOM ((char*)0xf0000000) /* determined empirically */
+# endif
+
+# ifdef S370
+    /* If this still works, and if anyone cares, this should probably   */
+    /* be moved to the S390 category.                                   */
+#   define MACH_TYPE "S370"
+#   define ALIGNMENT 4  /* Required by hardware */
+#   ifdef UTS4
+#       define OS_TYPE "UTS4"
+        extern int etext[];
+        extern int _etext[];
+        extern int _end[];
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x10000, (ptr_t)_etext)
+#       define DATASTART_IS_FUNC
+#       define DATAEND (ptr_t)(_end)
+#       define HEURISTIC2
+#   endif
+# endif
+
+# ifdef S390
+#   define MACH_TYPE "S390"
+#   ifndef __s390x__
+#   define ALIGNMENT 4
+#   define CPP_WORDSZ 32
+#   else
+#   define ALIGNMENT 8
+#   define CPP_WORDSZ 64
+#   ifndef HBLKSIZE
+#     define HBLKSIZE 4096
+#   endif
+#   endif
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       define DYNAMIC_LOADING
+        extern int __data_start[] __attribute__((__weak__));
+#       define DATASTART ((ptr_t)(__data_start))
+        extern int _end[] __attribute__((__weak__));
+#       define DATAEND (ptr_t)(_end)
+#       define CACHE_LINE_SIZE 256
+#       define GETPAGESIZE() 4096
+#   endif
+# endif
+
+# ifdef ARM32
+#   define CPP_WORDSZ 32
+#   define MACH_TYPE "ARM32"
+#   define ALIGNMENT 4
+#   ifdef NETBSD
+#       define OS_TYPE "NETBSD"
+#       define HEURISTIC2
+#       ifdef __ELF__
+           extern ptr_t GC_data_start;
+#          define DATASTART GC_data_start
+#          define DYNAMIC_LOADING
+#       else
+           extern char etext[];
+#          define DATASTART ((ptr_t)(etext))
+#       endif
+#   endif
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       undef STACK_GRAN
+#       define STACK_GRAN 0x10000000
+#       ifdef __ELF__
+#            define DYNAMIC_LOADING
+#            include <features.h>
+#            if defined(__GLIBC__) && __GLIBC__ >= 2 \
+                || defined(PLATFORM_ANDROID)
+#                define SEARCH_FOR_DATA_START
+#            else
+                 extern char **__environ;
+#                define DATASTART ((ptr_t)(&__environ))
+                              /* hideous kludge: __environ is the first */
+                              /* word in crt0.o, and delimits the start */
+                              /* of the data segment, no matter which   */
+                              /* ld options were passed through.        */
+                              /* We could use _etext instead, but that  */
+                              /* would include .rodata, which may       */
+                              /* contain large read-only data tables    */
+                              /* that we'd rather not scan.             */
+#            endif
+             extern int _end[];
+#            define DATAEND (ptr_t)(_end)
+#       else
+             extern int etext[];
+#            define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#       endif
+#   endif
+#   ifdef MSWINCE
+#     define OS_TYPE "MSWINCE"
+#     define DATAEND /* not needed */
+#   endif
+#   ifdef FREEBSD
+      /* FreeBSD/arm */
+#     define ALIGNMENT 4
+#     define OS_TYPE "FREEBSD"
+#     ifdef __ELF__
+#       define DYNAMIC_LOADING
+#     endif
+#     define HEURISTIC2
+      extern char etext[];
+#     define SEARCH_FOR_DATA_START
+#   endif
+#   ifdef DARWIN
+      /* iPhone */
+#     define OS_TYPE "DARWIN"
+#     define DYNAMIC_LOADING
+#     define DATASTART ((ptr_t) get_etext())
+#     define DATAEND   ((ptr_t) get_end())
+#     define STACKBOTTOM ((ptr_t) 0x30000000)
+#     ifndef USE_MMAP
+#       define USE_MMAP
+#     endif
+#     define USE_MMAP_ANON
+#     define MPROTECT_VDB
+#     include <unistd.h>
+#     define GETPAGESIZE() getpagesize()
+      /* FIXME: There seems to be some issues with trylock hanging on   */
+      /* darwin. This should be looked into some more.                  */
+#     define NO_PTHREAD_TRYLOCK
+#     ifndef NO_DYLD_BIND_FULLY_IMAGE
+#       define NO_DYLD_BIND_FULLY_IMAGE
+#     endif
+#   endif
+#   ifdef OPENBSD
+#     define ALIGNMENT 4
+#     define OS_TYPE "OPENBSD"
+#     ifdef GC_OPENBSD_THREADS
+#       define UTHREAD_SP_OFFSET 176
+#     else
+#       include <sys/param.h>
+#       include <uvm/uvm_extern.h>
+#       define STACKBOTTOM USRSTACK
+#     endif
+      extern int __data_start[];
+#     define DATASTART ((ptr_t)__data_start)
+      extern char _end[];
+#     define DATAEND ((ptr_t)(&_end))
+#     define DYNAMIC_LOADING
+#   endif
+#   ifdef NOSYS
+      /* __data_start is usually defined in the target linker script.  */
+      extern int __data_start[];
+#     define DATASTART (ptr_t)(__data_start)
+      /* __stack_base__ is set in newlib/libc/sys/arm/crt0.S  */
+      extern void *__stack_base__;
+#     define STACKBOTTOM ((ptr_t) (__stack_base__))
+#   endif
+#endif
+
+# ifdef CRIS
+#   define MACH_TYPE "CRIS"
+#   define CPP_WORDSZ 32
+#   define ALIGNMENT 1
+#   define OS_TYPE "LINUX"
+#   define DYNAMIC_LOADING
+#   define LINUX_STACKBOTTOM
+#   define SEARCH_FOR_DATA_START
+      extern int _end[];
+#   define DATAEND (ptr_t)(_end)
+# endif
+
+# if defined(SH) && !defined(SH4)
+#   define MACH_TYPE "SH"
+#   define ALIGNMENT 4
+#   ifdef MSWINCE
+#     define OS_TYPE "MSWINCE"
+#     define DATAEND /* not needed */
+#   endif
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     define LINUX_STACKBOTTOM
+#     define DYNAMIC_LOADING
+#     define SEARCH_FOR_DATA_START
+      extern int _end[];
+#     define DATAEND (ptr_t)(_end)
+#   endif
+#   ifdef NETBSD
+#      define OS_TYPE "NETBSD"
+#      define HEURISTIC2
+       extern ptr_t GC_data_start;
+#      define DATASTART GC_data_start
+#      define DYNAMIC_LOADING
+#   endif
+#   ifdef OPENBSD
+#      define OS_TYPE "OPENBSD"
+#      ifdef GC_OPENBSD_THREADS
+#        define UTHREAD_SP_OFFSET 332
+#      else
+#        include <sys/param.h>
+#        include <uvm/uvm_extern.h>
+#        define STACKBOTTOM USRSTACK
+#      endif
+       extern int __data_start[];
+#      define DATASTART ((ptr_t)__data_start)
+       extern char _end[];
+#      define DATAEND ((ptr_t)(&_end))
+#      define DYNAMIC_LOADING
+#   endif
+# endif
+
+# ifdef SH4
+#   define MACH_TYPE "SH4"
+#   define OS_TYPE "MSWINCE"
+#   define ALIGNMENT 4
+#   define DATAEND /* not needed */
+# endif
+
+# ifdef AVR32
+#   define MACH_TYPE "AVR32"
+#   define CPP_WORDSZ 32
+#   define ALIGNMENT 4
+#   define OS_TYPE "LINUX"
+#   define DYNAMIC_LOADING
+#   define LINUX_STACKBOTTOM
+#   define USE_GENERIC_PUSH_REGS
+#   define SEARCH_FOR_DATA_START
+    extern int _end[];
+#   define DATAEND (_end)
+# endif
+
+# ifdef M32R
+#   define CPP_WORDSZ 32
+#   define MACH_TYPE "M32R"
+#   define ALIGNMENT 4
+#   ifdef LINUX
+#     define OS_TYPE "LINUX"
+#     define LINUX_STACKBOTTOM
+#     undef STACK_GRAN
+#     define STACK_GRAN 0x10000000
+#     define DYNAMIC_LOADING
+#     define SEARCH_FOR_DATA_START
+      extern int _end[];
+#     define DATAEND (ptr_t)(_end)
+#   endif
+# endif
+
+
+# ifdef X86_64
+#   define MACH_TYPE "X86_64"
+#   ifdef __ILP32__
+#     define ALIGNMENT 4
+#     define CPP_WORDSZ 32
+#   else
+#     define ALIGNMENT 8
+#     define CPP_WORDSZ 64
+#   endif
+#   ifndef HBLKSIZE
+#     define HBLKSIZE 4096
+#   endif
+#   define CACHE_LINE_SIZE 64
+#   ifdef OPENBSD
+#       define OS_TYPE "OPENBSD"
+#       define ELF_CLASS ELFCLASS64
+#       ifdef GC_OPENBSD_THREADS
+#         define UTHREAD_SP_OFFSET 400
+#       else
+#         include <sys/param.h>
+#         include <uvm/uvm_extern.h>
+#         define STACKBOTTOM USRSTACK
+#       endif
+        extern int __data_start[];
+#       define DATASTART ((ptr_t)__data_start)
+        extern char _end[];
+#       define DATAEND ((ptr_t)(&_end))
+#       define DYNAMIC_LOADING
+#   endif
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+#       if !defined(GC_LINUX_THREADS) || !defined(REDIRECT_MALLOC)
+#           define MPROTECT_VDB
+#       else
+            /* We seem to get random errors in incremental mode,        */
+            /* possibly because Linux threads is itself a malloc client */
+            /* and can't deal with the signals.                         */
+#       endif
+#       ifdef __ELF__
+#            define DYNAMIC_LOADING
+#            ifdef UNDEFINED    /* includes ro data */
+               extern int _etext[];
+#              define DATASTART ((ptr_t)((((word) (_etext)) + 0xfff) & ~0xfff))
+#            endif
+#            include <features.h>
+#            define SEARCH_FOR_DATA_START
+             extern int _end[];
+#            define DATAEND (ptr_t)(_end)
+#       else
+             extern int etext[];
+#            define DATASTART ((ptr_t)((((word) (etext)) + 0xfff) & ~0xfff))
+#       endif
+#       if defined(__GNUC__) && __GNUC__ >= 3
+#           define PREFETCH(x) __builtin_prefetch((x), 0, 0)
+#           define PREFETCH_FOR_WRITE(x) __builtin_prefetch((x), 1)
+#       endif
+#       if defined(__GLIBC__) && !defined(__UCLIBC__)
+          /* At present, there's a bug in GLibc getcontext() on         */
+          /* Linux/x64 (it clears FPU exception mask).  We define this  */
+          /* macro to workaround it.                                    */
+          /* FIXME: This seems to be fixed in GLibc v2.14.              */
+#         define GETCONTEXT_FPU_EXCMASK_BUG
+#       endif
+#   endif
+#   ifdef DARWIN
+#     define OS_TYPE "DARWIN"
+#     define DARWIN_DONT_PARSE_STACK
+#     define DYNAMIC_LOADING
+      /* XXX: see get_end(3), get_etext() and get_end() should not be used. */
+      /* These aren't used when dyld support is enabled (it is by default)  */
+#     define DATASTART ((ptr_t) get_etext())
+#     define DATAEND   ((ptr_t) get_end())
+#     define STACKBOTTOM ((ptr_t) 0x7fff5fc00000)
+#     ifndef USE_MMAP
+#       define USE_MMAP
+#     endif
+#     define USE_MMAP_ANON
+#     define MPROTECT_VDB
+#     include <unistd.h>
+#     define GETPAGESIZE() getpagesize()
+      /* There seems to be some issues with trylock hanging on darwin.  */
+      /* This should be looked into some more.                          */
+#     define NO_PTHREAD_TRYLOCK
+#   endif
+#   ifdef FREEBSD
+#       define OS_TYPE "FREEBSD"
+#       ifndef GC_FREEBSD_THREADS
+#           define MPROTECT_VDB
+#       endif
+#       ifdef __GLIBC__
+#           define SIG_SUSPEND          (32+6)
+#           define SIG_THR_RESTART      (32+5)
+            extern int _end[];
+#           define DATAEND (ptr_t)(_end)
+#       else
+#           define SIG_SUSPEND SIGUSR1
+#           define SIG_THR_RESTART SIGUSR2
+#       endif
+#       define FREEBSD_STACKBOTTOM
+#       ifdef __ELF__
+#           define DYNAMIC_LOADING
+#       endif
+        extern char etext[];
+        ptr_t GC_FreeBSDGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_FreeBSDGetDataStart(0x1000, (ptr_t)etext)
+#       define DATASTART_IS_FUNC
+#   endif
+#   ifdef NETBSD
+#       define OS_TYPE "NETBSD"
+#       define HEURISTIC2
+#       ifdef __ELF__
+            extern ptr_t GC_data_start;
+#           define DATASTART GC_data_start
+#           define DYNAMIC_LOADING
+#       else
+#           define SEARCH_FOR_DATA_START
+#       endif
+#   endif
+#   ifdef SOLARIS
+#       define OS_TYPE "SOLARIS"
+#       define ELF_CLASS ELFCLASS64
+        extern int _etext[], _end[];
+        ptr_t GC_SysVGetDataStart(size_t, ptr_t);
+#       define DATASTART GC_SysVGetDataStart(0x1000, (ptr_t)_etext)
+#       define DATASTART_IS_FUNC
+#       define DATAEND (ptr_t)(_end)
+/*      # define STACKBOTTOM ((ptr_t)(_start)) worked through 2.7,      */
+/*      but reportedly breaks under 2.8.  It appears that the stack     */
+/*      base is a property of the executable, so this should not break  */
+/*      old executables.                                                */
+/*      HEURISTIC2 probably works, but this appears to be preferable.   */
+/*      Apparently USRSTACK is defined to be USERLIMIT, but in some     */
+/*      installations that's undefined.  We work around this with a     */
+/*      gross hack:                                                     */
+#       include <sys/vmparam.h>
+#       ifdef USERLIMIT
+          /* This should work everywhere, but doesn't.  */
+#         define STACKBOTTOM ((ptr_t) USRSTACK)
+#       else
+#         define HEURISTIC2
+#       endif
+/* At least in Solaris 2.5, PROC_VDB gives wrong values for dirty bits. */
+/* It appears to be fixed in 2.8 and 2.9.                               */
+#       ifdef SOLARIS25_PROC_VDB_BUG_FIXED
+#         define PROC_VDB
+#       endif
+#       ifndef GC_THREADS
+#         define MPROTECT_VDB
+#       endif
+#       define DYNAMIC_LOADING
+#       if !defined(USE_MMAP) && defined(REDIRECT_MALLOC)
+#         define USE_MMAP
+            /* Otherwise we now use calloc.  Mmap may result in the     */
+            /* heap interleaved with thread stacks, which can result in */
+            /* excessive blacklisting.  Sbrk is unusable since it       */
+            /* doesn't interact correctly with the system malloc.       */
+#       endif
+#       ifdef USE_MMAP
+#         define HEAP_START (ptr_t)0x40000000
+#       else
+#         define HEAP_START DATAEND
+#       endif
+#   endif
+#   ifdef MSWIN32
+#       define OS_TYPE "MSWIN32"
+                /* STACKBOTTOM and DATASTART are handled specially in   */
+                /* os_dep.c.                                            */
+#       if !defined(__GNUC__) || defined(__INTEL_COMPILER)
+          /* GCC does not currently support SetUnhandledExceptionFilter */
+          /* (does not generate SEH unwinding information) on x64.      */
+#         define MPROTECT_VDB
+#       endif
+#       define GWW_VDB
+#       define DATAEND  /* not needed */
+#   endif
+
+/** Nautilus-x86_64 */
+#   ifdef NAUT
+
+// Reference linker symbols as char arrays to avoid compiler warning
+extern char _data_start[];
+extern char _data_end[];
+
+#       include <nautilus/printk.h>
+#       include <nautilus/libccompat.h>
+
+#       undef LINUX
+#       undef USE_MMAP
+#       undef USE_MUNMAP
+#       undef MMAP_SUPPORTED
+#       undef ADD_HEAP_GUARD_PAGES
+#       undef DGUX
+#       undef UNIX_LIKE
+#       undef DYNAMIC_LOADING
+#       undef STACKBOTTOM
+#       undef HEURISTIC1
+#       undef HEURISTIC2
+#       undef PROC_VDB
+#       undef LINUX_STACKBOTTOM
+#       undef FIND_LEAK
+#       undef MPROTECT_VDB
+#       undef NEED_FIND_LIMIT
+#       undef HAVE_BUILTIN_UNWIND_INIT
+//#       define THREAD_GROWS_UP
+//#       define GC_THREADS // parallel marker
+#       define NAUT_THREADS
+#       define DEBUG_THREADS
+//#       define GC_PTHREADS
+//#       define GC_PTHREADS
+#       define GC_ASSERTIONS
+#       define OS_TYPE "NAUTILUS"
+#       define GETPAGESIZE() 4096
+//#       define GC_DEBUG
+#       define ALIGNMENT 8
+#       define DATASTART (void *)&_data_start
+#       define DATAEND (void *)&_data_end
+#       define GC_READ_ENV_FILE
+#       define NO_GETENV
+#       define STRTOULL simple_strtoull
+
+#       ifdef NAUT_THREADS
+#          define THREADS
+#       include <nautilus/thread.h>
+#       include "bdwgc_internal.h"
+#       endif
+
+#   endif 
+# endif /* X86_64 */
+
+# ifdef HEXAGON
+#   define CPP_WORDSZ 32
+#   define MACH_TYPE "HEXAGON"
+#   define ALIGNMENT 4
+#   ifdef LINUX
+#       define OS_TYPE "LINUX"
+#       define LINUX_STACKBOTTOM
+//#       define MPROTECT_VDB
+#       ifdef __ELF__
+#            define DYNAMIC_LOADING
+#            include <features.h>
+#            if defined(__GLIBC__) && __GLIBC__ >= 2
+#                define SEARCH_FOR_DATA_START
+#            else
+#                error --> unknown Hexagon libc configuration
+#            endif
+             extern int _end[];
+#            define DATAEND (ptr_t)(_end)
+#       else
+#            error --> bad Hexagon Linux configuration
+#       endif
+#   else
+#       error --> unknown Hexagon OS configuration
+#   endif
+# endif
+
+#if defined(LINUX_STACKBOTTOM) && defined(NO_PROC_STAT) \
+    && !defined(USE_LIBC_PRIVATES)
+    /* This combination will fail, since we have no way to get  */
+    /* the stack base.  Use HEURISTIC2 instead.                 */
+#   undef LINUX_STACKBOTTOM
+#   define HEURISTIC2
+    /* This may still fail on some architectures like IA64.     */
+    /* We tried ...                                             */
+#endif
+
+#if defined(LINUX) && defined(USE_MMAP)
+    /* The kernel may do a somewhat better job merging mappings etc.    */
+    /* with anonymous mappings.                                         */
+#   define USE_MMAP_ANON
+#endif
+
+#if defined(GC_LINUX_THREADS) && defined(REDIRECT_MALLOC) \
+    && !defined(USE_PROC_FOR_LIBRARIES)
+    /* Nptl allocates thread stacks with mmap, which is fine.  But it   */
+    /* keeps a cache of thread stacks.  Thread stacks contain the       */
+    /* thread control blocks.  These in turn contain a pointer to       */
+    /* (sizeof (void *) from the beginning of) the dtv for thread-local */
+    /* storage, which is calloc allocated.  If we don't scan the cached */
+    /* thread stacks, we appear to lose the dtv.  This tends to         */
+    /* result in something that looks like a bogus dtv count, which     */
+    /* tends to result in a memset call on a block that is way too      */
+    /* large.  Sometimes we're lucky and the process just dies ...      */
+    /* There seems to be a similar issue with some other memory         */
+    /* allocated by the dynamic loader.                                 */
+    /* This should be avoidable by either:                              */
+    /* - Defining USE_PROC_FOR_LIBRARIES here.                          */
+    /*   That performs very poorly, precisely because we end up         */
+    /*   scanning cached stacks.                                        */
+    /* - Have calloc look at its callers.                               */
+    /*   In spite of the fact that it is gross and disgusting.          */
+    /* In fact neither seems to suffice, probably in part because       */
+    /* even with USE_PROC_FOR_LIBRARIES, we don't scan parts of stack   */
+    /* segments that appear to be out of bounds.  Thus we actually      */
+    /* do both, which seems to yield the best results.                  */
+
+#   define USE_PROC_FOR_LIBRARIES
+#endif
+
+#ifndef STACK_GROWS_UP
+# define STACK_GROWS_DOWN
+#endif
+
+#ifndef CPP_WORDSZ
+# define CPP_WORDSZ 32
+#endif
+
+#ifndef OS_TYPE
+# define OS_TYPE ""
+#endif
+
+#ifndef DATAEND
+  extern int end[];
+# define DATAEND (ptr_t)(end)
+#endif
+
+#if defined(PLATFORM_ANDROID) && !defined(THREADS) \
+    && !defined(USE_GET_STACKBASE_FOR_MAIN)
+  /* Always use pthread_attr_getstack on Android ("-lpthread" option is  */
+  /* not needed to be specified manually) since GC_linux_main_stack_base */
+  /* causes app crash if invoked inside Dalvik VM.                       */
+# define USE_GET_STACKBASE_FOR_MAIN
+#endif
+
+#if (defined(SVR4) || defined(PLATFORM_ANDROID)) && !defined(GETPAGESIZE)
+# include <unistd.h>
+# define GETPAGESIZE()  sysconf(_SC_PAGESIZE)
+#endif
+
+#ifndef GETPAGESIZE
+# if defined(SOLARIS) || defined(IRIX5) || defined(LINUX) \
+     || defined(NETBSD) || defined(FREEBSD) || defined(HPUX)
+#   include <unistd.h>
+# endif
+# define GETPAGESIZE() getpagesize()
+#endif
+
+#if defined(SOLARIS) || defined(DRSNX) || defined(UTS4)
+        /* OS has SVR4 generic features.        */
+        /* Probably others also qualify.        */
+# define SVR4
+#endif
+
+#if defined(SOLARIS) || defined(DRSNX)
+        /* OS has SOLARIS style semi-undocumented interface     */
+        /* to dynamic loader.                                   */
+# define SOLARISDL
+        /* OS has SOLARIS style signal handlers.        */
+# define SUNOS5SIGS
+#endif
+
+#if defined(HPUX)
+# define SUNOS5SIGS
+#endif
+
+#if defined(FREEBSD) && (defined(__DragonFly__) || __FreeBSD__ >= 4 \
+                         || (__FreeBSD_kernel__ >= 4))
+# define SUNOS5SIGS
+#endif
+
+#if !defined(GC_EXPLICIT_SIGNALS_UNBLOCK) && defined(SUNOS5SIGS) \
+    && !defined(GC_NO_PTHREAD_SIGMASK)
+# define GC_EXPLICIT_SIGNALS_UNBLOCK
+#endif
+
+#ifdef GC_NETBSD_THREADS
+# define SIGRTMIN 33
+# define SIGRTMAX 63
+#endif
+
+#if defined(SVR4) || defined(LINUX) || defined(IRIX5) || defined(HPUX) \
+    || defined(OPENBSD) || defined(NETBSD) || defined(FREEBSD) \
+    || defined(DGUX) || defined(BSD) || defined(HURD) \
+    || defined(AIX) || defined(DARWIN) || defined(OSF1)
+#   define UNIX_LIKE      /* Basic Unix-like system calls work.   */
+#endif
+
+#if CPP_WORDSZ != 32 && CPP_WORDSZ != 64
+# error --> bad word size
+#endif
+
+#ifndef ALIGNMENT
+# error --> undefined ALIGNMENT
+#endif
+
+#ifdef PCR
+# undef DYNAMIC_LOADING
+# undef STACKBOTTOM
+# undef HEURISTIC1
+# undef HEURISTIC2
+# undef PROC_VDB
+# undef MPROTECT_VDB
+# define PCR_VDB
+#endif
+
+#if !defined(STACKBOTTOM) && (defined(ECOS) || defined(NOSYS))
+# error --> undefined STACKBOTTOM
+#endif
+
+#ifdef IGNORE_DYNAMIC_LOADING
+# undef DYNAMIC_LOADING
+#endif
+
+#if defined(SMALL_CONFIG) && !defined(GC_DISABLE_INCREMENTAL)
+  /* Presumably not worth the space it takes.   */
+# define GC_DISABLE_INCREMENTAL
+#endif
+
+#if defined(LINUX) || defined(FREEBSD) || defined(SOLARIS) || defined(IRIX5) \
+        || ((defined(USE_MMAP) || defined(USE_MUNMAP)) \
+        && !defined(MSWIN32) && !defined(MSWINCE))
+# define MMAP_SUPPORTED
+#endif
+
+#if defined(GC_DISABLE_INCREMENTAL) || defined(MANUAL_VDB)
+# undef GWW_VDB
+# undef MPROTECT_VDB
+# undef PCR_VDB
+# undef PROC_VDB
+#endif
+
+#ifdef GC_DISABLE_INCREMENTAL
+# undef CHECKSUMS
+#endif
+
+#ifdef USE_GLOBAL_ALLOC
+  /* Cannot pass MEM_WRITE_WATCH to GlobalAlloc().      */
+# undef GWW_VDB
+#endif
+
+#ifdef USE_MUNMAP
+  /* FIXME: Remove this undef if possible.      */
+# undef MPROTECT_VDB  /* Can't deal with address space holes.   */
+#endif
+
+/* PARALLEL_MARK does not cause undef MPROTECT_VDB any longer.  */
+
+#if defined(MPROTECT_VDB) && defined(GC_PREFER_MPROTECT_VDB)
+  /* Choose MPROTECT_VDB manually (if multiple strategies available).   */
+# undef PCR_VDB
+# undef PROC_VDB
+  /* #undef GWW_VDB - handled in os_dep.c       */
+#endif
+
+#ifdef PROC_VDB
+  /* Multi-VDB mode is not implemented. */
+# undef MPROTECT_VDB
+#endif
+
+#if !defined(PCR_VDB) && !defined(PROC_VDB) && !defined(MPROTECT_VDB) \
+    && !defined(GWW_VDB) && !defined(MANUAL_VDB) \
+    && !defined(GC_DISABLE_INCREMENTAL)
+# define DEFAULT_VDB
+#endif
+
+#if ((defined(UNIX_LIKE) && (defined(DARWIN) || defined(HURD) \
+                             || defined(OPENBSD) || defined(ARM32) \
+                             || defined(MIPS) || defined(AVR32))) \
+     || (defined(LINUX) && (defined(SPARC) || defined(M68K))) \
+     || (defined(RTEMS) && defined(I386))) && !defined(NO_GETCONTEXT)
+# define NO_GETCONTEXT
+#endif
+
+#ifndef PREFETCH
+# define PREFETCH(x)
+# define NO_PREFETCH
+#endif
+
+#ifndef PREFETCH_FOR_WRITE
+# define PREFETCH_FOR_WRITE(x)
+# define NO_PREFETCH_FOR_WRITE
+#endif
+
+#ifndef CACHE_LINE_SIZE
+# define CACHE_LINE_SIZE 32     /* Wild guess   */
+#endif
+
+#ifndef STATIC
+# ifndef NO_DEBUGGING
+#   define STATIC /* ignore to aid profiling and possibly debugging     */
+# else
+#   define STATIC static
+# endif
+#endif
+
+#if defined(LINUX) && (defined(USE_PROC_FOR_LIBRARIES) || defined(IA64) \
+                       || !defined(SMALL_CONFIG))
+# define NEED_PROC_MAPS
+#endif
+
+#if defined(LINUX) || defined(HURD) || defined(__GLIBC__)
+# define REGISTER_LIBRARIES_EARLY
+  /* We sometimes use dl_iterate_phdr, which may acquire an internal    */
+  /* lock.  This isn't safe after the world has stopped.  So we must    */
+  /* call GC_register_dynamic_libraries before stopping the world.      */
+  /* For performance reasons, this may be beneficial on other           */
+  /* platforms as well, though it should be avoided in win32.           */
+#endif /* LINUX */
+
+#if defined(SEARCH_FOR_DATA_START)
+  extern ptr_t GC_data_start;
+# define DATASTART GC_data_start
+#endif
+
+#ifndef CLEAR_DOUBLE
+# define CLEAR_DOUBLE(x) (((word*)(x))[0] = 0, ((word*)(x))[1] = 0)
+#endif
+
+#if defined(GC_LINUX_THREADS) && defined(REDIRECT_MALLOC) \
+    && !defined(INCLUDE_LINUX_THREAD_DESCR)
+  /* Will not work, since libc and the dynamic loader use thread        */
+  /* locals, sometimes as the only reference.                           */
+# define INCLUDE_LINUX_THREAD_DESCR
+#endif
+
+#if defined(GC_IRIX_THREADS) && !defined(IRIX5)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_LINUX_THREADS) && !defined(LINUX) && !defined(NACL)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_NETBSD_THREADS) && !defined(NETBSD)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_FREEBSD_THREADS) && !defined(FREEBSD)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_SOLARIS_THREADS) && !defined(SOLARIS)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_HPUX_THREADS) && !defined(HPUX)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_AIX_THREADS) && !defined(_AIX)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_GNU_THREADS) && !defined(HURD)
+# error --> inconsistent configuration
+#endif
+#if defined(GC_WIN32_THREADS) && !defined(MSWIN32) && !defined(CYGWIN32) \
+    && !defined(MSWINCE)
+# error --> inconsistent configuration
+#endif
+
+#if defined(PCR) || defined(GC_WIN32_THREADS) || defined(GC_PTHREADS) \
+    || defined(SN_TARGET_PS3)
+# define THREADS
+#endif
+
+#if defined(UNIX_LIKE) && defined(THREADS) && !defined(NO_CANCEL_SAFE) \
+    && !defined(PLATFORM_ANDROID)
+  /* Make the code cancellation-safe.  This basically means that we     */
+  /* ensure that cancellation requests are ignored while we are in      */
+  /* the collector.  This applies only to Posix deferred cancellation;  */
+  /* we don't handle Posix asynchronous cancellation.                   */
+  /* Note that this only works if pthread_setcancelstate is             */
+  /* async-signal-safe, at least in the absence of asynchronous         */
+  /* cancellation.  This appears to be true for the glibc version,      */
+  /* though it is not documented.  Without that assumption, there       */
+  /* seems to be no way to safely wait in a signal handler, which       */
+  /* we need to do for thread suspension.                               */
+  /* Also note that little other code appears to be cancellation-safe.  */
+  /* Hence it may make sense to turn this off for performance.          */
+# define CANCEL_SAFE
+#endif
+
+#ifdef CANCEL_SAFE
+# define IF_CANCEL(x) x
+#else
+# define IF_CANCEL(x) /* empty */
+#endif
+
+#if !defined(CAN_HANDLE_FORK) && !defined(NO_HANDLE_FORK) \
+    && ((defined(GC_PTHREADS) && !defined(HURD) && !defined(NACL) \
+         && !defined(PLATFORM_ANDROID) && !defined(GC_WIN32_PTHREADS)) \
+        || (defined(DARWIN) && defined(MPROTECT_VDB)) || defined(HANDLE_FORK))
+  /* Attempts (where supported and requested) to make GC_malloc work in */
+  /* a child process fork'ed from a multi-threaded parent.              */
+# define CAN_HANDLE_FORK
+#endif
+
+#if !defined(USE_MARK_BITS) && !defined(USE_MARK_BYTES) \
+    && defined(PARALLEL_MARK)
+   /* Minimize compare-and-swap usage.  */
+#  define USE_MARK_BYTES
+#endif
+
+#if defined(MSWINCE) && !defined(__CEGCC__) && !defined(NO_GETENV)
+# define NO_GETENV
+#endif
+
+#if (defined(NO_GETENV) || defined(MSWINCE)) && !defined(NO_GETENV_WIN32)
+# define NO_GETENV_WIN32
+#endif
+
+#ifndef STRTOULL
+# if defined(_WIN64) && !defined(__GNUC__)
+#   define STRTOULL _strtoui64
+# elif defined(_LLP64) || defined(__LLP64__) || defined(_WIN64)
+#   define STRTOULL strtoull
+# else
+    /* strtoul() fits since sizeof(long) >= sizeof(word).       */
+#   define STRTOULL strtoul
+# endif
+#endif /* !STRTOULL */
+
+#ifndef GC_WORD_C
+# if defined(_WIN64) && !defined(__GNUC__)
+#   define GC_WORD_C(val) val##ui64
+# elif defined(_LLP64) || defined(__LLP64__) || defined(_WIN64)
+#   define GC_WORD_C(val) val##ULL
+# else
+#   define GC_WORD_C(val) ((word)val##UL)
+# endif
+#endif /* !GC_WORD_C */
+
+#if defined(SPARC)
+# define ASM_CLEAR_CODE /* Stack clearing is crucial, and we    */
+                        /* include assembly code to do it well. */
+#endif
+
+/* Can we save call chain in objects for debugging?                     */
+/* SET NFRAMES (# of saved frames) and NARGS (#of args for each         */
+/* frame) to reasonable values for the platform.                        */
+/* Set SAVE_CALL_CHAIN if we can.  SAVE_CALL_COUNT can be specified     */
+/* at build time, though we feel free to adjust it slightly.            */
+/* Define NEED_CALLINFO if we either save the call stack or             */
+/* GC_ADD_CALLER is defined.                                            */
+/* GC_CAN_SAVE_CALL_STACKS is set in gc.h.                              */
+#if defined(SPARC)
+# define CAN_SAVE_CALL_ARGS
+#endif
+#if (defined(I386) || defined(X86_64)) \
+    && (defined(LINUX) || defined(__GLIBC__))
+  /* SAVE_CALL_CHAIN is supported if the code is compiled to save       */
+  /* frame pointers by default, i.e. no -fomit-frame-pointer flag.      */
+# define CAN_SAVE_CALL_ARGS
+#endif
+
+#if defined(SAVE_CALL_COUNT) && !defined(GC_ADD_CALLER) \
+    && defined(GC_CAN_SAVE_CALL_STACKS)
+# define SAVE_CALL_CHAIN
+#endif
+#ifdef SAVE_CALL_CHAIN
+# if defined(SAVE_CALL_NARGS) && defined(CAN_SAVE_CALL_ARGS)
+#   define NARGS SAVE_CALL_NARGS
+# else
+#   define NARGS 0      /* Number of arguments to save for each call.   */
+# endif
+#endif
+#ifdef SAVE_CALL_CHAIN
+# ifndef SAVE_CALL_COUNT
+#   define NFRAMES 6    /* Number of frames to save. Even for   */
+                        /* alignment reasons.                   */
+# else
+#   define NFRAMES ((SAVE_CALL_COUNT + 1) & ~1)
+# endif
+# define NEED_CALLINFO
+#endif /* SAVE_CALL_CHAIN */
+#ifdef GC_ADD_CALLER
+# define NFRAMES 1
+# define NARGS 0
+# define NEED_CALLINFO
+#endif
+
+#if defined(FREEBSD) && !defined(HAVE_DLADDR)
+  /* TODO: Define for Darwin, Linux, Solaris. */
+  /* TODO: Detect dladdr() presence by configure. */
+# define HAVE_DLADDR
+#endif
+
+#if defined(MAKE_BACK_GRAPH) && !defined(DBG_HDRS_ALL)
+# define DBG_HDRS_ALL
+#endif
+
+#if defined(POINTER_MASK) && !defined(POINTER_SHIFT)
+# define POINTER_SHIFT 0
+#endif
+
+#if defined(POINTER_SHIFT) && !defined(POINTER_MASK)
+# define POINTER_MASK ((GC_word)(-1))
+#endif
+
+#if !defined(FIXUP_POINTER) && defined(POINTER_MASK)
+# define FIXUP_POINTER(p) (p = ((p) & POINTER_MASK) << POINTER_SHIFT)
+#endif
+
+#if defined(FIXUP_POINTER)
+# define NEED_FIXUP_POINTER 1
+#else
+# define NEED_FIXUP_POINTER 0
+# define FIXUP_POINTER(p)
+#endif
+
+#if !defined(MARK_BIT_PER_GRANULE) && !defined(MARK_BIT_PER_OBJ)
+# define MARK_BIT_PER_GRANULE   /* Usually faster       */
+#endif
+
+/* Some static sanity tests.    */
+#if defined(MARK_BIT_PER_GRANULE) && defined(MARK_BIT_PER_OBJ)
+# error Define only one of MARK_BIT_PER_GRANULE and MARK_BIT_PER_OBJ.
+#endif
+
+#if defined(STACK_GROWS_UP) && defined(STACK_GROWS_DOWN)
+# error "Only one of STACK_GROWS_UP and STACK_GROWS_DOWN should be defd."
+#endif
+#if !defined(STACK_GROWS_UP) && !defined(STACK_GROWS_DOWN)
+# error "One of STACK_GROWS_UP and STACK_GROWS_DOWN should be defd."
+#endif
+
+#if defined(REDIRECT_MALLOC) && defined(THREADS) && !defined(LINUX)
+# error "REDIRECT_MALLOC with THREADS works at most on Linux."
+#endif
+
+#ifdef GC_PRIVATE_H
+        /* This relies on some type definitions from gc_priv.h, from    */
+        /* where it's normally included.                                */
+        /*                                                              */
+        /* How to get heap memory from the OS:                          */
+        /* Note that sbrk()-like allocation is preferred, since it      */
+        /* usually makes it possible to merge consecutively allocated   */
+        /* chunks.  It also avoids unintended recursion with            */
+        /* REDIRECT_MALLOC macro defined.                               */
+        /* GET_MEM() returns a HLKSIZE aligned chunk.                   */
+        /* 0 is taken to mean failure.                                  */
+        /* In case of MMAP_SUPPORTED, the argument must also be         */
+        /* a multiple of a physical page size.                          */
+        /* GET_MEM is currently not assumed to retrieve 0 filled space, */
+        /* though we should perhaps take advantage of the case in which */
+        /* does.                                                        */
+        struct hblk;    /* See gc_priv.h.       */
+# if defined(PCR)
+    char * real_malloc(size_t bytes);
+#   define GET_MEM(bytes) HBLKPTR(real_malloc((size_t)(bytes) + GC_page_size) \
+                                          + GC_page_size-1)
+# elif defined(OS2)
+    void * os2_alloc(size_t bytes);
+#   define GET_MEM(bytes) HBLKPTR((ptr_t)os2_alloc((size_t)(bytes) \
+                                            + GC_page_size) + GC_page_size-1)
+# elif defined(NEXT) || defined(DOS4GW) || defined(NONSTOP) \
+        || (defined(AMIGA) && !defined(GC_AMIGA_FASTALLOC)) \
+        || (defined(SOLARIS) && !defined(USE_MMAP)) || defined(RTEMS) \
+        || defined(__CC_ARM)
+#   define GET_MEM(bytes) HBLKPTR((size_t)calloc(1, \
+                                            (size_t)(bytes) + GC_page_size) \
+                                  + GC_page_size - 1)
+# elif defined(MSWIN32) || defined(CYGWIN32)
+    ptr_t GC_win32_get_mem(GC_word bytes);
+#   define GET_MEM(bytes) (struct hblk *)GC_win32_get_mem(bytes)
+# elif defined(MACOS)
+#   if defined(USE_TEMPORARY_MEMORY)
+      Ptr GC_MacTemporaryNewPtr(size_t size, Boolean clearMemory);
+#     define GET_MEM(bytes) HBLKPTR( \
+                        GC_MacTemporaryNewPtr((bytes) + GC_page_size, true) \
+                        + GC_page_size-1)
+#   else
+#     define GET_MEM(bytes) HBLKPTR(NewPtrClear((bytes) + GC_page_size) \
+                                    + GC_page_size-1)
+#   endif
+# elif defined(MSWINCE)
+    ptr_t GC_wince_get_mem(GC_word bytes);
+#   define GET_MEM(bytes) (struct hblk *)GC_wince_get_mem(bytes)
+# elif defined(AMIGA) && defined(GC_AMIGA_FASTALLOC)
+    void *GC_amiga_get_mem(size_t size);
+#   define GET_MEM(bytes) HBLKPTR((size_t) \
+                          GC_amiga_get_mem((size_t)(bytes) + GC_page_size) \
+                          + GC_page_size-1)
+# elif defined(SN_TARGET_PS3)
+    void *ps3_get_mem(size_t size);
+#   define GET_MEM(bytes) (struct hblk*)ps3_get_mem(bytes)
+# elif defined(NAUT)
+#   include <nautilus/mm.h>
+#   define GET_MEM(bytes) HBLKPTR(malloc((size_t)(bytes) + GC_page_size) \
+                                          + GC_page_size-1)
+# else
+    ptr_t GC_unix_get_mem(GC_word bytes);
+#   define GET_MEM(bytes) (struct hblk *)GC_unix_get_mem(bytes)
+# endif
+#endif /* GC_PRIVATE_H */
+
+#endif /* GCCONFIG_H */
diff --git a/src/gc/bdwgc/include/private/msvc_dbg.h b/src/gc/bdwgc/include/private/msvc_dbg.h
new file mode 100644
index 0000000..1d3030a
--- /dev/null
+++ b/src/gc/bdwgc/include/private/msvc_dbg.h
@@ -0,0 +1,69 @@
+/*
+  Copyright (c) 2004-2005 Andrei Polushin
+
+  Permission is hereby granted, free of charge,  to any person obtaining a copy
+  of this software and associated documentation files (the "Software"), to deal
+  in the Software without restriction,  including without limitation the rights
+  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+  copies of the Software, and to permit persons to whom the Software is
+  furnished to do so, subject to the following conditions:
+
+  The above copyright notice and this permission notice shall be included in
+  all copies or substantial portions of the Software.
+
+  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+  THE SOFTWARE.
+*/
+#ifndef _MSVC_DBG_H
+#define _MSVC_DBG_H
+
+#include <stdlib.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#if !MSVC_DBG_DLL
+#define MSVC_DBG_EXPORT
+#elif MSVC_DBG_BUILD
+#define MSVC_DBG_EXPORT __declspec(dllexport)
+#else
+#define MSVC_DBG_EXPORT __declspec(dllimport)
+#endif
+
+#ifndef MAX_SYM_NAME
+#define MAX_SYM_NAME 2000
+#endif
+
+typedef void*  HANDLE;
+typedef struct _CONTEXT CONTEXT;
+
+MSVC_DBG_EXPORT size_t GetStackFrames(size_t skip, void* frames[], size_t maxFrames);
+MSVC_DBG_EXPORT size_t GetStackFramesFromContext(HANDLE hProcess, HANDLE hThread, CONTEXT* context, size_t skip, void* frames[], size_t maxFrames);
+
+MSVC_DBG_EXPORT size_t GetModuleNameFromAddress(void* address, char* moduleName, size_t size);
+MSVC_DBG_EXPORT size_t GetModuleNameFromStack(size_t skip, char* moduleName, size_t size);
+
+MSVC_DBG_EXPORT size_t GetSymbolNameFromAddress(void* address, char* symbolName, size_t size, size_t* offsetBytes);
+MSVC_DBG_EXPORT size_t GetSymbolNameFromStack(size_t skip, char* symbolName, size_t size, size_t* offsetBytes);
+
+MSVC_DBG_EXPORT size_t GetFileLineFromAddress(void* address, char* fileName, size_t size, size_t* lineNumber, size_t* offsetBytes);
+MSVC_DBG_EXPORT size_t GetFileLineFromStack(size_t skip, char* fileName, size_t size, size_t* lineNumber, size_t* offsetBytes);
+
+MSVC_DBG_EXPORT size_t GetDescriptionFromAddress(void* address, const char* format, char* description, size_t size);
+MSVC_DBG_EXPORT size_t GetDescriptionFromStack(void*const frames[], size_t count, const char* format, char* description[], size_t size);
+
+/* Compatibility with <execinfo.h> */
+MSVC_DBG_EXPORT int    backtrace(void* addresses[], int count);
+MSVC_DBG_EXPORT char** backtrace_symbols(void*const addresses[], int count);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif/*_MSVC_DBG_H*/
diff --git a/src/gc/bdwgc/include/private/pthread_stop_world.h b/src/gc/bdwgc/include/private/pthread_stop_world.h
new file mode 100644
index 0000000..cb67d23
--- /dev/null
+++ b/src/gc/bdwgc/include/private/pthread_stop_world.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_PTHREAD_STOP_WORLD_H
+#define GC_PTHREAD_STOP_WORLD_H
+
+struct thread_stop_info {
+#   ifndef GC_OPENBSD_THREADS
+      word last_stop_count;     /* GC_last_stop_count value when thread */
+                                /* last successfully handled a suspend  */
+                                /* signal.                              */
+#   endif
+
+    ptr_t stack_ptr;            /* Valid only when stopped.             */
+
+#   ifdef NACL
+      /* Grab NACL_GC_REG_STORAGE_SIZE pointers off the stack when      */
+      /* going into a syscall.  20 is more than we need, but it's an    */
+      /* overestimate in case the instrumented function uses any callee */
+      /* saved registers, they may be pushed to the stack much earlier. */
+      /* Also, on amd64 'push' puts 8 bytes on the stack even though    */
+      /* our pointers are 4 bytes.                                      */
+#     define NACL_GC_REG_STORAGE_SIZE 20
+      ptr_t reg_storage[NACL_GC_REG_STORAGE_SIZE];
+#   endif
+};
+
+GC_INNER void GC_stop_init(void);
+
+#endif
diff --git a/src/gc/bdwgc/include/private/pthread_support.h b/src/gc/bdwgc/include/private/pthread_support.h
new file mode 100644
index 0000000..656f49a
--- /dev/null
+++ b/src/gc/bdwgc/include/private/pthread_support.h
@@ -0,0 +1,148 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#ifndef GC_PTHREAD_SUPPORT_H
+#define GC_PTHREAD_SUPPORT_H
+
+#include "private/gc_priv.h"
+
+#if (defined(GC_PTHREADS) && !defined(GC_WIN32_THREADS)) || defined(NAUT_THREADS)
+
+#ifdef NAUT_THREADS
+# include <nautilus/thread.h>
+# define pthread_t nk_thread_id_t
+# define THREAD_LOCAL_ALLOC
+# include "private/pthread_stop_world.h"
+#endif
+
+#if defined(GC_DARWIN_THREADS)
+# include "private/darwin_stop_world.h"
+#else
+# include "private/pthread_stop_world.h"
+#endif
+
+#ifdef THREAD_LOCAL_ALLOC
+# include "thread_local_alloc.h"
+#endif
+
+/* We use the allocation lock to protect thread-related data structures. */
+
+/* The set of all known threads.  We intercept thread creation and      */
+/* joins.                                                               */
+/* Protected by allocation/GC lock.                                     */
+/* Some of this should be declared volatile, but that's inconsistent    */
+/* with some library routine declarations.                              */
+//typedef struct GC_Thread_Rep {
+//struct GC_Thread_Rep * next;  /* More recently allocated threads    */
+/*                                   /\* with a given pthread id come       *\/ */
+/*                                   /\* first.  (All but the first are     *\/ */
+/*                                   /\* guaranteed to be dead, but we may  *\/ */
+/*                                   /\* not yet have registered the join.) *\/ */
+
+/*     pthread_t id; */
+/* #   ifdef PLATFORM_ANDROID */
+/*       pid_t kernel_id; */
+/* #   endif */
+/*     /\* Extra bookkeeping information the stopping code uses *\/ */
+/*     struct thread_stop_info stop_info; */
+
+/*     unsigned char flags; */
+/* #       define FINISHED 1       /\* Thread has exited.                   *\/ */
+/* #       define DETACHED 2       /\* Thread is treated as detached.       *\/ */
+/*                                 /\* Thread may really be detached, or    *\/ */
+/*                                 /\* it may have been explicitly          *\/ */
+/*                                 /\* registered, in which case we can     *\/ */
+/*                                 /\* deallocate its GC_Thread_Rep once    *\/ */
+/*                                 /\* it unregisters itself, since it      *\/ */
+/*                                 /\* may not return a GC pointer.         *\/ */
+/* #       define MAIN_THREAD 4    /\* True for the original thread only.   *\/ */
+/* #       define SUSPENDED_EXT 8  /\* Thread was suspended externally      *\/ */
+/*                                 /\* (this is not used by the unmodified  *\/ */
+/*                                 /\* GC itself at present).               *\/ */
+/* #       define DISABLED_GC 0x10 /\* Collections are disabled while the   *\/ */
+/*                                 /\* thread is exiting.                   *\/ */
+
+/*     unsigned char thread_blocked; */
+/*                                 /\* Protected by GC lock.                *\/ */
+/*                                 /\* Treated as a boolean value.  If set, *\/ */
+/*                                 /\* thread will acquire GC lock before   *\/ */
+/*                                 /\* doing any pointer manipulations, and *\/ */
+/*                                 /\* has set its SP value.  Thus it does  *\/ */
+/*                                 /\* not need to be sent a signal to stop *\/ */
+/*                                 /\* it.                                  *\/ */
+
+/*     unsigned short finalizer_skipped; */
+/*     unsigned char finalizer_nested; */
+/*                                 /\* Used by GC_check_finalizer_nested()  *\/ */
+/*                                 /\* to minimize the level of recursion   *\/ */
+/*                                 /\* when a client finalizer allocates    *\/ */
+/*                                 /\* memory (initially both are 0).       *\/ */
+
+/*     ptr_t stack_end;            /\* Cold end of the stack (except for    *\/ */
+/*                                 /\* main thread).                        *\/ */
+/* #   if defined(GC_DARWIN_THREADS) && !defined(DARWIN_DONT_PARSE_STACK) */
+/*       ptr_t topOfStack;         /\* Result of GC_FindTopOfStack(0);      *\/ */
+/*                                 /\* valid only if the thread is blocked; *\/ */
+/*                                 /\* non-NULL value means already set.    *\/ */
+/* #   endif */
+/* #   ifdef IA64 */
+/*         ptr_t backing_store_end; */
+/*         ptr_t backing_store_ptr; */
+/* #   endif */
+
+/*     struct GC_traced_stack_sect_s *traced_stack_sect; */
+/*                         /\* Points to the "frame" data held in stack by  *\/ */
+/*                         /\* the innermost GC_call_with_gc_active() of    *\/ */
+/*                         /\* this thread.  May be NULL.                   *\/ */
+
+/*     void * status;              /\* The value returned from the thread.  *\/ */
+/*                                 /\* Used only to avoid premature         *\/ */
+/*                                 /\* reclamation of any data it might     *\/ */
+/*                                 /\* reference.                           *\/ */
+/*                                 /\* This is unfortunately also the       *\/ */
+/*                                 /\* reason we need to intercept join     *\/ */
+/*                                 /\* and detach.                          *\/ */
+
+/* #   ifdef THREAD_LOCAL_ALLOC */
+/*         struct thread_local_freelists tlfs; */
+/* #   endif */
+/* } * GC_thread; */
+
+//# define THREAD_TABLE_SZ 256    /* Must be power of 2   */
+//GC_EXTERN volatile GC_thread GC_threads[THREAD_TABLE_SZ];
+
+//GC_EXTERN GC_bool GC_thr_initialized;
+
+
+GC_INNER nk_thread_t * GC_lookup_thread();
+
+
+GC_EXTERN GC_bool GC_in_thread_creation;
+        /* We may currently be in thread creation or destruction.       */
+        /* Only set to TRUE while allocation lock is held.              */
+        /* When set, it is OK to run GC from unknown thread.            */
+
+
+
+GC_INNER nk_thread_t* GC_start_rtn_prepare_thread(void *(**pstart)(void *),
+                                        void **pstart_arg,
+                                        struct GC_stack_base *sb, void *arg);
+GC_INNER void GC_thread_exit_proc(void *);
+
+#endif /* GC_PTHREADS && !GC_WIN32_THREADS */
+
+#endif /* GC_PTHREAD_SUPPORT_H */
diff --git a/src/gc/bdwgc/include/private/specific.h b/src/gc/bdwgc/include/private/specific.h
new file mode 100644
index 0000000..53a8d2f
--- /dev/null
+++ b/src/gc/bdwgc/include/private/specific.h
@@ -0,0 +1,98 @@
+/*
+ * This is a reimplementation of a subset of the pthread_getspecific/setspecific
+ * interface. This appears to outperform the standard linuxthreads one
+ * by a significant margin.
+ * The major restriction is that each thread may only make a single
+ * pthread_setspecific call on a single key.  (The current data structure
+ * doesn't really require that.  The restriction should be easily removable.)
+ * We don't currently support the destruction functions, though that
+ * could be done.
+ * We also currently assume that only one pthread_setspecific call
+ * can be executed at a time, though that assumption would be easy to remove
+ * by adding a lock.
+ */
+
+/* #ifdef NAUT */
+
+/* //# define AO_t int */
+/* //# define pthread_mutex_t int */
+/* //# define GC_approx_sp(void) */
+/* #else  */
+#include <errno.h>
+#include "atomic_ops.h"
+//#endif
+
+
+/* Called during key creation or setspecific.           */
+/* For the GC we already hold lock.                     */
+/* Currently allocated objects leak on thread exit.     */
+/* That's hard to fix, but OK if we allocate garbage    */
+/* collected memory.                                    */
+#define MALLOC_CLEAR(n) GC_INTERNAL_MALLOC(n, NORMAL)
+#define PREFIXED(name) GC_##name
+
+#define TS_CACHE_SIZE 1024
+#define CACHE_HASH(n) (((((long)n) >> 8) ^ (long)n) & (TS_CACHE_SIZE - 1))
+#define TS_HASH_SIZE 1024
+#define HASH(n) (((((long)n) >> 8) ^ (long)n) & (TS_HASH_SIZE - 1))
+
+/* An entry describing a thread-specific value for a given thread.      */
+/* All such accessible structures preserve the invariant that if either */
+/* thread is a valid pthread id or qtid is a valid "quick tread id"     */
+/* for a thread, then value holds the corresponding thread specific     */
+/* value.  This invariant must be preserved at ALL times, since         */
+/* asynchronous reads are allowed.                                      */
+typedef struct thread_specific_entry {
+        volatile AO_t qtid;     /* quick thread id, only for cache */
+        void * value;
+        struct thread_specific_entry *next;
+        pthread_t thread;
+} tse;
+
+/* We represent each thread-specific datum as two tables.  The first is */
+/* a cache, indexed by a "quick thread identifier".  The "quick" thread */
+/* identifier is an easy to compute value, which is guaranteed to       */
+/* determine the thread, though a thread may correspond to more than    */
+/* one value.  We typically use the address of a page in the stack.     */
+/* The second is a hash table, indexed by pthread_self().  It is used   */
+/* only as a backup.                                                    */
+
+/* Return the "quick thread id".  Default version.  Assumes page size,  */
+/* or at least thread stack separation, is at least 4K.                 */
+/* Must be defined so that it never returns 0.  (Page 0 can't really be */
+/* part of any stack, since that would make 0 a valid stack pointer.)   */
+#define quick_thread_id() (((unsigned long)GC_approx_sp()) >> 12)
+
+#define INVALID_QTID ((unsigned long)0)
+#define INVALID_THREADID ((pthread_t)0)
+
+typedef struct thread_specific_data {
+    tse * volatile cache[TS_CACHE_SIZE];
+                        /* A faster index to the hash table */
+    tse * hash[TS_HASH_SIZE];
+    pthread_mutex_t lock;
+} tsd;
+
+typedef tsd * PREFIXED(key_t);
+
+int PREFIXED(key_create) (tsd ** key_ptr, void (* destructor)(void *));
+int PREFIXED(setspecific) (tsd * key, void * value);
+void PREFIXED(remove_specific) (tsd * key);
+
+/* An internal version of getspecific that assumes a cache miss.        */
+void * PREFIXED(slow_getspecific) (tsd * key, unsigned long qtid,
+                                   tse * volatile * cache_entry);
+
+/* GC_INLINE is defined in gc_priv.h. */
+GC_INLINE void * PREFIXED(getspecific) (tsd * key)
+{
+    unsigned long qtid = quick_thread_id();
+    unsigned hash_val = CACHE_HASH(qtid);
+    tse * volatile * entry_ptr = key -> cache + hash_val;
+    tse * entry = *entry_ptr;   /* Must be loaded only once.    */
+    if (EXPECT(entry -> qtid == qtid, TRUE)) {
+      GC_ASSERT(entry -> thread == pthread_self());
+      return entry -> value;
+    }
+    return PREFIXED(slow_getspecific) (key, qtid, entry_ptr);
+}
diff --git a/src/gc/bdwgc/include/private/thread_local_alloc.h b/src/gc/bdwgc/include/private/thread_local_alloc.h
new file mode 100644
index 0000000..d015fcc
--- /dev/null
+++ b/src/gc/bdwgc/include/private/thread_local_alloc.h
@@ -0,0 +1,178 @@
+/*
+ * Copyright (c) 2000-2005 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* Included indirectly from a thread-library-specific file.     */
+/* This is the interface for thread-local allocation, whose     */
+/* implementation is mostly thread-library-independent.         */
+/* Here we describe only the interface that needs to be known   */
+/* and invoked from the thread support layer;  the actual       */
+/* implementation also exports GC_malloc and friends, which     */
+/* are declared in gc.h.                                        */
+
+#ifndef GC_THREAD_LOCAL_ALLOC_H
+#define GC_THREAD_LOCAL_ALLOC_H
+
+#include "private/gc_priv.h"
+
+#ifdef THREAD_LOCAL_ALLOC
+
+#include "gc_inline.h"
+
+#if defined(USE_HPUX_TLS)
+# error USE_HPUX_TLS macro was replaced by USE_COMPILER_TLS
+#endif
+
+/* #if !defined(USE_PTHREAD_SPECIFIC) && !defined(USE_WIN32_SPECIFIC) \ */
+/*     && !defined(USE_WIN32_COMPILER_TLS) && !defined(USE_COMPILER_TLS) \ */
+/*     && !defined(USE_CUSTOM_SPECIFIC) */
+/* # if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32) */
+/* #   if defined(__GNUC__) /\* Fixed for versions past 2.95? *\/ \ */
+/*        || defined(MSWINCE) */
+/* #     define USE_WIN32_SPECIFIC */
+/* #   else */
+/* #     define USE_WIN32_COMPILER_TLS */
+/* #   endif /\* !GNU *\/ */
+/* # elif defined(LINUX) && !defined(ARM32) && !defined(AVR32) \ */
+/*        && (__GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >=3)) */
+/* #   define USE_COMPILER_TLS */
+/* # elif defined(GC_DGUX386_THREADS) || defined(GC_OSF1_THREADS) \ */
+/*        || defined(GC_DARWIN_THREADS) || defined(GC_AIX_THREADS) \ */
+/*        || defined(GC_NETBSD_THREADS) || defined(GC_RTEMS_PTHREADS) */
+/* #   define USE_PTHREAD_SPECIFIC */
+/* # elif defined(GC_HPUX_THREADS) */
+/* #   ifdef __GNUC__ */
+/* #    define USE_PTHREAD_SPECIFIC */
+/*      /\* Empirically, as of gcc 3.3, USE_COMPILER_TLS doesn't work.      *\/ */
+/* #   else */
+/* #    define USE_COMPILER_TLS */
+/* #   endif */
+/* # else */
+/* #   define USE_CUSTOM_SPECIFIC  /\* Use our own. *\/ */
+/* # endif */
+/* #endif */
+
+#ifndef NAUT
+#include <stdlib.h>
+#endif
+
+#ifndef TINY_FREELISTS
+#define TINY_FREELISTS GC_TINY_FREELISTS
+#endif
+
+
+#ifndef NAUT
+/* One of these should be declared as the tlfs field in the     */
+/* structure pointed to by a GC_thread.                         */
+/* typedef struct thread_local_freelists { */
+/*   void * ptrfree_freelists[TINY_FREELISTS]; */
+/*   void * normal_freelists[TINY_FREELISTS]; */
+/* # ifdef GC_GCJ_SUPPORT */
+/*     void * gcj_freelists[TINY_FREELISTS]; */
+/* #   define ERROR_FL ((void *)(word)-1) */
+/*         /\* Value used for gcj_freelists[-1]; allocation is      *\/ */
+/*         /\* erroneous.                                           *\/ */
+/* # endif */
+/*   /\* Free lists contain either a pointer or a small count       *\/ */
+/*   /\* reflecting the number of granules allocated at that        *\/ */
+/*   /\* size.                                                      *\/ */
+/*   /\* 0 ==> thread-local allocation in use, free list            *\/ */
+/*   /\*       empty.                                               *\/ */
+/*   /\* > 0, <= DIRECT_GRANULES ==> Using global allocation,       *\/ */
+/*   /\*       too few objects of this size have been               *\/ */
+/*   /\*       allocated by this thread.                            *\/ */
+/*   /\* >= HBLKSIZE  => pointer to nonempty free list.             *\/ */
+/*   /\* > DIRECT_GRANULES, < HBLKSIZE ==> transition to            *\/ */
+/*   /\*    local alloc, equivalent to 0.                           *\/ */
+/* # define DIRECT_GRANULES (HBLKSIZE/GRANULE_BYTES) */
+/*         /\* Don't use local free lists for up to this much       *\/ */
+/*         /\* allocation.                                          *\/ */
+/* } *GC_tlfs; */
+
+#endif
+
+#if defined(USE_PTHREAD_SPECIFIC)
+# define GC_getspecific pthread_getspecific
+# define GC_setspecific pthread_setspecific
+# define GC_key_create pthread_key_create
+# define GC_remove_specific(key)  /* No need for cleanup on exit. */
+  typedef pthread_key_t GC_key_t;
+#elif defined(USE_COMPILER_TLS) || defined(USE_WIN32_COMPILER_TLS)
+# define GC_getspecific(x) (x)
+# define GC_setspecific(key, v) ((key) = (v), 0)
+# define GC_key_create(key, d) 0
+# define GC_remove_specific(key)  /* No need for cleanup on exit. */
+  typedef nk_tls_key_t GC_key_t;
+#elif defined(USE_WIN32_SPECIFIC)
+# ifndef WIN32_LEAN_AND_MEAN
+#   define WIN32_LEAN_AND_MEAN 1
+# endif
+# define NOSERVICE
+# include <windows.h>
+# define GC_getspecific TlsGetValue
+# define GC_setspecific(key, v) !TlsSetValue(key, v)
+        /* We assume 0 == success, msft does the opposite.      */
+# ifndef TLS_OUT_OF_INDEXES
+        /* this is currently missing in WinCE   */
+#   define TLS_OUT_OF_INDEXES (DWORD)0xFFFFFFFF
+# endif
+# define GC_key_create(key, d) \
+        ((d) != 0 || (*(key) = TlsAlloc()) == TLS_OUT_OF_INDEXES ? -1 : 0)
+# define GC_remove_specific(key)  /* No need for cleanup on exit. */
+        /* Need TlsFree on process exit/detach?   */
+  typedef DWORD GC_key_t;
+#elif defined(USE_CUSTOM_SPECIFIC)
+# include "private/specific.h"
+
+#elif defined(NAUT_THREADS)
+
+// Needs implementation
+# define GC_getspecific(x) nk_tls_get(x)
+# define GC_setspecific(key, v) nk_tls_set(key, v)
+# define GC_key_create(key, d) nk_tls_key_create(key, d)
+// nk_tls_key_create(&local_proc_key, thread_proc_free);
+# define GC_remove_specific(key)  /* No need for cleanup on exit. */
+  typedef nk_tls_key_t GC_key_t;
+
+#else
+# error implement me
+#endif
+
+/* Each thread structure must be initialized.   */
+/* This call must be made from the new thread.  */
+/* Caller holds allocation lock.                */
+GC_INNER void GC_init_thread_local(GC_tlfs p);
+
+/* Called when a thread is unregistered, or exits.      */
+/* We hold the allocator lock.                          */
+GC_INNER void GC_destroy_thread_local(GC_tlfs p);
+
+/* The thread support layer must arrange to mark thread-local   */
+/* free lists explicitly, since the link field is often         */
+/* invisible to the marker.  It knows how to find all threads;  */
+/* we take care of an individual thread freelist structure.     */
+GC_INNER void GC_mark_thread_local_fls_for(GC_tlfs p);
+
+extern
+#if defined(USE_COMPILER_TLS)
+  __thread
+#elif defined(USE_WIN32_COMPILER_TLS)
+  __declspec(thread)
+#endif
+//GC_key_t GC_thread_key;
+/* This is set up by the thread_local_alloc implementation.  No need    */
+/* for cleanup on thread exit.  But the thread support layer makes sure */
+/* that GC_thread_key is traced, if necessary.                          */
+
+#endif /* THREAD_LOCAL_ALLOC */
+
+#endif /* GC_THREAD_LOCAL_ALLOC_H */
diff --git a/src/gc/bdwgc/include/test.h b/src/gc/bdwgc/include/test.h
new file mode 100644
index 0000000..5284770
--- /dev/null
+++ b/src/gc/bdwgc/include/test.h
@@ -0,0 +1,15 @@
+
+
+#ifndef __BDWGC_TEST__
+#define __BDWGC_TEST__
+
+// Internal tests
+int huge_test();
+int realloc_test();
+int leak_test();
+
+int bdwgc_test_gc(); // Run all internal tets
+int bdwgc_test_leak_detector(); // Run all internal tets
+
+
+#endif
diff --git a/src/gc/bdwgc/include/weakpointer.h b/src/gc/bdwgc/include/weakpointer.h
new file mode 100644
index 0000000..84906b0
--- /dev/null
+++ b/src/gc/bdwgc/include/weakpointer.h
@@ -0,0 +1,221 @@
+#ifndef	_weakpointer_h_
+#define	_weakpointer_h_
+
+/****************************************************************************
+
+WeakPointer and CleanUp
+
+    Copyright (c) 1991 by Xerox Corporation.  All rights reserved.
+
+    THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+    OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+
+    Permission is hereby granted to copy this code for any purpose,
+    provided the above notices are retained on all copies.
+
+    Last modified on Mon Jul 17 18:16:01 PDT 1995 by ellis
+
+****************************************************************************/
+
+/****************************************************************************
+
+WeakPointer
+
+A weak pointer is a pointer to a heap-allocated object that doesn't
+prevent the object from being garbage collected. Weak pointers can be
+used to track which objects haven't yet been reclaimed by the
+collector. A weak pointer is deactivated when the collector discovers
+its referent object is unreachable by normal pointers (reachability
+and deactivation are defined more precisely below). A deactivated weak
+pointer remains deactivated forever.
+
+****************************************************************************/
+
+
+template< class T > class WeakPointer {
+public:
+
+WeakPointer( T* t = 0 )
+    /* Constructs a weak pointer for *t. t may be null. It is an error
+       if t is non-null and *t is not a collected object. */
+    {impl = _WeakPointer_New( t );}
+
+T* Pointer()
+    /* wp.Pointer() returns a pointer to the referent object of wp or
+       null if wp has been deactivated (because its referent object
+       has been discovered unreachable by the collector). */
+    {return (T*) _WeakPointer_Pointer( this->impl );}
+
+int operator==( WeakPointer< T > wp2 )
+    /* Given weak pointers wp1 and wp2, if wp1 == wp2, then wp1 and
+       wp2 refer to the same object. If wp1 != wp2, then either wp1
+       and wp2 don't refer to the same object, or if they do, one or
+       both of them has been deactivated. (Note: If objects t1 and t2
+       are never made reachable by their clean-up functions, then
+       WeakPointer<T>(t1) == WeakPointer<T>(t2) if and only t1 == t2.) */
+    {return _WeakPointer_Equal( this->impl, wp2.impl );}
+
+int Hash()
+    /* Returns a hash code suitable for use by multiplicative- and
+       division-based hash tables. If wp1 == wp2, then wp1.Hash() ==
+       wp2.Hash(). */
+    {return _WeakPointer_Hash( this->impl );}
+
+private:
+void* impl;
+};
+
+/*****************************************************************************
+
+CleanUp
+
+A garbage-collected object can have an associated clean-up function
+that will be invoked some time after the collector discovers the
+object is unreachable via normal pointers. Clean-up functions can be
+used to release resources such as open-file handles or window handles
+when their containing objects become unreachable.  If a C++ object has
+a non-empty explicit destructor (i.e. it contains programmer-written
+code), the destructor will be automatically registered as the object's
+initial clean-up function.
+
+There is no guarantee that the collector will detect every unreachable
+object (though it will find almost all of them). Clients should not
+rely on clean-up to cause some action to occur immediately -- clean-up
+is only a mechanism for improving resource usage.
+
+Every object with a clean-up function also has a clean-up queue. When
+the collector finds the object is unreachable, it enqueues it on its
+queue. The clean-up function is applied when the object is removed
+from the queue. By default, objects are enqueued on the garbage
+collector's queue, and the collector removes all objects from its
+queue after each collection. If a client supplies another queue for
+objects, it is his responsibility to remove objects (and cause their
+functions to be called) by polling it periodically.
+
+Clean-up queues allow clean-up functions accessing global data to
+synchronize with the main program. Garbage collection can occur at any
+time, and clean-ups invoked by the collector might access data in an
+inconsistent state. A client can control this by defining an explicit
+queue for objects and polling it at safe points.
+
+The following definitions are used by the specification below:
+
+Given a pointer t to a collected object, the base object BO(t) is the
+value returned by new when it created the object. (Because of multiple
+inheritance, t and BO(t) may not be the same address.)
+
+A weak pointer wp references an object *t if BO(wp.Pointer()) ==
+BO(t).
+
+***************************************************************************/
+
+template< class T, class Data > class CleanUp {
+public:
+
+static void Set( T* t, void c( Data* d, T* t ), Data* d = 0 )
+    /* Sets the clean-up function of object BO(t) to be <c, d>,
+       replacing any previously defined clean-up function for BO(t); c
+       and d can be null, but t cannot. Sets the clean-up queue for
+       BO(t) to be the collector's queue. When t is removed from its
+       clean-up queue, its clean-up will be applied by calling c(d,
+       t). It is an error if *t is not a collected object. */ 
+       {_CleanUp_Set( t, c, d );}
+
+static void Call( T* t )
+    /* Sets the new clean-up function for BO(t) to be null and, if the
+       old one is non-null, calls it immediately, even if BO(t) is
+       still reachable. Deactivates any weak pointers to BO(t). */
+       {_CleanUp_Call( t );}
+
+class Queue {public:
+    Queue()
+        /* Constructs a new queue. */
+            {this->head = _CleanUp_Queue_NewHead();}
+
+    void Set( T* t )
+        /* q.Set(t) sets the clean-up queue of BO(t) to be q. */
+            {_CleanUp_Queue_Set( this->head, t );}
+
+    int Call()
+        /* If q is non-empty, q.Call() removes the first object and
+           calls its clean-up function; does nothing if q is
+           empty. Returns true if there are more objects in the
+           queue. */
+           {return _CleanUp_Queue_Call( this->head );}
+
+    private:
+    void* head;
+    };
+};
+
+/**********************************************************************
+
+Reachability and Clean-up
+
+An object O is reachable if it can be reached via a non-empty path of
+normal pointers from the registers, stacks, global variables, or an
+object with a non-null clean-up function (including O itself),
+ignoring pointers from an object to itself.
+
+This definition of reachability ensures that if object B is accessible
+from object A (and not vice versa) and if both A and B have clean-up
+functions, then A will always be cleaned up before B. Note that as
+long as an object with a clean-up function is contained in a cycle of
+pointers, it will always be reachable and will never be cleaned up or
+collected.
+
+When the collector finds an unreachable object with a null clean-up
+function, it atomically deactivates all weak pointers referencing the
+object and recycles its storage. If object B is accessible from object
+A via a path of normal pointers, A will be discovered unreachable no
+later than B, and a weak pointer to A will be deactivated no later
+than a weak pointer to B.
+
+When the collector finds an unreachable object with a non-null
+clean-up function, the collector atomically deactivates all weak
+pointers referencing the object, redefines its clean-up function to be
+null, and enqueues it on its clean-up queue. The object then becomes
+reachable again and remains reachable at least until its clean-up
+function executes.
+
+The clean-up function is assured that its argument is the only
+accessible pointer to the object. Nothing prevents the function from
+redefining the object's clean-up function or making the object
+reachable again (for example, by storing the pointer in a global
+variable).
+
+If the clean-up function does not make its object reachable again and
+does not redefine its clean-up function, then the object will be
+collected by a subsequent collection (because the object remains
+unreachable and now has a null clean-up function). If the clean-up
+function does make its object reachable again and a clean-up function
+is subsequently redefined for the object, then the new clean-up
+function will be invoked the next time the collector finds the object
+unreachable.
+
+Note that a destructor for a collected object cannot safely redefine a
+clean-up function for its object, since after the destructor executes,
+the object has been destroyed into "raw memory". (In most
+implementations, destroying an object mutates its vtbl.)
+
+Finally, note that calling delete t on a collected object first
+deactivates any weak pointers to t and then invokes its clean-up
+function (destructor).
+
+**********************************************************************/
+
+extern "C" {
+    void* _WeakPointer_New( void* t );
+    void* _WeakPointer_Pointer( void* wp );
+    int _WeakPointer_Equal( void* wp1, void* wp2 );
+    int _WeakPointer_Hash( void* wp );
+    void _CleanUp_Set( void* t, void (*c)( void* d, void* t ), void* d );
+    void _CleanUp_Call( void* t );
+    void* _CleanUp_Queue_NewHead ();
+    void _CleanUp_Queue_Set( void* h, void* t );
+    int _CleanUp_Queue_Call( void* h );
+}
+
+#endif /* _weakpointer_h_ */
+
+
diff --git a/src/gc/bdwgc/libatomic_ops/Makefile b/src/gc/bdwgc/libatomic_ops/Makefile
new file mode 100644
index 0000000..88e1c87
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/Makefile
@@ -0,0 +1,21 @@
+CFLAGS += -Isrc/gc/bdwgc/libatomic_ops \
+	-Isrc/gc/bdwgc/libatomic_ops/atomic_ops \
+	-Isrc/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps \
+	-Isrc/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc \
+	-UHAVE_GETPAGESIZE \
+	-UHAVE_INTTYPES_H \
+	-UHAVE_MEMORY_H \
+	-UHAVE_MMAP \
+	-UHAVE_STDINT_H \
+	-UHAVE_STDLIB_H \
+	-UHAVE_STRINGS_H \
+	-UHAVE_SYS_PARAM_H \
+	-UHAVE_SYS_STAT_H \
+	-UHAVE_SYS_TYPES_H \
+	-UHAVE_UNISTD_H \
+	-USTDC_HEADERS \
+	-U_PTHREADS \
+	-U_REENTRANT \
+	-DNAUT
+obj-y += atomic_ops.o 
+
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops.c b/src/gc/bdwgc/libatomic_ops/atomic_ops.c
new file mode 100644
index 0000000..732563d
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops.c
@@ -0,0 +1,270 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Initialized data and out-of-line functions to support atomic_ops.h
+ * go here.  Currently this is needed only for pthread-based atomics
+ * emulation, or for compare-and-swap emulation.
+ * Pthreads emulation isn't useful on a native Windows platform, and
+ * cas emulation is not needed.  Thus we skip this on Windows.
+ */
+
+#if defined(HAVE_CONFIG_H)
+# include "config.h"
+#endif
+
+#if defined(__native_client__) && !defined(AO_USE_NO_SIGNALS) \
+    && !defined(AO_USE_NANOSLEEP)
+  /* Since NaCl is not recognized by configure yet, we do it here.      */
+# define AO_USE_NO_SIGNALS
+# define AO_USE_NANOSLEEP
+#endif
+
+
+#ifdef NAUT
+# include <nautilus/naut_types.h>
+# include <nautilus/timer.h>
+# define AO_USE_NO_SIGNALS
+#endif
+
+#if defined(AO_USE_WIN32_PTHREADS) && !defined(AO_USE_NO_SIGNALS)
+# define AO_USE_NO_SIGNALS
+#endif
+
+#if !defined(_MSC_VER) && !defined(__MINGW32__) && !defined(__BORLANDC__) \
+    || defined(AO_USE_NO_SIGNALS)
+
+#undef AO_REQUIRE_CAS
+
+#include <pthread.h>
+
+#ifndef AO_USE_NO_SIGNALS
+# include <signal.h>
+#endif
+
+#ifdef AO_USE_NANOSLEEP
+  /* This requires _POSIX_TIMERS feature. */
+# include <sys/time.h>
+# include <time.h>
+#elif defined(AO_USE_WIN32_PTHREADS)
+# include <windows.h> /* for Sleep() */
+#elif defined(_HPUX_SOURCE)
+# include <sys/time.h>
+#else
+# include <sys/select.h>
+#endif
+
+#include "atomic_ops.h"  /* Without cas emulation! */
+
+#ifndef AO_HAVE_double_t
+# include "atomic_ops/sysdeps/standard_ao_double_t.h"
+#endif
+
+/*
+ * Lock for pthreads-based implementation.
+ */
+
+pthread_mutex_t AO_pt_lock = PTHREAD_MUTEX_INITIALIZER;
+
+/*
+ * Out of line compare-and-swap emulation based on test and set.
+ *
+ * We use a small table of locks for different compare_and_swap locations.
+ * Before we update perform a compare-and-swap, we grab the corresponding
+ * lock.  Different locations may hash to the same lock, but since we
+ * never acquire more than one lock at a time, this can't deadlock.
+ * We explicitly disable signals while we perform this operation.
+ *
+ * FIXME: We should probably also support emulation based on Lamport
+ * locks, since we may not have test_and_set either.
+ */
+#define AO_HASH_SIZE 16
+
+#define AO_HASH(x) (((unsigned long)(x) >> 12) & (AO_HASH_SIZE-1))
+
+AO_TS_t AO_locks[AO_HASH_SIZE] = {
+  AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER,
+  AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER,
+  AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER,
+  AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER, AO_TS_INITIALIZER,
+};
+
+static AO_T dummy = 1;
+
+/* Spin for 2**n units. */
+void AO_spin(int n)
+{
+  int i;
+  AO_T j = AO_load(&dummy);
+
+  for (i = 0; i < (2 << n); ++i)
+    {
+       j *= 5;
+       j -= 4;
+    }
+  AO_store(&dummy, j);
+}
+
+
+void AO_pause(int n)
+{
+  if (n < 12)
+    AO_spin(n);
+  else
+    {
+#     ifdef AO_USE_NANOSLEEP
+        struct timespec ts;
+        ts.tv_sec = 0;
+        ts.tv_nsec = (n > 28 ? 100000 * 1000 : 1 << (n - 2));
+        nanosleep(&ts, 0);
+#     elif defined(AO_USE_WIN32_PTHREADS)
+        Sleep(n > 28 ? 100 : n < 22 ? 1 : 1 << (n - 22)); /* in millis */
+#     elif defined(NAUT)
+        nk_sleep(n > 28 ? 100000 * 1000 : 1 << (n - 2)); 
+#     else
+        struct timeval tv;
+        /* Short async-signal-safe sleep. */
+        tv.tv_sec = 0;
+        tv.tv_usec = n > 28 ? 100000 : 1 << (n - 12);
+        select(0, 0, 0, 0, &tv);
+#     endif
+    }
+}
+
+static void lock_ool(volatile AO_TS_t *l)
+{
+  int i = 0;
+
+  while (AO_test_and_set_acquire(l) == AO_TS_SET)
+    AO_pause(++i);
+}
+
+AO_INLINE void lock(volatile AO_TS_t *l)
+{
+  if (AO_test_and_set_acquire(l) == AO_TS_SET)
+    lock_ool(l);
+}
+
+AO_INLINE void unlock(volatile AO_TS_t *l)
+{
+  AO_CLEAR(l);
+}
+
+#ifndef AO_USE_NO_SIGNALS
+  static sigset_t all_sigs;
+  static volatile AO_t initialized = 0;
+  static volatile AO_TS_t init_lock = AO_TS_INITIALIZER;
+#endif
+
+int AO_compare_and_swap_emulation(volatile AO_t *addr, AO_t old,
+                                  AO_t new_val)
+{
+  AO_TS_t *my_lock = AO_locks + AO_HASH(addr);
+  int result;
+
+# ifndef AO_USE_NO_SIGNALS
+    sigset_t old_sigs;
+    if (!AO_load_acquire(&initialized))
+    {
+      lock(&init_lock);
+      if (!initialized) sigfillset(&all_sigs);
+      unlock(&init_lock);
+      AO_store_release(&initialized, 1);
+    }
+    sigprocmask(SIG_BLOCK, &all_sigs, &old_sigs);
+        /* Neither sigprocmask nor pthread_sigmask is 100%      */
+        /* guaranteed to work here.  Sigprocmask is not         */
+        /* guaranteed be thread safe, and pthread_sigmask       */
+        /* is not async-signal-safe.  Under linuxthreads,       */
+        /* sigprocmask may block some pthreads-internal         */
+        /* signals.  So long as we do that for short periods,   */
+        /* we should be OK.                                     */
+# endif
+  lock(my_lock);
+  if (*addr == old)
+    {
+      *addr = new_val;
+      result = 1;
+    }
+  else
+    result = 0;
+  unlock(my_lock);
+# ifndef AO_USE_NO_SIGNALS
+    sigprocmask(SIG_SETMASK, &old_sigs, NULL);
+# endif
+  return result;
+}
+
+int AO_compare_double_and_swap_double_emulation(volatile AO_double_t *addr,
+                                                AO_t old_val1, AO_t old_val2,
+                                                AO_t new_val1, AO_t new_val2)
+{
+  AO_TS_t *my_lock = AO_locks + AO_HASH(addr);
+  int result;
+
+# ifndef AO_USE_NO_SIGNALS
+    sigset_t old_sigs;
+    if (!AO_load_acquire(&initialized))
+    {
+      lock(&init_lock);
+      if (!initialized) sigfillset(&all_sigs);
+      unlock(&init_lock);
+      AO_store_release(&initialized, 1);
+    }
+    sigprocmask(SIG_BLOCK, &all_sigs, &old_sigs);
+        /* Neither sigprocmask nor pthread_sigmask is 100%      */
+        /* guaranteed to work here.  Sigprocmask is not         */
+        /* guaranteed be thread safe, and pthread_sigmask       */
+        /* is not async-signal-safe.  Under linuxthreads,       */
+        /* sigprocmask may block some pthreads-internal         */
+        /* signals.  So long as we do that for short periods,   */
+        /* we should be OK.                                     */
+# endif
+  lock(my_lock);
+  if (addr -> AO_val1 == old_val1 && addr -> AO_val2 == old_val2)
+    {
+      addr -> AO_val1 = new_val1;
+      addr -> AO_val2 = new_val2;
+      result = 1;
+    }
+  else
+    result = 0;
+  unlock(my_lock);
+# ifndef AO_USE_NO_SIGNALS
+    sigprocmask(SIG_SETMASK, &old_sigs, NULL);
+# endif
+  return result;
+}
+
+void AO_store_full_emulation(volatile AO_t *addr, AO_t val)
+{
+  AO_TS_t *my_lock = AO_locks + AO_HASH(addr);
+  lock(my_lock);
+  *addr = val;
+  unlock(my_lock);
+}
+
+#else /* Non-posix platform */
+
+extern int AO_non_posix_implementation_is_entirely_in_headers;
+
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops.h b/src/gc/bdwgc/libatomic_ops/atomic_ops.h
new file mode 100644
index 0000000..b4aadfa
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops.h
@@ -0,0 +1,389 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef ATOMIC_OPS_H
+
+#define ATOMIC_OPS_H
+
+#include <assert.h>
+#include <stddef.h>
+
+/* We define various atomic operations on memory in a           */
+/* machine-specific way.  Unfortunately, this is complicated    */
+/* by the fact that these may or may not be combined with       */
+/* various memory barriers.  Thus the actual operations we      */
+/* define have the form AO_<atomic-op>_<barrier>, for all       */
+/* plausible combinations of <atomic-op> and <barrier>.         */
+/* This of course results in a mild combinatorial explosion.    */
+/* To deal with it, we try to generate derived                  */
+/* definitions for as many of the combinations as we can, as    */
+/* automatically as possible.                                   */
+/*                                                              */
+/* Our assumption throughout is that the programmer will        */
+/* specify the least demanding operation and memory barrier     */
+/* that will guarantee correctness for the implementation.      */
+/* Our job is to find the least expensive way to implement it   */
+/* on the applicable hardware.  In many cases that will         */
+/* involve, for example, a stronger memory barrier, or a        */
+/* combination of hardware primitives.                          */
+/*                                                              */
+/* Conventions:                                                 */
+/* "plain" atomic operations are not guaranteed to include      */
+/* a barrier.  The suffix in the name specifies the barrier     */
+/* type.  Suffixes are:                                         */
+/* _release: Earlier operations may not be delayed past it.     */
+/* _acquire: Later operations may not move ahead of it.         */
+/* _read: Subsequent reads must follow this operation and       */
+/*        preceding reads.                                      */
+/* _write: Earlier writes precede both this operation and       */
+/*        later writes.                                         */
+/* _full: Ordered with respect to both earlier and later memops.*/
+/* _release_write: Ordered with respect to earlier writes.      */
+/* _acquire_read: Ordered with respect to later reads.          */
+/*                                                              */
+/* Currently we try to define the following atomic memory       */
+/* operations, in combination with the above barriers:          */
+/* AO_nop                                                       */
+/* AO_load                                                      */
+/* AO_store                                                     */
+/* AO_test_and_set (binary)                                     */
+/* AO_fetch_and_add                                             */
+/* AO_fetch_and_add1                                            */
+/* AO_fetch_and_sub1                                            */
+/* AO_or                                                        */
+/* AO_compare_and_swap                                          */
+/*                                                              */
+/* Note that atomicity guarantees are valid only if both        */
+/* readers and writers use AO_ operations to access the         */
+/* shared value, while ordering constraints are intended to     */
+/* apply all memory operations.  If a location can potentially  */
+/* be accessed simultaneously from multiple threads, and one of */
+/* those accesses may be a write access, then all such          */
+/* accesses to that location should be through AO_ primitives.  */
+/* However if AO_ operations enforce sufficient ordering to     */
+/* ensure that a location x cannot be accessed concurrently,    */
+/* or can only be read concurrently, then x can be accessed     */
+/* via ordinary references and assignments.                     */
+/*                                                              */
+/* Compare_and_exchange takes an address and an expected old    */
+/* value and a new value, and returns an int.  Nonzero          */
+/* indicates that it succeeded.                                 */
+/* Test_and_set takes an address, atomically replaces it by     */
+/* AO_TS_SET, and returns the prior value.                      */
+/* An AO_TS_t location can be reset with the                    */
+/* AO_CLEAR macro, which normally uses AO_store_release.        */
+/* AO_fetch_and_add takes an address and an AO_t increment      */
+/* value.  The AO_fetch_and_add1 and AO_fetch_and_sub1 variants */
+/* are provided, since they allow faster implementations on     */
+/* some hardware. AO_or atomically ors an AO_t value into a     */
+/* memory location, but does not provide access to the original.*/
+/*                                                              */
+/* We expect this list to grow slowly over time.                */
+/*                                                              */
+/* Note that AO_nop_full is a full memory barrier.              */
+/*                                                              */
+/* Note that if some data is initialized with                   */
+/*      data.x = ...; data.y = ...; ...                         */
+/*      AO_store_release_write(&data_is_initialized, 1)         */
+/* then data is guaranteed to be initialized after the test     */
+/*      if (AO_load_acquire_read(&data_is_initialized)) ...     */
+/* succeeds.  Furthermore, this should generate near-optimal    */
+/* code on all common platforms.                                */
+/*                                                              */
+/* All operations operate on unsigned AO_t, which               */
+/* is the natural word size, and usually unsigned long.         */
+/* It is possible to check whether a particular operation op    */
+/* is available on a particular platform by checking whether    */
+/* AO_HAVE_op is defined.  We make heavy use of these macros    */
+/* internally.                                                  */
+
+/* The rest of this file basically has three sections:          */
+/*                                                              */
+/* Some utility and default definitions.                        */
+/*                                                              */
+/* The architecture dependent section:                          */
+/* This defines atomic operations that have direct hardware     */
+/* support on a particular platform, mostly by including the    */
+/* appropriate compiler- and hardware-dependent file.           */
+/*                                                              */
+/* The synthesis section:                                       */
+/* This tries to define other atomic operations in terms of     */
+/* those that are explicitly available on the platform.         */
+/* This section is hardware independent.                        */
+/* We make no attempt to synthesize operations in ways that     */
+/* effectively introduce locks, except for the debugging/demo   */
+/* pthread-based implementation at the beginning.  A more       */
+/* realistic implementation that falls back to locks could be   */
+/* added as a higher layer.  But that would sacrifice           */
+/* usability from signal handlers.                              */
+/* The synthesis section is implemented almost entirely in      */
+/* atomic_ops/generalize.h.                                     */
+
+/* Some common defaults.  Overridden for some architectures.    */
+#define AO_t size_t
+
+/* The test_and_set primitive returns an AO_TS_VAL_t value.     */
+/* AO_TS_t is the type of an in-memory test-and-set location.   */
+
+#define AO_TS_INITIALIZER (AO_t)AO_TS_CLEAR
+
+/* Platform-dependent stuff:                                    */
+#if defined(__GNUC__) || defined(_MSC_VER) || defined(__INTEL_COMPILER) \
+        || defined(__DMC__) || defined(__WATCOMC__)
+# define AO_INLINE static __inline
+#elif defined(__sun)
+# define AO_INLINE static inline
+#else
+# define AO_INLINE static
+#endif
+
+#if defined(__GNUC__) && !defined(__INTEL_COMPILER)
+# define AO_compiler_barrier() __asm__ __volatile__("" : : : "memory")
+#elif defined(_MSC_VER) || defined(__DMC__) || defined(__BORLANDC__) \
+        || defined(__WATCOMC__)
+# if defined(_AMD64_) || defined(_M_X64) || _MSC_VER >= 1400
+#   if defined(_WIN32_WCE)
+/* #     include <cmnintrin.h> */
+#   elif defined(_MSC_VER)
+#     include <intrin.h>
+#   endif
+#   pragma intrinsic(_ReadWriteBarrier)
+#   define AO_compiler_barrier() _ReadWriteBarrier()
+        /* We assume this does not generate a fence instruction.        */
+        /* The documentation is a bit unclear.                          */
+# else
+#   define AO_compiler_barrier() __asm { }
+        /* The preceding implementation may be preferable here too.     */
+        /* But the documentation warns about VC++ 2003 and earlier.     */
+# endif
+#elif defined(__INTEL_COMPILER)
+# define AO_compiler_barrier() __memory_barrier() /* Too strong? IA64-only? */
+#elif defined(_HPUX_SOURCE)
+# if defined(__ia64)
+#   include <machine/sys/inline.h>
+#   define AO_compiler_barrier() _Asm_sched_fence()
+# else
+    /* FIXME - We dont know how to do this.  This is a guess.   */
+    /* And probably a bad one.                                  */
+    static volatile int AO_barrier_dummy;
+#   define AO_compiler_barrier() (void)(AO_barrier_dummy = AO_barrier_dummy)
+# endif
+#else
+  /* We conjecture that the following usually gives us the right        */
+  /* semantics or an error.                                             */
+# define AO_compiler_barrier() asm("")
+#endif
+
+#if defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/generic_pthread.h"
+#endif /* AO_USE_PTHREAD_DEFS */
+
+#if (defined(__CC_ARM) || defined(__ARMCC__)) && !defined(__GNUC__) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/armcc/arm_v6.h"
+# define AO_GENERALIZE_TWICE
+#endif
+
+#if defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS) \
+    && !defined(__INTEL_COMPILER)
+# if defined(__i386__)
+    /* We don't define AO_USE_SYNC_CAS_BUILTIN for x86 here because     */
+    /* it might require specifying additional options (like -march)     */
+    /* or additional link libraries (if -march is not specified).       */
+#   include "atomic_ops/sysdeps/gcc/x86.h"
+# endif /* __i386__ */
+# if defined(__x86_64__)
+#   if __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 2)
+      /* It is safe to use __sync CAS built-in on this architecture.    */
+#     define AO_USE_SYNC_CAS_BUILTIN
+#   endif
+#   ifdef __ILP32__
+#     ifndef AO_USE_PENTIUM4_INSTRS
+#       define AO_USE_PENTIUM4_INSTRS
+#     endif
+#     include "atomic_ops/sysdeps/gcc/x86.h"
+#   else
+#     include "atomic_ops/sysdeps/gcc/x86_64.h"
+#   endif
+# endif /* __x86_64__ */
+# if defined(__ia64__)
+#   include "atomic_ops/sysdeps/gcc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# endif /* __ia64__ */
+# if defined(__hppa__)
+#   include "atomic_ops/sysdeps/gcc/hppa.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __hppa__ */
+# if defined(__alpha__)
+#   include "atomic_ops/sysdeps/gcc/alpha.h"
+#   define AO_GENERALIZE_TWICE
+# endif /* __alpha__ */
+# if defined(__s390__)
+#   include "atomic_ops/sysdeps/gcc/s390.h"
+# endif /* __s390__ */
+# if defined(__sparc__)
+#   include "atomic_ops/sysdeps/gcc/sparc.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __sparc__ */
+# if defined(__m68k__)
+#   include "atomic_ops/sysdeps/gcc/m68k.h"
+# endif /* __m68k__ */
+# if defined(__powerpc__) || defined(__ppc__) || defined(__PPC__) \
+     || defined(__powerpc64__) || defined(__ppc64__)
+#   include "atomic_ops/sysdeps/gcc/powerpc.h"
+# endif /* __powerpc__ */
+# if defined(__arm__) && !defined(AO_USE_PTHREAD_DEFS)
+#   include "atomic_ops/sysdeps/gcc/arm.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __arm__ */
+# if defined(__cris__) || defined(CRIS)
+#   include "atomic_ops/sysdeps/gcc/cris.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+# if defined(__mips__)
+#   include "atomic_ops/sysdeps/gcc/mips.h"
+# endif /* __mips__ */
+# if defined(__sh__) || defined(SH4)
+#   include "atomic_ops/sysdeps/gcc/sh.h"
+#   define AO_CAN_EMUL_CAS
+# endif /* __sh__ */
+# if defined(__avr32__)
+#   include "atomic_ops/sysdeps/gcc/avr32.h"
+# endif
+# if defined(__hexagon__)
+#   include "atomic_ops/sysdeps/gcc/hexagon.h"
+# endif
+#endif /* __GNUC__ && !AO_USE_PTHREAD_DEFS */
+
+#if (defined(__IBMC__) || defined(__IBMCPP__)) && !defined(__GNUC__) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__powerpc__) || defined(__powerpc) || defined(__ppc__) \
+     || defined(__PPC__) || defined(_M_PPC) || defined(_ARCH_PPC) \
+     || defined(_ARCH_PWR)
+#   include "atomic_ops/sysdeps/ibmc/powerpc.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+#endif
+
+#if defined(__INTEL_COMPILER) && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__ia64__)
+#   include "atomic_ops/sysdeps/icc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+# if defined(__GNUC__)
+    /* Intel Compiler in GCC compatible mode */
+#   if defined(__i386__)
+#     include "atomic_ops/sysdeps/gcc/x86.h"
+#   endif /* __i386__ */
+#   if defined(__x86_64__)
+#     if __INTEL_COMPILER > 1110
+#       define AO_USE_SYNC_CAS_BUILTIN
+#     endif
+#     ifdef __ILP32__
+#       ifndef AO_USE_PENTIUM4_INSTRS
+#         define AO_USE_PENTIUM4_INSTRS
+#       endif
+#       include "atomic_ops/sysdeps/gcc/x86.h"
+#     else
+#       include "atomic_ops/sysdeps/gcc/x86_64.h"
+#     endif
+#   endif /* __x86_64__ */
+# endif
+#endif
+
+#if defined(_HPUX_SOURCE) && !defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS)
+# if defined(__ia64)
+#   include "atomic_ops/sysdeps/hpc/ia64.h"
+#   define AO_GENERALIZE_TWICE
+# else
+#   include "atomic_ops/sysdeps/hpc/hppa.h"
+#   define AO_CAN_EMUL_CAS
+# endif
+#endif
+
+#if defined(_MSC_VER) || defined(__DMC__) || defined(__BORLANDC__) \
+        || (defined(__WATCOMC__) && defined(__NT__))
+# if defined(_AMD64_) || defined(_M_X64)
+#   include "atomic_ops/sysdeps/msftc/x86_64.h"
+# elif defined(_M_IX86) || defined(x86)
+#   include "atomic_ops/sysdeps/msftc/x86.h"
+# elif defined(_M_ARM) || defined(ARM) || defined(_ARM_)
+#   include "atomic_ops/sysdeps/msftc/arm.h"
+#   define AO_GENERALIZE_TWICE
+# endif
+#endif
+
+#if defined(__sun) && !defined(__GNUC__) && !defined(AO_USE_PTHREAD_DEFS)
+  /* Note: use -DAO_USE_PTHREAD_DEFS if Sun CC does not handle inline asm. */
+# if defined(__i386)
+#   include "atomic_ops/sysdeps/sunc/x86.h"
+# endif /* __i386 */
+# if defined(__x86_64) || defined(__amd64)
+#   include "atomic_ops/sysdeps/sunc/x86_64.h"
+# endif /* __x86_64 */
+#endif
+
+#if !defined(__GNUC__) && (defined(sparc) || defined(__sparc)) \
+    && !defined(AO_USE_PTHREAD_DEFS)
+# include "atomic_ops/sysdeps/sunc/sparc.h"
+# define AO_CAN_EMUL_CAS
+#endif
+
+#if defined(AO_REQUIRE_CAS) && !defined(AO_HAVE_compare_and_swap) \
+    && !defined(AO_HAVE_compare_and_swap_full) \
+    && !defined(AO_HAVE_compare_and_swap_acquire)
+# if defined(AO_CAN_EMUL_CAS)
+#   include "atomic_ops/sysdeps/emul_cas.h"
+# else
+#  error Cannot implement AO_compare_and_swap_full on this architecture.
+# endif
+#endif /* AO_REQUIRE_CAS && !AO_HAVE_compare_and_swap ... */
+
+/* The most common way to clear a test-and-set location         */
+/* at the end of a critical section.                            */
+#if AO_AO_TS_T && !defined(AO_CLEAR)
+# define AO_CLEAR(addr) AO_store_release((AO_TS_t *)(addr), AO_TS_CLEAR)
+#endif
+#if AO_CHAR_TS_T && !defined(AO_CLEAR)
+# define AO_CLEAR(addr) AO_char_store_release((AO_TS_t *)(addr), AO_TS_CLEAR)
+#endif
+
+/* The generalization section.  */
+#if !defined(AO_GENERALIZE_TWICE) && defined(AO_CAN_EMUL_CAS) \
+    && !defined(AO_HAVE_compare_and_swap_full)
+# define AO_GENERALIZE_TWICE
+#endif
+
+/* Theoretically we should repeatedly include atomic_ops/generalize.h.  */
+/* In fact, we observe that this converges after a small fixed number   */
+/* of iterations, usually one.                                          */
+#include "atomic_ops/generalize.h"
+#ifdef AO_GENERALIZE_TWICE
+# include "atomic_ops/generalize.h"
+#endif
+
+/* For compatibility with version 0.4 and earlier       */
+#define AO_TS_T AO_TS_t
+#define AO_T AO_t
+#define AO_TS_VAL AO_TS_VAL_t
+
+#endif /* ATOMIC_OPS_H */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/Makefile b/src/gc/bdwgc/libatomic_ops/atomic_ops/Makefile
new file mode 100644
index 0000000..0059288
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/Makefile
@@ -0,0 +1,4 @@
+CFLAGS += -Isrc/gc/bdwgc/libatomic_ops/atomic_ops \
+	-Isrc/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps \
+	-Isrc/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc
+obj-y += sysdeps/
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.h
new file mode 100644
index 0000000..a54da20
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.h
@@ -0,0 +1,1797 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* char_load */
+#if defined(AO_HAVE_char_load_full) && !defined(AO_HAVE_char_load_acquire)
+# define AO_char_load_acquire(addr) AO_char_load_full(addr)
+# define AO_HAVE_char_load_acquire
+#endif
+
+#if defined(AO_HAVE_char_load_acquire) && !defined(AO_HAVE_char_load)
+# define AO_char_load(addr) AO_char_load_acquire(addr)
+# define AO_HAVE_char_load
+#endif
+
+#if defined(AO_HAVE_char_load_full) && !defined(AO_HAVE_char_load_read)
+# define AO_char_load_read(addr) AO_char_load_full(addr)
+# define AO_HAVE_char_load_read
+#endif
+
+#if !defined(AO_HAVE_char_load_acquire_read) \
+    && defined(AO_HAVE_char_load_acquire)
+# define AO_char_load_acquire_read(addr) AO_char_load_acquire(addr)
+# define AO_HAVE_char_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_char_load) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_load_acquire)
+  AO_INLINE unsigned char
+  AO_char_load_acquire(const volatile unsigned char *addr)
+  {
+    unsigned char result = AO_char_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_char_load_acquire
+#endif
+
+#if defined(AO_HAVE_char_load) && defined(AO_HAVE_nop_read) \
+    && !defined(AO_HAVE_char_load_read)
+  AO_INLINE unsigned char
+  AO_char_load_read(const volatile unsigned char *addr)
+  {
+    unsigned char result = AO_char_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_read();
+    return result;
+  }
+# define AO_HAVE_char_load_read
+#endif
+
+#if defined(AO_HAVE_char_load_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_load_full)
+# define AO_char_load_full(addr) (AO_nop_full(), AO_char_load_acquire(addr))
+# define AO_HAVE_char_load_full
+#endif
+
+#if !defined(AO_HAVE_char_load_acquire_read) \
+    && defined(AO_HAVE_char_load_read)
+# define AO_char_load_acquire_read(addr) AO_char_load_read(addr)
+# define AO_HAVE_char_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_char_load_acquire_read) && !defined(AO_HAVE_char_load)
+# define AO_char_load(addr) AO_char_load_acquire_read(addr)
+# define AO_HAVE_char_load
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_char_load_acquire_read)
+#   define AO_char_load_dd_acquire_read(addr) AO_char_load_acquire_read(addr)
+#   define AO_HAVE_char_load_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_char_load)
+#   define AO_char_load_dd_acquire_read(addr) AO_char_load(addr)
+#   define AO_HAVE_char_load_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* char_store */
+#if defined(AO_HAVE_char_store_full) && !defined(AO_HAVE_char_store_release)
+# define AO_char_store_release(addr,val) AO_char_store_full(addr,val)
+# define AO_HAVE_char_store_release
+#endif
+
+#if defined(AO_HAVE_char_store_release) && !defined(AO_HAVE_char_store)
+# define AO_char_store(addr, val) AO_char_store_release(addr,val)
+# define AO_HAVE_char_store
+#endif
+
+#if defined(AO_HAVE_char_store_full) && !defined(AO_HAVE_char_store_write)
+# define AO_char_store_write(addr,val) AO_char_store_full(addr,val)
+# define AO_HAVE_char_store_write
+#endif
+
+#if defined(AO_HAVE_char_store_release) \
+    && !defined(AO_HAVE_char_store_release_write)
+# define AO_char_store_release_write(addr, val) \
+                            AO_char_store_release(addr,val)
+# define AO_HAVE_char_store_release_write
+#endif
+
+#if defined(AO_HAVE_char_store_write) && !defined(AO_HAVE_char_store)
+# define AO_char_store(addr, val) AO_char_store_write(addr,val)
+# define AO_HAVE_char_store
+#endif
+
+#if defined(AO_HAVE_char_store) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_store_release)
+# define AO_char_store_release(addr,val) \
+                                (AO_nop_full(), AO_char_store(addr,val))
+# define AO_HAVE_char_store_release
+#endif
+
+#if defined(AO_HAVE_nop_write) && defined(AO_HAVE_char_store) \
+    && !defined(AO_HAVE_char_store_write)
+# define AO_char_store_write(addr, val) \
+                                (AO_nop_write(), AO_char_store(addr,val))
+# define AO_HAVE_char_store_write
+#endif
+
+#if defined(AO_HAVE_char_store_write) \
+    && !defined(AO_HAVE_char_store_release_write)
+# define AO_char_store_release_write(addr, val) AO_char_store_write(addr,val)
+# define AO_HAVE_char_store_release_write
+#endif
+
+#if defined(AO_HAVE_char_store_release) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_store_full)
+# define AO_char_store_full(addr, val) \
+                        (AO_char_store_release(addr, val), AO_nop_full())
+# define AO_HAVE_char_store_full
+#endif
+
+/* char_fetch_and_add */
+#if defined(AO_HAVE_char_compare_and_swap_full) \
+    && !defined(AO_HAVE_char_fetch_and_add_full)
+  AO_INLINE unsigned char
+  AO_char_fetch_and_add_full(volatile unsigned char *addr,
+                              unsigned char incr)
+  {
+    unsigned char old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_char_compare_and_swap_full(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_char_fetch_and_add_full
+#endif
+
+#if defined(AO_HAVE_char_compare_and_swap_acquire) \
+    && !defined(AO_HAVE_char_fetch_and_add_acquire)
+  AO_INLINE unsigned char
+  AO_char_fetch_and_add_acquire(volatile unsigned char *addr,
+                                 unsigned char incr)
+  {
+    unsigned char old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_char_compare_and_swap_acquire(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_char_fetch_and_add_acquire
+#endif
+
+#if defined(AO_HAVE_char_compare_and_swap_release) \
+    && !defined(AO_HAVE_char_fetch_and_add_release)
+  AO_INLINE unsigned char
+  AO_char_fetch_and_add_release(volatile unsigned char *addr,
+                                 unsigned char incr)
+  {
+    unsigned char old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_char_compare_and_swap_release(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_char_fetch_and_add_release
+#endif
+
+#if defined(AO_HAVE_char_compare_and_swap) \
+    && !defined(AO_HAVE_char_fetch_and_add)
+  AO_INLINE unsigned char
+  AO_char_fetch_and_add(volatile unsigned char *addr, unsigned char incr)
+  {
+    unsigned char old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_char_compare_and_swap(addr, old, old + incr));
+    return old;
+  }
+# define AO_HAVE_char_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_add_full)
+# if !defined(AO_HAVE_char_fetch_and_add_release)
+#   define AO_char_fetch_and_add_release(addr, val) \
+                                AO_char_fetch_and_add_full(addr, val)
+#   define AO_HAVE_char_fetch_and_add_release
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add_acquire)
+#   define AO_char_fetch_and_add_acquire(addr, val) \
+                                AO_char_fetch_and_add_full(addr, val)
+#   define AO_HAVE_char_fetch_and_add_acquire
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add_write)
+#   define AO_char_fetch_and_add_write(addr, val) \
+                                AO_char_fetch_and_add_full(addr, val)
+#   define AO_HAVE_char_fetch_and_add_write
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add_read)
+#   define AO_char_fetch_and_add_read(addr, val) \
+                                AO_char_fetch_and_add_full(addr, val)
+#   define AO_HAVE_char_fetch_and_add_read
+# endif
+#endif /* AO_HAVE_char_fetch_and_add_full */
+
+#if !defined(AO_HAVE_char_fetch_and_add) \
+    && defined(AO_HAVE_char_fetch_and_add_release)
+# define AO_char_fetch_and_add(addr, val) \
+                                AO_char_fetch_and_add_release(addr, val)
+# define AO_HAVE_char_fetch_and_add
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add) \
+    && defined(AO_HAVE_char_fetch_and_add_acquire)
+# define AO_char_fetch_and_add(addr, val) \
+                                AO_char_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_char_fetch_and_add
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add) \
+    && defined(AO_HAVE_char_fetch_and_add_write)
+# define AO_char_fetch_and_add(addr, val) \
+                                AO_char_fetch_and_add_write(addr, val)
+# define AO_HAVE_char_fetch_and_add
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add) \
+    && defined(AO_HAVE_char_fetch_and_add_read)
+# define AO_char_fetch_and_add(addr, val) \
+                                AO_char_fetch_and_add_read(addr, val)
+# define AO_HAVE_char_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_add_acquire) \
+    && defined(AO_HAVE_nop_full) && !defined(AO_HAVE_char_fetch_and_add_full)
+# define AO_char_fetch_and_add_full(addr, val) \
+                (AO_nop_full(), AO_char_fetch_and_add_acquire(addr, val))
+# define AO_HAVE_char_fetch_and_add_full
+#endif
+
+#if !defined(AO_HAVE_char_fetch_and_add_release_write) \
+    && defined(AO_HAVE_char_fetch_and_add_write)
+# define AO_char_fetch_and_add_release_write(addr, val) \
+                                AO_char_fetch_and_add_write(addr, val)
+# define AO_HAVE_char_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add_release_write) \
+    && defined(AO_HAVE_char_fetch_and_add_release)
+# define AO_char_fetch_and_add_release_write(addr, val) \
+                                AO_char_fetch_and_add_release(addr, val)
+# define AO_HAVE_char_fetch_and_add_release_write
+#endif
+
+#if !defined(AO_HAVE_char_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_add_read)
+# define AO_char_fetch_and_add_acquire_read(addr, val) \
+                                AO_char_fetch_and_add_read(addr, val)
+# define AO_HAVE_char_fetch_and_add_acquire_read
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_add_acquire)
+# define AO_char_fetch_and_add_acquire_read(addr, val) \
+                                AO_char_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_char_fetch_and_add_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_char_fetch_and_add_acquire_read)
+#   define AO_char_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_char_fetch_and_add_acquire_read(addr, val)
+#   define AO_HAVE_char_fetch_and_add_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_char_fetch_and_add)
+#   define AO_char_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_char_fetch_and_add(addr, val)
+#   define AO_HAVE_char_fetch_and_add_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* char_fetch_and_add1 */
+#if defined(AO_HAVE_char_fetch_and_add_full) \
+    && !defined(AO_HAVE_char_fetch_and_add1_full)
+# define AO_char_fetch_and_add1_full(addr) \
+                                AO_char_fetch_and_add_full(addr,1)
+# define AO_HAVE_char_fetch_and_add1_full
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_release) \
+    && !defined(AO_HAVE_char_fetch_and_add1_release)
+# define AO_char_fetch_and_add1_release(addr) \
+                                AO_char_fetch_and_add_release(addr,1)
+# define AO_HAVE_char_fetch_and_add1_release
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_char_fetch_and_add1_acquire)
+# define AO_char_fetch_and_add1_acquire(addr) \
+                                AO_char_fetch_and_add_acquire(addr,1)
+# define AO_HAVE_char_fetch_and_add1_acquire
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_write) \
+    && !defined(AO_HAVE_char_fetch_and_add1_write)
+# define AO_char_fetch_and_add1_write(addr) \
+                                AO_char_fetch_and_add_write(addr,1)
+# define AO_HAVE_char_fetch_and_add1_write
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_read) \
+    && !defined(AO_HAVE_char_fetch_and_add1_read)
+# define AO_char_fetch_and_add1_read(addr) \
+                                AO_char_fetch_and_add_read(addr,1)
+# define AO_HAVE_char_fetch_and_add1_read
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_char_fetch_and_add1_release_write)
+# define AO_char_fetch_and_add1_release_write(addr) \
+                                AO_char_fetch_and_add_release_write(addr,1)
+# define AO_HAVE_char_fetch_and_add1_release_write
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_char_fetch_and_add1_acquire_read)
+# define AO_char_fetch_and_add1_acquire_read(addr) \
+                                AO_char_fetch_and_add_acquire_read(addr,1)
+# define AO_HAVE_char_fetch_and_add1_acquire_read
+#endif
+#if defined(AO_HAVE_char_fetch_and_add) \
+    && !defined(AO_HAVE_char_fetch_and_add1)
+# define AO_char_fetch_and_add1(addr) AO_char_fetch_and_add(addr,1)
+# define AO_HAVE_char_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_add1_full)
+# if !defined(AO_HAVE_char_fetch_and_add1_release)
+#   define AO_char_fetch_and_add1_release(addr) \
+                                AO_char_fetch_and_add1_full(addr)
+#   define AO_HAVE_char_fetch_and_add1_release
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add1_acquire)
+#   define AO_char_fetch_and_add1_acquire(addr) \
+                                AO_char_fetch_and_add1_full(addr)
+#   define AO_HAVE_char_fetch_and_add1_acquire
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add1_write)
+#   define AO_char_fetch_and_add1_write(addr) \
+                                AO_char_fetch_and_add1_full(addr)
+#   define AO_HAVE_char_fetch_and_add1_write
+# endif
+# if !defined(AO_HAVE_char_fetch_and_add1_read)
+#   define AO_char_fetch_and_add1_read(addr) \
+                                AO_char_fetch_and_add1_full(addr)
+#   define AO_HAVE_char_fetch_and_add1_read
+# endif
+#endif /* AO_HAVE_char_fetch_and_add1_full */
+
+#if !defined(AO_HAVE_char_fetch_and_add1) \
+    && defined(AO_HAVE_char_fetch_and_add1_release)
+# define AO_char_fetch_and_add1(addr) AO_char_fetch_and_add1_release(addr)
+# define AO_HAVE_char_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1) \
+    && defined(AO_HAVE_char_fetch_and_add1_acquire)
+# define AO_char_fetch_and_add1(addr) AO_char_fetch_and_add1_acquire(addr)
+# define AO_HAVE_char_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1) \
+    && defined(AO_HAVE_char_fetch_and_add1_write)
+# define AO_char_fetch_and_add1(addr) AO_char_fetch_and_add1_write(addr)
+# define AO_HAVE_char_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1) \
+    && defined(AO_HAVE_char_fetch_and_add1_read)
+# define AO_char_fetch_and_add1(addr) AO_char_fetch_and_add1_read(addr)
+# define AO_HAVE_char_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_add1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_fetch_and_add1_full)
+# define AO_char_fetch_and_add1_full(addr) \
+                        (AO_nop_full(), AO_char_fetch_and_add1_acquire(addr))
+# define AO_HAVE_char_fetch_and_add1_full
+#endif
+
+#if !defined(AO_HAVE_char_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_char_fetch_and_add1_write)
+# define AO_char_fetch_and_add1_release_write(addr) \
+                                AO_char_fetch_and_add1_write(addr)
+# define AO_HAVE_char_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_char_fetch_and_add1_release)
+# define AO_char_fetch_and_add1_release_write(addr) \
+                                AO_char_fetch_and_add1_release(addr)
+# define AO_HAVE_char_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_add1_read)
+# define AO_char_fetch_and_add1_acquire_read(addr) \
+                                AO_char_fetch_and_add1_read(addr)
+# define AO_HAVE_char_fetch_and_add1_acquire_read
+#endif
+#if !defined(AO_HAVE_char_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_add1_acquire)
+# define AO_char_fetch_and_add1_acquire_read(addr) \
+                                AO_char_fetch_and_add1_acquire(addr)
+# define AO_HAVE_char_fetch_and_add1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_char_fetch_and_add1_acquire_read)
+#   define AO_char_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_char_fetch_and_add1_acquire_read(addr)
+#   define AO_HAVE_char_fetch_and_add1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_char_fetch_and_add1)
+#   define AO_char_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_char_fetch_and_add1(addr)
+#   define AO_HAVE_char_fetch_and_add1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* char_fetch_and_sub1 */
+#if defined(AO_HAVE_char_fetch_and_add_full) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_full)
+# define AO_char_fetch_and_sub1_full(addr) \
+                AO_char_fetch_and_add_full(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_full
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_release) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_release)
+# define AO_char_fetch_and_sub1_release(addr) \
+                AO_char_fetch_and_add_release(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_release
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_acquire)
+# define AO_char_fetch_and_sub1_acquire(addr) \
+                AO_char_fetch_and_add_acquire(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_acquire
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_write) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_write)
+# define AO_char_fetch_and_sub1_write(addr) \
+                AO_char_fetch_and_add_write(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_write
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_read) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_read)
+# define AO_char_fetch_and_sub1_read(addr) \
+                AO_char_fetch_and_add_read(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_read
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_release_write)
+# define AO_char_fetch_and_sub1_release_write(addr) \
+        AO_char_fetch_and_add_release_write(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_release_write
+#endif
+#if defined(AO_HAVE_char_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_acquire_read)
+# define AO_char_fetch_and_sub1_acquire_read(addr) \
+        AO_char_fetch_and_add_acquire_read(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1_acquire_read
+#endif
+#if defined(AO_HAVE_char_fetch_and_add) \
+    && !defined(AO_HAVE_char_fetch_and_sub1)
+# define AO_char_fetch_and_sub1(addr) \
+                AO_char_fetch_and_add(addr,(unsigned char)(-1))
+# define AO_HAVE_char_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_sub1_full)
+# if !defined(AO_HAVE_char_fetch_and_sub1_release)
+#   define AO_char_fetch_and_sub1_release(addr) \
+                                AO_char_fetch_and_sub1_full(addr)
+#   define AO_HAVE_char_fetch_and_sub1_release
+# endif
+# if !defined(AO_HAVE_char_fetch_and_sub1_acquire)
+#   define AO_char_fetch_and_sub1_acquire(addr) \
+                                AO_char_fetch_and_sub1_full(addr)
+#   define AO_HAVE_char_fetch_and_sub1_acquire
+# endif
+# if !defined(AO_HAVE_char_fetch_and_sub1_write)
+#   define AO_char_fetch_and_sub1_write(addr) \
+                                AO_char_fetch_and_sub1_full(addr)
+#   define AO_HAVE_char_fetch_and_sub1_write
+# endif
+# if !defined(AO_HAVE_char_fetch_and_sub1_read)
+#   define AO_char_fetch_and_sub1_read(addr) \
+                                AO_char_fetch_and_sub1_full(addr)
+#   define AO_HAVE_char_fetch_and_sub1_read
+# endif
+#endif /* AO_HAVE_char_fetch_and_sub1_full */
+
+#if !defined(AO_HAVE_char_fetch_and_sub1) \
+    && defined(AO_HAVE_char_fetch_and_sub1_release)
+# define AO_char_fetch_and_sub1(addr) AO_char_fetch_and_sub1_release(addr)
+# define AO_HAVE_char_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1) \
+    && defined(AO_HAVE_char_fetch_and_sub1_acquire)
+# define AO_char_fetch_and_sub1(addr) AO_char_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_char_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1) \
+    && defined(AO_HAVE_char_fetch_and_sub1_write)
+# define AO_char_fetch_and_sub1(addr) AO_char_fetch_and_sub1_write(addr)
+# define AO_HAVE_char_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1) \
+    && defined(AO_HAVE_char_fetch_and_sub1_read)
+# define AO_char_fetch_and_sub1(addr) AO_char_fetch_and_sub1_read(addr)
+# define AO_HAVE_char_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_char_fetch_and_sub1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_char_fetch_and_sub1_full)
+# define AO_char_fetch_and_sub1_full(addr) \
+                        (AO_nop_full(), AO_char_fetch_and_sub1_acquire(addr))
+# define AO_HAVE_char_fetch_and_sub1_full
+#endif
+
+#if !defined(AO_HAVE_char_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_char_fetch_and_sub1_write)
+# define AO_char_fetch_and_sub1_release_write(addr) \
+                                AO_char_fetch_and_sub1_write(addr)
+# define AO_HAVE_char_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_char_fetch_and_sub1_release)
+# define AO_char_fetch_and_sub1_release_write(addr) \
+                                AO_char_fetch_and_sub1_release(addr)
+# define AO_HAVE_char_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_sub1_read)
+# define AO_char_fetch_and_sub1_acquire_read(addr) \
+                                AO_char_fetch_and_sub1_read(addr)
+# define AO_HAVE_char_fetch_and_sub1_acquire_read
+#endif
+#if !defined(AO_HAVE_char_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_char_fetch_and_sub1_acquire)
+# define AO_char_fetch_and_sub1_acquire_read(addr) \
+                                AO_char_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_char_fetch_and_sub1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_char_fetch_and_sub1_acquire_read)
+#   define AO_char_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_char_fetch_and_sub1_acquire_read(addr)
+#   define AO_HAVE_char_fetch_and_sub1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_char_fetch_and_sub1)
+#   define AO_char_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_char_fetch_and_sub1(addr)
+#   define AO_HAVE_char_fetch_and_sub1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* short_load */
+#if defined(AO_HAVE_short_load_full) && !defined(AO_HAVE_short_load_acquire)
+# define AO_short_load_acquire(addr) AO_short_load_full(addr)
+# define AO_HAVE_short_load_acquire
+#endif
+
+#if defined(AO_HAVE_short_load_acquire) && !defined(AO_HAVE_short_load)
+# define AO_short_load(addr) AO_short_load_acquire(addr)
+# define AO_HAVE_short_load
+#endif
+
+#if defined(AO_HAVE_short_load_full) && !defined(AO_HAVE_short_load_read)
+# define AO_short_load_read(addr) AO_short_load_full(addr)
+# define AO_HAVE_short_load_read
+#endif
+
+#if !defined(AO_HAVE_short_load_acquire_read) \
+    && defined(AO_HAVE_short_load_acquire)
+# define AO_short_load_acquire_read(addr) AO_short_load_acquire(addr)
+# define AO_HAVE_short_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_short_load) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_load_acquire)
+  AO_INLINE unsigned short
+  AO_short_load_acquire(const volatile unsigned short *addr)
+  {
+    unsigned short result = AO_short_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_short_load_acquire
+#endif
+
+#if defined(AO_HAVE_short_load) && defined(AO_HAVE_nop_read) \
+    && !defined(AO_HAVE_short_load_read)
+  AO_INLINE unsigned short
+  AO_short_load_read(const volatile unsigned short *addr)
+  {
+    unsigned short result = AO_short_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_read();
+    return result;
+  }
+# define AO_HAVE_short_load_read
+#endif
+
+#if defined(AO_HAVE_short_load_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_load_full)
+# define AO_short_load_full(addr) (AO_nop_full(), AO_short_load_acquire(addr))
+# define AO_HAVE_short_load_full
+#endif
+
+#if !defined(AO_HAVE_short_load_acquire_read) \
+    && defined(AO_HAVE_short_load_read)
+# define AO_short_load_acquire_read(addr) AO_short_load_read(addr)
+# define AO_HAVE_short_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_short_load_acquire_read) && !defined(AO_HAVE_short_load)
+# define AO_short_load(addr) AO_short_load_acquire_read(addr)
+# define AO_HAVE_short_load
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_short_load_acquire_read)
+#   define AO_short_load_dd_acquire_read(addr) AO_short_load_acquire_read(addr)
+#   define AO_HAVE_short_load_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_short_load)
+#   define AO_short_load_dd_acquire_read(addr) AO_short_load(addr)
+#   define AO_HAVE_short_load_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* short_store */
+#if defined(AO_HAVE_short_store_full) && !defined(AO_HAVE_short_store_release)
+# define AO_short_store_release(addr,val) AO_short_store_full(addr,val)
+# define AO_HAVE_short_store_release
+#endif
+
+#if defined(AO_HAVE_short_store_release) && !defined(AO_HAVE_short_store)
+# define AO_short_store(addr, val) AO_short_store_release(addr,val)
+# define AO_HAVE_short_store
+#endif
+
+#if defined(AO_HAVE_short_store_full) && !defined(AO_HAVE_short_store_write)
+# define AO_short_store_write(addr,val) AO_short_store_full(addr,val)
+# define AO_HAVE_short_store_write
+#endif
+
+#if defined(AO_HAVE_short_store_release) \
+    && !defined(AO_HAVE_short_store_release_write)
+# define AO_short_store_release_write(addr, val) \
+                            AO_short_store_release(addr,val)
+# define AO_HAVE_short_store_release_write
+#endif
+
+#if defined(AO_HAVE_short_store_write) && !defined(AO_HAVE_short_store)
+# define AO_short_store(addr, val) AO_short_store_write(addr,val)
+# define AO_HAVE_short_store
+#endif
+
+#if defined(AO_HAVE_short_store) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_store_release)
+# define AO_short_store_release(addr,val) \
+                                (AO_nop_full(), AO_short_store(addr,val))
+# define AO_HAVE_short_store_release
+#endif
+
+#if defined(AO_HAVE_nop_write) && defined(AO_HAVE_short_store) \
+    && !defined(AO_HAVE_short_store_write)
+# define AO_short_store_write(addr, val) \
+                                (AO_nop_write(), AO_short_store(addr,val))
+# define AO_HAVE_short_store_write
+#endif
+
+#if defined(AO_HAVE_short_store_write) \
+    && !defined(AO_HAVE_short_store_release_write)
+# define AO_short_store_release_write(addr, val) AO_short_store_write(addr,val)
+# define AO_HAVE_short_store_release_write
+#endif
+
+#if defined(AO_HAVE_short_store_release) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_store_full)
+# define AO_short_store_full(addr, val) \
+                        (AO_short_store_release(addr, val), AO_nop_full())
+# define AO_HAVE_short_store_full
+#endif
+
+/* short_fetch_and_add */
+#if defined(AO_HAVE_short_compare_and_swap_full) \
+    && !defined(AO_HAVE_short_fetch_and_add_full)
+  AO_INLINE unsigned short
+  AO_short_fetch_and_add_full(volatile unsigned short *addr,
+                              unsigned short incr)
+  {
+    unsigned short old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_short_compare_and_swap_full(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_short_fetch_and_add_full
+#endif
+
+#if defined(AO_HAVE_short_compare_and_swap_acquire) \
+    && !defined(AO_HAVE_short_fetch_and_add_acquire)
+  AO_INLINE unsigned short
+  AO_short_fetch_and_add_acquire(volatile unsigned short *addr,
+                                 unsigned short incr)
+  {
+    unsigned short old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_short_compare_and_swap_acquire(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_short_fetch_and_add_acquire
+#endif
+
+#if defined(AO_HAVE_short_compare_and_swap_release) \
+    && !defined(AO_HAVE_short_fetch_and_add_release)
+  AO_INLINE unsigned short
+  AO_short_fetch_and_add_release(volatile unsigned short *addr,
+                                 unsigned short incr)
+  {
+    unsigned short old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_short_compare_and_swap_release(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_short_fetch_and_add_release
+#endif
+
+#if defined(AO_HAVE_short_compare_and_swap) \
+    && !defined(AO_HAVE_short_fetch_and_add)
+  AO_INLINE unsigned short
+  AO_short_fetch_and_add(volatile unsigned short *addr, unsigned short incr)
+  {
+    unsigned short old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_short_compare_and_swap(addr, old, old + incr));
+    return old;
+  }
+# define AO_HAVE_short_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_add_full)
+# if !defined(AO_HAVE_short_fetch_and_add_release)
+#   define AO_short_fetch_and_add_release(addr, val) \
+                                AO_short_fetch_and_add_full(addr, val)
+#   define AO_HAVE_short_fetch_and_add_release
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add_acquire)
+#   define AO_short_fetch_and_add_acquire(addr, val) \
+                                AO_short_fetch_and_add_full(addr, val)
+#   define AO_HAVE_short_fetch_and_add_acquire
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add_write)
+#   define AO_short_fetch_and_add_write(addr, val) \
+                                AO_short_fetch_and_add_full(addr, val)
+#   define AO_HAVE_short_fetch_and_add_write
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add_read)
+#   define AO_short_fetch_and_add_read(addr, val) \
+                                AO_short_fetch_and_add_full(addr, val)
+#   define AO_HAVE_short_fetch_and_add_read
+# endif
+#endif /* AO_HAVE_short_fetch_and_add_full */
+
+#if !defined(AO_HAVE_short_fetch_and_add) \
+    && defined(AO_HAVE_short_fetch_and_add_release)
+# define AO_short_fetch_and_add(addr, val) \
+                                AO_short_fetch_and_add_release(addr, val)
+# define AO_HAVE_short_fetch_and_add
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add) \
+    && defined(AO_HAVE_short_fetch_and_add_acquire)
+# define AO_short_fetch_and_add(addr, val) \
+                                AO_short_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_short_fetch_and_add
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add) \
+    && defined(AO_HAVE_short_fetch_and_add_write)
+# define AO_short_fetch_and_add(addr, val) \
+                                AO_short_fetch_and_add_write(addr, val)
+# define AO_HAVE_short_fetch_and_add
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add) \
+    && defined(AO_HAVE_short_fetch_and_add_read)
+# define AO_short_fetch_and_add(addr, val) \
+                                AO_short_fetch_and_add_read(addr, val)
+# define AO_HAVE_short_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_add_acquire) \
+    && defined(AO_HAVE_nop_full) && !defined(AO_HAVE_short_fetch_and_add_full)
+# define AO_short_fetch_and_add_full(addr, val) \
+                (AO_nop_full(), AO_short_fetch_and_add_acquire(addr, val))
+# define AO_HAVE_short_fetch_and_add_full
+#endif
+
+#if !defined(AO_HAVE_short_fetch_and_add_release_write) \
+    && defined(AO_HAVE_short_fetch_and_add_write)
+# define AO_short_fetch_and_add_release_write(addr, val) \
+                                AO_short_fetch_and_add_write(addr, val)
+# define AO_HAVE_short_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add_release_write) \
+    && defined(AO_HAVE_short_fetch_and_add_release)
+# define AO_short_fetch_and_add_release_write(addr, val) \
+                                AO_short_fetch_and_add_release(addr, val)
+# define AO_HAVE_short_fetch_and_add_release_write
+#endif
+
+#if !defined(AO_HAVE_short_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_add_read)
+# define AO_short_fetch_and_add_acquire_read(addr, val) \
+                                AO_short_fetch_and_add_read(addr, val)
+# define AO_HAVE_short_fetch_and_add_acquire_read
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_add_acquire)
+# define AO_short_fetch_and_add_acquire_read(addr, val) \
+                                AO_short_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_short_fetch_and_add_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_short_fetch_and_add_acquire_read)
+#   define AO_short_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_short_fetch_and_add_acquire_read(addr, val)
+#   define AO_HAVE_short_fetch_and_add_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_short_fetch_and_add)
+#   define AO_short_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_short_fetch_and_add(addr, val)
+#   define AO_HAVE_short_fetch_and_add_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* short_fetch_and_add1 */
+#if defined(AO_HAVE_short_fetch_and_add_full) \
+    && !defined(AO_HAVE_short_fetch_and_add1_full)
+# define AO_short_fetch_and_add1_full(addr) \
+                                AO_short_fetch_and_add_full(addr,1)
+# define AO_HAVE_short_fetch_and_add1_full
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_release) \
+    && !defined(AO_HAVE_short_fetch_and_add1_release)
+# define AO_short_fetch_and_add1_release(addr) \
+                                AO_short_fetch_and_add_release(addr,1)
+# define AO_HAVE_short_fetch_and_add1_release
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_short_fetch_and_add1_acquire)
+# define AO_short_fetch_and_add1_acquire(addr) \
+                                AO_short_fetch_and_add_acquire(addr,1)
+# define AO_HAVE_short_fetch_and_add1_acquire
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_write) \
+    && !defined(AO_HAVE_short_fetch_and_add1_write)
+# define AO_short_fetch_and_add1_write(addr) \
+                                AO_short_fetch_and_add_write(addr,1)
+# define AO_HAVE_short_fetch_and_add1_write
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_read) \
+    && !defined(AO_HAVE_short_fetch_and_add1_read)
+# define AO_short_fetch_and_add1_read(addr) \
+                                AO_short_fetch_and_add_read(addr,1)
+# define AO_HAVE_short_fetch_and_add1_read
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_short_fetch_and_add1_release_write)
+# define AO_short_fetch_and_add1_release_write(addr) \
+                                AO_short_fetch_and_add_release_write(addr,1)
+# define AO_HAVE_short_fetch_and_add1_release_write
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_short_fetch_and_add1_acquire_read)
+# define AO_short_fetch_and_add1_acquire_read(addr) \
+                                AO_short_fetch_and_add_acquire_read(addr,1)
+# define AO_HAVE_short_fetch_and_add1_acquire_read
+#endif
+#if defined(AO_HAVE_short_fetch_and_add) \
+    && !defined(AO_HAVE_short_fetch_and_add1)
+# define AO_short_fetch_and_add1(addr) AO_short_fetch_and_add(addr,1)
+# define AO_HAVE_short_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_add1_full)
+# if !defined(AO_HAVE_short_fetch_and_add1_release)
+#   define AO_short_fetch_and_add1_release(addr) \
+                                AO_short_fetch_and_add1_full(addr)
+#   define AO_HAVE_short_fetch_and_add1_release
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add1_acquire)
+#   define AO_short_fetch_and_add1_acquire(addr) \
+                                AO_short_fetch_and_add1_full(addr)
+#   define AO_HAVE_short_fetch_and_add1_acquire
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add1_write)
+#   define AO_short_fetch_and_add1_write(addr) \
+                                AO_short_fetch_and_add1_full(addr)
+#   define AO_HAVE_short_fetch_and_add1_write
+# endif
+# if !defined(AO_HAVE_short_fetch_and_add1_read)
+#   define AO_short_fetch_and_add1_read(addr) \
+                                AO_short_fetch_and_add1_full(addr)
+#   define AO_HAVE_short_fetch_and_add1_read
+# endif
+#endif /* AO_HAVE_short_fetch_and_add1_full */
+
+#if !defined(AO_HAVE_short_fetch_and_add1) \
+    && defined(AO_HAVE_short_fetch_and_add1_release)
+# define AO_short_fetch_and_add1(addr) AO_short_fetch_and_add1_release(addr)
+# define AO_HAVE_short_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1) \
+    && defined(AO_HAVE_short_fetch_and_add1_acquire)
+# define AO_short_fetch_and_add1(addr) AO_short_fetch_and_add1_acquire(addr)
+# define AO_HAVE_short_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1) \
+    && defined(AO_HAVE_short_fetch_and_add1_write)
+# define AO_short_fetch_and_add1(addr) AO_short_fetch_and_add1_write(addr)
+# define AO_HAVE_short_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1) \
+    && defined(AO_HAVE_short_fetch_and_add1_read)
+# define AO_short_fetch_and_add1(addr) AO_short_fetch_and_add1_read(addr)
+# define AO_HAVE_short_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_add1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_fetch_and_add1_full)
+# define AO_short_fetch_and_add1_full(addr) \
+                        (AO_nop_full(), AO_short_fetch_and_add1_acquire(addr))
+# define AO_HAVE_short_fetch_and_add1_full
+#endif
+
+#if !defined(AO_HAVE_short_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_short_fetch_and_add1_write)
+# define AO_short_fetch_and_add1_release_write(addr) \
+                                AO_short_fetch_and_add1_write(addr)
+# define AO_HAVE_short_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_short_fetch_and_add1_release)
+# define AO_short_fetch_and_add1_release_write(addr) \
+                                AO_short_fetch_and_add1_release(addr)
+# define AO_HAVE_short_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_add1_read)
+# define AO_short_fetch_and_add1_acquire_read(addr) \
+                                AO_short_fetch_and_add1_read(addr)
+# define AO_HAVE_short_fetch_and_add1_acquire_read
+#endif
+#if !defined(AO_HAVE_short_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_add1_acquire)
+# define AO_short_fetch_and_add1_acquire_read(addr) \
+                                AO_short_fetch_and_add1_acquire(addr)
+# define AO_HAVE_short_fetch_and_add1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_short_fetch_and_add1_acquire_read)
+#   define AO_short_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_short_fetch_and_add1_acquire_read(addr)
+#   define AO_HAVE_short_fetch_and_add1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_short_fetch_and_add1)
+#   define AO_short_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_short_fetch_and_add1(addr)
+#   define AO_HAVE_short_fetch_and_add1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* short_fetch_and_sub1 */
+#if defined(AO_HAVE_short_fetch_and_add_full) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_full)
+# define AO_short_fetch_and_sub1_full(addr) \
+                AO_short_fetch_and_add_full(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_full
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_release) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_release)
+# define AO_short_fetch_and_sub1_release(addr) \
+                AO_short_fetch_and_add_release(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_release
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_acquire)
+# define AO_short_fetch_and_sub1_acquire(addr) \
+                AO_short_fetch_and_add_acquire(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_acquire
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_write) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_write)
+# define AO_short_fetch_and_sub1_write(addr) \
+                AO_short_fetch_and_add_write(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_write
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_read) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_read)
+# define AO_short_fetch_and_sub1_read(addr) \
+                AO_short_fetch_and_add_read(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_read
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_release_write)
+# define AO_short_fetch_and_sub1_release_write(addr) \
+        AO_short_fetch_and_add_release_write(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_release_write
+#endif
+#if defined(AO_HAVE_short_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_acquire_read)
+# define AO_short_fetch_and_sub1_acquire_read(addr) \
+        AO_short_fetch_and_add_acquire_read(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1_acquire_read
+#endif
+#if defined(AO_HAVE_short_fetch_and_add) \
+    && !defined(AO_HAVE_short_fetch_and_sub1)
+# define AO_short_fetch_and_sub1(addr) \
+                AO_short_fetch_and_add(addr,(unsigned short)(-1))
+# define AO_HAVE_short_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_sub1_full)
+# if !defined(AO_HAVE_short_fetch_and_sub1_release)
+#   define AO_short_fetch_and_sub1_release(addr) \
+                                AO_short_fetch_and_sub1_full(addr)
+#   define AO_HAVE_short_fetch_and_sub1_release
+# endif
+# if !defined(AO_HAVE_short_fetch_and_sub1_acquire)
+#   define AO_short_fetch_and_sub1_acquire(addr) \
+                                AO_short_fetch_and_sub1_full(addr)
+#   define AO_HAVE_short_fetch_and_sub1_acquire
+# endif
+# if !defined(AO_HAVE_short_fetch_and_sub1_write)
+#   define AO_short_fetch_and_sub1_write(addr) \
+                                AO_short_fetch_and_sub1_full(addr)
+#   define AO_HAVE_short_fetch_and_sub1_write
+# endif
+# if !defined(AO_HAVE_short_fetch_and_sub1_read)
+#   define AO_short_fetch_and_sub1_read(addr) \
+                                AO_short_fetch_and_sub1_full(addr)
+#   define AO_HAVE_short_fetch_and_sub1_read
+# endif
+#endif /* AO_HAVE_short_fetch_and_sub1_full */
+
+#if !defined(AO_HAVE_short_fetch_and_sub1) \
+    && defined(AO_HAVE_short_fetch_and_sub1_release)
+# define AO_short_fetch_and_sub1(addr) AO_short_fetch_and_sub1_release(addr)
+# define AO_HAVE_short_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1) \
+    && defined(AO_HAVE_short_fetch_and_sub1_acquire)
+# define AO_short_fetch_and_sub1(addr) AO_short_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_short_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1) \
+    && defined(AO_HAVE_short_fetch_and_sub1_write)
+# define AO_short_fetch_and_sub1(addr) AO_short_fetch_and_sub1_write(addr)
+# define AO_HAVE_short_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1) \
+    && defined(AO_HAVE_short_fetch_and_sub1_read)
+# define AO_short_fetch_and_sub1(addr) AO_short_fetch_and_sub1_read(addr)
+# define AO_HAVE_short_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_short_fetch_and_sub1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_short_fetch_and_sub1_full)
+# define AO_short_fetch_and_sub1_full(addr) \
+                        (AO_nop_full(), AO_short_fetch_and_sub1_acquire(addr))
+# define AO_HAVE_short_fetch_and_sub1_full
+#endif
+
+#if !defined(AO_HAVE_short_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_short_fetch_and_sub1_write)
+# define AO_short_fetch_and_sub1_release_write(addr) \
+                                AO_short_fetch_and_sub1_write(addr)
+# define AO_HAVE_short_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_short_fetch_and_sub1_release)
+# define AO_short_fetch_and_sub1_release_write(addr) \
+                                AO_short_fetch_and_sub1_release(addr)
+# define AO_HAVE_short_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_sub1_read)
+# define AO_short_fetch_and_sub1_acquire_read(addr) \
+                                AO_short_fetch_and_sub1_read(addr)
+# define AO_HAVE_short_fetch_and_sub1_acquire_read
+#endif
+#if !defined(AO_HAVE_short_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_short_fetch_and_sub1_acquire)
+# define AO_short_fetch_and_sub1_acquire_read(addr) \
+                                AO_short_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_short_fetch_and_sub1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_short_fetch_and_sub1_acquire_read)
+#   define AO_short_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_short_fetch_and_sub1_acquire_read(addr)
+#   define AO_HAVE_short_fetch_and_sub1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_short_fetch_and_sub1)
+#   define AO_short_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_short_fetch_and_sub1(addr)
+#   define AO_HAVE_short_fetch_and_sub1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* int_load */
+#if defined(AO_HAVE_int_load_full) && !defined(AO_HAVE_int_load_acquire)
+# define AO_int_load_acquire(addr) AO_int_load_full(addr)
+# define AO_HAVE_int_load_acquire
+#endif
+
+#if defined(AO_HAVE_int_load_acquire) && !defined(AO_HAVE_int_load)
+# define AO_int_load(addr) AO_int_load_acquire(addr)
+# define AO_HAVE_int_load
+#endif
+
+#if defined(AO_HAVE_int_load_full) && !defined(AO_HAVE_int_load_read)
+# define AO_int_load_read(addr) AO_int_load_full(addr)
+# define AO_HAVE_int_load_read
+#endif
+
+#if !defined(AO_HAVE_int_load_acquire_read) \
+    && defined(AO_HAVE_int_load_acquire)
+# define AO_int_load_acquire_read(addr) AO_int_load_acquire(addr)
+# define AO_HAVE_int_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_int_load) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_load_acquire)
+  AO_INLINE unsigned int
+  AO_int_load_acquire(const volatile unsigned int *addr)
+  {
+    unsigned int result = AO_int_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_int_load_acquire
+#endif
+
+#if defined(AO_HAVE_int_load) && defined(AO_HAVE_nop_read) \
+    && !defined(AO_HAVE_int_load_read)
+  AO_INLINE unsigned int
+  AO_int_load_read(const volatile unsigned int *addr)
+  {
+    unsigned int result = AO_int_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_read();
+    return result;
+  }
+# define AO_HAVE_int_load_read
+#endif
+
+#if defined(AO_HAVE_int_load_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_load_full)
+# define AO_int_load_full(addr) (AO_nop_full(), AO_int_load_acquire(addr))
+# define AO_HAVE_int_load_full
+#endif
+
+#if !defined(AO_HAVE_int_load_acquire_read) \
+    && defined(AO_HAVE_int_load_read)
+# define AO_int_load_acquire_read(addr) AO_int_load_read(addr)
+# define AO_HAVE_int_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_int_load_acquire_read) && !defined(AO_HAVE_int_load)
+# define AO_int_load(addr) AO_int_load_acquire_read(addr)
+# define AO_HAVE_int_load
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_int_load_acquire_read)
+#   define AO_int_load_dd_acquire_read(addr) AO_int_load_acquire_read(addr)
+#   define AO_HAVE_int_load_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_int_load)
+#   define AO_int_load_dd_acquire_read(addr) AO_int_load(addr)
+#   define AO_HAVE_int_load_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* int_store */
+#if defined(AO_HAVE_int_store_full) && !defined(AO_HAVE_int_store_release)
+# define AO_int_store_release(addr,val) AO_int_store_full(addr,val)
+# define AO_HAVE_int_store_release
+#endif
+
+#if defined(AO_HAVE_int_store_release) && !defined(AO_HAVE_int_store)
+# define AO_int_store(addr, val) AO_int_store_release(addr,val)
+# define AO_HAVE_int_store
+#endif
+
+#if defined(AO_HAVE_int_store_full) && !defined(AO_HAVE_int_store_write)
+# define AO_int_store_write(addr,val) AO_int_store_full(addr,val)
+# define AO_HAVE_int_store_write
+#endif
+
+#if defined(AO_HAVE_int_store_release) \
+    && !defined(AO_HAVE_int_store_release_write)
+# define AO_int_store_release_write(addr, val) \
+                            AO_int_store_release(addr,val)
+# define AO_HAVE_int_store_release_write
+#endif
+
+#if defined(AO_HAVE_int_store_write) && !defined(AO_HAVE_int_store)
+# define AO_int_store(addr, val) AO_int_store_write(addr,val)
+# define AO_HAVE_int_store
+#endif
+
+#if defined(AO_HAVE_int_store) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_store_release)
+# define AO_int_store_release(addr,val) \
+                                (AO_nop_full(), AO_int_store(addr,val))
+# define AO_HAVE_int_store_release
+#endif
+
+#if defined(AO_HAVE_nop_write) && defined(AO_HAVE_int_store) \
+    && !defined(AO_HAVE_int_store_write)
+# define AO_int_store_write(addr, val) \
+                                (AO_nop_write(), AO_int_store(addr,val))
+# define AO_HAVE_int_store_write
+#endif
+
+#if defined(AO_HAVE_int_store_write) \
+    && !defined(AO_HAVE_int_store_release_write)
+# define AO_int_store_release_write(addr, val) AO_int_store_write(addr,val)
+# define AO_HAVE_int_store_release_write
+#endif
+
+#if defined(AO_HAVE_int_store_release) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_store_full)
+# define AO_int_store_full(addr, val) \
+                        (AO_int_store_release(addr, val), AO_nop_full())
+# define AO_HAVE_int_store_full
+#endif
+
+/* int_fetch_and_add */
+#if defined(AO_HAVE_int_compare_and_swap_full) \
+    && !defined(AO_HAVE_int_fetch_and_add_full)
+  AO_INLINE unsigned int
+  AO_int_fetch_and_add_full(volatile unsigned int *addr,
+                              unsigned int incr)
+  {
+    unsigned int old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_int_compare_and_swap_full(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_int_fetch_and_add_full
+#endif
+
+#if defined(AO_HAVE_int_compare_and_swap_acquire) \
+    && !defined(AO_HAVE_int_fetch_and_add_acquire)
+  AO_INLINE unsigned int
+  AO_int_fetch_and_add_acquire(volatile unsigned int *addr,
+                                 unsigned int incr)
+  {
+    unsigned int old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_int_compare_and_swap_acquire(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_int_fetch_and_add_acquire
+#endif
+
+#if defined(AO_HAVE_int_compare_and_swap_release) \
+    && !defined(AO_HAVE_int_fetch_and_add_release)
+  AO_INLINE unsigned int
+  AO_int_fetch_and_add_release(volatile unsigned int *addr,
+                                 unsigned int incr)
+  {
+    unsigned int old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_int_compare_and_swap_release(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_int_fetch_and_add_release
+#endif
+
+#if defined(AO_HAVE_int_compare_and_swap) \
+    && !defined(AO_HAVE_int_fetch_and_add)
+  AO_INLINE unsigned int
+  AO_int_fetch_and_add(volatile unsigned int *addr, unsigned int incr)
+  {
+    unsigned int old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_int_compare_and_swap(addr, old, old + incr));
+    return old;
+  }
+# define AO_HAVE_int_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_add_full)
+# if !defined(AO_HAVE_int_fetch_and_add_release)
+#   define AO_int_fetch_and_add_release(addr, val) \
+                                AO_int_fetch_and_add_full(addr, val)
+#   define AO_HAVE_int_fetch_and_add_release
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add_acquire)
+#   define AO_int_fetch_and_add_acquire(addr, val) \
+                                AO_int_fetch_and_add_full(addr, val)
+#   define AO_HAVE_int_fetch_and_add_acquire
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add_write)
+#   define AO_int_fetch_and_add_write(addr, val) \
+                                AO_int_fetch_and_add_full(addr, val)
+#   define AO_HAVE_int_fetch_and_add_write
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add_read)
+#   define AO_int_fetch_and_add_read(addr, val) \
+                                AO_int_fetch_and_add_full(addr, val)
+#   define AO_HAVE_int_fetch_and_add_read
+# endif
+#endif /* AO_HAVE_int_fetch_and_add_full */
+
+#if !defined(AO_HAVE_int_fetch_and_add) \
+    && defined(AO_HAVE_int_fetch_and_add_release)
+# define AO_int_fetch_and_add(addr, val) \
+                                AO_int_fetch_and_add_release(addr, val)
+# define AO_HAVE_int_fetch_and_add
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add) \
+    && defined(AO_HAVE_int_fetch_and_add_acquire)
+# define AO_int_fetch_and_add(addr, val) \
+                                AO_int_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_int_fetch_and_add
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add) \
+    && defined(AO_HAVE_int_fetch_and_add_write)
+# define AO_int_fetch_and_add(addr, val) \
+                                AO_int_fetch_and_add_write(addr, val)
+# define AO_HAVE_int_fetch_and_add
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add) \
+    && defined(AO_HAVE_int_fetch_and_add_read)
+# define AO_int_fetch_and_add(addr, val) \
+                                AO_int_fetch_and_add_read(addr, val)
+# define AO_HAVE_int_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_add_acquire) \
+    && defined(AO_HAVE_nop_full) && !defined(AO_HAVE_int_fetch_and_add_full)
+# define AO_int_fetch_and_add_full(addr, val) \
+                (AO_nop_full(), AO_int_fetch_and_add_acquire(addr, val))
+# define AO_HAVE_int_fetch_and_add_full
+#endif
+
+#if !defined(AO_HAVE_int_fetch_and_add_release_write) \
+    && defined(AO_HAVE_int_fetch_and_add_write)
+# define AO_int_fetch_and_add_release_write(addr, val) \
+                                AO_int_fetch_and_add_write(addr, val)
+# define AO_HAVE_int_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add_release_write) \
+    && defined(AO_HAVE_int_fetch_and_add_release)
+# define AO_int_fetch_and_add_release_write(addr, val) \
+                                AO_int_fetch_and_add_release(addr, val)
+# define AO_HAVE_int_fetch_and_add_release_write
+#endif
+
+#if !defined(AO_HAVE_int_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_add_read)
+# define AO_int_fetch_and_add_acquire_read(addr, val) \
+                                AO_int_fetch_and_add_read(addr, val)
+# define AO_HAVE_int_fetch_and_add_acquire_read
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_add_acquire)
+# define AO_int_fetch_and_add_acquire_read(addr, val) \
+                                AO_int_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_int_fetch_and_add_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_int_fetch_and_add_acquire_read)
+#   define AO_int_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_int_fetch_and_add_acquire_read(addr, val)
+#   define AO_HAVE_int_fetch_and_add_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_int_fetch_and_add)
+#   define AO_int_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_int_fetch_and_add(addr, val)
+#   define AO_HAVE_int_fetch_and_add_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* int_fetch_and_add1 */
+#if defined(AO_HAVE_int_fetch_and_add_full) \
+    && !defined(AO_HAVE_int_fetch_and_add1_full)
+# define AO_int_fetch_and_add1_full(addr) \
+                                AO_int_fetch_and_add_full(addr,1)
+# define AO_HAVE_int_fetch_and_add1_full
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_release) \
+    && !defined(AO_HAVE_int_fetch_and_add1_release)
+# define AO_int_fetch_and_add1_release(addr) \
+                                AO_int_fetch_and_add_release(addr,1)
+# define AO_HAVE_int_fetch_and_add1_release
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_int_fetch_and_add1_acquire)
+# define AO_int_fetch_and_add1_acquire(addr) \
+                                AO_int_fetch_and_add_acquire(addr,1)
+# define AO_HAVE_int_fetch_and_add1_acquire
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_write) \
+    && !defined(AO_HAVE_int_fetch_and_add1_write)
+# define AO_int_fetch_and_add1_write(addr) \
+                                AO_int_fetch_and_add_write(addr,1)
+# define AO_HAVE_int_fetch_and_add1_write
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_read) \
+    && !defined(AO_HAVE_int_fetch_and_add1_read)
+# define AO_int_fetch_and_add1_read(addr) \
+                                AO_int_fetch_and_add_read(addr,1)
+# define AO_HAVE_int_fetch_and_add1_read
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_int_fetch_and_add1_release_write)
+# define AO_int_fetch_and_add1_release_write(addr) \
+                                AO_int_fetch_and_add_release_write(addr,1)
+# define AO_HAVE_int_fetch_and_add1_release_write
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_int_fetch_and_add1_acquire_read)
+# define AO_int_fetch_and_add1_acquire_read(addr) \
+                                AO_int_fetch_and_add_acquire_read(addr,1)
+# define AO_HAVE_int_fetch_and_add1_acquire_read
+#endif
+#if defined(AO_HAVE_int_fetch_and_add) \
+    && !defined(AO_HAVE_int_fetch_and_add1)
+# define AO_int_fetch_and_add1(addr) AO_int_fetch_and_add(addr,1)
+# define AO_HAVE_int_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_add1_full)
+# if !defined(AO_HAVE_int_fetch_and_add1_release)
+#   define AO_int_fetch_and_add1_release(addr) \
+                                AO_int_fetch_and_add1_full(addr)
+#   define AO_HAVE_int_fetch_and_add1_release
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add1_acquire)
+#   define AO_int_fetch_and_add1_acquire(addr) \
+                                AO_int_fetch_and_add1_full(addr)
+#   define AO_HAVE_int_fetch_and_add1_acquire
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add1_write)
+#   define AO_int_fetch_and_add1_write(addr) \
+                                AO_int_fetch_and_add1_full(addr)
+#   define AO_HAVE_int_fetch_and_add1_write
+# endif
+# if !defined(AO_HAVE_int_fetch_and_add1_read)
+#   define AO_int_fetch_and_add1_read(addr) \
+                                AO_int_fetch_and_add1_full(addr)
+#   define AO_HAVE_int_fetch_and_add1_read
+# endif
+#endif /* AO_HAVE_int_fetch_and_add1_full */
+
+#if !defined(AO_HAVE_int_fetch_and_add1) \
+    && defined(AO_HAVE_int_fetch_and_add1_release)
+# define AO_int_fetch_and_add1(addr) AO_int_fetch_and_add1_release(addr)
+# define AO_HAVE_int_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1) \
+    && defined(AO_HAVE_int_fetch_and_add1_acquire)
+# define AO_int_fetch_and_add1(addr) AO_int_fetch_and_add1_acquire(addr)
+# define AO_HAVE_int_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1) \
+    && defined(AO_HAVE_int_fetch_and_add1_write)
+# define AO_int_fetch_and_add1(addr) AO_int_fetch_and_add1_write(addr)
+# define AO_HAVE_int_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1) \
+    && defined(AO_HAVE_int_fetch_and_add1_read)
+# define AO_int_fetch_and_add1(addr) AO_int_fetch_and_add1_read(addr)
+# define AO_HAVE_int_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_add1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_fetch_and_add1_full)
+# define AO_int_fetch_and_add1_full(addr) \
+                        (AO_nop_full(), AO_int_fetch_and_add1_acquire(addr))
+# define AO_HAVE_int_fetch_and_add1_full
+#endif
+
+#if !defined(AO_HAVE_int_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_int_fetch_and_add1_write)
+# define AO_int_fetch_and_add1_release_write(addr) \
+                                AO_int_fetch_and_add1_write(addr)
+# define AO_HAVE_int_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_int_fetch_and_add1_release)
+# define AO_int_fetch_and_add1_release_write(addr) \
+                                AO_int_fetch_and_add1_release(addr)
+# define AO_HAVE_int_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_add1_read)
+# define AO_int_fetch_and_add1_acquire_read(addr) \
+                                AO_int_fetch_and_add1_read(addr)
+# define AO_HAVE_int_fetch_and_add1_acquire_read
+#endif
+#if !defined(AO_HAVE_int_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_add1_acquire)
+# define AO_int_fetch_and_add1_acquire_read(addr) \
+                                AO_int_fetch_and_add1_acquire(addr)
+# define AO_HAVE_int_fetch_and_add1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_int_fetch_and_add1_acquire_read)
+#   define AO_int_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_int_fetch_and_add1_acquire_read(addr)
+#   define AO_HAVE_int_fetch_and_add1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_int_fetch_and_add1)
+#   define AO_int_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_int_fetch_and_add1(addr)
+#   define AO_HAVE_int_fetch_and_add1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* int_fetch_and_sub1 */
+#if defined(AO_HAVE_int_fetch_and_add_full) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_full)
+# define AO_int_fetch_and_sub1_full(addr) \
+                AO_int_fetch_and_add_full(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_full
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_release) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_release)
+# define AO_int_fetch_and_sub1_release(addr) \
+                AO_int_fetch_and_add_release(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_release
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_acquire)
+# define AO_int_fetch_and_sub1_acquire(addr) \
+                AO_int_fetch_and_add_acquire(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_acquire
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_write) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_write)
+# define AO_int_fetch_and_sub1_write(addr) \
+                AO_int_fetch_and_add_write(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_write
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_read) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_read)
+# define AO_int_fetch_and_sub1_read(addr) \
+                AO_int_fetch_and_add_read(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_read
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_release_write)
+# define AO_int_fetch_and_sub1_release_write(addr) \
+        AO_int_fetch_and_add_release_write(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_release_write
+#endif
+#if defined(AO_HAVE_int_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_acquire_read)
+# define AO_int_fetch_and_sub1_acquire_read(addr) \
+        AO_int_fetch_and_add_acquire_read(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1_acquire_read
+#endif
+#if defined(AO_HAVE_int_fetch_and_add) \
+    && !defined(AO_HAVE_int_fetch_and_sub1)
+# define AO_int_fetch_and_sub1(addr) \
+                AO_int_fetch_and_add(addr,(unsigned int)(-1))
+# define AO_HAVE_int_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_sub1_full)
+# if !defined(AO_HAVE_int_fetch_and_sub1_release)
+#   define AO_int_fetch_and_sub1_release(addr) \
+                                AO_int_fetch_and_sub1_full(addr)
+#   define AO_HAVE_int_fetch_and_sub1_release
+# endif
+# if !defined(AO_HAVE_int_fetch_and_sub1_acquire)
+#   define AO_int_fetch_and_sub1_acquire(addr) \
+                                AO_int_fetch_and_sub1_full(addr)
+#   define AO_HAVE_int_fetch_and_sub1_acquire
+# endif
+# if !defined(AO_HAVE_int_fetch_and_sub1_write)
+#   define AO_int_fetch_and_sub1_write(addr) \
+                                AO_int_fetch_and_sub1_full(addr)
+#   define AO_HAVE_int_fetch_and_sub1_write
+# endif
+# if !defined(AO_HAVE_int_fetch_and_sub1_read)
+#   define AO_int_fetch_and_sub1_read(addr) \
+                                AO_int_fetch_and_sub1_full(addr)
+#   define AO_HAVE_int_fetch_and_sub1_read
+# endif
+#endif /* AO_HAVE_int_fetch_and_sub1_full */
+
+#if !defined(AO_HAVE_int_fetch_and_sub1) \
+    && defined(AO_HAVE_int_fetch_and_sub1_release)
+# define AO_int_fetch_and_sub1(addr) AO_int_fetch_and_sub1_release(addr)
+# define AO_HAVE_int_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1) \
+    && defined(AO_HAVE_int_fetch_and_sub1_acquire)
+# define AO_int_fetch_and_sub1(addr) AO_int_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_int_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1) \
+    && defined(AO_HAVE_int_fetch_and_sub1_write)
+# define AO_int_fetch_and_sub1(addr) AO_int_fetch_and_sub1_write(addr)
+# define AO_HAVE_int_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1) \
+    && defined(AO_HAVE_int_fetch_and_sub1_read)
+# define AO_int_fetch_and_sub1(addr) AO_int_fetch_and_sub1_read(addr)
+# define AO_HAVE_int_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_int_fetch_and_sub1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_int_fetch_and_sub1_full)
+# define AO_int_fetch_and_sub1_full(addr) \
+                        (AO_nop_full(), AO_int_fetch_and_sub1_acquire(addr))
+# define AO_HAVE_int_fetch_and_sub1_full
+#endif
+
+#if !defined(AO_HAVE_int_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_int_fetch_and_sub1_write)
+# define AO_int_fetch_and_sub1_release_write(addr) \
+                                AO_int_fetch_and_sub1_write(addr)
+# define AO_HAVE_int_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_int_fetch_and_sub1_release)
+# define AO_int_fetch_and_sub1_release_write(addr) \
+                                AO_int_fetch_and_sub1_release(addr)
+# define AO_HAVE_int_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_sub1_read)
+# define AO_int_fetch_and_sub1_acquire_read(addr) \
+                                AO_int_fetch_and_sub1_read(addr)
+# define AO_HAVE_int_fetch_and_sub1_acquire_read
+#endif
+#if !defined(AO_HAVE_int_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_int_fetch_and_sub1_acquire)
+# define AO_int_fetch_and_sub1_acquire_read(addr) \
+                                AO_int_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_int_fetch_and_sub1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_int_fetch_and_sub1_acquire_read)
+#   define AO_int_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_int_fetch_and_sub1_acquire_read(addr)
+#   define AO_HAVE_int_fetch_and_sub1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_int_fetch_and_sub1)
+#   define AO_int_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_int_fetch_and_sub1(addr)
+#   define AO_HAVE_int_fetch_and_sub1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.template b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.template
new file mode 100644
index 0000000..a66988e
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize-small.template
@@ -0,0 +1,599 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* XSIZE_load */
+#if defined(AO_HAVE_XSIZE_load_full) && !defined(AO_HAVE_XSIZE_load_acquire)
+# define AO_XSIZE_load_acquire(addr) AO_XSIZE_load_full(addr)
+# define AO_HAVE_XSIZE_load_acquire
+#endif
+
+#if defined(AO_HAVE_XSIZE_load_acquire) && !defined(AO_HAVE_XSIZE_load)
+# define AO_XSIZE_load(addr) AO_XSIZE_load_acquire(addr)
+# define AO_HAVE_XSIZE_load
+#endif
+
+#if defined(AO_HAVE_XSIZE_load_full) && !defined(AO_HAVE_XSIZE_load_read)
+# define AO_XSIZE_load_read(addr) AO_XSIZE_load_full(addr)
+# define AO_HAVE_XSIZE_load_read
+#endif
+
+#if !defined(AO_HAVE_XSIZE_load_acquire_read) \
+    && defined(AO_HAVE_XSIZE_load_acquire)
+# define AO_XSIZE_load_acquire_read(addr) AO_XSIZE_load_acquire(addr)
+# define AO_HAVE_XSIZE_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_XSIZE_load) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_load_acquire)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_load_acquire(const volatile unsigned XCTYPE *addr)
+  {
+    unsigned XCTYPE result = AO_XSIZE_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_XSIZE_load_acquire
+#endif
+
+#if defined(AO_HAVE_XSIZE_load) && defined(AO_HAVE_nop_read) \
+    && !defined(AO_HAVE_XSIZE_load_read)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_load_read(const volatile unsigned XCTYPE *addr)
+  {
+    unsigned XCTYPE result = AO_XSIZE_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed    */
+    /* beyond it.                                                           */
+    AO_nop_read();
+    return result;
+  }
+# define AO_HAVE_XSIZE_load_read
+#endif
+
+#if defined(AO_HAVE_XSIZE_load_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_load_full)
+# define AO_XSIZE_load_full(addr) (AO_nop_full(), AO_XSIZE_load_acquire(addr))
+# define AO_HAVE_XSIZE_load_full
+#endif
+
+#if !defined(AO_HAVE_XSIZE_load_acquire_read) \
+    && defined(AO_HAVE_XSIZE_load_read)
+# define AO_XSIZE_load_acquire_read(addr) AO_XSIZE_load_read(addr)
+# define AO_HAVE_XSIZE_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_XSIZE_load_acquire_read) && !defined(AO_HAVE_XSIZE_load)
+# define AO_XSIZE_load(addr) AO_XSIZE_load_acquire_read(addr)
+# define AO_HAVE_XSIZE_load
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_XSIZE_load_acquire_read)
+#   define AO_XSIZE_load_dd_acquire_read(addr) AO_XSIZE_load_acquire_read(addr)
+#   define AO_HAVE_XSIZE_load_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_XSIZE_load)
+#   define AO_XSIZE_load_dd_acquire_read(addr) AO_XSIZE_load(addr)
+#   define AO_HAVE_XSIZE_load_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* XSIZE_store */
+#if defined(AO_HAVE_XSIZE_store_full) && !defined(AO_HAVE_XSIZE_store_release)
+# define AO_XSIZE_store_release(addr,val) AO_XSIZE_store_full(addr,val)
+# define AO_HAVE_XSIZE_store_release
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_release) && !defined(AO_HAVE_XSIZE_store)
+# define AO_XSIZE_store(addr, val) AO_XSIZE_store_release(addr,val)
+# define AO_HAVE_XSIZE_store
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_full) && !defined(AO_HAVE_XSIZE_store_write)
+# define AO_XSIZE_store_write(addr,val) AO_XSIZE_store_full(addr,val)
+# define AO_HAVE_XSIZE_store_write
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_release) \
+    && !defined(AO_HAVE_XSIZE_store_release_write)
+# define AO_XSIZE_store_release_write(addr, val) \
+                            AO_XSIZE_store_release(addr,val)
+# define AO_HAVE_XSIZE_store_release_write
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_write) && !defined(AO_HAVE_XSIZE_store)
+# define AO_XSIZE_store(addr, val) AO_XSIZE_store_write(addr,val)
+# define AO_HAVE_XSIZE_store
+#endif
+
+#if defined(AO_HAVE_XSIZE_store) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_store_release)
+# define AO_XSIZE_store_release(addr,val) \
+                                (AO_nop_full(), AO_XSIZE_store(addr,val))
+# define AO_HAVE_XSIZE_store_release
+#endif
+
+#if defined(AO_HAVE_nop_write) && defined(AO_HAVE_XSIZE_store) \
+    && !defined(AO_HAVE_XSIZE_store_write)
+# define AO_XSIZE_store_write(addr, val) \
+                                (AO_nop_write(), AO_XSIZE_store(addr,val))
+# define AO_HAVE_XSIZE_store_write
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_write) \
+    && !defined(AO_HAVE_XSIZE_store_release_write)
+# define AO_XSIZE_store_release_write(addr, val) AO_XSIZE_store_write(addr,val)
+# define AO_HAVE_XSIZE_store_release_write
+#endif
+
+#if defined(AO_HAVE_XSIZE_store_release) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_store_full)
+# define AO_XSIZE_store_full(addr, val) \
+                        (AO_XSIZE_store_release(addr, val), AO_nop_full())
+# define AO_HAVE_XSIZE_store_full
+#endif
+
+/* XSIZE_fetch_and_add */
+#if defined(AO_HAVE_XSIZE_compare_and_swap_full) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add_full)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_fetch_and_add_full(volatile unsigned XCTYPE *addr,
+                              unsigned XCTYPE incr)
+  {
+    unsigned XCTYPE old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_XSIZE_compare_and_swap_full(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_XSIZE_fetch_and_add_full
+#endif
+
+#if defined(AO_HAVE_XSIZE_compare_and_swap_acquire) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add_acquire)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_fetch_and_add_acquire(volatile unsigned XCTYPE *addr,
+                                 unsigned XCTYPE incr)
+  {
+    unsigned XCTYPE old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_XSIZE_compare_and_swap_acquire(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_XSIZE_fetch_and_add_acquire
+#endif
+
+#if defined(AO_HAVE_XSIZE_compare_and_swap_release) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add_release)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_fetch_and_add_release(volatile unsigned XCTYPE *addr,
+                                 unsigned XCTYPE incr)
+  {
+    unsigned XCTYPE old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_XSIZE_compare_and_swap_release(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_XSIZE_fetch_and_add_release
+#endif
+
+#if defined(AO_HAVE_XSIZE_compare_and_swap) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add)
+  AO_INLINE unsigned XCTYPE
+  AO_XSIZE_fetch_and_add(volatile unsigned XCTYPE *addr, unsigned XCTYPE incr)
+  {
+    unsigned XCTYPE old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_XSIZE_compare_and_swap(addr, old, old + incr));
+    return old;
+  }
+# define AO_HAVE_XSIZE_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_add_full)
+# if !defined(AO_HAVE_XSIZE_fetch_and_add_release)
+#   define AO_XSIZE_fetch_and_add_release(addr, val) \
+                                AO_XSIZE_fetch_and_add_full(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_release
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add_acquire)
+#   define AO_XSIZE_fetch_and_add_acquire(addr, val) \
+                                AO_XSIZE_fetch_and_add_full(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_acquire
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add_write)
+#   define AO_XSIZE_fetch_and_add_write(addr, val) \
+                                AO_XSIZE_fetch_and_add_full(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_write
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add_read)
+#   define AO_XSIZE_fetch_and_add_read(addr, val) \
+                                AO_XSIZE_fetch_and_add_full(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_read
+# endif
+#endif /* AO_HAVE_XSIZE_fetch_and_add_full */
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_release)
+# define AO_XSIZE_fetch_and_add(addr, val) \
+                                AO_XSIZE_fetch_and_add_release(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_acquire)
+# define AO_XSIZE_fetch_and_add(addr, val) \
+                                AO_XSIZE_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_write)
+# define AO_XSIZE_fetch_and_add(addr, val) \
+                                AO_XSIZE_fetch_and_add_write(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_read)
+# define AO_XSIZE_fetch_and_add(addr, val) \
+                                AO_XSIZE_fetch_and_add_read(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_add_acquire) \
+    && defined(AO_HAVE_nop_full) && !defined(AO_HAVE_XSIZE_fetch_and_add_full)
+# define AO_XSIZE_fetch_and_add_full(addr, val) \
+                (AO_nop_full(), AO_XSIZE_fetch_and_add_acquire(addr, val))
+# define AO_HAVE_XSIZE_fetch_and_add_full
+#endif
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_add_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_write)
+# define AO_XSIZE_fetch_and_add_release_write(addr, val) \
+                                AO_XSIZE_fetch_and_add_write(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_release)
+# define AO_XSIZE_fetch_and_add_release_write(addr, val) \
+                                AO_XSIZE_fetch_and_add_release(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add_release_write
+#endif
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_read)
+# define AO_XSIZE_fetch_and_add_acquire_read(addr, val) \
+                                AO_XSIZE_fetch_and_add_read(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add_acquire_read
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add_acquire)
+# define AO_XSIZE_fetch_and_add_acquire_read(addr, val) \
+                                AO_XSIZE_fetch_and_add_acquire(addr, val)
+# define AO_HAVE_XSIZE_fetch_and_add_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_XSIZE_fetch_and_add_acquire_read)
+#   define AO_XSIZE_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_XSIZE_fetch_and_add_acquire_read(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_XSIZE_fetch_and_add)
+#   define AO_XSIZE_fetch_and_add_dd_acquire_read(addr, val) \
+                                AO_XSIZE_fetch_and_add(addr, val)
+#   define AO_HAVE_XSIZE_fetch_and_add_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* XSIZE_fetch_and_add1 */
+#if defined(AO_HAVE_XSIZE_fetch_and_add_full) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_full)
+# define AO_XSIZE_fetch_and_add1_full(addr) \
+                                AO_XSIZE_fetch_and_add_full(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_full
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_release) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_release)
+# define AO_XSIZE_fetch_and_add1_release(addr) \
+                                AO_XSIZE_fetch_and_add_release(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_release
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_acquire)
+# define AO_XSIZE_fetch_and_add1_acquire(addr) \
+                                AO_XSIZE_fetch_and_add_acquire(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_acquire
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_write) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_write)
+# define AO_XSIZE_fetch_and_add1_write(addr) \
+                                AO_XSIZE_fetch_and_add_write(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_write
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_read) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_read)
+# define AO_XSIZE_fetch_and_add1_read(addr) \
+                                AO_XSIZE_fetch_and_add_read(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_read
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_release_write)
+# define AO_XSIZE_fetch_and_add1_release_write(addr) \
+                                AO_XSIZE_fetch_and_add_release_write(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_release_write
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_acquire_read)
+# define AO_XSIZE_fetch_and_add1_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_add_acquire_read(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1_acquire_read
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1)
+# define AO_XSIZE_fetch_and_add1(addr) AO_XSIZE_fetch_and_add(addr,1)
+# define AO_HAVE_XSIZE_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_add1_full)
+# if !defined(AO_HAVE_XSIZE_fetch_and_add1_release)
+#   define AO_XSIZE_fetch_and_add1_release(addr) \
+                                AO_XSIZE_fetch_and_add1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_release
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add1_acquire)
+#   define AO_XSIZE_fetch_and_add1_acquire(addr) \
+                                AO_XSIZE_fetch_and_add1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_acquire
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add1_write)
+#   define AO_XSIZE_fetch_and_add1_write(addr) \
+                                AO_XSIZE_fetch_and_add1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_write
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_add1_read)
+#   define AO_XSIZE_fetch_and_add1_read(addr) \
+                                AO_XSIZE_fetch_and_add1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_read
+# endif
+#endif /* AO_HAVE_XSIZE_fetch_and_add1_full */
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_release)
+# define AO_XSIZE_fetch_and_add1(addr) AO_XSIZE_fetch_and_add1_release(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_acquire)
+# define AO_XSIZE_fetch_and_add1(addr) AO_XSIZE_fetch_and_add1_acquire(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_write)
+# define AO_XSIZE_fetch_and_add1(addr) AO_XSIZE_fetch_and_add1_write(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_read)
+# define AO_XSIZE_fetch_and_add1(addr) AO_XSIZE_fetch_and_add1_read(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_add1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_add1_full)
+# define AO_XSIZE_fetch_and_add1_full(addr) \
+                        (AO_nop_full(), AO_XSIZE_fetch_and_add1_acquire(addr))
+# define AO_HAVE_XSIZE_fetch_and_add1_full
+#endif
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_write)
+# define AO_XSIZE_fetch_and_add1_release_write(addr) \
+                                AO_XSIZE_fetch_and_add1_write(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_release)
+# define AO_XSIZE_fetch_and_add1_release_write(addr) \
+                                AO_XSIZE_fetch_and_add1_release(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_read)
+# define AO_XSIZE_fetch_and_add1_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_add1_read(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1_acquire_read
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_add1_acquire)
+# define AO_XSIZE_fetch_and_add1_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_add1_acquire(addr)
+# define AO_HAVE_XSIZE_fetch_and_add1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_XSIZE_fetch_and_add1_acquire_read)
+#   define AO_XSIZE_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_add1_acquire_read(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_XSIZE_fetch_and_add1)
+#   define AO_XSIZE_fetch_and_add1_dd_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_add1(addr)
+#   define AO_HAVE_XSIZE_fetch_and_add1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* XSIZE_fetch_and_sub1 */
+#if defined(AO_HAVE_XSIZE_fetch_and_add_full) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_full)
+# define AO_XSIZE_fetch_and_sub1_full(addr) \
+                AO_XSIZE_fetch_and_add_full(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_full
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_release) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_release)
+# define AO_XSIZE_fetch_and_sub1_release(addr) \
+                AO_XSIZE_fetch_and_add_release(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_release
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire)
+# define AO_XSIZE_fetch_and_sub1_acquire(addr) \
+                AO_XSIZE_fetch_and_add_acquire(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_acquire
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_write) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_write)
+# define AO_XSIZE_fetch_and_sub1_write(addr) \
+                AO_XSIZE_fetch_and_add_write(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_write
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_read) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_read)
+# define AO_XSIZE_fetch_and_sub1_read(addr) \
+                AO_XSIZE_fetch_and_add_read(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_read
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_release_write)
+# define AO_XSIZE_fetch_and_sub1_release_write(addr) \
+        AO_XSIZE_fetch_and_add_release_write(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_release_write
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire_read)
+# define AO_XSIZE_fetch_and_sub1_acquire_read(addr) \
+        AO_XSIZE_fetch_and_add_acquire_read(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1_acquire_read
+#endif
+#if defined(AO_HAVE_XSIZE_fetch_and_add) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1)
+# define AO_XSIZE_fetch_and_sub1(addr) \
+                AO_XSIZE_fetch_and_add(addr,(unsigned XCTYPE)(-1))
+# define AO_HAVE_XSIZE_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_sub1_full)
+# if !defined(AO_HAVE_XSIZE_fetch_and_sub1_release)
+#   define AO_XSIZE_fetch_and_sub1_release(addr) \
+                                AO_XSIZE_fetch_and_sub1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_release
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire)
+#   define AO_XSIZE_fetch_and_sub1_acquire(addr) \
+                                AO_XSIZE_fetch_and_sub1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_acquire
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_sub1_write)
+#   define AO_XSIZE_fetch_and_sub1_write(addr) \
+                                AO_XSIZE_fetch_and_sub1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_write
+# endif
+# if !defined(AO_HAVE_XSIZE_fetch_and_sub1_read)
+#   define AO_XSIZE_fetch_and_sub1_read(addr) \
+                                AO_XSIZE_fetch_and_sub1_full(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_read
+# endif
+#endif /* AO_HAVE_XSIZE_fetch_and_sub1_full */
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_release)
+# define AO_XSIZE_fetch_and_sub1(addr) AO_XSIZE_fetch_and_sub1_release(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire)
+# define AO_XSIZE_fetch_and_sub1(addr) AO_XSIZE_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_write)
+# define AO_XSIZE_fetch_and_sub1(addr) AO_XSIZE_fetch_and_sub1_write(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_read)
+# define AO_XSIZE_fetch_and_sub1(addr) AO_XSIZE_fetch_and_sub1_read(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_XSIZE_fetch_and_sub1_full)
+# define AO_XSIZE_fetch_and_sub1_full(addr) \
+                        (AO_nop_full(), AO_XSIZE_fetch_and_sub1_acquire(addr))
+# define AO_HAVE_XSIZE_fetch_and_sub1_full
+#endif
+
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_write)
+# define AO_XSIZE_fetch_and_sub1_release_write(addr) \
+                                AO_XSIZE_fetch_and_sub1_write(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_release)
+# define AO_XSIZE_fetch_and_sub1_release_write(addr) \
+                                AO_XSIZE_fetch_and_sub1_release(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_read)
+# define AO_XSIZE_fetch_and_sub1_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_sub1_read(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1_acquire_read
+#endif
+#if !defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire)
+# define AO_XSIZE_fetch_and_sub1_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_XSIZE_fetch_and_sub1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_XSIZE_fetch_and_sub1_acquire_read)
+#   define AO_XSIZE_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_sub1_acquire_read(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_XSIZE_fetch_and_sub1)
+#   define AO_XSIZE_fetch_and_sub1_dd_acquire_read(addr) \
+                                AO_XSIZE_fetch_and_sub1(addr)
+#   define AO_HAVE_XSIZE_fetch_and_sub1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize.h
new file mode 100644
index 0000000..c9c9d2e
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/generalize.h
@@ -0,0 +1,1254 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Generalize atomic operations for atomic_ops.h.
+ * Should not be included directly.
+ *
+ * We make no attempt to define useless operations, such as
+ * AO_nop_acquire
+ * AO_nop_release
+ *
+ * We have also so far neglected to define some others, which
+ * do not appear likely to be useful, e.g. stores with acquire
+ * or read barriers.
+ *
+ * This file is sometimes included twice by atomic_ops.h.
+ * All definitions include explicit checks that we are not replacing
+ * an earlier definition.  In general, more desirable expansions
+ * appear earlier so that we are more likely to use them.
+ *
+ * We only make safe generalizations, except that by default we define
+ * the ...dd_acquire_read operations to be equivalent to those without
+ * a barrier.  On platforms for which this is unsafe, the platform-specific
+ * file must define AO_NO_DD_ORDERING.
+ */
+
+#ifndef ATOMIC_OPS_H
+# error Atomic_ops_generalize.h should not be included directly.
+#endif
+
+#if AO_CHAR_TS_T
+# define AO_TS_COMPARE_AND_SWAP_FULL(a,o,n) \
+                                AO_char_compare_and_swap_full(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP_ACQUIRE(a,o,n) \
+                                AO_char_compare_and_swap_acquire(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP_RELEASE(a,o,n) \
+                                AO_char_compare_and_swap_release(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP(a,o,n) AO_char_compare_and_swap(a,o,n)
+#endif
+
+#if AO_AO_TS_T
+# define AO_TS_COMPARE_AND_SWAP_FULL(a,o,n) AO_compare_and_swap_full(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP_ACQUIRE(a,o,n) \
+                                        AO_compare_and_swap_acquire(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP_RELEASE(a,o,n) \
+                                        AO_compare_and_swap_release(a,o,n)
+# define AO_TS_COMPARE_AND_SWAP(a,o,n) AO_compare_and_swap(a,o,n)
+#endif
+
+/* Generate test_and_set_full, if necessary and possible.       */
+#if !defined(AO_HAVE_test_and_set) && !defined(AO_HAVE_test_and_set_release) \
+    && !defined(AO_HAVE_test_and_set_acquire) \
+    && !defined(AO_HAVE_test_and_set_read) \
+    && !defined(AO_HAVE_test_and_set_full)
+# if (AO_AO_TS_T && defined(AO_HAVE_compare_and_swap_full)) \
+     || (AO_CHAR_TS_T && defined(AO_HAVE_char_compare_and_swap_full))
+    AO_INLINE AO_TS_VAL_t
+    AO_test_and_set_full(volatile AO_TS_t *addr)
+    {
+      if (AO_TS_COMPARE_AND_SWAP_FULL(addr, AO_TS_CLEAR, AO_TS_SET))
+        return AO_TS_CLEAR;
+      else
+        return AO_TS_SET;
+    }
+#   define AO_HAVE_test_and_set_full
+# endif /* AO_HAVE_compare_and_swap_full */
+
+# if (AO_AO_TS_T && defined(AO_HAVE_compare_and_swap_acquire)) \
+     || (AO_CHAR_TS_T && defined(AO_HAVE_char_compare_and_swap_acquire))
+    AO_INLINE AO_TS_VAL_t
+    AO_test_and_set_acquire(volatile AO_TS_t *addr)
+    {
+      if (AO_TS_COMPARE_AND_SWAP_ACQUIRE(addr, AO_TS_CLEAR, AO_TS_SET))
+        return AO_TS_CLEAR;
+      else
+        return AO_TS_SET;
+    }
+#   define AO_HAVE_test_and_set_acquire
+# endif /* AO_HAVE_compare_and_swap_acquire */
+
+# if (AO_AO_TS_T && defined(AO_HAVE_compare_and_swap_release)) \
+     || (AO_CHAR_TS_T && defined(AO_HAVE_char_compare_and_swap_release))
+    AO_INLINE AO_TS_VAL_t
+    AO_test_and_set_release(volatile AO_TS_t *addr)
+    {
+      if (AO_TS_COMPARE_AND_SWAP_RELEASE(addr, AO_TS_CLEAR, AO_TS_SET))
+        return AO_TS_CLEAR;
+      else
+        return AO_TS_SET;
+    }
+#   define AO_HAVE_test_and_set_release
+# endif /* AO_HAVE_compare_and_swap_release */
+
+# if (AO_AO_TS_T && defined(AO_HAVE_compare_and_swap)) \
+     || (AO_CHAR_TS_T && defined(AO_HAVE_char_compare_and_swap))
+    AO_INLINE AO_TS_VAL_t
+    AO_test_and_set(volatile AO_TS_t *addr)
+    {
+      if (AO_TS_COMPARE_AND_SWAP(addr, AO_TS_CLEAR, AO_TS_SET))
+        return AO_TS_CLEAR;
+      else
+        return AO_TS_SET;
+    }
+#   define AO_HAVE_test_and_set
+# endif /* AO_HAVE_compare_and_swap */
+
+# if defined(AO_HAVE_test_and_set) && defined(AO_HAVE_nop_full) \
+     && !defined(AO_HAVE_test_and_set_acquire)
+    AO_INLINE AO_TS_VAL_t
+    AO_test_and_set_acquire(volatile AO_TS_t *addr)
+    {
+      AO_TS_VAL_t result = AO_test_and_set(addr);
+      AO_nop_full();
+      return result;
+    }
+#   define AO_HAVE_test_and_set_acquire
+# endif
+#endif /* No prior test and set */
+
+/* Nop */
+#if !defined(AO_HAVE_nop)
+  AO_INLINE void AO_nop(void) {}
+# define AO_HAVE_nop
+#endif
+
+#if defined(AO_HAVE_test_and_set_full) && !defined(AO_HAVE_nop_full)
+  AO_INLINE void
+  AO_nop_full(void)
+  {
+    AO_TS_t dummy = AO_TS_INITIALIZER;
+    AO_test_and_set_full(&dummy);
+  }
+# define AO_HAVE_nop_full
+#endif
+
+#if defined(AO_HAVE_nop_acquire)
+# error AO_nop_acquire is useless: dont define.
+#endif
+#if defined(AO_HAVE_nop_release)
+# error AO_nop_release is useless: dont define.
+#endif
+
+#if defined(AO_HAVE_nop_full) && !defined(AO_HAVE_nop_read)
+# define AO_nop_read() AO_nop_full()
+# define AO_HAVE_nop_read
+#endif
+
+#if defined(AO_HAVE_nop_full) && !defined(AO_HAVE_nop_write)
+# define AO_nop_write() AO_nop_full()
+# define AO_HAVE_nop_write
+#endif
+
+/* Load */
+#if defined(AO_HAVE_load_full) && !defined(AO_HAVE_load_acquire)
+# define AO_load_acquire(addr) AO_load_full(addr)
+# define AO_HAVE_load_acquire
+#endif
+
+#if defined(AO_HAVE_load_acquire) && !defined(AO_HAVE_load)
+# define AO_load(addr) AO_load_acquire(addr)
+# define AO_HAVE_load
+#endif
+
+#if defined(AO_HAVE_load_full) && !defined(AO_HAVE_load_read)
+# define AO_load_read(addr) AO_load_full(addr)
+# define AO_HAVE_load_read
+#endif
+
+#if !defined(AO_HAVE_load_acquire_read) && defined(AO_HAVE_load_acquire)
+# define AO_load_acquire_read(addr) AO_load_acquire(addr)
+# define AO_HAVE_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_load) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_load_acquire)
+  AO_INLINE AO_t
+  AO_load_acquire(const volatile AO_t *addr)
+  {
+    AO_t result = AO_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed */
+    /* beyond it.                                                        */
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_load_acquire
+#endif
+
+#if defined(AO_HAVE_load) && defined(AO_HAVE_nop_read) \
+    && !defined(AO_HAVE_load_read)
+  AO_INLINE AO_t
+  AO_load_read(const volatile AO_t *addr)
+  {
+    AO_t result = AO_load(addr);
+    /* Acquire barrier would be useless, since the load could be delayed */
+    /* beyond it.                                                        */
+    AO_nop_read();
+    return result;
+  }
+# define AO_HAVE_load_read
+#endif
+
+#if defined(AO_HAVE_load_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_load_full)
+# define AO_load_full(addr) (AO_nop_full(), AO_load_acquire(addr))
+# define AO_HAVE_load_full
+#endif
+
+#if !defined(AO_HAVE_load_acquire_read) && defined(AO_HAVE_load_read)
+# define AO_load_acquire_read(addr) AO_load_read(addr)
+# define AO_HAVE_load_acquire_read
+#endif
+
+#if defined(AO_HAVE_load_acquire_read) && !defined(AO_HAVE_load)
+# define AO_load(addr) AO_load_acquire_read(addr)
+# define AO_HAVE_load
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_load_acquire_read)
+#   define AO_load_dd_acquire_read(addr) AO_load_acquire_read(addr)
+#   define AO_HAVE_load_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_load)
+#   define AO_load_dd_acquire_read(addr) AO_load(addr)
+#   define AO_HAVE_load_dd_acquire_read
+# endif
+#endif
+
+/* Store */
+#if defined(AO_HAVE_store_full) && !defined(AO_HAVE_store_release)
+# define AO_store_release(addr,val) AO_store_full(addr,val)
+# define AO_HAVE_store_release
+#endif
+
+#if defined(AO_HAVE_store_release) && !defined(AO_HAVE_store)
+# define AO_store(addr,val) AO_store_release(addr,val)
+# define AO_HAVE_store
+#endif
+
+#if defined(AO_HAVE_store_full) && !defined(AO_HAVE_store_write)
+# define AO_store_write(addr,val) AO_store_full(addr,val)
+# define AO_HAVE_store_write
+#endif
+
+#if defined(AO_HAVE_store_release) && !defined(AO_HAVE_store_release_write)
+# define AO_store_release_write(addr,val) AO_store_release(addr,val)
+# define AO_HAVE_store_release_write
+#endif
+
+#if defined(AO_HAVE_store_write) && !defined(AO_HAVE_store)
+# define AO_store(addr,val) AO_store_write(addr,val)
+# define AO_HAVE_store
+#endif
+
+#if defined(AO_HAVE_store) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_store_release)
+# define AO_store_release(addr,val) (AO_nop_full(), AO_store(addr,val))
+# define AO_HAVE_store_release
+#endif
+
+#if defined(AO_HAVE_nop_write) && defined(AO_HAVE_store) \
+    && !defined(AO_HAVE_store_write)
+# define AO_store_write(addr,val) (AO_nop_write(), AO_store(addr,val))
+# define AO_HAVE_store_write
+#endif
+
+#if defined(AO_HAVE_store_write) && !defined(AO_HAVE_store_release_write)
+# define AO_store_release_write(addr,val) AO_store_write(addr,val)
+# define AO_HAVE_store_release_write
+#endif
+
+#if defined(AO_HAVE_store_release) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_store_full)
+# define AO_store_full(addr,val) (AO_store_release(addr,val), AO_nop_full())
+# define AO_HAVE_store_full
+#endif
+
+/* NEC LE-IT: Test and set */
+#if defined(AO_HAVE_test_and_set) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_test_and_set_release)
+# define AO_test_and_set_release(addr) (AO_nop_full(), AO_test_and_set(addr))
+# define AO_HAVE_test_and_set_release
+#endif
+
+#if defined(AO_HAVE_test_and_set) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_test_and_set_acquire)
+  AO_INLINE AO_TS_VAL_t
+  AO_test_and_set_acquire(volatile AO_TS_t *addr)
+  {
+    AO_TS_VAL_t result = AO_test_and_set(addr);
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_test_and_set_acquire
+#endif
+
+/* Fetch_and_add */
+/* We first try to implement fetch_and_add variants in terms    */
+/* of the corresponding compare_and_swap variants to minimize   */
+/* adding barriers.                                             */
+#if defined(AO_HAVE_compare_and_swap_full) \
+    && !defined(AO_HAVE_fetch_and_add_full)
+  AO_INLINE AO_t
+  AO_fetch_and_add_full(volatile AO_t *addr, AO_t incr)
+  {
+    AO_t old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_compare_and_swap_full(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_fetch_and_add_full
+#endif
+
+#if defined(AO_HAVE_compare_and_swap_acquire) \
+    && !defined(AO_HAVE_fetch_and_add_acquire)
+  AO_INLINE AO_t
+  AO_fetch_and_add_acquire(volatile AO_t *addr, AO_t incr)
+  {
+    AO_t old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_compare_and_swap_acquire(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_fetch_and_add_acquire
+#endif
+
+#if defined(AO_HAVE_compare_and_swap_release) \
+    && !defined(AO_HAVE_fetch_and_add_release)
+  AO_INLINE AO_t
+  AO_fetch_and_add_release(volatile AO_t *addr, AO_t incr)
+  {
+    AO_t old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_compare_and_swap_release(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_fetch_and_add_release
+#endif
+
+#if defined(AO_HAVE_compare_and_swap) && !defined(AO_HAVE_fetch_and_add)
+  AO_INLINE AO_t
+  AO_fetch_and_add(volatile AO_t *addr, AO_t incr)
+  {
+    AO_t old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_compare_and_swap(addr, old, old+incr));
+    return old;
+  }
+# define AO_HAVE_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_fetch_and_add_full)
+# if !defined(AO_HAVE_fetch_and_add_release)
+#   define AO_fetch_and_add_release(addr,val) AO_fetch_and_add_full(addr,val)
+#   define AO_HAVE_fetch_and_add_release
+# endif
+# if !defined(AO_HAVE_fetch_and_add_acquire)
+#   define AO_fetch_and_add_acquire(addr,val) AO_fetch_and_add_full(addr,val)
+#   define AO_HAVE_fetch_and_add_acquire
+# endif
+# if !defined(AO_HAVE_fetch_and_add_write)
+#   define AO_fetch_and_add_write(addr,val) AO_fetch_and_add_full(addr,val)
+#   define AO_HAVE_fetch_and_add_write
+# endif
+# if !defined(AO_HAVE_fetch_and_add_read)
+#   define AO_fetch_and_add_read(addr,val) AO_fetch_and_add_full(addr,val)
+#   define AO_HAVE_fetch_and_add_read
+# endif
+#endif /* AO_HAVE_fetch_and_add_full */
+
+#if !defined(AO_HAVE_fetch_and_add) && defined(AO_HAVE_fetch_and_add_release)
+# define AO_fetch_and_add(addr,val) AO_fetch_and_add_release(addr,val)
+# define AO_HAVE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_fetch_and_add) && defined(AO_HAVE_fetch_and_add_acquire)
+# define AO_fetch_and_add(addr,val) AO_fetch_and_add_acquire(addr,val)
+# define AO_HAVE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_fetch_and_add) && defined(AO_HAVE_fetch_and_add_write)
+# define AO_fetch_and_add(addr,val) AO_fetch_and_add_write(addr,val)
+# define AO_HAVE_fetch_and_add
+#endif
+#if !defined(AO_HAVE_fetch_and_add) && defined(AO_HAVE_fetch_and_add_read)
+# define AO_fetch_and_add(addr,val) AO_fetch_and_add_read(addr,val)
+# define AO_HAVE_fetch_and_add
+#endif
+
+#if defined(AO_HAVE_fetch_and_add_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_fetch_and_add_full)
+# define AO_fetch_and_add_full(addr,val) \
+                        (AO_nop_full(), AO_fetch_and_add_acquire(addr,val))
+# define AO_HAVE_fetch_and_add_full
+#endif
+
+#if !defined(AO_HAVE_fetch_and_add_release_write) \
+    && defined(AO_HAVE_fetch_and_add_write)
+# define AO_fetch_and_add_release_write(addr,val) \
+                                        AO_fetch_and_add_write(addr,val)
+# define AO_HAVE_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_add_release_write) \
+    && defined(AO_HAVE_fetch_and_add_release)
+# define AO_fetch_and_add_release_write(addr,val) \
+                                        AO_fetch_and_add_release(addr,val)
+# define AO_HAVE_fetch_and_add_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_fetch_and_add_read)
+# define AO_fetch_and_add_acquire_read(addr,val) \
+                                        AO_fetch_and_add_read(addr,val)
+# define AO_HAVE_fetch_and_add_acquire_read
+#endif
+#if !defined(AO_HAVE_fetch_and_add_acquire_read) \
+    && defined(AO_HAVE_fetch_and_add_acquire)
+# define AO_fetch_and_add_acquire_read(addr,val) \
+                                        AO_fetch_and_add_acquire(addr,val)
+# define AO_HAVE_fetch_and_add_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_fetch_and_add_acquire_read)
+#   define AO_fetch_and_add_dd_acquire_read(addr,val) \
+                                AO_fetch_and_add_acquire_read(addr,val)
+#   define AO_HAVE_fetch_and_add_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_fetch_and_add)
+#   define AO_fetch_and_add_dd_acquire_read(addr,val) \
+                                        AO_fetch_and_add(addr,val)
+#   define AO_HAVE_fetch_and_add_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* Fetch_and_add1 */
+
+#if defined(AO_HAVE_fetch_and_add_full) \
+    && !defined(AO_HAVE_fetch_and_add1_full)
+# define AO_fetch_and_add1_full(addr) AO_fetch_and_add_full(addr,1)
+# define AO_HAVE_fetch_and_add1_full
+#endif
+#if defined(AO_HAVE_fetch_and_add_release) \
+    && !defined(AO_HAVE_fetch_and_add1_release)
+# define AO_fetch_and_add1_release(addr) AO_fetch_and_add_release(addr,1)
+# define AO_HAVE_fetch_and_add1_release
+#endif
+#if defined(AO_HAVE_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_fetch_and_add1_acquire)
+# define AO_fetch_and_add1_acquire(addr) AO_fetch_and_add_acquire(addr,1)
+# define AO_HAVE_fetch_and_add1_acquire
+#endif
+#if defined(AO_HAVE_fetch_and_add_write) \
+    && !defined(AO_HAVE_fetch_and_add1_write)
+# define AO_fetch_and_add1_write(addr) AO_fetch_and_add_write(addr,1)
+# define AO_HAVE_fetch_and_add1_write
+#endif
+#if defined(AO_HAVE_fetch_and_add_read) \
+    && !defined(AO_HAVE_fetch_and_add1_read)
+# define AO_fetch_and_add1_read(addr) AO_fetch_and_add_read(addr,1)
+# define AO_HAVE_fetch_and_add1_read
+#endif
+#if defined(AO_HAVE_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_fetch_and_add1_release_write)
+# define AO_fetch_and_add1_release_write(addr) \
+                                        AO_fetch_and_add_release_write(addr,1)
+# define AO_HAVE_fetch_and_add1_release_write
+#endif
+#if defined(AO_HAVE_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_fetch_and_add1_acquire_read)
+# define AO_fetch_and_add1_acquire_read(addr) \
+                                        AO_fetch_and_add_acquire_read(addr,1)
+# define AO_HAVE_fetch_and_add1_acquire_read
+#endif
+#if defined(AO_HAVE_fetch_and_add) && !defined(AO_HAVE_fetch_and_add1)
+# define AO_fetch_and_add1(addr) AO_fetch_and_add(addr,1)
+# define AO_HAVE_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_fetch_and_add1_full)
+# if !defined(AO_HAVE_fetch_and_add1_release)
+#   define AO_fetch_and_add1_release(addr) AO_fetch_and_add1_full(addr)
+#   define AO_HAVE_fetch_and_add1_release
+# endif
+# if !defined(AO_HAVE_fetch_and_add1_acquire)
+#   define AO_fetch_and_add1_acquire(addr) AO_fetch_and_add1_full(addr)
+#   define AO_HAVE_fetch_and_add1_acquire
+# endif
+# if !defined(AO_HAVE_fetch_and_add1_write)
+#   define AO_fetch_and_add1_write(addr) AO_fetch_and_add1_full(addr)
+#   define AO_HAVE_fetch_and_add1_write
+# endif
+# if !defined(AO_HAVE_fetch_and_add1_read)
+#   define AO_fetch_and_add1_read(addr) AO_fetch_and_add1_full(addr)
+#   define AO_HAVE_fetch_and_add1_read
+# endif
+#endif /* AO_HAVE_fetch_and_add1_full */
+
+#if !defined(AO_HAVE_fetch_and_add1) \
+    && defined(AO_HAVE_fetch_and_add1_release)
+# define AO_fetch_and_add1(addr) AO_fetch_and_add1_release(addr)
+# define AO_HAVE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_fetch_and_add1) \
+    && defined(AO_HAVE_fetch_and_add1_acquire)
+# define AO_fetch_and_add1(addr) AO_fetch_and_add1_acquire(addr)
+# define AO_HAVE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_fetch_and_add1) && defined(AO_HAVE_fetch_and_add1_write)
+# define AO_fetch_and_add1(addr) AO_fetch_and_add1_write(addr)
+# define AO_HAVE_fetch_and_add1
+#endif
+#if !defined(AO_HAVE_fetch_and_add1) && defined(AO_HAVE_fetch_and_add1_read)
+# define AO_fetch_and_add1(addr) AO_fetch_and_add1_read(addr)
+# define AO_HAVE_fetch_and_add1
+#endif
+
+#if defined(AO_HAVE_fetch_and_add1_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_fetch_and_add1_full)
+# define AO_fetch_and_add1_full(addr) \
+                        (AO_nop_full(), AO_fetch_and_add1_acquire(addr))
+# define AO_HAVE_fetch_and_add1_full
+#endif
+
+#if !defined(AO_HAVE_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_fetch_and_add1_write)
+# define AO_fetch_and_add1_release_write(addr) AO_fetch_and_add1_write(addr)
+# define AO_HAVE_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_add1_release_write) \
+    && defined(AO_HAVE_fetch_and_add1_release)
+# define AO_fetch_and_add1_release_write(addr) AO_fetch_and_add1_release(addr)
+# define AO_HAVE_fetch_and_add1_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_fetch_and_add1_read)
+# define AO_fetch_and_add1_acquire_read(addr) AO_fetch_and_add1_read(addr)
+# define AO_HAVE_fetch_and_add1_acquire_read
+#endif
+#if !defined(AO_HAVE_fetch_and_add1_acquire_read) \
+    && defined(AO_HAVE_fetch_and_add1_acquire)
+# define AO_fetch_and_add1_acquire_read(addr) AO_fetch_and_add1_acquire(addr)
+# define AO_HAVE_fetch_and_add1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_fetch_and_add1_acquire_read)
+#   define AO_fetch_and_add1_dd_acquire_read(addr) \
+                                        AO_fetch_and_add1_acquire_read(addr)
+#   define AO_HAVE_fetch_and_add1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_fetch_and_add1)
+#   define AO_fetch_and_add1_dd_acquire_read(addr) AO_fetch_and_add1(addr)
+#   define AO_HAVE_fetch_and_add1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* Fetch_and_sub1 */
+#if defined(AO_HAVE_fetch_and_add_full) \
+    && !defined(AO_HAVE_fetch_and_sub1_full)
+# define AO_fetch_and_sub1_full(addr) AO_fetch_and_add_full(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_full
+#endif
+#if defined(AO_HAVE_fetch_and_add_release) \
+    && !defined(AO_HAVE_fetch_and_sub1_release)
+# define AO_fetch_and_sub1_release(addr) \
+                                AO_fetch_and_add_release(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_release
+#endif
+#if defined(AO_HAVE_fetch_and_add_acquire) \
+    && !defined(AO_HAVE_fetch_and_sub1_acquire)
+# define AO_fetch_and_sub1_acquire(addr) \
+                                AO_fetch_and_add_acquire(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_acquire
+#endif
+#if defined(AO_HAVE_fetch_and_add_write) \
+    && !defined(AO_HAVE_fetch_and_sub1_write)
+# define AO_fetch_and_sub1_write(addr) AO_fetch_and_add_write(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_write
+#endif
+#if defined(AO_HAVE_fetch_and_add_read) \
+    && !defined(AO_HAVE_fetch_and_sub1_read)
+# define AO_fetch_and_sub1_read(addr) AO_fetch_and_add_read(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_read
+#endif
+#if defined(AO_HAVE_fetch_and_add_release_write) \
+    && !defined(AO_HAVE_fetch_and_sub1_release_write)
+# define AO_fetch_and_sub1_release_write(addr) \
+                        AO_fetch_and_add_release_write(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_release_write
+#endif
+#if defined(AO_HAVE_fetch_and_add_acquire_read) \
+    && !defined(AO_HAVE_fetch_and_sub1_acquire_read)
+# define AO_fetch_and_sub1_acquire_read(addr) \
+                        AO_fetch_and_add_acquire_read(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1_acquire_read
+#endif
+#if defined(AO_HAVE_fetch_and_add) && !defined(AO_HAVE_fetch_and_sub1)
+# define AO_fetch_and_sub1(addr) AO_fetch_and_add(addr,(AO_t)(-1))
+# define AO_HAVE_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_fetch_and_sub1_full)
+# if !defined(AO_HAVE_fetch_and_sub1_release)
+#   define AO_fetch_and_sub1_release(addr) AO_fetch_and_sub1_full(addr)
+#   define AO_HAVE_fetch_and_sub1_release
+# endif
+# if !defined(AO_HAVE_fetch_and_sub1_acquire)
+#   define AO_fetch_and_sub1_acquire(addr) AO_fetch_and_sub1_full(addr)
+#   define AO_HAVE_fetch_and_sub1_acquire
+# endif
+# if !defined(AO_HAVE_fetch_and_sub1_write)
+#   define AO_fetch_and_sub1_write(addr) AO_fetch_and_sub1_full(addr)
+#   define AO_HAVE_fetch_and_sub1_write
+# endif
+# if !defined(AO_HAVE_fetch_and_sub1_read)
+#   define AO_fetch_and_sub1_read(addr) AO_fetch_and_sub1_full(addr)
+#   define AO_HAVE_fetch_and_sub1_read
+# endif
+#endif /* AO_HAVE_fetch_and_sub1_full */
+
+#if !defined(AO_HAVE_fetch_and_sub1) \
+    && defined(AO_HAVE_fetch_and_sub1_release)
+# define AO_fetch_and_sub1(addr) AO_fetch_and_sub1_release(addr)
+# define AO_HAVE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1) \
+    && defined(AO_HAVE_fetch_and_sub1_acquire)
+# define AO_fetch_and_sub1(addr) AO_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1) && defined(AO_HAVE_fetch_and_sub1_write)
+# define AO_fetch_and_sub1(addr) AO_fetch_and_sub1_write(addr)
+# define AO_HAVE_fetch_and_sub1
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1) && defined(AO_HAVE_fetch_and_sub1_read)
+# define AO_fetch_and_sub1(addr) AO_fetch_and_sub1_read(addr)
+# define AO_HAVE_fetch_and_sub1
+#endif
+
+#if defined(AO_HAVE_fetch_and_sub1_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_fetch_and_sub1_full)
+# define AO_fetch_and_sub1_full(addr) \
+                        (AO_nop_full(), AO_fetch_and_sub1_acquire(addr))
+# define AO_HAVE_fetch_and_sub1_full
+#endif
+
+#if !defined(AO_HAVE_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_fetch_and_sub1_write)
+# define AO_fetch_and_sub1_release_write(addr) AO_fetch_and_sub1_write(addr)
+# define AO_HAVE_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1_release_write) \
+    && defined(AO_HAVE_fetch_and_sub1_release)
+# define AO_fetch_and_sub1_release_write(addr) AO_fetch_and_sub1_release(addr)
+# define AO_HAVE_fetch_and_sub1_release_write
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_fetch_and_sub1_read)
+# define AO_fetch_and_sub1_acquire_read(addr) AO_fetch_and_sub1_read(addr)
+# define AO_HAVE_fetch_and_sub1_acquire_read
+#endif
+#if !defined(AO_HAVE_fetch_and_sub1_acquire_read) \
+    && defined(AO_HAVE_fetch_and_sub1_acquire)
+# define AO_fetch_and_sub1_acquire_read(addr) AO_fetch_and_sub1_acquire(addr)
+# define AO_HAVE_fetch_and_sub1_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_fetch_and_sub1_acquire_read)
+#   define AO_fetch_and_sub1_dd_acquire_read(addr) \
+                                        AO_fetch_and_sub1_acquire_read(addr)
+#   define AO_HAVE_fetch_and_sub1_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_fetch_and_sub1)
+#   define AO_fetch_and_sub1_dd_acquire_read(addr) AO_fetch_and_sub1(addr)
+#   define AO_HAVE_fetch_and_sub1_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* Atomic or */
+#if defined(AO_HAVE_compare_and_swap_full) && !defined(AO_HAVE_or_full)
+  AO_INLINE void
+  AO_or_full(volatile AO_t *addr, AO_t incr)
+  {
+    AO_t old;
+    do
+      {
+        old = *addr;
+      }
+    while (!AO_compare_and_swap_full(addr, old, old | incr));
+  }
+# define AO_HAVE_or_full
+#endif
+
+#if defined(AO_HAVE_or_full)
+# if !defined(AO_HAVE_or_release)
+#   define AO_or_release(addr,val) AO_or_full(addr,val)
+#   define AO_HAVE_or_release
+# endif
+# if !defined(AO_HAVE_or_acquire)
+#   define AO_or_acquire(addr,val) AO_or_full(addr,val)
+#   define AO_HAVE_or_acquire
+# endif
+# if !defined(AO_HAVE_or_write)
+#   define AO_or_write(addr,val) AO_or_full(addr,val)
+#   define AO_HAVE_or_write
+# endif
+# if !defined(AO_HAVE_or_read)
+#   define AO_or_read(addr,val) AO_or_full(addr,val)
+#   define AO_HAVE_or_read
+# endif
+#endif /* AO_HAVE_or_full */
+
+#if !defined(AO_HAVE_or) && defined(AO_HAVE_or_release)
+# define AO_or(addr,val) AO_or_release(addr,val)
+# define AO_HAVE_or
+#endif
+#if !defined(AO_HAVE_or) && defined(AO_HAVE_or_acquire)
+# define AO_or(addr,val) AO_or_acquire(addr,val)
+# define AO_HAVE_or
+#endif
+#if !defined(AO_HAVE_or) && defined(AO_HAVE_or_write)
+# define AO_or(addr,val) AO_or_write(addr,val)
+# define AO_HAVE_or
+#endif
+#if !defined(AO_HAVE_or) && defined(AO_HAVE_or_read)
+# define AO_or(addr,val) AO_or_read(addr,val)
+# define AO_HAVE_or
+#endif
+
+#if defined(AO_HAVE_or_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_or_full)
+# define AO_or_full(addr,val) (AO_nop_full(), AO_or_acquire(addr,val))
+# define AO_HAVE_or_full
+#endif
+
+#if !defined(AO_HAVE_or_release_write) && defined(AO_HAVE_or_write)
+# define AO_or_release_write(addr,val) AO_or_write(addr,val)
+# define AO_HAVE_or_release_write
+#endif
+#if !defined(AO_HAVE_or_release_write) && defined(AO_HAVE_or_release)
+# define AO_or_release_write(addr,val) AO_or_release(addr,val)
+# define AO_HAVE_or_release_write
+#endif
+#if !defined(AO_HAVE_or_acquire_read) && defined(AO_HAVE_or_read)
+# define AO_or_acquire_read(addr,val) AO_or_read(addr,val)
+# define AO_HAVE_or_acquire_read
+#endif
+#if !defined(AO_HAVE_or_acquire_read) && defined(AO_HAVE_or_acquire)
+# define AO_or_acquire_read(addr,val) AO_or_acquire(addr,val)
+# define AO_HAVE_or_acquire_read
+#endif
+
+/* dd_acquire_read is meaningless.      */
+
+/* Test_and_set */
+#if defined(AO_HAVE_test_and_set_full)
+# if !defined(AO_HAVE_test_and_set_release)
+#   define AO_test_and_set_release(addr) AO_test_and_set_full(addr)
+#   define AO_HAVE_test_and_set_release
+# endif
+# if !defined(AO_HAVE_test_and_set_acquire)
+#   define AO_test_and_set_acquire(addr) AO_test_and_set_full(addr)
+#   define AO_HAVE_test_and_set_acquire
+# endif
+# if !defined(AO_HAVE_test_and_set_write)
+#   define AO_test_and_set_write(addr) AO_test_and_set_full(addr)
+#   define AO_HAVE_test_and_set_write
+# endif
+# if !defined(AO_HAVE_test_and_set_read)
+#   define AO_test_and_set_read(addr) AO_test_and_set_full(addr)
+#   define AO_HAVE_test_and_set_read
+# endif
+#endif /* AO_HAVE_test_and_set_full */
+
+#if !defined(AO_HAVE_test_and_set) && defined(AO_HAVE_test_and_set_release)
+# define AO_test_and_set(addr) AO_test_and_set_release(addr)
+# define AO_HAVE_test_and_set
+#endif
+#if !defined(AO_HAVE_test_and_set) && defined(AO_HAVE_test_and_set_acquire)
+# define AO_test_and_set(addr) AO_test_and_set_acquire(addr)
+# define AO_HAVE_test_and_set
+#endif
+#if !defined(AO_HAVE_test_and_set) && defined(AO_HAVE_test_and_set_write)
+# define AO_test_and_set(addr) AO_test_and_set_write(addr)
+# define AO_HAVE_test_and_set
+#endif
+#if !defined(AO_HAVE_test_and_set) && defined(AO_HAVE_test_and_set_read)
+# define AO_test_and_set(addr) AO_test_and_set_read(addr)
+# define AO_HAVE_test_and_set
+#endif
+
+#if defined(AO_HAVE_test_and_set_acquire) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_test_and_set_full)
+# define AO_test_and_set_full(addr) \
+                        (AO_nop_full(), AO_test_and_set_acquire(addr))
+# define AO_HAVE_test_and_set_full
+#endif
+
+#if !defined(AO_HAVE_test_and_set_release_write) \
+    && defined(AO_HAVE_test_and_set_write)
+# define AO_test_and_set_release_write(addr) AO_test_and_set_write(addr)
+# define AO_HAVE_test_and_set_release_write
+#endif
+#if !defined(AO_HAVE_test_and_set_release_write) \
+    && defined(AO_HAVE_test_and_set_release)
+# define AO_test_and_set_release_write(addr) AO_test_and_set_release(addr)
+# define AO_HAVE_test_and_set_release_write
+#endif
+#if !defined(AO_HAVE_test_and_set_acquire_read) \
+    && defined(AO_HAVE_test_and_set_read)
+# define AO_test_and_set_acquire_read(addr) AO_test_and_set_read(addr)
+# define AO_HAVE_test_and_set_acquire_read
+#endif
+#if !defined(AO_HAVE_test_and_set_acquire_read) \
+    && defined(AO_HAVE_test_and_set_acquire)
+# define AO_test_and_set_acquire_read(addr) AO_test_and_set_acquire(addr)
+# define AO_HAVE_test_and_set_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_test_and_set_acquire_read)
+#   define AO_test_and_set_dd_acquire_read(addr) \
+                                        AO_test_and_set_acquire_read(addr)
+#   define AO_HAVE_test_and_set_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_test_and_set)
+#   define AO_test_and_set_dd_acquire_read(addr) AO_test_and_set(addr)
+#   define AO_HAVE_test_and_set_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* Compare_and_swap */
+#if defined(AO_HAVE_compare_and_swap) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_and_swap_acquire)
+  AO_INLINE int
+  AO_compare_and_swap_acquire(volatile AO_t *addr, AO_t old, AO_t new_val)
+  {
+    int result = AO_compare_and_swap(addr, old, new_val);
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_compare_and_swap_acquire
+#endif
+#if defined(AO_HAVE_compare_and_swap) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_and_swap_release)
+# define AO_compare_and_swap_release(addr,old,new_val) \
+                        (AO_nop_full(), AO_compare_and_swap(addr,old,new_val))
+# define AO_HAVE_compare_and_swap_release
+#endif
+#if defined(AO_HAVE_compare_and_swap_full)
+# if !defined(AO_HAVE_compare_and_swap_release)
+#   define AO_compare_and_swap_release(addr,old,new_val) \
+                                AO_compare_and_swap_full(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_release
+# endif
+# if !defined(AO_HAVE_compare_and_swap_acquire)
+#   define AO_compare_and_swap_acquire(addr,old,new_val) \
+                                AO_compare_and_swap_full(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_acquire
+# endif
+# if !defined(AO_HAVE_compare_and_swap_write)
+#   define AO_compare_and_swap_write(addr,old,new_val) \
+                                AO_compare_and_swap_full(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_write
+# endif
+# if !defined(AO_HAVE_compare_and_swap_read)
+#   define AO_compare_and_swap_read(addr,old,new_val) \
+                                AO_compare_and_swap_full(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_read
+# endif
+#endif /* AO_HAVE_compare_and_swap_full */
+
+#if !defined(AO_HAVE_compare_and_swap) \
+    && defined(AO_HAVE_compare_and_swap_release)
+# define AO_compare_and_swap(addr,old,new_val) \
+                                AO_compare_and_swap_release(addr,old,new_val)
+# define AO_HAVE_compare_and_swap
+#endif
+#if !defined(AO_HAVE_compare_and_swap) \
+    && defined(AO_HAVE_compare_and_swap_acquire)
+# define AO_compare_and_swap(addr,old,new_val) \
+                                AO_compare_and_swap_acquire(addr,old,new_val)
+# define AO_HAVE_compare_and_swap
+#endif
+#if !defined(AO_HAVE_compare_and_swap) \
+    && defined(AO_HAVE_compare_and_swap_write)
+# define AO_compare_and_swap(addr,old,new_val) \
+                                AO_compare_and_swap_write(addr,old,new_val)
+# define AO_HAVE_compare_and_swap
+#endif
+#if !defined(AO_HAVE_compare_and_swap) \
+    && defined(AO_HAVE_compare_and_swap_read)
+# define AO_compare_and_swap(addr,old,new_val) \
+                                AO_compare_and_swap_read(addr,old,new_val)
+# define AO_HAVE_compare_and_swap
+#endif
+
+#if defined(AO_HAVE_compare_and_swap_acquire) \
+    && defined(AO_HAVE_nop_full) && !defined(AO_HAVE_compare_and_swap_full)
+# define AO_compare_and_swap_full(addr,old,new_val) \
+                (AO_nop_full(), AO_compare_and_swap_acquire(addr,old,new_val))
+# define AO_HAVE_compare_and_swap_full
+#endif
+
+#if !defined(AO_HAVE_compare_and_swap_release_write) \
+    && defined(AO_HAVE_compare_and_swap_write)
+# define AO_compare_and_swap_release_write(addr,old,new_val) \
+                                AO_compare_and_swap_write(addr,old,new_val)
+# define AO_HAVE_compare_and_swap_release_write
+#endif
+#if !defined(AO_HAVE_compare_and_swap_release_write) \
+    && defined(AO_HAVE_compare_and_swap_release)
+# define AO_compare_and_swap_release_write(addr,old,new_val) \
+                                AO_compare_and_swap_release(addr,old,new_val)
+# define AO_HAVE_compare_and_swap_release_write
+#endif
+#if !defined(AO_HAVE_compare_and_swap_acquire_read) \
+    && defined(AO_HAVE_compare_and_swap_read)
+# define AO_compare_and_swap_acquire_read(addr,old,new_val) \
+                                AO_compare_and_swap_read(addr,old,new_val)
+# define AO_HAVE_compare_and_swap_acquire_read
+#endif
+#if !defined(AO_HAVE_compare_and_swap_acquire_read) \
+    && defined(AO_HAVE_compare_and_swap_acquire)
+# define AO_compare_and_swap_acquire_read(addr,old,new_val) \
+                                AO_compare_and_swap_acquire(addr,old,new_val)
+# define AO_HAVE_compare_and_swap_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_compare_and_swap_acquire_read)
+#   define AO_compare_and_swap_dd_acquire_read(addr,old,new_val) \
+                        AO_compare_and_swap_acquire_read(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_compare_and_swap)
+#   define AO_compare_and_swap_dd_acquire_read(addr,old,new_val) \
+                                AO_compare_and_swap(addr,old,new_val)
+#   define AO_HAVE_compare_and_swap_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+#include "generalize-small.h"
+
+/* Compare_double_and_swap_double */
+#if defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_double_and_swap_double_acquire)
+  AO_INLINE int
+  AO_compare_double_and_swap_double_acquire(volatile AO_double_t *addr,
+                                            AO_t o1, AO_t o2,
+                                            AO_t n1, AO_t n2)
+  {
+    int result = AO_compare_double_and_swap_double(addr, o1, o2, n1, n2);
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_compare_double_and_swap_double_acquire
+#endif
+#if defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_double_and_swap_double_release)
+# define AO_compare_double_and_swap_double_release(addr,o1,o2,n1,n2) \
+      (AO_nop_full(), AO_compare_double_and_swap_double(addr,o1,o2,n1,n2))
+# define AO_HAVE_compare_double_and_swap_double_release
+#endif
+#if defined(AO_HAVE_compare_double_and_swap_double_full)
+# if !defined(AO_HAVE_compare_double_and_swap_double_release)
+#   define AO_compare_double_and_swap_double_release(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_full(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_release
+# endif
+# if !defined(AO_HAVE_compare_double_and_swap_double_acquire)
+#   define AO_compare_double_and_swap_double_acquire(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_full(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_acquire
+# endif
+# if !defined(AO_HAVE_compare_double_and_swap_double_write)
+#   define AO_compare_double_and_swap_double_write(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_full(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_write
+# endif
+# if !defined(AO_HAVE_compare_double_and_swap_double_read)
+#   define AO_compare_double_and_swap_double_read(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_full(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_read
+# endif
+#endif /* AO_HAVE_compare_double_and_swap_double_full */
+
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_compare_double_and_swap_double_release)
+# define AO_compare_double_and_swap_double(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_release(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_compare_double_and_swap_double_acquire)
+# define AO_compare_double_and_swap_double(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_acquire(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_compare_double_and_swap_double_write)
+# define AO_compare_double_and_swap_double(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_write(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && defined(AO_HAVE_compare_double_and_swap_double_read)
+# define AO_compare_double_and_swap_double(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_read(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double
+#endif
+
+#if defined(AO_HAVE_compare_double_and_swap_double_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_double_and_swap_double_full)
+# define AO_compare_double_and_swap_double_full(addr,o1,o2,n1,n2) \
+                (AO_nop_full(), \
+                 AO_compare_double_and_swap_double_acquire(addr,o1,o2,n1,n2))
+# define AO_HAVE_compare_double_and_swap_double_full
+#endif
+
+#if !defined(AO_HAVE_compare_double_and_swap_double_release_write) \
+    && defined(AO_HAVE_compare_double_and_swap_double_write)
+# define AO_compare_double_and_swap_double_release_write(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_write(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double_release_write
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double_release_write) \
+    && defined(AO_HAVE_compare_double_and_swap_double_release)
+# define AO_compare_double_and_swap_double_release_write(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_release(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double_release_write
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double_acquire_read) \
+    && defined(AO_HAVE_compare_double_and_swap_double_read)
+# define AO_compare_double_and_swap_double_acquire_read(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_read(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double_acquire_read
+#endif
+#if !defined(AO_HAVE_compare_double_and_swap_double_acquire_read) \
+    && defined(AO_HAVE_compare_double_and_swap_double_acquire)
+# define AO_compare_double_and_swap_double_acquire_read(addr,o1,o2,n1,n2) \
+                AO_compare_double_and_swap_double_acquire(addr,o1,o2,n1,n2)
+# define AO_HAVE_compare_double_and_swap_double_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_compare_double_and_swap_double_acquire_read)
+#   define AO_compare_double_and_swap_double_dd_acquire_read(addr,o1,o2,n1,n2) \
+             AO_compare_double_and_swap_double_acquire_read(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_compare_double_and_swap_double)
+#   define AO_compare_double_and_swap_double_dd_acquire_read(addr,o1,o2,n1,n2) \
+                        AO_compare_double_and_swap_double(addr,o1,o2,n1,n2)
+#   define AO_HAVE_compare_double_and_swap_double_dd_acquire_read
+# endif
+#endif /* !AO_NO_DD_ORDERING */
+
+/* Compare_and_swap_double */
+#if defined(AO_HAVE_compare_and_swap_double) && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_and_swap_double_acquire)
+  AO_INLINE int
+  AO_compare_and_swap_double_acquire(volatile AO_double_t *addr,
+                                            AO_t o1,
+                                            AO_t n1, AO_t n2)
+  {
+    int result = AO_compare_and_swap_double(addr, o1, n1, n2);
+    AO_nop_full();
+    return result;
+  }
+# define AO_HAVE_compare_and_swap_double_acquire
+#endif
+#if defined(AO_HAVE_compare_and_swap_double) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_and_swap_double_release)
+# define AO_compare_and_swap_double_release(addr,o1,n1,n2) \
+                (AO_nop_full(), AO_compare_and_swap_double(addr,o1,n1,n2))
+# define AO_HAVE_compare_and_swap_double_release
+#endif
+#if defined(AO_HAVE_compare_and_swap_double_full)
+# if !defined(AO_HAVE_compare_and_swap_double_release)
+#   define AO_compare_and_swap_double_release(addr,o1,n1,n2) \
+                                AO_compare_and_swap_double_full(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_release
+# endif
+# if !defined(AO_HAVE_compare_and_swap_double_acquire)
+#   define AO_compare_and_swap_double_acquire(addr,o1,n1,n2) \
+                                AO_compare_and_swap_double_full(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_acquire
+# endif
+# if !defined(AO_HAVE_compare_and_swap_double_write)
+#   define AO_compare_and_swap_double_write(addr,o1,n1,n2) \
+                                AO_compare_and_swap_double_full(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_write
+# endif
+# if !defined(AO_HAVE_compare_and_swap_double_read)
+#   define AO_compare_and_swap_double_read(addr,o1,n1,n2) \
+                                AO_compare_and_swap_double_full(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_read
+# endif
+#endif /* AO_HAVE_compare_and_swap_double_full */
+
+#if !defined(AO_HAVE_compare_and_swap_double) \
+    && defined(AO_HAVE_compare_and_swap_double_release)
+# define AO_compare_and_swap_double(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_release(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double) \
+    && defined(AO_HAVE_compare_and_swap_double_acquire)
+# define AO_compare_and_swap_double(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_acquire(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double) \
+    && defined(AO_HAVE_compare_and_swap_double_write)
+# define AO_compare_and_swap_double(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_write(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double) \
+    && defined(AO_HAVE_compare_and_swap_double_read)
+# define AO_compare_and_swap_double(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_read(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double
+#endif
+
+#if defined(AO_HAVE_compare_and_swap_double_acquire) \
+    && defined(AO_HAVE_nop_full) \
+    && !defined(AO_HAVE_compare_and_swap_double_full)
+# define AO_compare_and_swap_double_full(addr,o1,n1,n2) \
+        (AO_nop_full(), AO_compare_and_swap_double_acquire(addr,o1,n1,n2))
+# define AO_HAVE_compare_and_swap_double_full
+#endif
+
+#if !defined(AO_HAVE_compare_and_swap_double_release_write) \
+    && defined(AO_HAVE_compare_and_swap_double_write)
+# define AO_compare_and_swap_double_release_write(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_write(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double_release_write
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double_release_write) \
+    && defined(AO_HAVE_compare_and_swap_double_release)
+# define AO_compare_and_swap_double_release_write(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_release(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double_release_write
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double_acquire_read) \
+    && defined(AO_HAVE_compare_and_swap_double_read)
+# define AO_compare_and_swap_double_acquire_read(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_read(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double_acquire_read
+#endif
+#if !defined(AO_HAVE_compare_and_swap_double_acquire_read) \
+    && defined(AO_HAVE_compare_and_swap_double_acquire)
+# define AO_compare_and_swap_double_acquire_read(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_acquire(addr,o1,n1,n2)
+# define AO_HAVE_compare_and_swap_double_acquire_read
+#endif
+
+#ifdef AO_NO_DD_ORDERING
+# if defined(AO_HAVE_compare_and_swap_double_acquire_read)
+#   define AO_compare_and_swap_double_dd_acquire_read(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double_acquire_read(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_dd_acquire_read
+# endif
+#else
+# if defined(AO_HAVE_compare_and_swap_double)
+#   define AO_compare_and_swap_double_dd_acquire_read(addr,o1,n1,n2) \
+                        AO_compare_and_swap_double(addr,o1,n1,n2)
+#   define AO_HAVE_compare_and_swap_double_dd_acquire_read
+# endif
+#endif
+
+/* NEC LE-IT: Convenience functions for AO_double compare and swap which */
+/* types and reads easier in code                                        */
+#if defined(AO_HAVE_compare_double_and_swap_double_release) \
+    && !defined(AO_HAVE_double_compare_and_swap_release)
+  AO_INLINE int
+  AO_double_compare_and_swap_release(volatile AO_double_t *addr,
+                                     AO_double_t old_val, AO_double_t new_val)
+  {
+    return AO_compare_double_and_swap_double_release(addr,
+                                          old_val.AO_val1, old_val.AO_val2,
+                                          new_val.AO_val1, new_val.AO_val2);
+  }
+# define AO_HAVE_double_compare_and_swap_release
+#endif
+
+#if defined(AO_HAVE_compare_double_and_swap_double_acquire) \
+    && !defined(AO_HAVE_double_compare_and_swap_acquire)
+  AO_INLINE int
+  AO_double_compare_and_swap_acquire(volatile AO_double_t *addr,
+                                     AO_double_t old_val, AO_double_t new_val)
+  {
+    return AO_compare_double_and_swap_double_acquire(addr,
+                                          old_val.AO_val1, old_val.AO_val2,
+                                          new_val.AO_val1, new_val.AO_val2);
+  }
+# define AO_HAVE_double_compare_and_swap_acquire
+#endif
+
+#if defined(AO_HAVE_compare_double_and_swap_double_full) \
+    && !defined(AO_HAVE_double_compare_and_swap_full)
+  AO_INLINE int
+  AO_double_compare_and_swap_full(volatile AO_double_t *addr,
+                                  AO_double_t old_val, AO_double_t new_val)
+  {
+    return AO_compare_double_and_swap_double_full(addr,
+                                          old_val.AO_val1, old_val.AO_val2,
+                                          new_val.AO_val1, new_val.AO_val2);
+  }
+# define AO_HAVE_double_compare_and_swap_full
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/README b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/README
new file mode 100644
index 0000000..605699f
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/README
@@ -0,0 +1,7 @@
+There are two kinds of entities in this directory:
+
+- Subdirectories corresponding to specific compilers (or compiler/OS combinations).
+  Each of these includes one or more architecture-specific headers.
+
+- More generic header files corresponding to a particular ordering and/or
+  atomicity property that might be shared by multiple hardware platforms.
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/acquire_release_volatile.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/acquire_release_volatile.h
new file mode 100644
index 0000000..6d54af9
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/acquire_release_volatile.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file adds definitions appropriate for environments in which an AO_t
+ * volatile load has acquire semantics, and an AO_t volatile store has release
+ * semantics.  This is arguably supposed to be true with the standard Itanium
+ * software conventions.
+ */
+
+/*
+ * Empirically gcc/ia64 does some reordering of ordinary operations around volatiles
+ * even when we think it shouldn't.  Gcc 3.3 and earlier could reorder a volatile store
+ * with another store.  As of March 2005, gcc pre-4 reused previously computed
+ * common subexpressions across a volatile load.
+ * Hence we now add compiler barriers for gcc.
+ */
+#if !defined(AO_GCC_BARRIER)
+#  if defined(__GNUC__)
+#    define AO_GCC_BARRIER() AO_compiler_barrier()
+#  else
+#    define AO_GCC_BARRIER()
+#  endif
+#endif
+
+AO_INLINE AO_t
+AO_load_acquire(const volatile AO_t *p)
+{
+  AO_t result = *p;
+  /* A normal volatile load generates an ld.acq         */
+  AO_GCC_BARRIER();
+  return result;
+}
+#define AO_HAVE_load_acquire
+
+AO_INLINE void
+AO_store_release(volatile AO_t *p, AO_t val)
+{
+  AO_GCC_BARRIER();
+  /* A normal volatile store generates an st.rel        */
+  *p = val;
+}
+#define AO_HAVE_store_release
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/aligned_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/aligned_atomic_load_store.h
new file mode 100644
index 0000000..d24fe1d
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/aligned_atomic_load_store.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Definitions for architectures on which loads and stores of AO_t are  */
+/* atomic fo all legal alignments.                                      */
+
+AO_INLINE AO_t
+AO_load(const volatile AO_t *addr)
+{
+  assert(((size_t)addr & (sizeof(AO_t) - 1)) == 0);
+  /* Cast away the volatile for architectures where             */
+  /* volatile adds barrier semantics.                           */
+  return *(AO_t *)addr;
+}
+#define AO_HAVE_load
+
+AO_INLINE void
+AO_store(volatile AO_t *addr, AO_t new_val)
+{
+  assert(((size_t)addr & (sizeof(AO_t) - 1)) == 0);
+  (*(AO_t *)addr) = new_val;
+}
+#define AO_HAVE_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_acquire_release_volatile.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_acquire_release_volatile.h
new file mode 100644
index 0000000..6787387
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_acquire_release_volatile.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ * 
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ * 
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ * 
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE. 
+ */
+
+/*
+ * Describes architectures on which volatile AO_t, unsigned char, unsigned
+ * short, and unsigned int loads and stores have acquire/release semantics for
+ * all normally legal alignments.
+ */
+#include "acquire_release_volatile.h"
+#include "char_acquire_release_volatile.h"
+#include "short_acquire_release_volatile.h"
+#include "int_acquire_release_volatile.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_aligned_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_aligned_atomic_load_store.h
new file mode 100644
index 0000000..db258df
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_aligned_atomic_load_store.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Describes architectures on which AO_t, unsigned char, unsigned short,
+ * and unsigned int loads and stores are atomic for all normally legal
+ * alignments.
+ */
+#include "aligned_atomic_load_store.h"
+#include "char_atomic_load_store.h"
+#include "short_aligned_atomic_load_store.h"
+#include "int_aligned_atomic_load_store.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_atomic_load_store.h
new file mode 100644
index 0000000..248d9a6
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/all_atomic_load_store.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Describes architectures on which AO_t, unsigned char, unsigned short,
+ * and unsigned int loads and stores are atomic for all normally legal
+ * alignments.
+ */
+#include "atomic_load_store.h"
+#include "char_atomic_load_store.h"
+#include "short_atomic_load_store.h"
+#include "int_atomic_load_store.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ao_t_is_int.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ao_t_is_int.h
new file mode 100644
index 0000000..5dbfca2
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ao_t_is_int.h
@@ -0,0 +1,126 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Inclusion of this file signifies that AO_t is in fact int.  Hence
+ * any AO_... operations can also serve as AO_int_... operations.
+ * We currently define only the more important ones here, and allow for
+ * the normal generalization process to define the others.
+ * We should probably add others in the future.
+ */
+
+#if defined(AO_HAVE_compare_and_swap_full) && \
+    !defined(AO_HAVE_int_compare_and_swap_full)
+#  define AO_int_compare_and_swap_full(addr, old, new_val) \
+                AO_compare_and_swap_full((volatile AO_t *)(addr), \
+                                        (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap_full
+# endif
+
+#if defined(AO_HAVE_compare_and_swap_acquire) && \
+    !defined(AO_HAVE_int_compare_and_swap_acquire)
+#  define AO_int_compare_and_swap_acquire(addr, old, new_val) \
+                AO_compare_and_swap_acquire((volatile AO_t *)(addr), \
+                                            (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap_acquire
+# endif
+
+#if defined(AO_HAVE_compare_and_swap_release) && \
+    !defined(AO_HAVE_int_compare_and_swap_release)
+#  define AO_int_compare_and_swap_release(addr, old, new_val) \
+                AO_compare_and_swap_release((volatile AO_t *)(addr), \
+                                         (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap_release
+# endif
+
+#if defined(AO_HAVE_compare_and_swap_write) && \
+    !defined(AO_HAVE_int_compare_and_swap_write)
+#  define AO_int_compare_and_swap_write(addr, old, new_val) \
+                AO_compare_and_swap_write((volatile AO_t *)(addr), \
+                                          (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap_write
+# endif
+
+#if defined(AO_HAVE_compare_and_swap_read) && \
+    !defined(AO_HAVE_int_compare_and_swap_read)
+#  define AO_int_compare_and_swap_read(addr, old, new_val) \
+                AO_compare_and_swap_read((volatile AO_t *)(addr), \
+                                         (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap_read
+# endif
+
+#if defined(AO_HAVE_compare_and_swap) && \
+    !defined(AO_HAVE_int_compare_and_swap)
+#  define AO_int_compare_and_swap(addr, old, new_val) \
+                AO_compare_and_swap((volatile AO_t *)(addr), \
+                                    (AO_t)(old), (AO_t)(new_val))
+#  define AO_HAVE_int_compare_and_swap
+# endif
+
+#if defined(AO_HAVE_load_acquire) && \
+    !defined(AO_HAVE_int_load_acquire)
+#  define AO_int_load_acquire(addr) \
+        (unsigned)AO_load_acquire((const volatile AO_t *)(addr))
+#  define AO_HAVE_int_load_acquire
+# endif
+
+#if defined(AO_HAVE_store_release) && \
+    !defined(AO_HAVE_int_store_release)
+#  define AO_int_store_release(addr, val) \
+        AO_store_release((volatile AO_t *)(addr), (AO_t)(val))
+#  define AO_HAVE_int_store_release
+# endif
+
+#if defined(AO_HAVE_fetch_and_add_full) && \
+    !defined(AO_HAVE_int_fetch_and_add_full)
+#  define AO_int_fetch_and_add_full(addr, incr) \
+        (unsigned)AO_fetch_and_add_full((volatile AO_t *)(addr), (AO_t)(incr))
+#  define AO_HAVE_int_fetch_and_add_full
+# endif
+
+#if defined(AO_HAVE_fetch_and_add1_acquire) && \
+    !defined(AO_HAVE_int_fetch_and_add1_acquire)
+#  define AO_int_fetch_and_add1_acquire(addr) \
+        (unsigned)AO_fetch_and_add1_acquire((volatile AO_t *)(addr))
+#  define AO_HAVE_int_fetch_and_add1_acquire
+# endif
+
+#if defined(AO_HAVE_fetch_and_add1_release) && \
+    !defined(AO_HAVE_int_fetch_and_add1_release)
+#  define AO_int_fetch_and_add1_release(addr) \
+        (unsigned)AO_fetch_and_add1_release((volatile AO_t *)(addr))
+#  define AO_HAVE_int_fetch_and_add1_release
+# endif
+
+#if defined(AO_HAVE_fetch_and_sub1_acquire) && \
+    !defined(AO_HAVE_int_fetch_and_sub1_acquire)
+#  define AO_int_fetch_and_sub1_acquire(addr) \
+        (unsigned)AO_fetch_and_sub1_acquire((volatile AO_t *)(addr))
+#  define AO_HAVE_int_fetch_and_sub1_acquire
+# endif
+
+#if defined(AO_HAVE_fetch_and_sub1_release) && \
+    !defined(AO_HAVE_int_fetch_and_sub1_release)
+#  define AO_int_fetch_and_sub1_release(addr) \
+        (unsigned)AO_fetch_and_sub1_release((volatile AO_t *)(addr))
+#  define AO_HAVE_int_fetch_and_sub1_release
+# endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/armcc/arm_v6.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/armcc/arm_v6.h
new file mode 100644
index 0000000..78b99a5
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/armcc/arm_v6.h
@@ -0,0 +1,234 @@
+/*
+ * Copyright (c) 2007 by NEC LE-IT:               All rights reserved.
+ * A transcription of ARMv6 atomic operations for the ARM Realview Toolchain.
+ * This code works with armcc from RVDS 3.1
+ * This is based on work in gcc/arm.h by
+ *   Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *   Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ *   Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "../test_and_set_t_is_ao_t.h" /* Probably suboptimal */
+
+#if __TARGET_ARCH_ARM < 6
+Dont use with ARM instruction sets lower than v6
+#else
+
+#include "../standard_ao_double_t.h"
+
+/* NEC LE-IT: ARMv6 is the first architecture providing support for simple LL/SC
+ * A data memory barrier must be raised via CP15 command (see documentation).
+ *
+ * ARMv7 is compatible to ARMv6 but has a simpler command for issuing a
+ * memory barrier (DMB). Raising it via CP15 should still work as told me by the
+ * support engineers. If it turns out to be much quicker than we should implement
+ * custom code for ARMv7 using the asm { dmb } command.
+ *
+ * If only a single processor is used, we can define AO_UNIPROCESSOR
+ * and do not need to access CP15 for ensuring a DMB at all.
+*/
+
+AO_INLINE void
+AO_nop_full(void)
+{
+# ifndef AO_UNIPROCESSOR
+    unsigned int dest=0;
+    /* issue an data memory barrier (keeps ordering of memory transactions */
+    /* before and after this operation)                                    */
+    __asm {
+            mcr p15,0,dest,c7,c10,5
+            };
+# else
+    AO_compiler_barrier();
+# endif
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_t
+AO_load(const volatile AO_t *addr)
+{
+        /* Cast away the volatile in case it adds fence semantics */
+        return (*(const AO_t *)addr);
+}
+#define AO_HAVE_load
+
+/* NEC LE-IT: atomic "store" - according to ARM documentation this is
+ * the only safe way to set variables also used in LL/SC environment.
+ * A direct write won't be recognized by the LL/SC construct in other CPUs.
+ *
+ * HB: Based on subsequent discussion, I think it would be OK to use an
+ * ordinary store here if we knew that interrupt handlers always cleared
+ * the reservation.  They should, but there is some doubt that this is
+ * currently always the case for e.g. Linux.
+*/
+AO_INLINE void AO_store(volatile AO_t *addr, AO_t value)
+{
+        unsigned long tmp;
+
+retry:
+__asm {
+        ldrex   tmp, [addr]
+        strex   tmp, value, [addr]
+        teq     tmp, #0
+        bne     retry
+        };
+}
+#define AO_HAVE_store
+
+/* NEC LE-IT: replace the SWAP as recommended by ARM:
+
+   "Applies to: ARM11 Cores
+        Though the SWP instruction will still work with ARM V6 cores, it is recommended
+        to use the new V6 synchronization instructions. The SWP instruction produces
+        locked read and write accesses which are atomic, i.e. another operation cannot
+        be done between these locked accesses which ties up external bus (AHB,AXI)
+        bandwidth and can increase worst case interrupt latencies. LDREX,STREX are
+        more flexible, other instructions can be done between the LDREX and STREX accesses.
+   "
+*/
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set(volatile AO_TS_t *addr) {
+
+        AO_TS_VAL_t oldval;
+        unsigned long tmp;
+        unsigned long one = 1;
+retry:
+__asm {
+        ldrex   oldval, [addr]
+        strex   tmp, one, [addr]
+        teq     tmp, #0
+        bne     retry
+        }
+
+        return oldval;
+}
+#define AO_HAVE_test_and_set
+
+/* NEC LE-IT: fetch and add for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_add(volatile AO_t *p, AO_t incr)
+{
+        unsigned long tmp,tmp2;
+        AO_t result;
+
+retry:
+__asm {
+        ldrex   result, [p]
+        add     tmp, incr, result
+        strex   tmp2, tmp, [p]
+        teq     tmp2, #0
+        bne     retry
+        }
+
+        return result;
+}
+#define AO_HAVE_fetch_and_add
+
+/* NEC LE-IT: fetch and add1 for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_add1(volatile AO_t *p)
+{
+        unsigned long tmp,tmp2;
+        AO_t result;
+
+retry:
+__asm {
+        ldrex   result, [p]
+        add     tmp, result, #1
+        strex   tmp2, tmp, [p]
+        teq     tmp2, #0
+        bne     retry
+        }
+
+        return result;
+}
+#define AO_HAVE_fetch_and_add1
+
+/* NEC LE-IT: fetch and sub for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_sub1(volatile AO_t *p)
+{
+        unsigned long tmp,tmp2;
+        AO_t result;
+
+retry:
+__asm {
+        ldrex   result, [p]
+        sub     tmp, result, #1
+        strex   tmp2, tmp, [p]
+        teq     tmp2, #0
+        bne     retry
+        }
+
+        return result;
+}
+#define AO_HAVE_fetch_and_sub1
+
+/* NEC LE-IT: compare and swap */
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old_val, AO_t new_val)
+{
+         AO_t result,tmp;
+
+retry:
+__asm__ {
+        mov     result, #2
+        ldrex   tmp, [addr]
+        teq     tmp, old_val
+#     ifdef __thumb__
+        it      eq
+#     endif
+        strexeq result, new_val, [addr]
+        teq     result, #1
+        beq     retry
+        }
+
+        return !(result&2);
+}
+#define AO_HAVE_compare_and_swap
+
+/* helper functions for the Realview compiler: LDREXD is not usable
+ * with inline assembler, so use the "embedded" assembler as
+ * suggested by ARM Dev. support (June 2008). */
+__asm inline double_ptr_storage load_ex(volatile AO_double_t *addr) {
+        LDREXD r0,r1,[r0]
+}
+
+__asm inline int store_ex(AO_t val1, AO_t val2, volatile AO_double_t *addr) {
+        STREXD r3,r0,r1,[r2]
+        MOV    r0,r3
+}
+
+AO_INLINE int
+AO_compare_double_and_swap_double(volatile AO_double_t *addr,
+                                  AO_t old_val1, AO_t old_val2,
+                                  AO_t new_val1, AO_t new_val2)
+{
+        double_ptr_storage old_val =
+                        ((double_ptr_storage)old_val2 << 32) | old_val1;
+        double_ptr_storage tmp;
+        int result;
+
+        while(1) {
+                tmp = load_ex(addr);
+                if(tmp != old_val)      return 0;
+                result = store_ex(new_val1, new_val2, addr);
+                if(!result)     return 1;
+        }
+}
+#define AO_HAVE_compare_double_and_swap_double
+
+#endif // __TARGET_ARCH_ARM
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/atomic_load_store.h
new file mode 100644
index 0000000..1210891
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/atomic_load_store.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Definitions for architectures on which loads and stores of AO_t are  */
+/* atomic for all legal alignments.                                     */
+
+AO_INLINE AO_t
+AO_load(const volatile AO_t *addr)
+{
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(const AO_t *)addr);
+}
+#define AO_HAVE_load
+
+AO_INLINE void
+AO_store(volatile AO_t *addr, AO_t new_val)
+{
+  (*(AO_t *)addr) = new_val;
+}
+#define AO_HAVE_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_acquire_release_volatile.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_acquire_release_volatile.h
new file mode 100644
index 0000000..c988488
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_acquire_release_volatile.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file adds definitions appropriate for environments in which an unsigned char
+ * volatile load has acquire semantics, and an unsigned char volatile store has release
+ * semantics.  This is true with the standard Itanium ABI.
+ */
+#if !defined(AO_GCC_BARRIER)
+#  if defined(__GNUC__)
+#    define AO_GCC_BARRIER() AO_compiler_barrier()
+#  else
+#    define AO_GCC_BARRIER()
+#  endif
+#endif
+
+AO_INLINE unsigned char
+AO_char_load_acquire(const volatile unsigned char *p)
+{
+  unsigned char result = *p;
+  /* A normal volatile load generates an ld.acq         */
+  AO_GCC_BARRIER();
+  return result;
+}
+#define AO_HAVE_char_load_acquire
+
+AO_INLINE void
+AO_char_store_release(volatile unsigned char *p, unsigned char val)
+{
+  AO_GCC_BARRIER();
+  /* A normal volatile store generates an st.rel        */
+  *p = val;
+}
+#define AO_HAVE_char_store_release
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_atomic_load_store.h
new file mode 100644
index 0000000..ae7005a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/char_atomic_load_store.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Definitions for architectures on which loads and stores of unsigned  */
+/* char are atomic for all legal alignments.                            */
+
+AO_INLINE unsigned char
+AO_char_load(const volatile unsigned char *addr)
+{
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(const unsigned char *)addr);
+}
+#define AO_HAVE_char_load
+
+AO_INLINE void
+AO_char_store(volatile unsigned char *addr, unsigned char new_val)
+{
+  (*(unsigned char *)addr) = new_val;
+}
+#define AO_HAVE_char_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/emul_cas.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/emul_cas.h
new file mode 100644
index 0000000..cd013f4
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/emul_cas.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Ensure, if at all possible, that AO_compare_and_swap_full() is
+ * available.  The emulation should be brute-force signal-safe, even
+ * though it actually blocks.
+ * Including this file will generate an error if AO_compare_and_swap_full()
+ * cannot be made available.
+ * This will be included from platform-specific atomic_ops files
+ * if appropriate, and if AO_REQUIRE_CAS is defined.  It should not be
+ * included directly, especially since it affects the implementation
+ * of other atomic update primitives.
+ * The implementation assumes that only AO_store_XXX and AO_test_and_set_XXX
+ * variants are defined, and that AO_test_and_set_XXX is not used to
+ * operate on compare_and_swap locations.
+ */
+
+#if !defined(ATOMIC_OPS_H)
+#  error This file should not be included directly.
+#endif
+
+#ifndef AO_HAVE_double_t
+# include "standard_ao_double_t.h"
+#endif
+
+int AO_compare_and_swap_emulation(volatile AO_t *addr, AO_t old,
+                                  AO_t new_val);
+
+int AO_compare_double_and_swap_double_emulation(volatile AO_double_t *addr,
+                                                AO_t old_val1, AO_t old_val2,
+                                                AO_t new_val1, AO_t new_val2);
+
+void AO_store_full_emulation(volatile AO_t *addr, AO_t val);
+
+#define AO_compare_and_swap_full(addr, old, newval) \
+       AO_compare_and_swap_emulation(addr, old, newval)
+#define AO_HAVE_compare_and_swap_full
+
+#ifndef AO_HAVE_compare_double_and_swap_double_full
+# define AO_compare_double_and_swap_double_full(addr, old1, old2, \
+                                                newval1, newval2) \
+        AO_compare_double_and_swap_double_emulation(addr, old1, old2, \
+                                                    newval1, newval2)
+# define AO_HAVE_compare_double_and_swap_double_full
+#endif
+
+#undef AO_store
+#undef AO_HAVE_store
+#undef AO_store_write
+#undef AO_HAVE_store_write
+#undef AO_store_release
+#undef AO_HAVE_store_release
+#undef AO_store_full
+#undef AO_HAVE_store_full
+#define AO_store_full(addr, val) AO_store_full_emulation(addr, val)
+#define AO_HAVE_store_full
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/alpha.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/alpha.h
new file mode 100644
index 0000000..f90a86a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/alpha.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "../atomic_load_store.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+
+#define AO_NO_DD_ORDERING
+        /* Data dependence does not imply read ordering.        */
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__("mb" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE void
+AO_nop_write(void)
+{
+  __asm__ __volatile__("wmb" : : : "memory");
+}
+#define AO_HAVE_nop_write
+
+/* mb should be used for AO_nop_read().  That's the default.    */
+
+/* We believe that ldq_l ... stq_c does not imply any memory barrier.   */
+/* We should add an explicit fetch_and_add definition.                  */
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr,
+                    AO_t old, AO_t new_val)
+{
+  unsigned long was_equal;
+  unsigned long temp;
+
+  __asm__ __volatile__(
+                     "1:     ldq_l %0,%1\n"
+                     "       cmpeq %0,%4,%2\n"
+                     "       mov %3,%0\n"
+                     "       beq %2,2f\n"
+                     "       stq_c %0,%1\n"
+                     "       beq %0,1b\n"
+                     "2:\n"
+                     : "=&r" (temp), "+m" (*addr), "=&r" (was_equal)
+                     : "r" (new_val), "Ir" (old)
+                     :"memory");
+  return (int)was_equal;
+}
+#define AO_HAVE_compare_and_swap
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/arm.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/arm.h
new file mode 100644
index 0000000..6d34188
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/arm.h
@@ -0,0 +1,358 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "../test_and_set_t_is_ao_t.h" /* Probably suboptimal */
+
+/* NEC LE-IT: ARMv6 is the first architecture providing support for     */
+/* simple LL/SC.  A data memory barrier must be raised via CP15 command */
+/* (see documentation).                                                 */
+/* ARMv7 is compatible to ARMv6 but has a simpler command for issuing   */
+/* a memory barrier (DMB). Raising it via CP15 should still work as     */
+/* told me by the support engineers. If it turns out to be much quicker */
+/* than we should implement custom code for ARMv7 using the asm { dmb } */
+/* instruction.                                                         */
+/* If only a single processor is used, we can define AO_UNIPROCESSOR    */
+/* and do not need to access CP15 for ensuring a DMB.                   */
+
+#if defined(__thumb__) && !defined(__thumb2__)
+  /* Thumb One mode does not have ARM "mcr", "swp" and some load/store  */
+  /* instructions, so we temporarily switch to ARM mode and go back     */
+  /* afterwards (clobbering "r3" register).                             */
+# define AO_THUMB_GO_ARM \
+           "       adr     r3, 4f\n" \
+           "       bx      r3\n" \
+           "      .align\n" \
+           "      .arm\n" \
+           "4:\n"
+# define AO_THUMB_RESTORE_MODE \
+           "       adr     r3, 5f + 1\n" \
+           "       bx      r3\n" \
+           "       .thumb\n" \
+           "5:\n"
+# define AO_THUMB_SWITCH_CLOBBERS "r3",
+#else
+# define AO_THUMB_GO_ARM /* empty */
+# define AO_THUMB_RESTORE_MODE /* empty */
+# define AO_THUMB_SWITCH_CLOBBERS /* empty */
+#endif /* !__thumb__ */
+
+/* NEC LE-IT: gcc has no way to easily check the arm architecture       */
+/* but it defines only one (or several) of __ARM_ARCH_x__ to be true.   */
+#if !defined(__ARM_ARCH_2__) && !defined(__ARM_ARCH_3__) \
+    && !defined(__ARM_ARCH_3M__) && !defined(__ARM_ARCH_4__) \
+    && !defined(__ARM_ARCH_4T__) \
+    && ((!defined(__ARM_ARCH_5__) && !defined(__ARM_ARCH_5E__) \
+         && !defined(__ARM_ARCH_5T__) && !defined(__ARM_ARCH_5TE__) \
+         && !defined(__ARM_ARCH_5TEJ__) && !defined(__ARM_ARCH_6M__)) \
+        || defined(__ARM_ARCH_7__) || defined(__ARM_ARCH_7A__))
+
+#include "../standard_ao_double_t.h"
+
+AO_INLINE void
+AO_nop_full(void)
+{
+# ifndef AO_UNIPROCESSOR
+    unsigned dest = 0;
+
+    /* Issue a data memory barrier (keeps ordering of memory    */
+    /* transactions before and after this operation).           */
+    __asm__ __volatile__("@AO_nop_full\n"
+      AO_THUMB_GO_ARM
+      "       mcr p15,0,%0,c7,c10,5\n"
+      AO_THUMB_RESTORE_MODE
+      : "=&r"(dest)
+      : /* empty */
+      : AO_THUMB_SWITCH_CLOBBERS "memory");
+# else
+    AO_compiler_barrier();
+# endif
+}
+#define AO_HAVE_nop_full
+
+/* NEC LE-IT: AO_t load is simple reading */
+AO_INLINE AO_t
+AO_load(const volatile AO_t *addr)
+{
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(const AO_t *)addr);
+}
+#define AO_HAVE_load
+
+/* NEC LE-IT: atomic "store" - according to ARM documentation this is
+ * the only safe way to set variables also used in LL/SC environment.
+ * A direct write won't be recognized by the LL/SC construct on the _same_ CPU.
+ *
+ * Support engineers response for behaviour of ARMv6:
+ *
+   Core1        Core2          SUCCESS
+   ===================================
+   LDREX(x)
+   STREX(x)                    Yes
+   -----------------------------------
+   LDREX(x)
+                STR(x)
+   STREX(x)                    No
+   -----------------------------------
+   LDREX(x)
+   STR(x)
+   STREX(x)                    Yes
+   -----------------------------------
+ *
+ * ARMv7 behaves similar, see documentation CortexA8 TRM, point 8.5
+ *
+ * HB: I think this is only a problem if interrupt handlers do not clear
+ * the reservation, as they almost certainly should.  Probably change this back
+ * in a while?
+*/
+AO_INLINE void AO_store(volatile AO_t *addr, AO_t value)
+{
+  AO_t flag;
+
+  __asm__ __volatile__("@AO_store\n"
+    AO_THUMB_GO_ARM
+    "1:     ldrex   %0, [%2]\n"
+    "       strex   %0, %3, [%2]\n"
+    "       teq     %0, #0\n"
+    "       bne     1b\n"
+    AO_THUMB_RESTORE_MODE
+    : "=&r"(flag), "+m"(*addr)
+    : "r" (addr), "r"(value)
+    : AO_THUMB_SWITCH_CLOBBERS "cc");
+}
+#define AO_HAVE_store
+
+/* NEC LE-IT: replace the SWAP as recommended by ARM:
+   "Applies to: ARM11 Cores
+      Though the SWP instruction will still work with ARM V6 cores, it is
+      recommended to use the new V6 synchronization instructions.  The SWP
+      instruction produces 'locked' read and write accesses which are atomic,
+      i.e. another operation cannot be done between these locked accesses which
+      ties up external bus (AHB, AXI) bandwidth and can increase worst case
+      interrupt latencies. LDREX, STREX are more flexible, other instructions
+      can be done between the LDREX and STREX accesses."
+*/
+#if !defined(AO_FORCE_USE_SWP) || defined(__thumb2__)
+  /* But, on the other hand, there could be a considerable performance  */
+  /* degradation in case of a race.  Eg., test_atomic.c executing       */
+  /* test_and_set test on a dual-core ARMv7 processor using LDREX/STREX */
+  /* showed around 35 times lower performance than that using SWP.      */
+  /* To force use of SWP instruction, use -D AO_FORCE_USE_SWP option    */
+  /* (this is ignored in the Thumb-2 mode as SWP is missing there).     */
+  AO_INLINE AO_TS_VAL_t
+  AO_test_and_set(volatile AO_TS_t *addr)
+  {
+    AO_TS_VAL_t oldval;
+    unsigned long flag;
+
+    __asm__ __volatile__("@AO_test_and_set\n"
+      AO_THUMB_GO_ARM
+      "1:     ldrex   %0, [%3]\n"
+      "       strex   %1, %4, [%3]\n"
+      "       teq     %1, #0\n"
+      "       bne     1b\n"
+      AO_THUMB_RESTORE_MODE
+      : "=&r"(oldval), "=&r"(flag), "+m"(*addr)
+      : "r"(addr), "r"(1)
+      : AO_THUMB_SWITCH_CLOBBERS "cc");
+    return oldval;
+  }
+# define AO_HAVE_test_and_set
+#endif /* !AO_FORCE_USE_SWP */
+
+/* NEC LE-IT: fetch and add for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_add(volatile AO_t *p, AO_t incr)
+{
+  unsigned long flag, tmp;
+  AO_t result;
+
+  __asm__ __volatile__("@AO_fetch_and_add\n"
+    AO_THUMB_GO_ARM
+    "1:     ldrex   %0, [%5]\n"         /* get original         */
+    "       add     %2, %0, %4\n"       /* sum up in incr       */
+    "       strex   %1, %2, [%5]\n"     /* store them           */
+    "       teq     %1, #0\n"
+    "       bne     1b\n"
+    AO_THUMB_RESTORE_MODE
+    : "=&r"(result), "=&r"(flag), "=&r"(tmp), "+m"(*p) /* 0..3 */
+    : "r"(incr), "r"(p)                                /* 4..5 */
+    : AO_THUMB_SWITCH_CLOBBERS "cc");
+  return result;
+}
+#define AO_HAVE_fetch_and_add
+
+/* NEC LE-IT: fetch and add1 for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_add1(volatile AO_t *p)
+{
+  unsigned long flag, tmp;
+  AO_t result;
+
+  __asm__ __volatile__("@AO_fetch_and_add1\n"
+    AO_THUMB_GO_ARM
+    "1:     ldrex   %0, [%4]\n"         /* get original */
+    "       add     %1, %0, #1\n"       /* increment */
+    "       strex   %2, %1, [%4]\n"     /* store them */
+    "       teq     %2, #0\n"
+    "       bne     1b\n"
+    AO_THUMB_RESTORE_MODE
+    : "=&r"(result), "=&r"(tmp), "=&r"(flag), "+m"(*p)
+    : "r"(p)
+    : AO_THUMB_SWITCH_CLOBBERS "cc");
+  return result;
+}
+#define AO_HAVE_fetch_and_add1
+
+/* NEC LE-IT: fetch and sub for ARMv6 */
+AO_INLINE AO_t
+AO_fetch_and_sub1(volatile AO_t *p)
+{
+  unsigned long flag, tmp;
+  AO_t result;
+
+  __asm__ __volatile__("@AO_fetch_and_sub1\n"
+    AO_THUMB_GO_ARM
+    "1:     ldrex   %0, [%4]\n"         /* get original */
+    "       sub     %1, %0, #1\n"       /* decrement */
+    "       strex   %2, %1, [%4]\n"     /* store them */
+    "       teq     %2, #0\n"
+    "       bne     1b\n"
+    AO_THUMB_RESTORE_MODE
+    : "=&r"(result), "=&r"(tmp), "=&r"(flag), "+m"(*p)
+    : "r"(p)
+    : AO_THUMB_SWITCH_CLOBBERS "cc");
+  return result;
+}
+#define AO_HAVE_fetch_and_sub1
+
+/* NEC LE-IT: compare and swap */
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old_val, AO_t new_val)
+{
+  AO_t result, tmp;
+
+  __asm__ __volatile__("@AO_compare_and_swap\n"
+    AO_THUMB_GO_ARM
+    "1:     mov     %0, #2\n"           /* store a flag */
+    "       ldrex   %1, [%3]\n"         /* get original */
+    "       teq     %1, %4\n"           /* see if match */
+#   ifdef __thumb2__
+      "       it      eq\n"
+#   endif
+    "       strexeq %0, %5, [%3]\n"     /* store new one if matched */
+    "       teq     %0, #1\n"
+    "       beq     1b\n"               /* if update failed, repeat */
+    AO_THUMB_RESTORE_MODE
+    : "=&r"(result), "=&r"(tmp), "+m"(*addr)
+    : "r"(addr), "r"(old_val), "r"(new_val)
+    : AO_THUMB_SWITCH_CLOBBERS "cc");
+  return !(result&2);   /* if succeded, return 1, else 0 */
+}
+#define AO_HAVE_compare_and_swap
+
+#if !defined(__ARM_ARCH_6__) && !defined(__ARM_ARCH_6J__) \
+    && !defined(__ARM_ARCH_6T2__) && !defined(__ARM_ARCH_6Z__) \
+    && !defined(__ARM_ARCH_6ZT2__) && (!defined(__thumb__) \
+        || (defined(__thumb2__) && !defined(__ARM_ARCH_7__) \
+            && !defined(__ARM_ARCH_7M__) && !defined(__ARM_ARCH_7EM__))) \
+    && (!defined(__clang__) || (__clang_major__ > 3) \
+         || (__clang_major__ == 3 && __clang_minor__ >= 3))
+  /* LDREXD/STREXD present in ARMv6K/M+ (see gas/config/tc-arm.c)       */
+  /* In the Thumb mode, this works only starting from ARMv7 (except for */
+  /* the base and 'M' models).  Clang3.2 (and earlier) does not         */
+  /* allocate register pairs for LDREXD/STREXD properly (besides,       */
+  /* Clang3.1 does not support "%H<r>" operand specification).          */
+  AO_INLINE int
+  AO_compare_double_and_swap_double(volatile AO_double_t *addr,
+                                    AO_t old_val1, AO_t old_val2,
+                                    AO_t new_val1, AO_t new_val2)
+  {
+    double_ptr_storage old_val =
+                        ((double_ptr_storage)old_val2 << 32) | old_val1;
+    double_ptr_storage new_val =
+                        ((double_ptr_storage)new_val2 << 32) | new_val1;
+    double_ptr_storage tmp;
+    int result = 1;
+
+    do {
+      __asm__ __volatile__("@AO_compare_double_and_swap_double\n"
+        "       ldrexd  %0, %H0, [%1]\n" /* get original to r1 & r2 */
+        : "=&r"(tmp)
+        : "r"(addr)
+        : "cc");
+      if (tmp != old_val)
+        break;
+      __asm__ __volatile__(
+        "       strexd  %0, %3, %H3, [%2]\n" /* store new one if matched */
+        : "=&r"(result), "+m"(*addr)
+        : "r" (addr), "r" (new_val)
+        : "cc");
+    } while (result);
+    return !result;   /* if succeded, return 1 else 0 */
+  }
+# define AO_HAVE_compare_double_and_swap_double
+#endif
+
+#else
+/* pre ARMv6 architectures ... */
+
+/* I found a slide set that, if I read it correctly, claims that        */
+/* Loads followed by either a Load or Store are ordered, but nothing    */
+/* else is.                                                             */
+/* It appears that SWP is the only simple memory barrier.               */
+#include "../all_atomic_load_store.h"
+
+/* The code should run correctly on a multi-core ARMv6+ as well.        */
+/* There is only a single concern related to AO_store (defined in       */
+/* atomic_load_store.h file):                                           */
+/* HB: Based on subsequent discussion, I think it would be OK to use an */
+/* ordinary store here if we knew that interrupt handlers always        */
+/* cleared the reservation.  They should, but there is some doubt that  */
+/* this is currently always the case, e.g., for Linux.                  */
+
+/* ARMv6M does not support ARM mode.    */
+#endif /* __ARM_ARCH_x */
+
+#if !defined(AO_HAVE_test_and_set_full) && !defined(AO_HAVE_test_and_set) \
+    && !defined(__ARM_ARCH_2__) && !defined(__ARM_ARCH_6M__)
+  AO_INLINE AO_TS_VAL_t
+  AO_test_and_set_full(volatile AO_TS_t *addr)
+  {
+    AO_TS_VAL_t oldval;
+    /* SWP on ARM is very similar to XCHG on x86.                       */
+    /* The first operand is the result, the second the value            */
+    /* to be stored.  Both registers must be different from addr.       */
+    /* Make the address operand an early clobber output so it           */
+    /* doesn't overlap with the other operands.  The early clobber      */
+    /* on oldval is necessary to prevent the compiler allocating        */
+    /* them to the same register if they are both unused.               */
+
+    __asm__ __volatile__("@AO_test_and_set_full\n"
+      AO_THUMB_GO_ARM
+      "       swp %0, %2, [%3]\n"
+                /* Ignore GCC "SWP is deprecated for this architecture" */
+                /* warning here (for ARMv6+).                           */
+      AO_THUMB_RESTORE_MODE
+      : "=&r"(oldval), "=&r"(addr)
+      : "r"(1), "1"(addr)
+      : AO_THUMB_SWITCH_CLOBBERS "memory");
+    return oldval;
+  }
+# define AO_HAVE_test_and_set_full
+#endif /* !AO_HAVE_test_and_set[_full] */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/avr32.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/avr32.h
new file mode 100644
index 0000000..7a2fbed
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/avr32.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright (C) 2009 Bradley Smith <brad@brad-smith.co.uk>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included
+ * in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "../all_atomic_load_store.h"
+
+#include "../ordered.h" /* There are no multiprocessor implementations. */
+
+#include "../test_and_set_t_is_ao_t.h"
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+        register long ret;
+
+        __asm__ __volatile__(
+                "xchg %[oldval], %[mem], %[newval]"
+                : [oldval] "=&r"(ret)
+                : [mem] "r"(addr), [newval] "r"(1)
+                : "memory");
+
+        return (AO_TS_VAL_t)ret;
+}
+#define AO_HAVE_test_and_set_full
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+       register long ret;
+
+       __asm__ __volatile__(
+               "1: ssrf    5\n"
+               "   ld.w    %[res], %[mem]\n"
+               "   eor     %[res], %[oldval]\n"
+               "   brne    2f\n"
+               "   stcond  %[mem], %[newval]\n"
+               "   brne    1b\n"
+               "2:\n"
+               : [res] "=&r"(ret), [mem] "=m"(*addr)
+               : "m"(*addr), [newval] "r"(new_val), [oldval] "r"(old)
+               : "cc", "memory");
+
+       return (int)ret;
+}
+#define AO_HAVE_compare_and_swap_full
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/cris.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/cris.h
new file mode 100644
index 0000000..cbca1e7
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/cris.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * Most of this code originally comes from Hans-Peter Nilsson.  It is included
+ * here with his permission.
+ *
+ * This version has not been tested.  It was coped here from a GC
+ * patch so that we wouldn't lose the code in the upgrade to gc7.
+ */
+
+#include "../all_atomic_load_store.h"
+
+#include "../ordered.h"  /* There are no multiprocessor implementations. */
+
+#include "../test_and_set_t_is_ao_t.h"
+
+/*
+ * The architecture apparently supports an "f" flag which is
+ * set on preemption.  This essentially gives us load-locked,
+ * store-conditional primitives, though I'm not quite sure how
+ * this would work on a hypothetical multiprocessor.  -HB
+ *
+ * For details, see
+ * http://developer.axis.com/doc/hardware/etrax100lx/prog_man/
+ *      1_architectural_description.pdf
+ *
+ * Presumably many other primitives (notably CAS, including the double-
+ * width versions) could be implemented in this manner, if someone got
+ * around to it.
+ */
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr) {
+    /* Ripped from linuxthreads/sysdeps/cris/pt-machine.h */
+    register unsigned long int ret;
+
+    /* Note the use of a dummy output of *addr to expose the write.  The
+       memory barrier is to stop *other* writes being moved past this code.  */
+      __asm__ __volatile__("clearf\n"
+                           "0:\n\t"
+                           "movu.b [%2],%0\n\t"
+                           "ax\n\t"
+                           "move.b %3,[%2]\n\t"
+                           "bwf 0b\n\t"
+                           "clearf"
+                           : "=&r" (ret), "=m" (*addr)
+                           : "r" (addr), "r" ((int) 1), "m" (*addr)
+                           : "memory");
+    return ret;
+}
+#define AO_HAVE_test_and_set_full
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hexagon.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hexagon.h
new file mode 100644
index 0000000..62ad0da
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hexagon.h
@@ -0,0 +1,98 @@
+/*
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "../all_aligned_atomic_load_store.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+
+/* There's also "isync" and "barrier"; however, for all current CPU     */
+/* versions, "syncht" should suffice.  Likewise, it seems that the      */
+/* auto-defined versions of *_acquire, *_release or *_full suffice for  */
+/* all current ISA implementations.                                     */
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__("syncht" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+/* The Hexagon has load-locked, store-conditional primitives, and so    */
+/* resulting code is very nearly identical to that of PowerPC.          */
+
+AO_INLINE AO_t
+AO_fetch_and_add(volatile AO_t *addr, AO_t incr)
+{
+  AO_t oldval;
+  AO_t newval;
+  __asm__ __volatile__(
+     "1:\n"
+     "  %0 = memw_locked(%3);\n"        /* load and reserve            */
+     "  %1 = add (%0,%4);\n"            /* increment                   */
+     "  memw_locked(%3,p1) = %1;\n"     /* store conditional           */
+     "  if (!p1) jump 1b;\n"            /* retry if lost reservation   */
+     : "=&r"(oldval), "=&r"(newval), "+m"(*addr)
+     : "r"(addr), "r"(incr)
+     : "memory", "p1");
+  return oldval;
+}
+#define AO_HAVE_fetch_and_add
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set(volatile AO_TS_t *addr)
+{
+  int oldval;
+  int locked_value = 1;
+
+  __asm__ __volatile__(
+     "1:\n"
+     "  %0 = memw_locked(%2);\n"        /* load and reserve            */
+     "  {\n"
+     "    p2 = cmp.eq(%0,#0);\n"        /* if load is not zero,        */
+     "    if (!p2.new) jump:nt 2f; \n"  /* we are done                 */
+     "  }\n"
+     "  memw_locked(%2,p1) = %3;\n"     /* else store conditional      */
+     "  if (!p1) jump 1b;\n"            /* retry if lost reservation   */
+     "2:\n"                             /* oldval is zero if we set    */
+     : "=&r"(oldval), "+m"(*addr)
+     : "r"(addr), "r"(locked_value)
+     : "memory", "p1", "p2");
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set
+
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  AO_t __oldval;
+  int result = 0;
+  __asm__ __volatile__(
+     "1:\n"
+     "  %0 = memw_locked(%3);\n"        /* load and reserve            */
+     "  {\n"
+     "    p2 = cmp.eq(%0,%4);\n"        /* if load is not equal to     */
+     "    if (!p2.new) jump:nt 2f; \n"  /* old, fail                   */
+     "  }\n"
+     "  memw_locked(%3,p1) = %5;\n"     /* else store conditional      */
+     "  if (!p1) jump 1b;\n"            /* retry if lost reservation   */
+     "  %1 = #1\n"                      /* success, result = 1         */
+     "2:\n"
+     : "=&r" (__oldval), "+r" (result), "+m"(*addr)
+     : "r" (addr), "r" (old), "r" (new_val)
+     : "p1", "p2", "memory"
+  );
+  return result;
+}
+#define AO_HAVE_compare_and_swap
+
+/* Generalize first to define more AO_int_... primitives.       */
+#include "../../generalize.h"
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hppa.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hppa.h
new file mode 100644
index 0000000..e7365e0
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/hppa.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * Modified by Carlos O'Donell <carlos@baldric.uwo.ca>, 2003
+ *      - Added self-aligning lock.
+ *
+ */
+
+#include "../all_atomic_load_store.h"
+
+/* Some architecture set descriptions include special "ordered" memory  */
+/* operations.  As far as we can tell, no existing processors actually  */
+/* require those.  Nor does it appear likely that future processors     */
+/* will.                                                                */
+#include "../ordered.h"
+
+/* GCC will not guarantee the alignment we need, use four lock words    */
+/* and select the correctly aligned datum. See the glibc 2.3.2          */
+/* linuxthread port for the original implementation.                    */
+struct AO_pa_clearable_loc {
+  int data[4];
+};
+
+#undef AO_TS_INITIALIZER
+#define AO_TS_t struct AO_pa_clearable_loc
+#define AO_TS_INITIALIZER {1,1,1,1}
+/* Switch meaning of set and clear, since we only have an atomic clear  */
+/* instruction.                                                         */
+typedef enum {AO_PA_TS_set = 0, AO_PA_TS_clear = 1} AO_PA_TS_val;
+#define AO_TS_VAL_t AO_PA_TS_val
+#define AO_TS_CLEAR AO_PA_TS_clear
+#define AO_TS_SET AO_PA_TS_set
+
+/* The hppa only has one atomic read and modify memory operation,       */
+/* load and clear, so hppa spinlocks must use zero to signify that      */
+/* someone is holding the lock.  The address used for the ldcw          */
+/* semaphore must be 16-byte aligned.                                   */
+
+#define __ldcw(a) ({ \
+  volatile unsigned int __ret;                                  \
+  __asm__ __volatile__("ldcw 0(%2),%0"                          \
+                      : "=r" (__ret), "=m" (*(a)) : "r" (a));   \
+  __ret;                                                        \
+})
+
+/* Because malloc only guarantees 8-byte alignment for malloc'd data,   */
+/* and GCC only guarantees 8-byte alignment for stack locals, we can't  */
+/* be assured of 16-byte alignment for atomic lock data even if we      */
+/* specify "__attribute ((aligned(16)))" in the type declaration.  So,  */
+/* we use a struct containing an array of four ints for the atomic lock */
+/* type and dynamically select the 16-byte aligned int from the array   */
+/* for the semaphore.                                                   */
+#define __PA_LDCW_ALIGNMENT 16
+#define __ldcw_align(a) ({ \
+  unsigned long __ret = (unsigned long) a;                      \
+  __ret += __PA_LDCW_ALIGNMENT - 1;                                     \
+  __ret &= ~(__PA_LDCW_ALIGNMENT - 1);                                  \
+  (volatile unsigned int *) __ret;                                      \
+})
+
+/* Works on PA 1.1 and PA 2.0 systems */
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t * addr)
+{
+  volatile unsigned int *a = __ldcw_align (addr);
+  return (AO_TS_VAL_t) __ldcw (a);
+}
+#define AO_HAVE_test_and_set_full
+
+AO_INLINE void
+AO_pa_clear(volatile AO_TS_t * addr)
+{
+  volatile unsigned int *a = __ldcw_align (addr);
+  AO_compiler_barrier();
+  *a = 1;
+}
+#define AO_CLEAR(addr) AO_pa_clear(addr)
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/ia64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/ia64.h
new file mode 100644
index 0000000..6c5e221
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/ia64.h
@@ -0,0 +1,285 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "../all_atomic_load_store.h"
+
+#include "../all_acquire_release_volatile.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#ifdef _ILP32
+  /* 32-bit HP/UX code. */
+  /* This requires pointer "swizzling".  Pointers need to be expanded   */
+  /* to 64 bits using the addp4 instruction before use.  This makes it  */
+  /* hard to share code, but we try anyway.                             */
+# define AO_LEN "4"
+  /* We assume that addr always appears in argument position 1 in asm   */
+  /* code.  If it is clobbered due to swizzling, we also need it in     */
+  /* second position.  Any later arguments are referenced symbolically, */
+  /* so that we don't have to worry about their position.  This requires*/
+  /* gcc 3.1, but you shouldn't be using anything older than that on    */
+  /* IA64 anyway.                                                       */
+  /* The AO_MASK macro is a workaround for the fact that HP/UX gcc      */
+  /* appears to otherwise store 64-bit pointers in ar.ccv, i.e. it      */
+  /* doesn't appear to clear high bits in a pointer value we pass into  */
+  /* assembly code, even if it is supposedly of type AO_t.              */
+# define AO_IN_ADDR "1"(addr)
+# define AO_OUT_ADDR , "=r"(addr)
+# define AO_SWIZZLE "addp4 %1=0,%1;;\n"
+# define AO_MASK(ptr) __asm__ __volatile__("zxt4 %1=%1": "=r"(ptr) : "0"(ptr))
+#else
+# define AO_LEN "8"
+# define AO_IN_ADDR "r"(addr)
+# define AO_OUT_ADDR
+# define AO_SWIZZLE
+# define AO_MASK(ptr) /* empty */
+#endif
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__("mf" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_t
+AO_fetch_and_add1_acquire (volatile AO_t *addr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ (AO_SWIZZLE
+                        "fetchadd" AO_LEN ".acq %0=[%1],1":
+                        "=r" (result) AO_OUT_ADDR: AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_add1_release (volatile AO_t *addr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ (AO_SWIZZLE
+                        "fetchadd" AO_LEN ".rel %0=[%1],1":
+                        "=r" (result) AO_OUT_ADDR: AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add1_release
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_acquire (volatile AO_t *addr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ (AO_SWIZZLE
+                        "fetchadd" AO_LEN ".acq %0=[%1],-1":
+                        "=r" (result) AO_OUT_ADDR: AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_sub1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_release (volatile AO_t *addr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ (AO_SWIZZLE
+                        "fetchadd" AO_LEN ".rel %0=[%1],-1":
+                        "=r" (result) AO_OUT_ADDR: AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_sub1_release
+
+#ifndef _ILP32
+
+AO_INLINE unsigned int
+AO_int_fetch_and_add1_acquire (volatile unsigned int *addr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("fetchadd4.acq %0=[%1],1":
+                        "=r" (result): AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_add1_acquire
+
+AO_INLINE unsigned int
+AO_int_fetch_and_add1_release (volatile unsigned int *addr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("fetchadd4.rel %0=[%1],1":
+                        "=r" (result): AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_add1_release
+
+AO_INLINE unsigned int
+AO_int_fetch_and_sub1_acquire (volatile unsigned int *addr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("fetchadd4.acq %0=[%1],-1":
+                        "=r" (result): AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_sub1_acquire
+
+AO_INLINE unsigned int
+AO_int_fetch_and_sub1_release (volatile unsigned int *addr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("fetchadd4.rel %0=[%1],-1":
+                        "=r" (result): AO_IN_ADDR :"memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_sub1_release
+
+#endif /* !_ILP32 */
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+  AO_MASK(old);
+  __asm__ __volatile__(AO_SWIZZLE
+                       "mov ar.ccv=%[old] ;; cmpxchg" AO_LEN
+                       ".acq %0=[%1],%[new_val],ar.ccv"
+                       : "=r"(oldval) AO_OUT_ADDR
+                       : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"(old)
+                       : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+  AO_MASK(old);
+  __asm__ __volatile__(AO_SWIZZLE
+                       "mov ar.ccv=%[old] ;; cmpxchg" AO_LEN
+                       ".rel %0=[%1],%[new_val],ar.ccv"
+                       : "=r"(oldval) AO_OUT_ADDR
+                       : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"(old)
+                       : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_char_compare_and_swap_acquire(volatile unsigned char *addr,
+                                 unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+  __asm__ __volatile__(AO_SWIZZLE
+               "mov ar.ccv=%[old] ;; cmpxchg1.acq %0=[%1],%[new_val],ar.ccv"
+               : "=r"(oldval) AO_OUT_ADDR
+               : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"((AO_t)old)
+               : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_char_compare_and_swap_acquire
+
+AO_INLINE int
+AO_char_compare_and_swap_release(volatile unsigned char *addr,
+                                 unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+  __asm__ __volatile__(AO_SWIZZLE
+                "mov ar.ccv=%[old] ;; cmpxchg1.rel %0=[%1],%[new_val],ar.ccv"
+                : "=r"(oldval) AO_OUT_ADDR
+                : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"((AO_t)old)
+                : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_char_compare_and_swap_release
+
+AO_INLINE int
+AO_short_compare_and_swap_acquire(volatile unsigned short *addr,
+                                  unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+  __asm__ __volatile__(AO_SWIZZLE
+                "mov ar.ccv=%[old] ;; cmpxchg2.acq %0=[%1],%[new_val],ar.ccv"
+                : "=r"(oldval) AO_OUT_ADDR
+                : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"((AO_t)old)
+                : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_short_compare_and_swap_acquire
+
+AO_INLINE int
+AO_short_compare_and_swap_release(volatile unsigned short *addr,
+                                  unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+  __asm__ __volatile__(AO_SWIZZLE
+                "mov ar.ccv=%[old] ;; cmpxchg2.rel %0=[%1],%[new_val],ar.ccv"
+                : "=r"(oldval) AO_OUT_ADDR
+                : AO_IN_ADDR, [new_val]"r"(new_val), [old]"r"((AO_t)old)
+                : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_short_compare_and_swap_release
+
+#ifndef _ILP32
+
+AO_INLINE int
+AO_int_compare_and_swap_acquire(volatile unsigned int *addr,
+                                unsigned int old, unsigned int new_val)
+{
+  unsigned int oldval;
+  __asm__ __volatile__("mov ar.ccv=%3 ;; cmpxchg4.acq %0=[%1],%2,ar.ccv"
+                       : "=r"(oldval)
+                       : AO_IN_ADDR, "r"(new_val), "r"((AO_t)old) : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_int_compare_and_swap_acquire
+
+AO_INLINE int
+AO_int_compare_and_swap_release(volatile unsigned int *addr,
+                                unsigned int old, unsigned int new_val)
+{
+  unsigned int oldval;
+  __asm__ __volatile__("mov ar.ccv=%3 ;; cmpxchg4.rel %0=[%1],%2,ar.ccv"
+                       : "=r"(oldval)
+                       : AO_IN_ADDR, "r"(new_val), "r"((AO_t)old) : "memory");
+  return (oldval == old);
+}
+#define AO_HAVE_int_compare_and_swap_release
+
+#endif /* !_ILP32 */
+
+/* FIXME: Add compare_and_swap_double as soon as there is widely        */
+/* available hardware that implements it.                               */
+
+/* FIXME: Add compare_double_and_swap_double for the _ILP32 case.       */
+
+#ifdef _ILP32
+  /* Generalize first to define more AO_int_... primitives.     */
+# include "../../generalize.h"
+# include "../ao_t_is_int.h"
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/m68k.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/m68k.h
new file mode 100644
index 0000000..e7d435e
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/m68k.h
@@ -0,0 +1,67 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/* The cas instruction causes an emulation trap for the */
+/* 060 with a misaligned pointer, so let's avoid this.  */
+#undef AO_t
+typedef unsigned long AO_t __attribute__ ((aligned (4)));
+
+/* FIXME.  Very incomplete.  */
+#include "../all_aligned_atomic_load_store.h"
+
+/* Are there any m68k multiprocessors still around?     */
+/* AFAIK, Alliants were sequentially consistent.        */
+#include "../ordered.h"
+
+#include "../test_and_set_t_is_char.h"
+
+/* Contributed by Tony Mantler or new.  Should be changed to MIT license? */
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr) {
+  AO_TS_t oldval;
+
+  /* The value at addr is semi-phony.   */
+  /* 'tas' sets bit 7 while the return  */
+  /* value pretends all bits were set,  */
+  /* which at least matches AO_TS_SET.  */
+  __asm__ __volatile__(
+                "tas %1; sne %0"
+                : "=d" (oldval), "=m" (*addr)
+                : "m" (*addr)
+                : "memory");
+  /* This cast works due to the above.  */
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr,
+                         AO_t old, AO_t new_val)
+{
+  char result;
+
+  __asm__ __volatile__(
+                "cas.l %3,%4,%1; seq %0"
+                : "=d" (result), "=m" (*addr)
+                : "m" (*addr), "d" (old), "d" (new_val)
+                : "memory");
+  return -result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/mips.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/mips.h
new file mode 100644
index 0000000..5350518
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/mips.h
@@ -0,0 +1,102 @@
+/*
+ * Copyright (c) 2005,2007  Thiemo Seufer <ths@networkno.de>
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/*
+ * FIXME:  This should probably make finer distinctions.  SGI MIPS is
+ * much more strongly ordered, and in fact closer to sequentially
+ * consistent.  This is really aimed at modern embedded implementations.
+ * It looks to me like this assumes a 32-bit ABI.  -HB
+ */
+
+#include "../all_aligned_atomic_load_store.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+#include "../standard_ao_double_t.h"
+
+/* Data dependence does not imply read ordering.  */
+#define AO_NO_DD_ORDERING
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__(
+      "       .set push           \n"
+      "       .set mips2          \n"
+      "       .set noreorder      \n"
+      "       .set nomacro        \n"
+      "       sync                \n"
+      "       .set pop              "
+      : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  register int was_equal = 0;
+  register int temp;
+
+  __asm__ __volatile__(
+      "       .set push           \n"
+      "       .set mips2          \n"
+      "       .set noreorder      \n"
+      "       .set nomacro        \n"
+      "1:     ll      %0, %1      \n"
+      "       bne     %0, %4, 2f  \n"
+      "        move   %0, %3      \n"
+      "       sc      %0, %1      \n"
+      "       .set pop            \n"
+      "       beqz    %0, 1b      \n"
+      "       li      %2, 1       \n"
+      "2:                           "
+      : "=&r" (temp), "+m" (*addr), "+r" (was_equal)
+      : "r" (new_val), "r" (old)
+      : "memory");
+  return was_equal;
+}
+#define AO_HAVE_compare_and_swap
+
+/* FIXME: I think the implementations below should be automatically     */
+/* generated if we omit them.  - HB                                     */
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  int result = AO_compare_and_swap(addr, old, new_val);
+  AO_nop_full();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  AO_nop_full();
+  return AO_compare_and_swap(addr, old, new_val);
+}
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  int result;
+  AO_nop_full();
+  result = AO_compare_and_swap(addr, old, new_val);
+  AO_nop_full();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+/*
+ * FIXME: We should also implement fetch_and_add and or primitives
+ * directly.
+ */
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/powerpc.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/powerpc.h
new file mode 100644
index 0000000..83d7d39
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/powerpc.h
@@ -0,0 +1,287 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/* Memory model documented at http://www-106.ibm.com/developerworks/    */
+/* eserver/articles/archguide.html and (clearer)                        */
+/* http://www-106.ibm.com/developerworks/eserver/articles/powerpc.html. */
+/* There appears to be no implicit ordering between any kind of         */
+/* independent memory references.                                       */
+/* Architecture enforces some ordering based on control dependence.     */
+/* I don't know if that could help.                                     */
+/* Data-dependent loads are always ordered.                             */
+/* Based on the above references, eieio is intended for use on          */
+/* uncached memory, which we don't support.  It does not order loads    */
+/* from cached memory.                                                  */
+/* Thanks to Maged Michael, Doug Lea, and Roger Hoover for helping to   */
+/* track some of this down and correcting my misunderstandings. -HB     */
+/* Earl Chew subsequently contributed further fixes & additions.        */
+
+#include "../all_aligned_atomic_load_store.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+        /* There seems to be no byte equivalent of lwarx, so this       */
+        /* may really be what we want, at least in the 32-bit case.     */
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__("sync" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+/* lwsync apparently works for everything but a StoreLoad barrier.      */
+AO_INLINE void
+AO_lwsync(void)
+{
+#ifdef __NO_LWSYNC__
+  __asm__ __volatile__("sync" : : : "memory");
+#else
+  __asm__ __volatile__("lwsync" : : : "memory");
+#endif
+}
+
+#define AO_nop_write() AO_lwsync()
+#define AO_HAVE_nop_write
+
+#define AO_nop_read() AO_lwsync()
+#define AO_HAVE_nop_read
+
+/* We explicitly specify load_acquire, since it is important, and can   */
+/* be implemented relatively cheaply.  It could be implemented          */
+/* with an ordinary load followed by a lwsync.  But the general wisdom  */
+/* seems to be that a data dependent branch followed by an isync is     */
+/* cheaper.  And the documentation is fairly explicit that this also    */
+/* has acquire semantics.                                               */
+/* ppc64 uses ld not lwz */
+AO_INLINE AO_t
+AO_load_acquire(const volatile AO_t *addr)
+{
+  AO_t result;
+#if defined(__powerpc64__) || defined(__ppc64__) || defined(__64BIT__)
+   __asm__ __volatile__ (
+    "ld%U1%X1 %0,%1\n"
+    "cmpw %0,%0\n"
+    "bne- 1f\n"
+    "1: isync\n"
+    : "=r" (result)
+    : "m"(*addr) : "memory", "cr0");
+#else
+  /* FIXME: We should get gcc to allocate one of the condition  */
+  /* registers.  I always got "impossible constraint" when I    */
+  /* tried the "y" constraint.                                  */
+  __asm__ __volatile__ (
+    "lwz%U1%X1 %0,%1\n"
+    "cmpw %0,%0\n"
+    "bne- 1f\n"
+    "1: isync\n"
+    : "=r" (result)
+    : "m"(*addr) : "memory", "cc");
+#endif
+  return result;
+}
+#define AO_HAVE_load_acquire
+
+/* We explicitly specify store_release, since it relies         */
+/* on the fact that lwsync is also a LoadStore barrier.         */
+AO_INLINE void
+AO_store_release(volatile AO_t *addr, AO_t value)
+{
+  AO_lwsync();
+  *addr = value;
+}
+#define AO_HAVE_store_release
+
+/* This is similar to the code in the garbage collector.  Deleting      */
+/* this and having it synthesized from compare_and_swap would probably  */
+/* only cost us a load immediate instruction.                           */
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set(volatile AO_TS_t *addr) {
+#if defined(__powerpc64__) || defined(__ppc64__) || defined(__64BIT__)
+/* Completely untested.  And we should be using smaller objects anyway. */
+  unsigned long oldval;
+  unsigned long temp = 1; /* locked value */
+
+  __asm__ __volatile__(
+               "1:ldarx %0,0,%1\n"   /* load and reserve               */
+               "cmpdi %0, 0\n"       /* if load is                     */
+               "bne 2f\n"            /*   non-zero, return already set */
+               "stdcx. %2,0,%1\n"    /* else store conditional         */
+               "bne- 1b\n"           /* retry if lost reservation      */
+               "2:\n"                /* oldval is zero if we set       */
+              : "=&r"(oldval)
+              : "r"(addr), "r"(temp)
+              : "memory", "cr0");
+#else
+  int oldval;
+  int temp = 1; /* locked value */
+
+  __asm__ __volatile__(
+               "1:lwarx %0,0,%1\n"   /* load and reserve               */
+               "cmpwi %0, 0\n"       /* if load is                     */
+               "bne 2f\n"            /*   non-zero, return already set */
+               "stwcx. %2,0,%1\n"    /* else store conditional         */
+               "bne- 1b\n"           /* retry if lost reservation      */
+               "2:\n"                /* oldval is zero if we set       */
+              : "=&r"(oldval)
+              : "r"(addr), "r"(temp)
+              : "memory", "cr0");
+#endif
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_acquire(volatile AO_TS_t *addr) {
+  AO_TS_VAL_t result = AO_test_and_set(addr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_test_and_set_acquire
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_release(volatile AO_TS_t *addr) {
+  AO_lwsync();
+  return AO_test_and_set(addr);
+}
+#define AO_HAVE_test_and_set_release
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr) {
+  AO_TS_VAL_t result;
+  AO_lwsync();
+  result = AO_test_and_set(addr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_test_and_set_full
+
+AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  AO_t oldval;
+  int result = 0;
+#if defined(__powerpc64__) || defined(__ppc64__) || defined(__64BIT__)
+/* FIXME: Completely untested.  */
+  __asm__ __volatile__(
+               "1:ldarx %0,0,%2\n"   /* load and reserve              */
+               "cmpd %0, %4\n"      /* if load is not equal to  */
+               "bne 2f\n"            /*   old, fail                     */
+               "stdcx. %3,0,%2\n"    /* else store conditional         */
+               "bne- 1b\n"           /* retry if lost reservation      */
+               "li %1,1\n"           /* result = 1;                     */
+               "2:\n"
+              : "=&r"(oldval), "=&r"(result)
+              : "r"(addr), "r"(new_val), "r"(old), "1"(result)
+              : "memory", "cr0");
+#else
+  __asm__ __volatile__(
+               "1:lwarx %0,0,%2\n"   /* load and reserve              */
+               "cmpw %0, %4\n"      /* if load is not equal to  */
+               "bne 2f\n"            /*   old, fail                     */
+               "stwcx. %3,0,%2\n"    /* else store conditional         */
+               "bne- 1b\n"           /* retry if lost reservation      */
+               "li %1,1\n"           /* result = 1;                     */
+               "2:\n"
+              : "=&r"(oldval), "=&r"(result)
+              : "r"(addr), "r"(new_val), "r"(old), "1"(result)
+              : "memory", "cr0");
+#endif
+  return result;
+}
+#define AO_HAVE_compare_and_swap
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  int result = AO_compare_and_swap(addr, old, new_val);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  AO_lwsync();
+  return AO_compare_and_swap(addr, old, new_val);
+}
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  int result;
+  AO_lwsync();
+  result = AO_compare_and_swap(addr, old, new_val);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+AO_INLINE AO_t
+AO_fetch_and_add(volatile AO_t *addr, AO_t incr) {
+  AO_t oldval;
+  AO_t newval;
+#if defined(__powerpc64__) || defined(__ppc64__) || defined(__64BIT__)
+/* FIXME: Completely untested.                                          */
+  __asm__ __volatile__(
+               "1:ldarx %0,0,%2\n"   /* load and reserve                */
+               "add %1,%0,%3\n"      /* increment                       */
+               "stdcx. %1,0,%2\n"    /* store conditional               */
+               "bne- 1b\n"           /* retry if lost reservation       */
+              : "=&r"(oldval), "=&r"(newval)
+               : "r"(addr), "r"(incr)
+              : "memory", "cr0");
+#else
+  __asm__ __volatile__(
+               "1:lwarx %0,0,%2\n"   /* load and reserve                */
+               "add %1,%0,%3\n"      /* increment                       */
+               "stwcx. %1,0,%2\n"    /* store conditional               */
+               "bne- 1b\n"           /* retry if lost reservation       */
+              : "=&r"(oldval), "=&r"(newval)
+               : "r"(addr), "r"(incr)
+              : "memory", "cr0");
+#endif
+  return oldval;
+}
+#define AO_HAVE_fetch_and_add
+
+AO_INLINE AO_t
+AO_fetch_and_add_acquire(volatile AO_t *addr, AO_t incr) {
+  AO_t result = AO_fetch_and_add(addr, incr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_fetch_and_add_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_add_release(volatile AO_t *addr, AO_t incr) {
+  AO_lwsync();
+  return AO_fetch_and_add(addr, incr);
+}
+#define AO_HAVE_fetch_and_add_release
+
+AO_INLINE AO_t
+AO_fetch_and_add_full(volatile AO_t *addr, AO_t incr) {
+  AO_t result;
+  AO_lwsync();
+  result = AO_fetch_and_add(addr, incr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_fetch_and_add_full
+
+#if defined(__powerpc64__) || defined(__ppc64__) || defined(__64BIT__)
+#else
+# include "../ao_t_is_int.h"
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/s390.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/s390.h
new file mode 100644
index 0000000..7cc9c36
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/s390.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/* FIXME: untested.                                             */
+/* The relevant documentation appears to be at                  */
+/* http://publibz.boulder.ibm.com/epubs/pdf/dz9zr003.pdf        */
+/* around page 5-96.  Apparently:                               */
+/* - Memory references in general are atomic only for a single  */
+/*   byte.  But it appears that the most common load/store      */
+/*   instructions also guarantee atomicity for aligned          */
+/*   operands of standard types.  WE FOOLISHLY ASSUME that      */
+/*   compilers only generate those.  If that turns out to be    */
+/*   wrong, we need inline assembly code for AO_load and        */
+/*   AO_store.                                                  */
+/* - A store followed by a load is unordered since the store    */
+/*   may be delayed.  Otherwise everything is ordered.          */
+/* - There is a hardware compare-and-swap (CS) instruction.     */
+
+#include "../all_aligned_atomic_load_store.h"
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+/* FIXME: Is there a way to do byte-sized test-and-set? */
+
+/* FIXME: AO_nop_full should probably be implemented directly.  */
+/* It appears that certain BCR instructions have that effect.   */
+/* Presumably they're cheaper than CS?                          */
+
+AO_INLINE int AO_compare_and_swap_full(volatile AO_t *addr,
+                                       AO_t old, AO_t new_val)
+{
+  int retval;
+  __asm__ __volatile__ (
+# ifndef __s390x__
+    "     cs  %1,%2,0(%3)\n"
+# else
+    "     csg %1,%2,0(%3)\n"
+# endif
+  "     ipm %0\n"
+  "     srl %0,28\n"
+  : "=&d" (retval), "+d" (old)
+  : "d" (new_val), "a" (addr)
+  : "cc", "memory");
+  return retval == 0;
+}
+#define AO_HAVE_compare_and_swap_full
+
+/* FIXME: Add double-wide compare-and-swap for 32-bit executables.      */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sh.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sh.h
new file mode 100644
index 0000000..933037a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sh.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (c) 2009 by Takashi YOSHII. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "../all_atomic_load_store.h"
+#include "../ordered.h"
+
+/* sh has tas.b(byte) only */
+#include "../test_and_set_t_is_char.h"
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+  int oldval;
+  __asm__ __volatile__(
+        "tas.b @%1; movt %0"
+        : "=r" (oldval)
+        : "r" (addr)
+        : "t", "memory");
+  return oldval? AO_TS_CLEAR : AO_TS_SET;
+}
+#define AO_HAVE_test_and_set_full
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sparc.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sparc.h
new file mode 100644
index 0000000..41bc2f5
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/sparc.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+/* FIXME.  Very incomplete.  No support for sparc64.    */
+/* Non-ancient SPARCs provide compare-and-swap (casa).  */
+/* We should make that available.                       */
+
+#include "../all_atomic_load_store.h"
+
+/* Real SPARC code uses TSO:                            */
+#include "../ordered_except_wr.h"
+
+/* Test_and_set location is just a byte.                */
+#include "../test_and_set_t_is_char.h"
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr) {
+   AO_TS_VAL_t oldval;
+
+   __asm__ __volatile__("ldstub %1,%0"
+                        : "=r"(oldval), "=m"(*addr)
+                        : "m"(*addr) : "memory");
+   return oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+#ifndef AO_NO_SPARC_V9
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val) {
+  char ret;
+  __asm__ __volatile__ ("membar #StoreLoad | #LoadLoad\n\t"
+#                       if defined(__arch64__)
+                          "casx [%2],%0,%1\n\t"
+#                       else
+                          "cas [%2],%0,%1\n\t" /* 32-bit version */
+#                       endif
+                        "membar #StoreLoad | #StoreStore\n\t"
+                        "cmp %0,%1\n\t"
+                        "be,a 0f\n\t"
+                        "mov 1,%0\n\t"/* one insn after branch always executed */
+                        "clr %0\n\t"
+                        "0:\n\t"
+                        : "=r" (ret), "+r" (new_val)
+                        : "r" (addr), "0" (old)
+                        : "memory", "cc");
+  return (int)ret;
+}
+#define AO_HAVE_compare_and_swap_full
+#endif /* !AO_NO_SPARC_V9 */
+
+/* FIXME: This needs to be extended for SPARC v8 and v9.        */
+/* SPARC V8 also has swap.  V9 has CAS.                         */
+/* There are barriers like membar #LoadStore.                   */
+/* CASA (32-bit) and CASXA(64-bit) instructions were            */
+/* added in V9.                                                 */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86.h
new file mode 100644
index 0000000..3348845
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86.h
@@ -0,0 +1,229 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Some of the machine specific code was borrowed from our GC distribution.
+ */
+
+/* The following really assume we have a 486 or better.  Unfortunately  */
+/* gcc doesn't define a suitable feature test macro based on command    */
+/* line options.                                                        */
+/* We should perhaps test dynamically.                                  */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations, except for some old WinChips, appear       */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore both the WinChips, and the fact that the official specs    */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include "../standard_ao_double_t.h"
+
+#if defined(AO_USE_PENTIUM4_INSTRS)
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__("mfence" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+#else
+
+/* We could use the cpuid instruction.  But that seems to be slower     */
+/* than the default implementation based on test_and_set_full.  Thus    */
+/* we omit that bit of misinformation here.                             */
+
+#endif
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+/* Really only works for 486 and later */
+AO_INLINE AO_t
+AO_fetch_and_add_full (volatile AO_t *p, AO_t incr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ ("lock; xaddl %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE unsigned char
+AO_char_fetch_and_add_full (volatile unsigned char *p, unsigned char incr)
+{
+  unsigned char result;
+
+  __asm__ __volatile__ ("lock; xaddb %0, %1" :
+                        "=q" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_char_fetch_and_add_full
+
+AO_INLINE unsigned short
+AO_short_fetch_and_add_full (volatile unsigned short *p, unsigned short incr)
+{
+  unsigned short result;
+
+  __asm__ __volatile__ ("lock; xaddw %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_short_fetch_and_add_full
+
+/* Really only works for 486 and later */
+AO_INLINE void
+AO_or_full (volatile AO_t *p, AO_t incr)
+{
+  __asm__ __volatile__ ("lock; orl %1, %0" :
+                        "=m" (*p) : "r" (incr), "m" (*p) : "memory");
+}
+#define AO_HAVE_or_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+  unsigned char oldval;
+  /* Note: the "xchg" instruction does not need a "lock" prefix */
+  __asm__ __volatile__("xchgb %0, %1"
+                : "=q"(oldval), "=m"(*addr)
+                : "0"((unsigned char)0xff), "m"(*addr) : "memory");
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+# ifdef AO_USE_SYNC_CAS_BUILTIN
+    return (int)__sync_bool_compare_and_swap(addr, old, new_val);
+# else
+    char result;
+    __asm__ __volatile__("lock; cmpxchgl %3, %0; setz %1"
+                         : "=m" (*addr), "=a" (result)
+                         : "m" (*addr), "r" (new_val), "a" (old) : "memory");
+    return (int)result;
+# endif
+}
+#define AO_HAVE_compare_and_swap_full
+
+#if !defined(__x86_64__)
+
+/* Returns nonzero if the comparison succeeded. */
+/* Really requires at least a Pentium.          */
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+  char result;
+# ifdef __PIC__
+    AO_t saved_ebx;
+
+    /* If PIC is turned on, we cannot use ebx as it is reserved for the */
+    /* GOT pointer.  We should save and restore ebx.  The proposed      */
+    /* solution is not so efficient as the older alternatives using     */
+    /* push ebx or edi as new_val1 (w/o clobbering edi and temporary    */
+    /* local variable usage) but it is more portable (it works even if  */
+    /* ebx is not used as GOT pointer, and it works for the buggy GCC   */
+    /* releases that incorrectly evaluate memory operands offset in the */
+    /* inline assembly after push).                                     */
+#   ifdef __OPTIMIZE__
+      __asm__ __volatile__("mov %%ebx, %2\n\t" /* save ebx */
+                           "lea %0, %%edi\n\t" /* in case addr is in ebx */
+                           "mov %7, %%ebx\n\t" /* load new_val1 */
+                           "lock; cmpxchg8b (%%edi)\n\t"
+                           "mov %2, %%ebx\n\t" /* restore ebx */
+                           "setz %1"
+                        : "=m" (*addr), "=a" (result), "=m" (saved_ebx)
+                        : "m" (*addr), "d" (old_val2), "a" (old_val1),
+                          "c" (new_val2), "m" (new_val1)
+                        : "%edi", "memory");
+#   else
+      /* A less-efficient code manually preserving edi if GCC invoked   */
+      /* with -O0 option (otherwise it fails while finding a register   */
+      /* in class 'GENERAL_REGS').                                      */
+      AO_t saved_edi;
+      __asm__ __volatile__("mov %%edi, %3\n\t" /* save edi */
+                           "mov %%ebx, %2\n\t" /* save ebx */
+                           "lea %0, %%edi\n\t" /* in case addr is in ebx */
+                           "mov %8, %%ebx\n\t" /* load new_val1 */
+                           "lock; cmpxchg8b (%%edi)\n\t"
+                           "mov %2, %%ebx\n\t" /* restore ebx */
+                           "mov %3, %%edi\n\t" /* restore edi */
+                           "setz %1"
+                        : "=m" (*addr), "=a" (result),
+                          "=m" (saved_ebx), "=m" (saved_edi)
+                        : "m" (*addr), "d" (old_val2), "a" (old_val1),
+                          "c" (new_val2), "m" (new_val1) : "memory");
+#   endif
+# else
+    /* For non-PIC mode, this operation could be simplified (and be     */
+    /* faster) by using ebx as new_val1 (GCC would refuse to compile    */
+    /* such code for PIC mode).                                         */
+    __asm__ __volatile__("lock; cmpxchg8b %0; setz %1"
+                       : "=m" (*addr), "=a" (result)
+                       : "m" (*addr), "d" (old_val2), "a" (old_val1),
+                         "c" (new_val2), "b" (new_val1) : "memory");
+# endif
+  return (int) result;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+
+#else /* x86_64 && ILP32 */
+
+  /* X32 has native support for 64-bit integer operations (AO_double_t  */
+  /* is a 64-bit integer and we could use 64-bit cmpxchg).              */
+  /* This primitive is used by compare_double_and_swap_double_full.     */
+  AO_INLINE int
+  AO_double_compare_and_swap_full(volatile AO_double_t *addr,
+                                  AO_double_t old_val, AO_double_t new_val)
+  {
+    /* It is safe to use __sync CAS built-in here.      */
+    return __sync_bool_compare_and_swap(&addr->AO_whole,
+                                        old_val.AO_whole, new_val.AO_whole
+                                        /* empty protection list */);
+  }
+# define AO_HAVE_double_compare_and_swap_full
+
+  /* TODO: Remove if generalized.       */
+  AO_INLINE int
+  AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                         AO_t old_val1, AO_t old_val2,
+                                         AO_t new_val1, AO_t new_val2)
+  {
+    AO_double_t old_w;
+    AO_double_t new_w;
+    old_w.AO_val1 = old_val1;
+    old_w.AO_val2 = old_val2;
+    new_w.AO_val1 = new_val1;
+    new_w.AO_val2 = new_val2;
+    return AO_double_compare_and_swap_full(addr, old_w, new_w);
+  }
+# define AO_HAVE_compare_double_and_swap_double_full
+
+#endif /* x86_64 && ILP32 */
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86_64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86_64.h
new file mode 100644
index 0000000..b2e920a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/gcc/x86_64.h
@@ -0,0 +1,175 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Some of the machine specific code was borrowed from our GC distribution.
+ */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations appear                                      */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore the fact that the official specs                           */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include "../standard_ao_double_t.h"
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  /* Note: "mfence" (SSE2) is supported on all x86_64/amd64 chips.      */
+  __asm__ __volatile__("mfence" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+AO_INLINE AO_t
+AO_fetch_and_add_full (volatile AO_t *p, AO_t incr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ ("lock; xadd %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE unsigned char
+AO_char_fetch_and_add_full (volatile unsigned char *p, unsigned char incr)
+{
+  unsigned char result;
+
+  __asm__ __volatile__ ("lock; xaddb %0, %1" :
+                        "=q" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_char_fetch_and_add_full
+
+AO_INLINE unsigned short
+AO_short_fetch_and_add_full (volatile unsigned short *p, unsigned short incr)
+{
+  unsigned short result;
+
+  __asm__ __volatile__ ("lock; xaddw %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_short_fetch_and_add_full
+
+AO_INLINE unsigned int
+AO_int_fetch_and_add_full (volatile unsigned int *p, unsigned int incr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("lock; xaddl %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr), "m" (*p)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_add_full
+
+AO_INLINE void
+AO_or_full (volatile AO_t *p, AO_t incr)
+{
+  __asm__ __volatile__ ("lock; or %1, %0" :
+                        "=m" (*p) : "r" (incr), "m" (*p) : "memory");
+}
+#define AO_HAVE_or_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+  unsigned char oldval;
+  /* Note: the "xchg" instruction does not need a "lock" prefix */
+  __asm__ __volatile__("xchgb %0, %1"
+                : "=q"(oldval), "=m"(*addr)
+                : "0"((unsigned char)0xff), "m"(*addr) : "memory");
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+# ifdef AO_USE_SYNC_CAS_BUILTIN
+    return (int)__sync_bool_compare_and_swap(addr, old, new_val);
+# else
+    char result;
+    __asm__ __volatile__("lock; cmpxchg %3, %0; setz %1"
+                         : "=m" (*addr), "=a" (result)
+                         : "m" (*addr), "r" (new_val), "a" (old) : "memory");
+    return (int) result;
+# endif
+}
+#define AO_HAVE_compare_and_swap_full
+
+#ifdef AO_CMPXCHG16B_AVAILABLE
+
+/* NEC LE-IT: older AMD Opterons are missing this instruction.
+ * On these machines SIGILL will be thrown.
+ * Define AO_WEAK_DOUBLE_CAS_EMULATION to have an emulated
+ * (lock based) version available */
+/* HB: Changed this to not define either by default.  There are
+ * enough machines and tool chains around on which cmpxchg16b
+ * doesn't work.  And the emulation is unsafe by our usual rules.
+ * However both are clearly useful in certain cases.
+ */
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+  char result;
+  __asm__ __volatile__("lock; cmpxchg16b %0; setz %1"
+                       : "=m"(*addr), "=a"(result)
+                       : "m"(*addr), "d" (old_val2), "a" (old_val1),
+                         "c" (new_val2), "b" (new_val1) : "memory");
+  return (int) result;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+
+#else
+/* this one provides spinlock based emulation of CAS implemented in     */
+/* atomic_ops.c.  We probably do not want to do this here, since it is  */
+/* not atomic with respect to other kinds of updates of *addr.  On the  */
+/* other hand, this may be a useful facility on occasion.               */
+#ifdef AO_WEAK_DOUBLE_CAS_EMULATION
+int AO_compare_double_and_swap_double_emulation(volatile AO_double_t *addr,
+                                                AO_t old_val1, AO_t old_val2,
+                                                AO_t new_val1, AO_t new_val2);
+
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+  return AO_compare_double_and_swap_double_emulation(addr, old_val1, old_val2,
+                                                     new_val1, new_val2);
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+#endif /* AO_WEAK_DOUBLE_CAS_EMULATION */
+
+#endif /* !AO_CMPXCHG16B_AVAILABLE */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/generic_pthread.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/generic_pthread.h
new file mode 100644
index 0000000..55a1967
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/generic_pthread.h
@@ -0,0 +1,269 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* The following is useful primarily for debugging and documentation.   */
+/* We define various atomic operations by acquiring a global pthread    */
+/* lock.  The resulting implementation will perform poorly, but should  */
+/* be correct unless it is used from signal handlers.                   */
+/* We assume that all pthread operations act like full memory barriers. */
+/* (We believe that is the intent of the specification.)                */
+
+#include <pthread.h>
+
+#include "test_and_set_t_is_ao_t.h"
+        /* This is not necessarily compatible with the native           */
+        /* implementation.  But those can't be safely mixed anyway.     */
+
+/* We define only the full barrier variants, and count on the           */
+/* generalization section below to fill in the rest.                    */
+extern pthread_mutex_t AO_pt_lock;
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_t
+AO_load_full(const volatile AO_t *addr)
+{
+  AO_t result;
+  pthread_mutex_lock(&AO_pt_lock);
+  result = *addr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return result;
+}
+#define AO_HAVE_load_full
+
+AO_INLINE void
+AO_store_full(volatile AO_t *addr, AO_t val)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  *addr = val;
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_store_full
+
+AO_INLINE unsigned char
+AO_char_load_full(const volatile unsigned char *addr)
+{
+  unsigned char result;
+  pthread_mutex_lock(&AO_pt_lock);
+  result = *addr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return result;
+}
+#define AO_HAVE_char_load_full
+
+AO_INLINE void
+AO_char_store_full(volatile unsigned char *addr, unsigned char val)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  *addr = val;
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_char_store_full
+
+AO_INLINE unsigned short
+AO_short_load_full(const volatile unsigned short *addr)
+{
+  unsigned short result;
+  pthread_mutex_lock(&AO_pt_lock);
+  result = *addr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return result;
+}
+#define AO_HAVE_short_load_full
+
+AO_INLINE void
+AO_short_store_full(volatile unsigned short *addr, unsigned short val)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  *addr = val;
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_short_store_full
+
+AO_INLINE unsigned int
+AO_int_load_full(const volatile unsigned int *addr)
+{
+  unsigned int result;
+  pthread_mutex_lock(&AO_pt_lock);
+  result = *addr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return result;
+}
+#define AO_HAVE_int_load_full
+
+AO_INLINE void
+AO_int_store_full(volatile unsigned int *addr, unsigned int val)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  *addr = val;
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_int_store_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+  AO_TS_VAL_t result;
+  pthread_mutex_lock(&AO_pt_lock);
+  result = (AO_TS_VAL_t)(*addr);
+  *addr = AO_TS_SET;
+  pthread_mutex_unlock(&AO_pt_lock);
+  assert(result == AO_TS_SET || result == AO_TS_CLEAR);
+  return result;
+}
+#define AO_HAVE_test_and_set_full
+
+AO_INLINE AO_t
+AO_fetch_and_add_full(volatile AO_t *p, AO_t incr)
+{
+  AO_t tmp;
+
+  pthread_mutex_lock(&AO_pt_lock);
+  tmp = *p;
+  *p = tmp + incr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return tmp;
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE unsigned char
+AO_char_fetch_and_add_full(volatile unsigned char *p, unsigned char incr)
+{
+  unsigned char tmp;
+
+  pthread_mutex_lock(&AO_pt_lock);
+  tmp = *p;
+  *p = tmp + incr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return tmp;
+}
+#define AO_HAVE_char_fetch_and_add_full
+
+AO_INLINE unsigned short
+AO_short_fetch_and_add_full(volatile unsigned short *p, unsigned short incr)
+{
+  unsigned short tmp;
+
+  pthread_mutex_lock(&AO_pt_lock);
+  tmp = *p;
+  *p = tmp + incr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return tmp;
+}
+#define AO_HAVE_short_fetch_and_add_full
+
+AO_INLINE unsigned int
+AO_int_fetch_and_add_full(volatile unsigned int *p, unsigned int incr)
+{
+  unsigned int tmp;
+
+  pthread_mutex_lock(&AO_pt_lock);
+  tmp = *p;
+  *p = tmp + incr;
+  pthread_mutex_unlock(&AO_pt_lock);
+  return tmp;
+}
+#define AO_HAVE_int_fetch_and_add_full
+
+AO_INLINE void
+AO_or_full(volatile AO_t *p, AO_t incr)
+{
+  AO_t tmp;
+
+  pthread_mutex_lock(&AO_pt_lock);
+  tmp = *p;
+  *p = (tmp | incr);
+  pthread_mutex_unlock(&AO_pt_lock);
+}
+#define AO_HAVE_or_full
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  if (*addr == old)
+    {
+      *addr = new_val;
+      pthread_mutex_unlock(&AO_pt_lock);
+      return 1;
+    }
+  else
+    pthread_mutex_unlock(&AO_pt_lock);
+    return 0;
+}
+#define AO_HAVE_compare_and_swap_full
+
+/* Unlike real architectures, we define both double-width CAS variants. */
+
+typedef struct {
+        AO_t AO_val1;
+        AO_t AO_val2;
+} AO_double_t;
+#define AO_HAVE_double_t
+
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old1, AO_t old2,
+                                       AO_t new1, AO_t new2)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  if (addr -> AO_val1 == old1 && addr -> AO_val2 == old2)
+    {
+      addr -> AO_val1 = new1;
+      addr -> AO_val2 = new2;
+      pthread_mutex_unlock(&AO_pt_lock);
+      return 1;
+    }
+  else
+    pthread_mutex_unlock(&AO_pt_lock);
+    return 0;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+
+AO_INLINE int
+AO_compare_and_swap_double_full(volatile AO_double_t *addr,
+                                AO_t old1,
+                                AO_t new1, AO_t new2)
+{
+  pthread_mutex_lock(&AO_pt_lock);
+  if (addr -> AO_val1 == old1)
+    {
+      addr -> AO_val1 = new1;
+      addr -> AO_val2 = new2;
+      pthread_mutex_unlock(&AO_pt_lock);
+      return 1;
+    }
+  else
+    pthread_mutex_unlock(&AO_pt_lock);
+    return 0;
+}
+#define AO_HAVE_compare_and_swap_double_full
+
+/* We can't use hardware loads and stores, since they don't     */
+/* interact correctly with atomic updates.                      */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/hppa.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/hppa.h
new file mode 100644
index 0000000..b34d6bc
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/hppa.h
@@ -0,0 +1,100 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * Derived from the corresponding header file for gcc.
+ *
+ */
+
+#include "../atomic_load_store.h"
+
+/* Some architecture set descriptions include special "ordered" memory  */
+/* operations.  As far as we can tell, no existing processors actually  */
+/* require those.  Nor does it appear likely that future processors     */
+/* will.                                                                */
+/* FIXME:                                                               */
+/* The PA emulator on Itanium may obey weaker restrictions.             */
+/* There should be a mode in which we don't assume sequential           */
+/* consistency here.                                                    */
+#include "../ordered.h"
+
+#include <machine/inline.h>
+
+/* GCC will not guarantee the alignment we need, use four lock words    */
+/* and select the correctly aligned datum. See the glibc 2.3.2          */
+/* linuxthread port for the original implementation.                    */
+struct AO_pa_clearable_loc {
+  int data[4];
+};
+
+#undef AO_TS_INITIALIZER
+#define AO_TS_t struct AO_pa_clearable_loc
+#define AO_TS_INITIALIZER {1,1,1,1}
+/* Switch meaning of set and clear, since we only have an atomic clear  */
+/* instruction.                                                         */
+typedef enum {AO_PA_TS_set = 0, AO_PA_TS_clear = 1} AO_PA_TS_val;
+#define AO_TS_VAL_t AO_PA_TS_val
+#define AO_TS_CLEAR AO_PA_TS_clear
+#define AO_TS_SET AO_PA_TS_set
+
+/* The hppa only has one atomic read and modify memory operation,       */
+/* load and clear, so hppa spinlocks must use zero to signify that      */
+/* someone is holding the lock.  The address used for the ldcw          */
+/* semaphore must be 16-byte aligned.                                   */
+
+#define __ldcw(a, ret)  \
+  _LDCWX(0 /* index */, 0 /* s */, a /* base */, ret);
+
+/* Because malloc only guarantees 8-byte alignment for malloc'd data,   */
+/* and GCC only guarantees 8-byte alignment for stack locals, we can't  */
+/* be assured of 16-byte alignment for atomic lock data even if we      */
+/* specify "__attribute ((aligned(16)))" in the type declaration.  So,  */
+/* we use a struct containing an array of four ints for the atomic lock */
+/* type and dynamically select the 16-byte aligned int from the array   */
+/* for the semaphore.                                                   */
+#define __PA_LDCW_ALIGNMENT 16
+
+#define __ldcw_align(a, ret) { \
+  ret = (unsigned long) a;                      \
+  ret += __PA_LDCW_ALIGNMENT - 1;                                       \
+  ret &= ~(__PA_LDCW_ALIGNMENT - 1);                                    \
+}
+
+/* Works on PA 1.1 and PA 2.0 systems */
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t * addr)
+{
+  register unsigned int ret;
+  register unsigned long a;
+  __ldcw_align (addr, a);
+  __ldcw (a, ret);
+  return ret;
+}
+#define AO_HAVE_test_and_set_full
+
+AO_INLINE void
+AO_pa_clear(volatile AO_TS_t * addr)
+{
+  unsigned long a;
+  __ldcw_align (addr,a);
+  AO_compiler_barrier();
+  *(volatile unsigned int *)a = 1;
+}
+#define AO_CLEAR(addr) AO_pa_clear(addr)
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/ia64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/ia64.h
new file mode 100644
index 0000000..3c538fd
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/hpc/ia64.h
@@ -0,0 +1,166 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file specifies Itanimum primitives for use with the HP compiler
+ * under HP/UX.  We use intrinsics instead of the inline assembly code in the
+ * gcc file.
+ */
+
+#include "../all_atomic_load_store.h"
+
+#include "../all_acquire_release_volatile.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include <machine/sys/inline.h>
+
+#ifdef __LP64__
+# define AO_T_FASIZE _FASZ_D
+# define AO_T_SIZE _SZ_D
+#else
+# define AO_T_FASIZE _FASZ_W
+# define AO_T_SIZE _SZ_W
+#endif
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  _Asm_mf();
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_t
+AO_fetch_and_add1_acquire (volatile AO_t *p)
+{
+  return _Asm_fetchadd(AO_T_FASIZE, _SEM_ACQ, p, 1,
+                       _LDHINT_NONE, _DOWN_MEM_FENCE);
+}
+#define AO_HAVE_fetch_and_add1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_add1_release (volatile AO_t *p)
+{
+  return _Asm_fetchadd(AO_T_FASIZE, _SEM_REL, p, 1,
+                       _LDHINT_NONE, _UP_MEM_FENCE);
+}
+#define AO_HAVE_fetch_and_add1_release
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_acquire (volatile AO_t *p)
+{
+  return _Asm_fetchadd(AO_T_FASIZE, _SEM_ACQ, p, -1,
+                       _LDHINT_NONE, _DOWN_MEM_FENCE);
+}
+#define AO_HAVE_fetch_and_sub1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_release (volatile AO_t *p)
+{
+  return _Asm_fetchadd(AO_T_FASIZE, _SEM_REL, p, -1,
+                       _LDHINT_NONE, _UP_MEM_FENCE);
+}
+#define AO_HAVE_fetch_and_sub1_release
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+
+  _Asm_mov_to_ar(_AREG_CCV, old, _DOWN_MEM_FENCE);
+  oldval = _Asm_cmpxchg(AO_T_SIZE, _SEM_ACQ, addr,
+                        new_val, _LDHINT_NONE, _DOWN_MEM_FENCE);
+  return (oldval == old);
+}
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+  _Asm_mov_to_ar(_AREG_CCV, old, _UP_MEM_FENCE);
+  oldval = _Asm_cmpxchg(AO_T_SIZE, _SEM_REL, addr,
+                        new_val, _LDHINT_NONE, _UP_MEM_FENCE);
+  /* Hopefully the compiler knows not to reorder the above two? */
+  return (oldval == old);
+}
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_char_compare_and_swap_acquire(volatile unsigned char *addr,
+                                 unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+
+  _Asm_mov_to_ar(_AREG_CCV, old, _DOWN_MEM_FENCE);
+  oldval = _Asm_cmpxchg(_SZ_B, _SEM_ACQ, addr,
+                        new_val, _LDHINT_NONE, _DOWN_MEM_FENCE);
+  return (oldval == old);
+}
+#define AO_HAVE_char_compare_and_swap_acquire
+
+AO_INLINE int
+AO_char_compare_and_swap_release(volatile unsigned char *addr,
+                                 unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+  _Asm_mov_to_ar(_AREG_CCV, old, _UP_MEM_FENCE);
+  oldval = _Asm_cmpxchg(_SZ_B, _SEM_REL, addr,
+                        new_val, _LDHINT_NONE, _UP_MEM_FENCE);
+  /* Hopefully the compiler knows not to reorder the above two? */
+  return (oldval == old);
+}
+#define AO_HAVE_char_compare_and_swap_release
+
+AO_INLINE int
+AO_short_compare_and_swap_acquire(volatile unsigned short *addr,
+                                 unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+
+  _Asm_mov_to_ar(_AREG_CCV, old, _DOWN_MEM_FENCE);
+  oldval = _Asm_cmpxchg(_SZ_B, _SEM_ACQ, addr,
+                        new_val, _LDHINT_NONE, _DOWN_MEM_FENCE);
+  return (oldval == old);
+}
+#define AO_HAVE_short_compare_and_swap_acquire
+
+AO_INLINE int
+AO_short_compare_and_swap_release(volatile unsigned short *addr,
+                                 unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+  _Asm_mov_to_ar(_AREG_CCV, old, _UP_MEM_FENCE);
+  oldval = _Asm_cmpxchg(_SZ_B, _SEM_REL, addr,
+                        new_val, _LDHINT_NONE, _UP_MEM_FENCE);
+  /* Hopefully the compiler knows not to reorder the above two? */
+  return (oldval == old);
+}
+#define AO_HAVE_short_compare_and_swap_release
+
+#ifndef __LP64__
+  /* Generalize first to define more AO_int_... primitives.     */
+# include "../../generalize.h"
+# include "../ao_t_is_int.h"
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ibmc/powerpc.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ibmc/powerpc.h
new file mode 100644
index 0000000..4378dca
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ibmc/powerpc.h
@@ -0,0 +1,124 @@
+/* FIXME.  This is only a placeholder for the AIX compiler.             */
+/* It doesn't work.  Please send a patch.                               */
+/* Memory model documented at http://www-106.ibm.com/developerworks/    */
+/* eserver/articles/archguide.html and (clearer)                        */
+/* http://www-106.ibm.com/developerworks/eserver/articles/powerpc.html. */
+/* There appears to be no implicit ordering between any kind of         */
+/* independent memory references.                                       */
+/* Architecture enforces some ordering based on control dependence.     */
+/* I don't know if that could help.                                     */
+/* Data-dependent loads are always ordered.                             */
+/* Based on the above references, eieio is intended for use on          */
+/* uncached memory, which we don't support.  It does not order loads    */
+/* from cached memory.                                                  */
+/* Thanks to Maged Michael, Doug Lea, and Roger Hoover for helping to   */
+/* track some of this down and correcting my misunderstandings. -HB     */
+
+#include "../all_aligned_atomic_load_store.h"
+
+void AO_sync(void);
+#pragma mc_func AO_sync { "7c0004ac" }
+
+#ifdef __NO_LWSYNC__
+# define AO_lwsync AO_sync
+#else
+  void AO_lwsync(void);
+#pragma mc_func AO_lwsync { "7c2004ac" }
+#endif
+
+#define AO_nop_write() AO_lwsync()
+#define AO_HAVE_nop_write
+
+#define AO_nop_read() AO_lwsync()
+#define AO_HAVE_nop_read
+
+/* We explicitly specify load_acquire and store_release, since these    */
+/* rely on the fact that lwsync is also a LoadStore barrier.            */
+AO_INLINE AO_t
+AO_load_acquire(const volatile AO_t *addr)
+{
+  AO_t result = *addr;
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_load_acquire
+
+AO_INLINE void
+AO_store_release(volatile AO_t *addr, AO_t value)
+{
+  AO_lwsync();
+  *addr = value;
+}
+#define AO_HAVE_store_release
+
+/* This is similar to the code in the garbage collector.  Deleting      */
+/* this and having it synthesized from compare_and_swap would probably  */
+/* only cost us a load immediate instruction.                           */
+/*AO_INLINE AO_TS_VAL_t
+AO_test_and_set(volatile AO_TS_t *addr) {
+# error FIXME Implement me
+}
+#define AO_HAVE_test_and_set*/
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_acquire(volatile AO_TS_t *addr) {
+  AO_TS_VAL_t result = AO_test_and_set(addr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_test_and_set_acquire
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_release(volatile AO_TS_t *addr) {
+  AO_lwsync();
+  return AO_test_and_set(addr);
+}
+#define AO_HAVE_test_and_set_release
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr) {
+  AO_TS_VAL_t result;
+  AO_lwsync();
+  result = AO_test_and_set(addr);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_test_and_set_full
+
+/*AO_INLINE int
+AO_compare_and_swap(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+# error FIXME Implement me
+}
+#define AO_HAVE_compare_and_swap*/
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  int result = AO_compare_and_swap(addr, old, new_val);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  AO_lwsync();
+  return AO_compare_and_swap(addr, old, new_val);
+}
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  int result;
+  AO_lwsync();
+  result = AO_compare_and_swap(addr, old, new_val);
+  AO_lwsync();
+  return result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+/* FIXME: We should also implement fetch_and_add and or primitives      */
+/* directly.                                                            */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/icc/ia64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/icc/ia64.h
new file mode 100644
index 0000000..d0ca804
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/icc/ia64.h
@@ -0,0 +1,224 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file specifies Itanimum primitives for use with the Intel (ecc)
+ * compiler.  We use intrinsics instead of the inline assembly code in the
+ * gcc file.
+ */
+
+#include "../all_atomic_load_store.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include <ia64intrin.h>
+
+/* The acquire release semantics of volatile can be turned off.  And volatile   */
+/* operations in icc9 don't imply ordering with respect to other nonvolatile    */
+/* operations.                                                                  */
+
+#define AO_INTEL_PTR_t void *
+
+AO_INLINE AO_t
+AO_load_acquire(const volatile AO_t *p)
+{
+  return (AO_t)(__ld8_acq((AO_INTEL_PTR_t)p));
+}
+#define AO_HAVE_load_acquire
+
+AO_INLINE void
+AO_store_release(volatile AO_t *p, AO_t val)
+{
+  __st8_rel((AO_INTEL_PTR_t)p, (__int64)val);
+}
+#define AO_HAVE_store_release
+
+AO_INLINE unsigned char
+AO_char_load_acquire(const volatile unsigned char *p)
+{
+  /* A normal volatile load generates an ld.acq         */
+  return (__ld1_acq((AO_INTEL_PTR_t)p));
+}
+#define AO_HAVE_char_load_acquire
+
+AO_INLINE void
+AO_char_store_release(volatile unsigned char *p, unsigned char val)
+{
+  __st1_rel((AO_INTEL_PTR_t)p, val);
+}
+#define AO_HAVE_char_store_release
+
+AO_INLINE unsigned short
+AO_short_load_acquire(const volatile unsigned short *p)
+{
+  /* A normal volatile load generates an ld.acq         */
+  return (__ld2_acq((AO_INTEL_PTR_t)p));
+}
+#define AO_HAVE_short_load_acquire
+
+AO_INLINE void
+AO_short_store_release(volatile unsigned short *p, unsigned short val)
+{
+  __st2_rel((AO_INTEL_PTR_t)p, val);
+}
+#define AO_HAVE_short_store_release
+
+AO_INLINE unsigned int
+AO_int_load_acquire(const volatile unsigned int *p)
+{
+  /* A normal volatile load generates an ld.acq         */
+  return (__ld4_acq((AO_INTEL_PTR_t)p));
+}
+#define AO_HAVE_int_load_acquire
+
+AO_INLINE void
+AO_int_store_release(volatile unsigned int *p, unsigned int val)
+{
+  __st4_rel((AO_INTEL_PTR_t)p, val);
+}
+#define AO_HAVE_int_store_release
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __mf();
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_t
+AO_fetch_and_add1_acquire (volatile AO_t *p)
+{
+  return __fetchadd8_acq((unsigned __int64 *)p, 1);
+}
+#define AO_HAVE_fetch_and_add1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_add1_release (volatile AO_t *p)
+{
+  return __fetchadd8_rel((unsigned __int64 *)p, 1);
+}
+
+#define AO_HAVE_fetch_and_add1_release
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_acquire (volatile AO_t *p)
+{
+  return __fetchadd8_acq((unsigned __int64 *)p, -1);
+}
+
+#define AO_HAVE_fetch_and_sub1_acquire
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_release (volatile AO_t *p)
+{
+  return __fetchadd8_rel((unsigned __int64 *)p, -1);
+}
+
+#define AO_HAVE_fetch_and_sub1_release
+
+AO_INLINE int
+AO_compare_and_swap_acquire(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+  oldval = _InterlockedCompareExchange64_acq(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_compare_and_swap_acquire
+
+AO_INLINE int
+AO_compare_and_swap_release(volatile AO_t *addr,
+                             AO_t old, AO_t new_val)
+{
+  AO_t oldval;
+  oldval = _InterlockedCompareExchange64_rel(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_compare_and_swap_release
+
+AO_INLINE int
+AO_char_compare_and_swap_acquire(volatile unsigned char *addr,
+                                 unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+  oldval = _InterlockedCompareExchange8_acq(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_char_compare_and_swap_acquire
+
+AO_INLINE int
+AO_char_compare_and_swap_release(volatile unsigned char *addr,
+                            unsigned char old, unsigned char new_val)
+{
+  unsigned char oldval;
+  oldval = _InterlockedCompareExchange8_rel(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_char_compare_and_swap_release
+
+AO_INLINE int
+AO_short_compare_and_swap_acquire(volatile unsigned short *addr,
+                                 unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+  oldval = _InterlockedCompareExchange16_acq(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_short_compare_and_swap_acquire
+
+AO_INLINE int
+AO_short_compare_and_swap_release(volatile unsigned short *addr,
+                            unsigned short old, unsigned short new_val)
+{
+  unsigned short oldval;
+  oldval = _InterlockedCompareExchange16_rel(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_short_compare_and_swap_release
+
+AO_INLINE int
+AO_int_compare_and_swap_acquire(volatile unsigned int *addr,
+                                 unsigned int old, unsigned int new_val)
+{
+  unsigned int oldval;
+  oldval = _InterlockedCompareExchange_acq(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_int_compare_and_swap_acquire
+
+AO_INLINE int
+AO_int_compare_and_swap_release(volatile unsigned int *addr,
+                            unsigned int old, unsigned int new_val)
+{
+  unsigned int oldval;
+  oldval = _InterlockedCompareExchange_rel(addr, new_val, old);
+  return (oldval == old);
+}
+
+#define AO_HAVE_int_compare_and_swap_release
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_acquire_release_volatile.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_acquire_release_volatile.h
new file mode 100644
index 0000000..01037a2
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_acquire_release_volatile.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file adds definitions appropriate for environments in which an unsigned
+ * int volatile load has acquire semantics, and an unsigned short volatile
+ * store has release semantics.  This is true with the standard Itanium ABI.
+ */
+#if !defined(AO_GCC_BARRIER)
+#  if defined(__GNUC__)
+#    define AO_GCC_BARRIER() AO_compiler_barrier()
+#  else
+#    define AO_GCC_BARRIER()
+#  endif
+#endif
+
+AO_INLINE unsigned int
+AO_int_load_acquire(const volatile unsigned int *p)
+{
+  unsigned int result = *p;
+  /* A normal volatile load generates an ld.acq         */
+  AO_GCC_BARRIER();
+  return result;
+}
+#define AO_HAVE_int_load_acquire
+
+AO_INLINE void
+AO_int_store_release(volatile unsigned int *p, unsigned int val)
+{
+  AO_GCC_BARRIER();
+  /* A normal volatile store generates an st.rel        */
+  *p = val;
+}
+#define AO_HAVE_int_store_release
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_aligned_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_aligned_atomic_load_store.h
new file mode 100644
index 0000000..1dcb3b2
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_aligned_atomic_load_store.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Definitions for architectures on which loads and stores of unsigned  */
+/* int are atomic for all legal alignments.                             */
+
+AO_INLINE unsigned int
+AO_int_load(const volatile unsigned int *addr)
+{
+  assert(((size_t)addr & (sizeof(unsigned int) - 1)) == 0);
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(unsigned int *)addr);
+}
+#define AO_HAVE_int_load
+
+AO_INLINE void
+AO_int_store(volatile unsigned int *addr, unsigned int new_val)
+{
+  assert(((size_t)addr & (sizeof(unsigned int) - 1)) == 0);
+  (*(unsigned int *)addr) = new_val;
+}
+#define AO_HAVE_int_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_atomic_load_store.h
new file mode 100644
index 0000000..0c3777b
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/int_atomic_load_store.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Definitions for architectures on which loads and stores of unsigned  */
+/* int are atomic for all legal alignments.                             */
+
+AO_INLINE unsigned int
+AO_int_load(const volatile unsigned int *addr)
+{
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(const unsigned int *)addr);
+}
+#define AO_HAVE_int_load
+
+AO_INLINE void
+AO_int_store(volatile unsigned int *addr, unsigned int new_val)
+{
+  (*(unsigned int *)addr) = new_val;
+}
+#define AO_HAVE_int_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/arm.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/arm.h
new file mode 100644
index 0000000..614c9e9
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/arm.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef AO_ASSUME_WINDOWS98
+  /* CAS is always available */
+# define AO_ASSUME_WINDOWS98
+#endif
+#include "common32_defs.h"
+/* FIXME: Do _InterlockedOps really have a full memory barrier?         */
+/* (MSDN WinCE docs say nothing about it.)                              */
+
+#if _M_ARM >= 6
+/* ARMv6 is the first architecture providing support for simple LL/SC.  */
+
+#include "../standard_ao_double_t.h"
+
+/* If only a single processor is used, we can define AO_UNIPROCESSOR.   */
+#ifdef AO_UNIPROCESSOR
+  AO_INLINE void AO_nop_full(void)
+  {
+    AO_compiler_barrier();
+  }
+# define AO_HAVE_nop_full
+#else
+/* AO_nop_full() is emulated using AO_test_and_set_full().              */
+#endif
+
+#include "../test_and_set_t_is_ao_t.h"
+/* AO_test_and_set() is emulated using CAS.                             */
+
+AO_INLINE AO_t
+AO_load(const volatile AO_t *addr)
+{
+  /* Cast away the volatile in case it adds fence semantics */
+  return (*(const AO_t *)addr);
+}
+#define AO_HAVE_load
+
+AO_INLINE void
+AO_store_full(volatile AO_t *addr, AO_t value)
+{
+  /* Emulate atomic store using CAS.    */
+  AO_t old = AO_load(addr);
+  AO_t current;
+# ifdef AO_OLD_STYLE_INTERLOCKED_COMPARE_EXCHANGE
+    while ((current = (AO_t)_InterlockedCompareExchange(
+                                (PVOID AO_INTERLOCKED_VOLATILE *)addr,
+                                (PVOID)value, (PVOID)old)) != old)
+      old = current;
+# else
+    while ((current = (AO_t)_InterlockedCompareExchange(
+                                (LONG AO_INTERLOCKED_VOLATILE *)addr,
+                                (LONG)value, (LONG)old)) != old)
+      old = current;
+# endif
+}
+#define AO_HAVE_store_full
+
+/* FIXME: implement AO_compare_double_and_swap_double() */
+
+#else /* _M_ARM < 6 */
+
+/* Some ARM slide set, if it has been read correctly, claims that Loads */
+/* followed by either a Load or a Store are ordered, but nothing        */
+/* else is.  It appears that SWP is the only simple memory barrier.     */
+#include "../all_atomic_load_store.h"
+
+#include "../test_and_set_t_is_ao_t.h"
+/* AO_test_and_set_full() is emulated using CAS.                        */
+
+#endif /* _M_ARM < 6 */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/common32_defs.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/common32_defs.h
new file mode 100644
index 0000000..5d519f0
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/common32_defs.h
@@ -0,0 +1,118 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* This file contains AO primitives based on VC++ built-in intrinsic    */
+/* functions commonly available across 32-bit architectures.            */
+
+/* This file should be included from arch-specific header files.        */
+/* Define AO_USE_INTERLOCKED_INTRINSICS if _Interlocked primitives      */
+/* (used below) are available as intrinsic ones for a target arch       */
+/* (otherwise "Interlocked" functions family is used instead).          */
+/* Define AO_ASSUME_WINDOWS98 if CAS is available.                      */
+
+#include <windows.h>
+        /* Seems like over-kill, but that's what MSDN recommends.       */
+        /* And apparently winbase.h is not always self-contained.       */
+
+#if _MSC_VER < 1310 || !defined(AO_USE_INTERLOCKED_INTRINSICS)
+
+# define _InterlockedIncrement       InterlockedIncrement
+# define _InterlockedDecrement       InterlockedDecrement
+# define _InterlockedExchange        InterlockedExchange
+# define _InterlockedExchangeAdd     InterlockedExchangeAdd
+# define _InterlockedCompareExchange InterlockedCompareExchange
+
+# define AO_INTERLOCKED_VOLATILE /**/
+
+#else /* elif _MSC_VER >= 1310 */
+
+# if _MSC_VER >= 1400
+#   ifndef _WIN32_WCE
+#     include <intrin.h>
+#   endif
+
+#   pragma intrinsic (_ReadWriteBarrier)
+
+# else /* elif _MSC_VER < 1400 */
+#  ifdef __cplusplus
+     extern "C" {
+#  endif
+   LONG __cdecl _InterlockedIncrement(LONG volatile *);
+   LONG __cdecl _InterlockedDecrement(LONG volatile *);
+   LONG __cdecl _InterlockedExchangeAdd(LONG volatile *, LONG);
+   LONG __cdecl _InterlockedExchange(LONG volatile *, LONG);
+   LONG __cdecl _InterlockedCompareExchange(LONG volatile *,
+                                        LONG /* Exchange */, LONG /* Comp */);
+#  ifdef __cplusplus
+     }
+#  endif
+# endif /* _MSC_VER < 1400 */
+
+# pragma intrinsic (_InterlockedIncrement)
+# pragma intrinsic (_InterlockedDecrement)
+# pragma intrinsic (_InterlockedExchange)
+# pragma intrinsic (_InterlockedExchangeAdd)
+# pragma intrinsic (_InterlockedCompareExchange)
+
+# define AO_INTERLOCKED_VOLATILE volatile
+
+#endif /* _MSC_VER >= 1310 */
+
+AO_INLINE AO_t
+AO_fetch_and_add_full(volatile AO_t *p, AO_t incr)
+{
+  return _InterlockedExchangeAdd((LONG AO_INTERLOCKED_VOLATILE *)p,
+                                 (LONG)incr);
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE AO_t
+AO_fetch_and_add1_full(volatile AO_t *p)
+{
+  return _InterlockedIncrement((LONG AO_INTERLOCKED_VOLATILE *)p) - 1;
+}
+#define AO_HAVE_fetch_and_add1_full
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_full(volatile AO_t *p)
+{
+  return _InterlockedDecrement((LONG AO_INTERLOCKED_VOLATILE *)p) + 1;
+}
+#define AO_HAVE_fetch_and_sub1_full
+
+#ifdef AO_ASSUME_WINDOWS98
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+# ifdef AO_OLD_STYLE_INTERLOCKED_COMPARE_EXCHANGE
+    return _InterlockedCompareExchange((PVOID AO_INTERLOCKED_VOLATILE *)addr,
+                                       (PVOID)new_val, (PVOID)old)
+           == (PVOID)old;
+# else
+    return _InterlockedCompareExchange((LONG AO_INTERLOCKED_VOLATILE *)addr,
+                                       (LONG)new_val, (LONG)old)
+           == (LONG)old;
+# endif
+}
+# define AO_HAVE_compare_and_swap_full
+#endif /* AO_ASSUME_WINDOWS98 */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86.h
new file mode 100644
index 0000000..3ab17a1
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86.h
@@ -0,0 +1,120 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* If AO_ASSUME_WINDOWS98 is defined, we assume Windows 98 or newer.    */
+/* If AO_ASSUME_VISTA is defined, we assume Windows Server 2003, Vista  */
+/* or later.                                                            */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations, except for some old WinChips, appear       */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore both the WinChips, and the fact that the official specs    */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#ifndef AO_USE_INTERLOCKED_INTRINSICS
+  /* _Interlocked primitives (Inc, Dec, Xchg, Add) are always available */
+# define AO_USE_INTERLOCKED_INTRINSICS
+#endif
+#include "common32_defs.h"
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+/* Unfortunately mfence doesn't exist everywhere.               */
+/* IsProcessorFeaturePresent(PF_COMPARE_EXCHANGE128) is         */
+/* probably a conservative test for it?                         */
+
+#if defined(AO_USE_PENTIUM4_INSTRS)
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm { mfence }
+}
+#define AO_HAVE_nop_full
+
+#else
+
+/* We could use the cpuid instruction.  But that seems to be slower     */
+/* than the default implementation based on test_and_set_full.  Thus    */
+/* we omit that bit of misinformation here.                             */
+
+#endif
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+    __asm
+    {
+        mov     eax,0xff                ; /* AO_TS_SET */
+        mov     ebx,addr                ;
+        xchg    byte ptr [ebx],al       ;
+    }
+    /* Ignore possible "missing return value" warning here. */
+}
+#define AO_HAVE_test_and_set_full
+
+#ifdef _WIN64
+#  error wrong architecture
+#endif
+
+#ifdef AO_ASSUME_VISTA
+
+/* NEC LE-IT: whenever we run on a pentium class machine we have that
+ * certain function */
+
+#include "../standard_ao_double_t.h"
+#pragma intrinsic (_InterlockedCompareExchange64)
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+    __int64 oldv = (__int64)old_val1 | ((__int64)old_val2 << 32);
+    __int64 newv = (__int64)new_val1 | ((__int64)new_val2 << 32);
+    return _InterlockedCompareExchange64((__int64 volatile *)addr,
+                                       newv, oldv) == oldv;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+
+#ifdef __cplusplus
+AO_INLINE int
+AO_double_compare_and_swap_full(volatile AO_double_t *addr,
+                                AO_double_t old_val, AO_double_t new_val)
+{
+    return _InterlockedCompareExchange64((__int64 volatile *)addr,
+                new_val.AO_whole, old_val.AO_whole) == old_val.AO_whole;
+}
+#define AO_HAVE_double_compare_and_swap_full
+#endif /* __cplusplus */
+
+#endif /* AO_ASSUME_VISTA */
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86_64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86_64.h
new file mode 100644
index 0000000..135a053
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/msftc/x86_64.h
@@ -0,0 +1,158 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations appear                                      */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore the fact that the official specs                           */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#ifdef AO_ASM_X64_AVAILABLE
+# include "../test_and_set_t_is_char.h"
+#else
+# include "../test_and_set_t_is_ao_t.h"
+#endif
+
+#include "../standard_ao_double_t.h"
+
+#include <windows.h>
+        /* Seems like over-kill, but that's what MSDN recommends.       */
+        /* And apparently winbase.h is not always self-contained.       */
+
+/* Assume _MSC_VER >= 1400 */
+#include <intrin.h>
+
+#pragma intrinsic (_ReadWriteBarrier)
+
+#pragma intrinsic (_InterlockedIncrement64)
+#pragma intrinsic (_InterlockedDecrement64)
+#pragma intrinsic (_InterlockedExchange64)
+#pragma intrinsic (_InterlockedExchangeAdd64)
+#pragma intrinsic (_InterlockedCompareExchange64)
+
+AO_INLINE AO_t
+AO_fetch_and_add_full (volatile AO_t *p, AO_t incr)
+{
+  return _InterlockedExchangeAdd64((LONGLONG volatile *)p, (LONGLONG)incr);
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE AO_t
+AO_fetch_and_add1_full (volatile AO_t *p)
+{
+  return _InterlockedIncrement64((LONGLONG volatile *)p) - 1;
+}
+#define AO_HAVE_fetch_and_add1_full
+
+AO_INLINE AO_t
+AO_fetch_and_sub1_full (volatile AO_t *p)
+{
+  return _InterlockedDecrement64((LONGLONG volatile *)p) + 1;
+}
+#define AO_HAVE_fetch_and_sub1_full
+
+AO_INLINE int
+AO_compare_and_swap_full(volatile AO_t *addr,
+                         AO_t old, AO_t new_val)
+{
+    return _InterlockedCompareExchange64((LONGLONG volatile *)addr,
+                                         (LONGLONG)new_val, (LONGLONG)old)
+           == (LONGLONG)old;
+}
+#define AO_HAVE_compare_and_swap_full
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+#ifdef AO_ASM_X64_AVAILABLE
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  /* Note: "mfence" (SSE2) is supported on all x86_64/amd64 chips.      */
+  __asm { mfence }
+}
+#define AO_HAVE_nop_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr)
+{
+    __asm
+    {
+        mov     rax,AO_TS_SET           ;
+        mov     rbx,addr                ;
+        xchg    byte ptr [rbx],al       ;
+    }
+}
+#define AO_HAVE_test_and_set_full
+
+#endif /* AO_ASM_X64_AVAILABLE */
+
+#ifdef AO_CMPXCHG16B_AVAILABLE
+/* AO_compare_double_and_swap_double_full needs implementation for Win64.
+ * Also see ../gcc/x86_64.h for partial old Opteron workaround.
+ */
+
+# if _MSC_VER >= 1500
+
+#pragma intrinsic (_InterlockedCompareExchange128)
+
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+   __int64 comparandResult[2];
+   comparandResult[0] = old_val1; /* low */
+   comparandResult[1] = old_val2; /* high */
+   return _InterlockedCompareExchange128((volatile __int64 *)addr,
+                new_val2 /* high */, new_val1 /* low */, comparandResult);
+}
+#   define AO_HAVE_compare_double_and_swap_double_full
+
+# elif defined(AO_ASM_X64_AVAILABLE)
+    /* If there is no intrinsic _InterlockedCompareExchange128 then we  */
+    /* need basically what's given below.                               */
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+        __asm
+        {
+                mov     rdx,QWORD PTR [old_val2]        ;
+                mov     rax,QWORD PTR [old_val1]        ;
+                mov     rcx,QWORD PTR [new_val2]        ;
+                mov     rbx,QWORD PTR [new_val1]        ;
+                lock cmpxchg16b [addr]                  ;
+                setz    rax                             ;
+        }
+}
+#   define AO_HAVE_compare_double_and_swap_double_full
+# endif /* _MSC_VER >= 1500 || AO_ASM_X64_AVAILABLE */
+
+#endif /* AO_CMPXCHG16B_AVAILABLE */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered.h
new file mode 100644
index 0000000..ba9822d
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright (c) 2003 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* These are common definitions for architectures that provide  */
+/* processor ordered memory operations.                         */
+
+#include "ordered_except_wr.h"
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  AO_compiler_barrier();
+}
+#define AO_HAVE_nop_full
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered_except_wr.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered_except_wr.h
new file mode 100644
index 0000000..da8b13a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/ordered_except_wr.h
@@ -0,0 +1,91 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * These are common definitions for architectures that provide processor
+ * ordered memory operations except that a later read may pass an
+ * earlier write.  Real x86 implementations seem to be in this category,
+ * except apparently for some IDT WinChips, which we ignore.
+ */
+
+#include "read_ordered.h"
+
+AO_INLINE void
+AO_nop_write(void)
+{
+  AO_compiler_barrier();
+  /* sfence according to Intel docs.  Pentium 3 and up. */
+  /* Unnecessary for cached accesses?                   */
+}
+#define AO_HAVE_nop_write
+
+#if defined(AO_HAVE_store)
+  AO_INLINE void
+  AO_store_write(volatile AO_t *addr, AO_t val)
+  {
+    AO_compiler_barrier();
+    AO_store(addr, val);
+  }
+# define AO_HAVE_store_write
+
+# define AO_store_release(addr, val) AO_store_write(addr, val)
+# define AO_HAVE_store_release
+#endif /* AO_HAVE_store */
+
+#if defined(AO_HAVE_char_store)
+  AO_INLINE void
+  AO_char_store_write(volatile unsigned char *addr, unsigned char val)
+  {
+    AO_compiler_barrier();
+    AO_char_store(addr, val);
+  }
+# define AO_HAVE_char_store_write
+
+# define AO_char_store_release(addr, val) AO_char_store_write(addr, val)
+# define AO_HAVE_char_store_release
+#endif /* AO_HAVE_char_store */
+
+#if defined(AO_HAVE_short_store)
+  AO_INLINE void
+  AO_short_store_write(volatile unsigned short *addr, unsigned short val)
+  {
+    AO_compiler_barrier();
+    AO_short_store(addr, val);
+  }
+# define AO_HAVE_short_store_write
+
+# define AO_short_store_release(addr, val) AO_short_store_write(addr, val)
+# define AO_HAVE_short_store_release
+#endif /* AO_HAVE_short_store */
+
+#if defined(AO_HAVE_int_store)
+  AO_INLINE void
+  AO_int_store_write(volatile unsigned int *addr, unsigned int val)
+  {
+    AO_compiler_barrier();
+    AO_int_store(addr, val);
+  }
+# define AO_HAVE_int_store_write
+
+# define AO_int_store_release(addr, val) AO_int_store_write(addr, val)
+# define AO_HAVE_int_store_release
+#endif /* AO_HAVE_int_store */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/read_ordered.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/read_ordered.h
new file mode 100644
index 0000000..986cd3a
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/read_ordered.h
@@ -0,0 +1,91 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * These are common definitions for architectures that provide processor
+ * ordered memory operations except that a later read may pass an
+ * earlier write.  Real x86 implementations seem to be in this category,
+ * except apparently for some IDT WinChips, which we ignore.
+ */
+
+AO_INLINE void
+AO_nop_read(void)
+{
+  AO_compiler_barrier();
+}
+#define AO_HAVE_nop_read
+
+#ifdef AO_HAVE_load
+  AO_INLINE AO_t
+  AO_load_read(const volatile AO_t *addr)
+  {
+    AO_t result = AO_load(addr);
+    AO_compiler_barrier();
+    return result;
+  }
+# define AO_HAVE_load_read
+
+# define AO_load_acquire(addr) AO_load_read(addr)
+# define AO_HAVE_load_acquire
+#endif /* AO_HAVE_load */
+
+#ifdef AO_HAVE_char_load
+  AO_INLINE unsigned char
+  AO_char_load_read(const volatile unsigned char *addr)
+  {
+    unsigned char result = AO_char_load(addr);
+    AO_compiler_barrier();
+    return result;
+  }
+# define AO_HAVE_char_load_read
+
+# define AO_char_load_acquire(addr) AO_char_load_read(addr)
+# define AO_HAVE_char_load_acquire
+#endif /* AO_HAVE_char_load */
+
+#ifdef AO_HAVE_short_load
+  AO_INLINE unsigned short
+  AO_short_load_read(const volatile unsigned short *addr)
+  {
+    unsigned short result = AO_short_load(addr);
+    AO_compiler_barrier();
+    return result;
+  }
+# define AO_HAVE_short_load_read
+
+# define AO_short_load_acquire(addr) AO_short_load_read(addr)
+# define AO_HAVE_short_load_acquire
+#endif /* AO_HAVE_short_load */
+
+#ifdef AO_HAVE_int_load
+  AO_INLINE unsigned int
+  AO_int_load_read(const volatile unsigned int *addr)
+  {
+    unsigned int result = AO_int_load(addr);
+    AO_compiler_barrier();
+    return result;
+  }
+# define AO_HAVE_int_load_read
+
+# define AO_int_load_acquire(addr) AO_int_load_read(addr)
+# define AO_HAVE_int_load_acquire
+#endif /* AO_HAVE_int_load */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_acquire_release_volatile.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_acquire_release_volatile.h
new file mode 100644
index 0000000..dcf3c04
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_acquire_release_volatile.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (c) 2003-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * This file adds definitions appropriate for environments in which an unsigned short
+ * volatile load has acquire semantics, and an unsigned short volatile store has release
+ * semantics.  This is true with the standard Itanium ABI.
+ */
+#if !defined(AO_GCC_BARRIER)
+#  if defined(__GNUC__)
+#    define AO_GCC_BARRIER() AO_compiler_barrier()
+#  else
+#    define AO_GCC_BARRIER()
+#  endif
+#endif
+
+AO_INLINE unsigned short
+AO_short_load_acquire(const volatile unsigned short *p)
+{
+  unsigned short result = *p;
+  /* A normal volatile load generates an ld.acq         */
+  AO_GCC_BARRIER();
+  return result;
+}
+#define AO_HAVE_short_load_acquire
+
+AO_INLINE void
+AO_short_store_release(volatile unsigned short *p, unsigned short val)
+{
+  AO_GCC_BARRIER();
+  /* A normal volatile store generates an st.rel        */
+  *p = val;
+}
+#define AO_HAVE_short_store_release
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_aligned_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_aligned_atomic_load_store.h
new file mode 100644
index 0000000..1340934
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_aligned_atomic_load_store.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Definitions for architectures on which loads and stores of unsigned short
+ * are atomic for all legal alignments.
+ */
+
+AO_INLINE unsigned short
+AO_short_load(const volatile unsigned short *addr)
+{
+  assert(((size_t)addr & (sizeof(unsigned short) - 1)) == 0);
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(unsigned short *)addr);
+}
+#define AO_HAVE_short_load
+
+AO_INLINE void
+AO_short_store(volatile unsigned short *addr, unsigned short new_val)
+{
+  assert(((size_t)addr & (sizeof(unsigned short) - 1)) == 0);
+  (*(unsigned short *)addr) = new_val;
+}
+#define AO_HAVE_short_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_atomic_load_store.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_atomic_load_store.h
new file mode 100644
index 0000000..3f3794c
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/short_atomic_load_store.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (c) 2003 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * Definitions for architectures on which loads and stores of unsigned short
+ * are atomic for all legal alignments.
+ */
+
+AO_INLINE unsigned short
+AO_short_load(const volatile unsigned short *addr)
+{
+  /* Cast away the volatile for architectures like IA64 where   */
+  /* volatile adds barrier semantics.                           */
+  return (*(const unsigned short *)addr);
+}
+#define AO_HAVE_short_load
+
+AO_INLINE void
+AO_short_store(volatile unsigned short *addr, unsigned short new_val)
+{
+  (*(unsigned short *)addr) = new_val;
+}
+#define AO_HAVE_short_store
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/standard_ao_double_t.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/standard_ao_double_t.h
new file mode 100644
index 0000000..38db06f
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/standard_ao_double_t.h
@@ -0,0 +1,27 @@
+/*
+ * NEC LE-IT: For 64-bit OS we extend the double type to hold two int64's
+ *
+ * x86-64 (except for x32): __m128 serves as placeholder which also requires
+ *    the compiler to align it on 16 byte boundary (as required by cmpxchg16).
+ * Similar things could be done for PowerPC 64-bit using a VMX data type...
+ */
+
+#if ((defined(__x86_64__) && __GNUC__ >= 4) || defined(_WIN64)) \
+  && !defined(__ILP32__) && !defined(NAUT)
+# include <xmmintrin.h>
+  typedef __m128 double_ptr_storage;
+#elif defined(_WIN32) && !defined(__GNUC__)
+  typedef unsigned __int64 double_ptr_storage;
+#else
+  typedef unsigned long long double_ptr_storage;
+#endif
+# define AO_HAVE_DOUBLE_PTR_STORAGE
+
+typedef union {
+    double_ptr_storage AO_whole;
+    struct {AO_t AO_v1; AO_t AO_v2;} AO_parts;
+} AO_double_t;
+
+#define AO_HAVE_double_t
+#define AO_val1 AO_parts.AO_v1
+#define AO_val2 AO_parts.AO_v2
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.S b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.S
new file mode 100644
index 0000000..81f0ef0
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.S
@@ -0,0 +1,5 @@
+	.seg 	"text"
+	.globl	AO_test_and_set_full
+AO_test_and_set_full:
+	retl
+	  ldstub	[%o0],%o0
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.h
new file mode 100644
index 0000000..1083a99
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/sparc.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "../all_atomic_load_store.h"
+
+/* Real SPARC code uses TSO:                            */
+#include "../ordered_except_wr.h"
+
+/* Test_and_set location is just a byte.                */
+#include "../test_and_set_t_is_char.h"
+
+extern AO_TS_VAL_t
+AO_test_and_set_full(volatile AO_TS_t *addr);
+/* Implemented in separate .S file, for now.    */
+#define AO_HAVE_test_and_set_full
+
+/* FIXME: Like the gcc version, this needs to be extended for V8        */
+/* and V9.                                                              */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86.h
new file mode 100644
index 0000000..fd8e132
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86.h
@@ -0,0 +1,164 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Some of the machine specific code was borrowed from our GC distribution.
+ */
+
+/* The following really assume we have a 486 or better.                 */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations, except for some old WinChips, appear       */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore both the WinChips, and the fact that the official specs    */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include "../standard_ao_double_t.h"
+
+#if defined(AO_USE_PENTIUM4_INSTRS)
+AO_INLINE void
+AO_nop_full(void)
+{
+  __asm__ __volatile__ ("mfence" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+#else
+
+/* We could use the cpuid instruction.  But that seems to be slower     */
+/* than the default implementation based on test_and_set_full.  Thus    */
+/* we omit that bit of misinformation here.                             */
+
+#endif
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+/* Really only works for 486 and later */
+AO_INLINE AO_t
+AO_fetch_and_add_full (volatile AO_t *p, AO_t incr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ ("lock; xaddl %0, %1" :
+                        "=r" (result), "+m" (*p) : "0" (incr)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE unsigned char
+AO_char_fetch_and_add_full (volatile unsigned char *p, unsigned char incr)
+{
+  unsigned char result;
+
+  __asm__ __volatile__ ("lock; xaddb %0, %1" :
+                        "=q" (result), "+m" (*p) : "0" (incr)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_char_fetch_and_add_full
+
+AO_INLINE unsigned short
+AO_short_fetch_and_add_full (volatile unsigned short *p, unsigned short incr)
+{
+  unsigned short result;
+
+  __asm__ __volatile__ ("lock; xaddw %0, %1" :
+                        "=r" (result), "+m" (*p) : "0" (incr)
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_short_fetch_and_add_full
+
+/* Really only works for 486 and later */
+AO_INLINE void
+AO_or_full (volatile AO_t *p, AO_t incr)
+{
+  __asm__ __volatile__ ("lock; orl %1, %0" :
+                        "+m" (*p) : "r" (incr)
+                        : "memory");
+}
+#define AO_HAVE_or_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full (volatile AO_TS_t *addr)
+{
+  AO_TS_t oldval;
+  /* Note: the "xchg" instruction does not need a "lock" prefix */
+  __asm__ __volatile__ ("xchg %b0, %1"
+                        : "=q" (oldval), "+m" (*addr)
+                        : "0" (0xff)
+                        : "memory");
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full (volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  char result;
+  __asm__ __volatile__ ("lock; cmpxchgl %2, %0; setz %1"
+                        : "+m" (*addr), "=a" (result)
+                        : "r" (new_val), "a"(old) : "memory");
+  return (int) result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+#if 0
+/* FIXME: not tested (and probably wrong). Besides,     */
+/* it tickles a bug in Sun C 5.10 (when optimizing).    */
+/* Returns nonzero if the comparison succeeded. */
+/* Really requires at least a Pentium.          */
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+  char result;
+#if __PIC__
+  /* If PIC is turned on, we can't use %ebx as it is reserved for the
+     GOT pointer.  We can save and restore %ebx because GCC won't be
+     using it for anything else (such as any of the m operands) */
+  __asm__ __volatile__("pushl %%ebx;"   /* save ebx used for PIC GOT ptr */
+                       "movl %6,%%ebx;" /* move new_val1 to %ebx */
+                       "lock; cmpxchg8b %0; setz %1;"
+                       "pop %%ebx;"     /* restore %ebx */
+                       : "=m"(*addr), "=a"(result)
+                       : "m"(*addr), "d" (old_val2), "a" (old_val1),
+                         "c" (new_val2), "m" (new_val1) : "memory");
+#else
+  /* We can't just do the same thing in non-PIC mode, because GCC
+   * might be using %ebx as the memory operand.  We could have ifdef'd
+   * in a clobber, but there's no point doing the push/pop if we don't
+   * have to. */
+  __asm__ __volatile__("lock; cmpxchg8b %0; setz %1;"
+                       : "=m"(*addr), "=a"(result)
+                       : /* "m"(*addr), */ "d" (old_val2), "a" (old_val1),
+                         "c" (new_val2), "b" (new_val1) : "memory");
+#endif
+  return (int) result;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+#endif
+
+#include "../ao_t_is_int.h"
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86_64.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86_64.h
new file mode 100644
index 0000000..5c66f42
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/sunc/x86_64.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2003 by Hewlett-Packard Company. All rights reserved.
+ *
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Some of the machine specific code was borrowed from our GC distribution.
+ */
+
+#include "../all_aligned_atomic_load_store.h"
+
+/* Real X86 implementations, appear                                     */
+/* to enforce ordering between memory operations, EXCEPT that a later   */
+/* read can pass earlier writes, presumably due to the visible          */
+/* presence of store buffers.                                           */
+/* We ignore the fact that the official specs                           */
+/* seem to be much weaker (and arguably too weak to be usable).         */
+
+#include "../ordered_except_wr.h"
+
+#include "../test_and_set_t_is_char.h"
+
+#include "../standard_ao_double_t.h"
+
+AO_INLINE void
+AO_nop_full(void)
+{
+  /* Note: "mfence" (SSE2) is supported on all x86_64/amd64 chips.      */
+  __asm__ __volatile__ ("mfence" : : : "memory");
+}
+#define AO_HAVE_nop_full
+
+/* As far as we can tell, the lfence and sfence instructions are not    */
+/* currently needed or useful for cached memory accesses.               */
+
+AO_INLINE AO_t
+AO_fetch_and_add_full (volatile AO_t *p, AO_t incr)
+{
+  AO_t result;
+
+  __asm__ __volatile__ ("lock; xaddq %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr) /* , "m" (*p) */
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_fetch_and_add_full
+
+AO_INLINE unsigned char
+AO_char_fetch_and_add_full (volatile unsigned char *p, unsigned char incr)
+{
+  unsigned char result;
+
+  __asm__ __volatile__ ("lock; xaddb %0, %1" :
+                        "=q" (result), "=m" (*p) : "0" (incr) /* , "m" (*p) */
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_char_fetch_and_add_full
+
+AO_INLINE unsigned short
+AO_short_fetch_and_add_full (volatile unsigned short *p, unsigned short incr)
+{
+  unsigned short result;
+
+  __asm__ __volatile__ ("lock; xaddw %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr) /* , "m" (*p) */
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_short_fetch_and_add_full
+
+AO_INLINE unsigned int
+AO_int_fetch_and_add_full (volatile unsigned int *p, unsigned int incr)
+{
+  unsigned int result;
+
+  __asm__ __volatile__ ("lock; xaddl %0, %1" :
+                        "=r" (result), "=m" (*p) : "0" (incr) /* , "m" (*p) */
+                        : "memory");
+  return result;
+}
+#define AO_HAVE_int_fetch_and_add_full
+
+AO_INLINE void
+AO_or_full (volatile AO_t *p, AO_t incr)
+{
+  __asm__ __volatile__ ("lock; orq %1, %0" :
+                        "=m" (*p) : "r" (incr) /* , "m" (*p) */
+                        : "memory");
+}
+#define AO_HAVE_or_full
+
+AO_INLINE AO_TS_VAL_t
+AO_test_and_set_full (volatile AO_TS_t *addr)
+{
+  AO_TS_t oldval;
+  /* Note: the "xchg" instruction does not need a "lock" prefix */
+  __asm__ __volatile__ ("xchg %b0, %1"
+                        : "=q"(oldval), "=m"(*addr)
+                        : "0"(0xff) /* , "m"(*addr) */
+                        : "memory");
+  return (AO_TS_VAL_t)oldval;
+}
+#define AO_HAVE_test_and_set_full
+
+/* Returns nonzero if the comparison succeeded. */
+AO_INLINE int
+AO_compare_and_swap_full (volatile AO_t *addr, AO_t old, AO_t new_val)
+{
+  char result;
+  __asm__ __volatile__ ("lock; cmpxchgq %2, %0; setz %1"
+                        : "=m"(*addr), "=a"(result)
+                        : "r" (new_val), "a"(old) : "memory");
+  return (int) result;
+}
+#define AO_HAVE_compare_and_swap_full
+
+#ifdef AO_CMPXCHG16B_AVAILABLE
+/* NEC LE-IT: older AMD Opterons are missing this instruction.
+ * On these machines SIGILL will be thrown.
+ * Define AO_WEAK_DOUBLE_CAS_EMULATION to have an emulated
+ * (lock based) version available */
+/* HB: Changed this to not define either by default.  There are
+ * enough machines and tool chains around on which cmpxchg16b
+ * doesn't work.  And the emulation is unsafe by our usual rules.
+ * However both are clearly useful in certain cases.
+ */
+AO_INLINE int
+AO_compare_double_and_swap_double_full (volatile AO_double_t *addr,
+                                        AO_t old_val1, AO_t old_val2,
+                                        AO_t new_val1, AO_t new_val2)
+{
+  char result;
+  __asm__ __volatile__ ("lock; cmpxchg16b %0; setz %1"
+                        : "=m"(*addr), "=a"(result)
+                        : /* "m" (*addr), */ "d" (old_val2), "a" (old_val1),
+                          "c" (new_val2), "b" (new_val1) : "memory");
+  return (int) result;
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+#else
+/* this one provides spinlock based emulation of CAS implemented in     */
+/* atomic_ops.c.  We probably do not want to do this here, since it is  */
+/* not atomic with respect to other kinds of updates of *addr.  On the  */
+/* other hand, this may be a useful facility on occasion.               */
+#ifdef AO_WEAK_DOUBLE_CAS_EMULATION
+int AO_compare_double_and_swap_double_emulation(volatile AO_double_t *addr,
+                                                AO_t old_val1, AO_t old_val2,
+                                                AO_t new_val1, AO_t new_val2);
+
+AO_INLINE int
+AO_compare_double_and_swap_double_full(volatile AO_double_t *addr,
+                                       AO_t old_val1, AO_t old_val2,
+                                       AO_t new_val1, AO_t new_val2)
+{
+        return AO_compare_double_and_swap_double_emulation(addr,
+                                                           old_val1, old_val2,
+                                                           new_val1, new_val2);
+}
+#define AO_HAVE_compare_double_and_swap_double_full
+#endif /* AO_WEAK_DOUBLE_CAS_EMULATION */
+#endif /* AO_CMPXCHG16B_AVAILABLE */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_ao_t.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_ao_t.h
new file mode 100644
index 0000000..606f7ac
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_ao_t.h
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * These are common definitions for architectures on which test_and_set
+ * operates on pointer-sized quantities, the "clear" value contains
+ * all zeroes, and the "set" value contains only one lowest bit set.
+ * This can be used if test_and_set is synthesized from compare_and_swap.
+ */
+typedef enum {AO_TS_clear = 0, AO_TS_set = 1} AO_TS_val;
+#define AO_TS_VAL_t AO_TS_val
+#define AO_TS_CLEAR AO_TS_clear
+#define AO_TS_SET AO_TS_set
+
+#define AO_TS_t AO_t
+
+#define AO_AO_TS_T 1
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_char.h b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_char.h
new file mode 100644
index 0000000..8e265aa
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops/sysdeps/test_and_set_t_is_char.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (c) 2004 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * These are common definitions for architectures on which test_and_set
+ * operates on byte sized quantities, the "clear" value contains
+ * all zeroes, and the "set" value contains all ones.
+ */
+
+#define AO_TS_t unsigned char
+typedef enum {AO_BYTE_TS_clear = 0, AO_BYTE_TS_set = 0xff} AO_BYTE_TS_val;
+#define AO_TS_VAL_t AO_BYTE_TS_val
+#define AO_TS_CLEAR AO_BYTE_TS_clear
+#define AO_TS_SET AO_BYTE_TS_set
+
+#define AO_CHAR_TS_T 1
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.c b/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.c
new file mode 100644
index 0000000..c2401f3
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.c
@@ -0,0 +1,321 @@
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ * Original Author: Hans Boehm
+ *
+ * This file may be redistributed and/or modified under the
+ * terms of the GNU General Public License as published by the Free Software
+ * Foundation; either version 2, or (at your option) any later version.
+ *
+ * It is distributed in the hope that it will be useful, but WITHOUT ANY
+ * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ * FOR A PARTICULAR PURPOSE.  See the GNU General Public License in the
+ * file doc/COPYING for more details.
+ */
+
+#if defined(HAVE_CONFIG_H)
+# include "config.h"
+#endif
+
+#define AO_REQUIRE_CAS
+#include "atomic_ops_stack.h"
+#include <string.h>     /* for ffs, which is assumed reentrant. */
+#include <stdlib.h>
+#ifdef AO_TRACE_MALLOC
+# include <stdio.h>
+# include <pthread.h>
+#endif
+
+/*
+ * We round up each allocation request to the next power of two
+ * minus one word.
+ * We keep one stack of free objects for each size.  Each object
+ * has an initial word (offset -sizeof(AO_t) from the visible pointer)
+ * which contains either
+ *      The binary log of the object size in bytes (small objects)
+ *      The object size (a multiple of CHUNK_SIZE) for large objects.
+ * The second case only arises if mmap-based allocation is supported.
+ * We align the user-visible part of each object on a GRANULARITY
+ * byte boundary.  That means that the actual (hidden) start of
+ * the object starts a word before this boundary.
+ */
+
+#ifndef LOG_MAX_SIZE
+# define LOG_MAX_SIZE 16
+        /* We assume that 2**LOG_MAX_SIZE is a multiple of page size. */
+#endif
+
+#ifndef ALIGNMENT
+# define ALIGNMENT 16
+        /* Assumed to be at least sizeof(AO_t).         */
+#endif
+
+#define CHUNK_SIZE (1 << LOG_MAX_SIZE)
+
+#ifndef AO_INITIAL_HEAP_SIZE
+#  define AO_INITIAL_HEAP_SIZE (2*(LOG_MAX_SIZE+1)*CHUNK_SIZE)
+#endif
+
+char AO_initial_heap[AO_INITIAL_HEAP_SIZE];
+
+static volatile AO_t initial_heap_ptr = (AO_t)AO_initial_heap;
+static volatile char *initial_heap_lim = AO_initial_heap + AO_INITIAL_HEAP_SIZE;
+
+#if defined(HAVE_MMAP)
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <sys/mman.h>
+
+#if defined(MAP_ANONYMOUS) || defined(MAP_ANON)
+# define USE_MMAP_ANON
+#endif
+
+#ifdef USE_MMAP_FIXED
+# define GC_MMAP_FLAGS MAP_FIXED | MAP_PRIVATE
+        /* Seems to yield better performance on Solaris 2, but can      */
+        /* be unreliable if something is already mapped at the address. */
+#else
+# define GC_MMAP_FLAGS MAP_PRIVATE
+#endif
+
+#ifdef USE_MMAP_ANON
+# ifdef MAP_ANONYMOUS
+#   define OPT_MAP_ANON MAP_ANONYMOUS
+# else
+#   define OPT_MAP_ANON MAP_ANON
+# endif
+#else
+# define OPT_MAP_ANON 0
+#endif
+
+static volatile AO_t mmap_enabled = 0;
+
+void
+AO_malloc_enable_mmap(void)
+{
+# if defined(__sun)
+    AO_store_release(&mmap_enabled, 1);
+            /* Workaround for Sun CC */
+# else
+    AO_store(&mmap_enabled, 1);
+# endif
+}
+
+static char *get_mmaped(size_t sz)
+{
+  char * result;
+# ifdef USE_MMAP_ANON
+#   define zero_fd -1
+# else
+    int zero_fd;
+# endif
+
+  assert(!(sz & (CHUNK_SIZE - 1)));
+  if (!mmap_enabled)
+    return 0;
+
+# ifndef USE_MMAP_ANON
+    zero_fd = open("/dev/zero", O_RDONLY);
+    if (zero_fd == -1)
+      return 0;
+# endif
+  result = mmap(0, sz, PROT_READ | PROT_WRITE,
+                GC_MMAP_FLAGS | OPT_MAP_ANON, zero_fd, 0/* offset */);
+# ifndef USE_MMAP_ANON
+    close(zero_fd);
+# endif
+  if (result == MAP_FAILED)
+    result = 0;
+  return result;
+}
+
+/* Allocate an object of size (incl. header) of size > CHUNK_SIZE.      */
+/* sz includes space for an AO_t-sized header.                          */
+static char *
+AO_malloc_large(size_t sz)
+{
+ char * result;
+ /* The header will force us to waste ALIGNMENT bytes, incl. header.    */
+   sz += ALIGNMENT;
+ /* Round to multiple of CHUNK_SIZE.    */
+   sz = (sz + CHUNK_SIZE - 1) & ~(CHUNK_SIZE - 1);
+ result = get_mmaped(sz);
+ if (result == 0) return 0;
+ result += ALIGNMENT;
+ ((AO_t *)result)[-1] = (AO_t)sz;
+ return result;
+}
+
+static void
+AO_free_large(char * p)
+{
+  AO_t sz = ((AO_t *)p)[-1];
+  if (munmap(p - ALIGNMENT, (size_t)sz) != 0)
+    abort();  /* Programmer error.  Not really async-signal-safe, but ... */
+}
+
+
+#else /*  No MMAP */
+
+void
+AO_malloc_enable_mmap(void)
+{
+}
+
+static char *get_mmaped(size_t sz)
+{
+  return 0;
+}
+
+static char *
+AO_malloc_large(size_t sz)
+{
+  return 0;
+}
+
+static void
+AO_free_large(char * p)
+{
+  abort();  /* Programmer error.  Not really async-signal-safe, but ... */
+}
+
+#endif /* No MMAP */
+
+static char *
+get_chunk(void)
+{
+  char *initial_ptr;
+  char *my_chunk_ptr;
+  char * my_lim;
+
+retry:
+  initial_ptr = (char *)AO_load(&initial_heap_ptr);
+  my_chunk_ptr = (char *)(((AO_t)initial_ptr + (ALIGNMENT - 1))
+                          & ~(ALIGNMENT - 1));
+  if (initial_ptr != my_chunk_ptr)
+    {
+      /* Align correctly.  If this fails, someone else did it for us.   */
+      AO_compare_and_swap_acquire(&initial_heap_ptr, (AO_t)initial_ptr,
+                                  (AO_t)my_chunk_ptr);
+    }
+  my_lim = my_chunk_ptr + CHUNK_SIZE;
+  if (my_lim <= initial_heap_lim)
+    {
+      if (!AO_compare_and_swap(&initial_heap_ptr, (AO_t)my_chunk_ptr,
+                                                  (AO_t)my_lim))
+        goto retry;
+      return my_chunk_ptr;
+    }
+  /* We failed.  The initial heap is used up.   */
+  my_chunk_ptr = get_mmaped(CHUNK_SIZE);
+  assert (!((AO_t)my_chunk_ptr & (ALIGNMENT-1)));
+  return my_chunk_ptr;
+}
+
+/* Object free lists.  Ith entry corresponds to objects */
+/* of total size 2**i bytes.                                    */
+AO_stack_t AO_free_list[LOG_MAX_SIZE+1];
+
+/* Chunk free list, linked through first word in chunks.        */
+/* All entries of size CHUNK_SIZE.                              */
+AO_stack_t AO_chunk_free_list;
+
+/* Break up the chunk, and add it to the object free list for   */
+/* the given size.  Sz must be a power of two.                  */
+/* We have exclusive access to chunk.                           */
+static void
+add_chunk_as(void * chunk, size_t sz, unsigned log_sz)
+{
+  char *first = (char *)chunk + ALIGNMENT - sizeof(AO_t);
+  char *limit = (char *)chunk + CHUNK_SIZE - sz;
+  char *next, *p;
+
+  for (p = first; p <= limit; p = next) {
+    next = p + sz;
+    AO_stack_push(AO_free_list+log_sz, (AO_t *)p);
+  }
+}
+
+static int msbs[16] = {0, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4};
+
+/* Return the position of the most significant set bit in the   */
+/* argument.                                                    */
+/* We follow the conventions of ffs(), i.e. the least           */
+/* significant bit is number one.                               */
+int msb(size_t s)
+{
+  int result = 0;
+  int v;
+  if ((s & 0xff) != s) {
+    /* The following shift often generates warnings on 32-bit arch's    */
+    /* That's OK, because it will never be executed there.              */
+    /* Doing the shift only in a conditional expression suppresses the  */
+    /* warning with the modern compilers.                               */
+    if (sizeof(size_t) > 4 && (v = s >> 32) != 0)
+      {
+        s = v;
+        result += 32;
+      }
+    if ((s >> 16) != 0)
+      {
+        s >>= 16;
+        result += 16;
+      }
+    if ((s >> 8) != 0)
+      {
+        s >>= 8;
+        result += 8;
+      }
+  }
+  if (s > 15)
+    {
+      s >>= 4;
+      result += 4;
+    }
+  result += msbs[s];
+  return result;
+}
+
+void *
+AO_malloc(size_t sz)
+{
+  AO_t *result;
+  size_t adj_sz = sz + sizeof(AO_t);
+  int log_sz;
+  if (sz > CHUNK_SIZE)
+    return AO_malloc_large(sz);
+  log_sz = msb(adj_sz-1);
+  result = AO_stack_pop(AO_free_list+log_sz);
+  while (0 == result) {
+    void * chunk = get_chunk();
+    if (0 == chunk) return 0;
+    adj_sz = 1 << log_sz;
+    add_chunk_as(chunk, adj_sz, log_sz);
+    result = AO_stack_pop(AO_free_list+log_sz);
+  }
+  *result = log_sz;
+# ifdef AO_TRACE_MALLOC
+    fprintf(stderr, "%x: AO_malloc(%lu) = %p\n",
+                    (int)pthread_self(), (unsigned long)sz, result+1);
+# endif
+  return result + 1;
+}
+
+void
+AO_free(void *p)
+{
+  char *base = (char *)p - sizeof(AO_t);
+  int log_sz;
+
+  if (0 == p) return;
+  log_sz = *(AO_t *)base;
+# ifdef AO_TRACE_MALLOC
+    fprintf(stderr, "%x: AO_free(%p sz:%lu)\n", (int)pthread_self(), p,
+            (unsigned long)(log_sz > LOG_MAX_SIZE? log_sz : (1 << log_sz)));
+# endif
+  if (log_sz > LOG_MAX_SIZE)
+    AO_free_large(p);
+  else
+    AO_stack_push(AO_free_list+log_sz, (AO_t *)base);
+}
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.h b/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.h
new file mode 100644
index 0000000..41987ed
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops_malloc.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Almost lock-free malloc implementation based on stack implementation. */
+/* See README.malloc file for detailed usage rules.                      */
+
+#ifndef AO_ATOMIC_H
+#define AO_ATOMIC_H
+
+#include <stdlib.h>     /* For size_t */
+
+#include "atomic_ops_stack.h"
+
+#ifdef AO_STACK_IS_LOCK_FREE
+# define AO_MALLOC_IS_LOCK_FREE
+#endif
+
+void AO_free(void *);
+
+void * AO_malloc(size_t);
+
+/* Allow use of mmap to grow the heap.  No-op on some platforms.        */
+void AO_malloc_enable_mmap(void);
+
+#endif /* !AO_ATOMIC_H */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.c b/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.c
new file mode 100644
index 0000000..1a430df
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.c
@@ -0,0 +1,310 @@
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ * Original Author: Hans Boehm
+ *
+ * This file may be redistributed and/or modified under the
+ * terms of the GNU General Public License as published by the Free Software
+ * Foundation; either version 2, or (at your option) any later version.
+ *
+ * It is distributed in the hope that it will be useful, but WITHOUT ANY
+ * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ * FOR A PARTICULAR PURPOSE.  See the GNU General Public License in the
+ * file doc/COPYING for more details.
+ */
+
+#if defined(HAVE_CONFIG_H)
+# include "config.h"
+#endif
+
+#include <string.h>
+#include <stdlib.h>
+#include <assert.h>
+#define AO_REQUIRE_CAS
+#include "atomic_ops_stack.h"
+
+#if defined(_MSC_VER) \
+    || defined(_WIN32) && !defined(__CYGWIN32__) && !defined(__CYGWIN__)
+  /* AO_pause not defined elsewhere */
+  /* FIXME: At least AO_spin should be factored out.    */
+#include <windows.h>
+
+static AO_t dummy = 1;
+
+/* Spin for 2**n units. */
+static void AO_spin(int n)
+{
+  int i;
+  AO_T j = AO_load(&dummy);
+
+  for (i = 0; i < (2 << n); ++i)
+    {
+       j *= 5;
+       j -= 4;
+    }
+  AO_store(&dummy, j);
+}
+
+void AO_pause(int n)
+{
+    if (n < 12)
+      AO_spin(n);
+    else
+      {
+        DWORD msecs;
+
+        /* Short async-signal-safe sleep. */
+        msecs = n > 28 ? 100 : n < 22 ? 1 : 1 << (n - 22); /* in millis */
+        Sleep(msecs);
+      }
+}
+
+#else
+
+/* AO_pause is available elsewhere */
+
+extern void AO_pause(int);
+
+#endif
+
+#ifdef AO_USE_ALMOST_LOCK_FREE
+
+/* LIFO linked lists based on compare-and-swap.  We need to avoid       */
+/* the case of a node deletion and reinsertion while I'm deleting       */
+/* it, since that may cause my CAS to succeed eventhough the next       */
+/* pointer is now wrong.  Our solution is not fully lock-free, but it   */
+/* is good enough for signal handlers, provided we have a suitably low  */
+/* bound on the number of recursive signal handler reentries.           */
+/* A list consists of a first pointer and a blacklist                   */
+/* of pointer values that are currently being removed.  No list element */
+/* on the blacklist may be inserted.  If we would otherwise do so, we   */
+/* are allowed to insert a variant that differs only in the least       */
+/* significant, ignored, bits.  If the list is full, we wait.           */
+
+/* Crucial observation: A particular padded pointer x (i.e. pointer     */
+/* plus arbitrary low order bits) can never be newly inserted into      */
+/* a list while it's in the corresponding auxiliary data structure.     */
+
+/* The second argument is a pointer to the link field of the element    */
+/* to be inserted.                                                      */
+/* Both list headers and link fields contain "perturbed" pointers, i.e. */
+/* pointers with extra bits "or"ed into the low order bits.             */
+void
+AO_stack_push_explicit_aux_release(volatile AO_t *list, AO_t *x,
+                                   AO_stack_aux *a)
+{
+  AO_t x_bits = (AO_t)x;
+  AO_t next;
+
+  /* No deletions of x can start here, since x is not currently in the  */
+  /* list.                                                              */
+ retry:
+# if AO_BL_SIZE == 2
+  {
+    /* Start all loads as close to concurrently as possible. */
+    AO_t entry1 = AO_load(a -> AO_stack_bl);
+    AO_t entry2 = AO_load(a -> AO_stack_bl + 1);
+    if (entry1 == x_bits || entry2 == x_bits)
+      {
+        /* Entry is currently being removed.  Change it a little.       */
+          ++x_bits;
+          if ((x_bits & AO_BIT_MASK) == 0)
+            /* Version count overflowed;         */
+            /* EXTREMELY unlikely, but possible. */
+            x_bits = (AO_t)x;
+        goto retry;
+      }
+  }
+# else
+  {
+    int i;
+    for (i = 0; i < AO_BL_SIZE; ++i)
+      {
+        if (AO_load(a -> AO_stack_bl + i) == x_bits)
+          {
+            /* Entry is currently being removed.  Change it a little.   */
+              ++x_bits;
+              if ((x_bits & AO_BIT_MASK) == 0)
+                /* Version count overflowed;         */
+                /* EXTREMELY unlikely, but possible. */
+                x_bits = (AO_t)x;
+            goto retry;
+          }
+      }
+  }
+# endif
+  /* x_bits is not currently being deleted */
+  do
+    {
+      next = AO_load(list);
+      *x = next;
+    }
+  while(!AO_compare_and_swap_release(list, next, x_bits));
+}
+
+/*
+ * I concluded experimentally that checking a value first before
+ * performing a compare-and-swap is usually beneficial on X86, but
+ * slows things down appreciably with contention on Itanium.
+ * Since the Itanium behavior makes more sense to me (more cache line
+ * movement unless we're mostly reading, but back-off should guard
+ * against that), we take Itanium as the default.  Measurements on
+ * other multiprocessor architectures would be useful.  (On a uniprocessor,
+ * the initial check is almost certainly a very small loss.) - HB
+ */
+#ifdef __i386__
+# define PRECHECK(a) (a) == 0 &&
+#else
+# define PRECHECK(a)
+#endif
+
+AO_t *
+AO_stack_pop_explicit_aux_acquire(volatile AO_t *list, AO_stack_aux * a)
+{
+  unsigned i;
+  int j = 0;
+  AO_t first;
+  AO_t * first_ptr;
+  AO_t next;
+
+ retry:
+  first = AO_load(list);
+  if (0 == first) return 0;
+  /* Insert first into aux black list.                                  */
+  /* This may spin if more than AO_BL_SIZE removals using auxiliary     */
+  /* structure a are currently in progress.                             */
+  for (i = 0; ; )
+    {
+      if (PRECHECK(a -> AO_stack_bl[i])
+          AO_compare_and_swap_acquire(a->AO_stack_bl+i, 0, first))
+        break;
+      ++i;
+      if ( i >= AO_BL_SIZE )
+        {
+          i = 0;
+          AO_pause(++j);
+        }
+    }
+  assert(i >= 0 && i < AO_BL_SIZE);
+  assert(a -> AO_stack_bl[i] == first);
+  /* First is on the auxiliary black list.  It may be removed by        */
+  /* another thread before we get to it, but a new insertion of x       */
+  /* cannot be started here.                                            */
+  /* Only we can remove it from the black list.                         */
+  /* We need to make sure that first is still the first entry on the    */
+  /* list.  Otherwise it's possible that a reinsertion of it was        */
+  /* already started before we added the black list entry.              */
+  if (first != AO_load(list)) {
+    AO_store_release(a->AO_stack_bl+i, 0);
+    goto retry;
+  }
+  first_ptr = AO_REAL_NEXT_PTR(first);
+  next = AO_load(first_ptr);
+  if (!AO_compare_and_swap_release(list, first, next)) {
+    AO_store_release(a->AO_stack_bl+i, 0);
+    goto retry;
+  }
+  assert(*list != first);
+  /* Since we never insert an entry on the black list, this cannot have */
+  /* succeeded unless first remained on the list while we were running. */
+  /* Thus its next link cannot have changed out from under us, and we   */
+  /* removed exactly one entry and preserved the rest of the list.      */
+  /* Note that it is quite possible that an additional entry was        */
+  /* inserted and removed while we were running; this is OK since the   */
+  /* part of the list following first must have remained unchanged, and */
+  /* first must again have been at the head of the list when the        */
+  /* compare_and_swap succeeded.                                        */
+  AO_store_release(a->AO_stack_bl+i, 0);
+  return first_ptr;
+}
+
+#else /* ! USE_ALMOST_LOCK_FREE */
+
+/* Better names for fields in AO_stack_t */
+#define ptr AO_val2
+#define version AO_val1
+
+#if defined(AO_HAVE_compare_double_and_swap_double)
+
+void AO_stack_push_release(AO_stack_t *list, AO_t *element)
+{
+    AO_t next;
+
+    do {
+      next = AO_load(&(list -> ptr));
+      *element = next;
+    } while (!AO_compare_and_swap_release
+                    ( &(list -> ptr), next, (AO_t) element));
+    /* This uses a narrow CAS here, an old optimization suggested       */
+    /* by Treiber.  Pop is still safe, since we run into the ABA        */
+    /* problem only if there were both intervening "pop"s and "push"es. */
+    /* In that case we still see a change in the version number.        */
+}
+
+AO_t *AO_stack_pop_acquire(AO_stack_t *list)
+{
+#   ifdef __clang__
+      AO_t *volatile cptr;
+                        /* Use volatile to workaround a bug in          */
+                        /* clang-1.1/x86 causing test_stack failure.    */
+#   else
+      AO_t *cptr;
+#   endif
+    AO_t next;
+    AO_t cversion;
+
+    do {
+      /* Version must be loaded first.  */
+      cversion = AO_load_acquire(&(list -> version));
+      cptr = (AO_t *)AO_load(&(list -> ptr));
+      if (cptr == 0) return 0;
+      next = *cptr;
+    } while (!AO_compare_double_and_swap_double_release
+                    (list, cversion, (AO_t) cptr, cversion+1, (AO_t) next));
+    return cptr;
+}
+
+
+#elif defined(AO_HAVE_compare_and_swap_double)
+
+/* Needed for future IA64 processors.  No current clients? */
+
+#error Untested!  Probably doesnt work.
+
+/* We have a wide CAS, but only does an AO_t-wide comparison.   */
+/* We can't use the Treiber optimization, since we only check   */
+/* for an unchanged version number, not an unchanged pointer.   */
+void AO_stack_push_release(AO_stack_t *list, AO_t *element)
+{
+    AO_t version;
+    AO_t next_ptr;
+
+    do {
+      /* Again version must be loaded first, for different reason.      */
+      version = AO_load_acquire(&(list -> version));
+      next_ptr = AO_load(&(list -> ptr));
+      *element = next_ptr;
+    } while (!AO_compare_and_swap_double_release(
+                           list, version,
+                           version+1, (AO_t) element));
+}
+
+AO_t *AO_stack_pop_acquire(AO_stack_t *list)
+{
+    AO_t *cptr;
+    AO_t next;
+    AO_t cversion;
+
+    do {
+      cversion = AO_load_acquire(&(list -> version));
+      cptr = (AO_t *)AO_load(&(list -> ptr));
+      if (cptr == 0) return 0;
+      next = *cptr;
+    } while (!AO_compare_double_and_swap_double_release
+                    (list, cversion, (AO_t) cptr, cversion+1, next));
+    return cptr;
+}
+
+
+#endif /* AO_HAVE_compare_and_swap_double */
+
+#endif /* ! USE_ALMOST_LOCK_FREE */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.h b/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.h
new file mode 100644
index 0000000..6c8b5bb
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops_stack.h
@@ -0,0 +1,188 @@
+/*
+ * The implementation of the routines described here is covered by the GPL.
+ * This header file is covered by the following license:
+ */
+
+/*
+ * Copyright (c) 2005 Hewlett-Packard Development Company, L.P.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* Almost lock-free LIFO linked lists (linked stacks).  */
+#ifndef AO_STACK_H
+#define AO_STACK_H
+
+#include "atomic_ops.h"
+
+#if !defined(AO_HAVE_compare_double_and_swap_double) \
+    && !defined(AO_HAVE_compare_double_and_swap) \
+    && defined(AO_HAVE_compare_and_swap)
+# define AO_USE_ALMOST_LOCK_FREE
+#else
+  /* If we have no compare-and-swap operation defined, we assume        */
+  /* that we will actually be using CAS emulation.  If we do that,      */
+  /* it's cheaper to use the version-based implementation.              */
+# define AO_STACK_IS_LOCK_FREE
+#endif
+
+/*
+ * These are not guaranteed to be completely lock-free.
+ * List insertion may spin under extremely unlikely conditions.
+ * It cannot deadlock due to recursive reentry unless AO_list_remove
+ * is called while at least AO_BL_SIZE activations of
+ * AO_list_remove are currently active in the same thread, i.e.
+ * we must have at least AO_BL_SIZE recursive signal handler
+ * invocations.
+ *
+ * All operations take an AO_list_aux argument.  It is safe to
+ * share a single AO_list_aux structure among all lists, but that
+ * may increase contention.  Any given list must always be accessed
+ * with the same AO_list_aux structure.
+ *
+ * We make some machine-dependent assumptions:
+ *   - We have a compare-and-swap operation.
+ *   - At least _AO_N_BITS low order bits in pointers are
+ *     zero and normally unused.
+ *   - size_t and pointers have the same size.
+ *
+ * We do use a fully lock-free implementation if double-width
+ * compare-and-swap operations are available.
+ */
+
+#ifdef AO_USE_ALMOST_LOCK_FREE
+/* The number of low order pointer bits we can use for a small  */
+/* version number.                                              */
+# if defined(__LP64__) || defined(_LP64) || defined(_WIN64)
+   /* WIN64 isn't really supported yet. */
+#  define AO_N_BITS 3
+# else
+#  define AO_N_BITS 2
+# endif
+
+# define AO_BIT_MASK ((1 << AO_N_BITS) - 1)
+/*
+ * AO_stack_aux should be treated as opaque.
+ * It is fully defined here, so it can be allocated, and to facilitate
+ * debugging.
+ */
+#ifndef AO_BL_SIZE
+#  define AO_BL_SIZE 2
+#endif
+
+#if AO_BL_SIZE > (1 << AO_N_BITS)
+#  error AO_BL_SIZE too big
+#endif
+
+typedef struct AO__stack_aux {
+  volatile AO_t AO_stack_bl[AO_BL_SIZE];
+} AO_stack_aux;
+
+/* The stack implementation knows only about the location of    */
+/* link fields in nodes, and nothing about the rest of the      */
+/* stack elements.  Link fields hold an AO_t, which is not      */
+/* necessarily a real pointer.  This converts the AO_t to a     */
+/* real (AO_t *) which is either o, or points at the link       */
+/* field in the next node.                                      */
+#define AO_REAL_NEXT_PTR(x) (AO_t *)((x) & ~AO_BIT_MASK)
+
+/* The following two routines should not normally be used directly.     */
+/* We make them visible here for the rare cases in which it makes sense */
+/* to share the an AO_stack_aux between stacks.                         */
+void
+AO_stack_push_explicit_aux_release(volatile AO_t *list, AO_t *x,
+                                  AO_stack_aux *);
+
+AO_t *
+AO_stack_pop_explicit_aux_acquire(volatile AO_t *list, AO_stack_aux *);
+
+/* And now AO_stack_t for the real interface:                           */
+
+typedef struct AO__stack {
+  volatile AO_t AO_ptr;
+  AO_stack_aux AO_aux;
+} AO_stack_t;
+
+#define AO_STACK_INITIALIZER {0}
+
+AO_INLINE void AO_stack_init(AO_stack_t *list)
+{
+# if AO_BL_SIZE == 2
+    list -> AO_aux.AO_stack_bl[0] = 0;
+    list -> AO_aux.AO_stack_bl[1] = 0;
+# else
+    int i;
+    for (i = 0; i < AO_BL_SIZE; ++i)
+      list -> AO_aux.AO_stack_bl[i] = 0;
+# endif
+  list -> AO_ptr = 0;
+}
+
+/* Convert an AO_stack_t to a pointer to the link field in      */
+/* the first element.                                           */
+#define AO_REAL_HEAD_PTR(x) AO_REAL_NEXT_PTR((x).AO_ptr)
+
+#define AO_stack_push_release(l, e) \
+        AO_stack_push_explicit_aux_release(&((l)->AO_ptr), e, &((l)->AO_aux))
+#define AO_HAVE_stack_push_release
+
+#define AO_stack_pop_acquire(l) \
+        AO_stack_pop_explicit_aux_acquire(&((l)->AO_ptr), &((l)->AO_aux))
+#define AO_HAVE_stack_pop_acquire
+
+# else /* Use fully non-blocking data structure, wide CAS       */
+
+#ifndef AO_HAVE_double_t
+  /* Can happen if we're using CAS emulation, since we don't want to    */
+  /* force that here, in case other atomic_ops clients don't want it.   */
+# include "atomic_ops/sysdeps/standard_ao_double_t.h"
+#endif
+
+typedef volatile AO_double_t AO_stack_t;
+/* AO_val1 is version, AO_val2 is pointer.      */
+
+#define AO_STACK_INITIALIZER {0}
+
+AO_INLINE void AO_stack_init(AO_stack_t *list)
+{
+  list -> AO_val1 = 0;
+  list -> AO_val2 = 0;
+}
+
+#define AO_REAL_HEAD_PTR(x) (AO_t *)((x).AO_val2)
+#define AO_REAL_NEXT_PTR(x) (AO_t *)(x)
+
+void AO_stack_push_release(AO_stack_t *list, AO_t *new_element);
+#define AO_HAVE_stack_push_release
+AO_t * AO_stack_pop_acquire(AO_stack_t *list);
+#define AO_HAVE_stack_pop_acquire
+
+#endif /* Wide CAS case */
+
+#if defined(AO_HAVE_stack_push_release) && !defined(AO_HAVE_stack_push)
+# define AO_stack_push(l, e) AO_stack_push_release(l, e)
+# define AO_HAVE_stack_push
+#endif
+
+#if defined(AO_HAVE_stack_pop_acquire) && !defined(AO_HAVE_stack_pop)
+# define AO_stack_pop(l) AO_stack_pop_acquire(l)
+# define AO_HAVE_stack_pop
+#endif
+
+#endif /* !AO_STACK_H */
diff --git a/src/gc/bdwgc/libatomic_ops/atomic_ops_sysdeps.S b/src/gc/bdwgc/libatomic_ops/atomic_ops_sysdeps.S
new file mode 100644
index 0000000..f586f23
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/atomic_ops_sysdeps.S
@@ -0,0 +1,9 @@
+/*
+ * Include the appropriate system-dependent assembly file, if any.
+ * This is used only if the platform supports neither inline assembly
+ * code, nor appropriate compiler intrinsics.
+ */
+
+#if !defined(__GNUC__) && (defined(sparc) || defined(__sparc))
+#  include "atomic_ops/sysdeps/sunc/sparc.S"
+#endif
diff --git a/src/gc/bdwgc/libatomic_ops/config.h.in b/src/gc/bdwgc/libatomic_ops/config.h.in
new file mode 100644
index 0000000..ac02b70
--- /dev/null
+++ b/src/gc/bdwgc/libatomic_ops/config.h.in
@@ -0,0 +1,70 @@
+/* src/config.h.in.  Generated from configure.ac by autoheader.  */
+
+/* Define to 1 if you have the `getpagesize' function. */
+#undef HAVE_GETPAGESIZE
+
+/* Define to 1 if you have the <inttypes.h> header file. */
+#undef HAVE_INTTYPES_H
+
+/* Define to 1 if you have the <memory.h> header file. */
+#undef HAVE_MEMORY_H
+
+/* Define to 1 if you have a working `mmap' system call. */
+#undef HAVE_MMAP
+
+/* Define to 1 if you have the <stdint.h> header file. */
+#undef HAVE_STDINT_H
+
+/* Define to 1 if you have the <stdlib.h> header file. */
+#undef HAVE_STDLIB_H
+
+/* Define to 1 if you have the <strings.h> header file. */
+#undef HAVE_STRINGS_H
+
+/* Define to 1 if you have the <string.h> header file. */
+#undef HAVE_STRING_H
+
+/* Define to 1 if you have the <sys/param.h> header file. */
+#undef HAVE_SYS_PARAM_H
+
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#undef HAVE_SYS_STAT_H
+
+/* Define to 1 if you have the <sys/types.h> header file. */
+#undef HAVE_SYS_TYPES_H
+
+/* Define to 1 if you have the <unistd.h> header file. */
+#undef HAVE_UNISTD_H
+
+/* Name of package */
+#undef PACKAGE
+
+/* Define to the address where bug reports for this package should be sent. */
+#undef PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#undef PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#undef PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#undef PACKAGE_TARNAME
+
+/* Define to the home page for this package. */
+#undef PACKAGE_URL
+
+/* Define to the version of this package. */
+#undef PACKAGE_VERSION
+
+/* Define to 1 if you have the ANSI C header files. */
+#undef STDC_HEADERS
+
+/* Version number of package */
+#undef VERSION
+
+/* Indicates the use of pthreads (NetBSD). */
+#undef _PTHREADS
+
+/* Required define if using POSIX threads. */
+#undef _REENTRANT
diff --git a/src/gc/bdwgc/ltmain.sh b/src/gc/bdwgc/ltmain.sh
new file mode 100644
index 0000000..a356aca
--- /dev/null
+++ b/src/gc/bdwgc/ltmain.sh
@@ -0,0 +1,9661 @@
+
+# libtool (GNU libtool) 2.4.2
+# Written by Gordon Matzigkeit <gord@gnu.ai.mit.edu>, 1996
+
+# Copyright (C) 1996, 1997, 1998, 1999, 2000, 2001, 2003, 2004, 2005, 2006,
+# 2007, 2008, 2009, 2010, 2011 Free Software Foundation, Inc.
+# This is free software; see the source for copying conditions.  There is NO
+# warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+
+# GNU Libtool is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 2 of the License, or
+# (at your option) any later version.
+#
+# As a special exception to the GNU General Public License,
+# if you distribute this file as part of a program or library that
+# is built using GNU Libtool, you may include this file under the
+# same distribution terms that you use for the rest of that program.
+#
+# GNU Libtool is distributed in the hope that it will be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+# General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GNU Libtool; see the file COPYING.  If not, a copy
+# can be downloaded from http://www.gnu.org/licenses/gpl.html,
+# or obtained by writing to the Free Software Foundation, Inc.,
+# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+
+# Usage: $progname [OPTION]... [MODE-ARG]...
+#
+# Provide generalized library-building support services.
+#
+#       --config             show all configuration variables
+#       --debug              enable verbose shell tracing
+#   -n, --dry-run            display commands without modifying any files
+#       --features           display basic configuration information and exit
+#       --mode=MODE          use operation mode MODE
+#       --preserve-dup-deps  don't remove duplicate dependency libraries
+#       --quiet, --silent    don't print informational messages
+#       --no-quiet, --no-silent
+#                            print informational messages (default)
+#       --no-warn            don't display warning messages
+#       --tag=TAG            use configuration variables from tag TAG
+#   -v, --verbose            print more informational messages than default
+#       --no-verbose         don't print the extra informational messages
+#       --version            print version information
+#   -h, --help, --help-all   print short, long, or detailed help message
+#
+# MODE must be one of the following:
+#
+#         clean              remove files from the build directory
+#         compile            compile a source file into a libtool object
+#         execute            automatically set library path, then run a program
+#         finish             complete the installation of libtool libraries
+#         install            install libraries or executables
+#         link               create a library or an executable
+#         uninstall          remove libraries from an installed directory
+#
+# MODE-ARGS vary depending on the MODE.  When passed as first option,
+# `--mode=MODE' may be abbreviated as `MODE' or a unique abbreviation of that.
+# Try `$progname --help --mode=MODE' for a more detailed description of MODE.
+#
+# When reporting a bug, please describe a test case to reproduce it and
+# include the following information:
+#
+#         host-triplet:	$host
+#         shell:		$SHELL
+#         compiler:		$LTCC
+#         compiler flags:		$LTCFLAGS
+#         linker:		$LD (gnu? $with_gnu_ld)
+#         $progname:	(GNU libtool) 2.4.2 Debian-2.4.2-1.7ubuntu1
+#         automake:	$automake_version
+#         autoconf:	$autoconf_version
+#
+# Report bugs to <bug-libtool@gnu.org>.
+# GNU libtool home page: <http://www.gnu.org/software/libtool/>.
+# General help using GNU software: <http://www.gnu.org/gethelp/>.
+
+PROGRAM=libtool
+PACKAGE=libtool
+VERSION="2.4.2 Debian-2.4.2-1.7ubuntu1"
+TIMESTAMP=""
+package_revision=1.3337
+
+# Be Bourne compatible
+if test -n "${ZSH_VERSION+set}" && (emulate sh) >/dev/null 2>&1; then
+  emulate sh
+  NULLCMD=:
+  # Zsh 3.x and 4.x performs word splitting on ${1+"$@"}, which
+  # is contrary to our usage.  Disable this feature.
+  alias -g '${1+"$@"}'='"$@"'
+  setopt NO_GLOB_SUBST
+else
+  case `(set -o) 2>/dev/null` in *posix*) set -o posix;; esac
+fi
+BIN_SH=xpg4; export BIN_SH # for Tru64
+DUALCASE=1; export DUALCASE # for MKS sh
+
+# A function that is used when there is no print builtin or printf.
+func_fallback_echo ()
+{
+  eval 'cat <<_LTECHO_EOF
+$1
+_LTECHO_EOF'
+}
+
+# NLS nuisances: We save the old values to restore during execute mode.
+lt_user_locale=
+lt_safe_locale=
+for lt_var in LANG LANGUAGE LC_ALL LC_CTYPE LC_COLLATE LC_MESSAGES
+do
+  eval "if test \"\${$lt_var+set}\" = set; then
+          save_$lt_var=\$$lt_var
+          $lt_var=C
+	  export $lt_var
+	  lt_user_locale=\"$lt_var=\\\$save_\$lt_var; \$lt_user_locale\"
+	  lt_safe_locale=\"$lt_var=C; \$lt_safe_locale\"
+	fi"
+done
+LC_ALL=C
+LANGUAGE=C
+export LANGUAGE LC_ALL
+
+$lt_unset CDPATH
+
+
+# Work around backward compatibility issue on IRIX 6.5. On IRIX 6.4+, sh
+# is ksh but when the shell is invoked as "sh" and the current value of
+# the _XPG environment variable is not equal to 1 (one), the special
+# positional parameter $0, within a function call, is the name of the
+# function.
+progpath="$0"
+
+
+
+: ${CP="cp -f"}
+test "${ECHO+set}" = set || ECHO=${as_echo-'printf %s\n'}
+: ${MAKE="make"}
+: ${MKDIR="mkdir"}
+: ${MV="mv -f"}
+: ${RM="rm -f"}
+: ${SHELL="${CONFIG_SHELL-/bin/sh}"}
+: ${Xsed="$SED -e 1s/^X//"}
+
+# Global variables:
+EXIT_SUCCESS=0
+EXIT_FAILURE=1
+EXIT_MISMATCH=63  # $? = 63 is used to indicate version mismatch to missing.
+EXIT_SKIP=77	  # $? = 77 is used to indicate a skipped test to automake.
+
+exit_status=$EXIT_SUCCESS
+
+# Make sure IFS has a sensible default
+lt_nl='
+'
+IFS=" 	$lt_nl"
+
+dirname="s,/[^/]*$,,"
+basename="s,^.*/,,"
+
+# func_dirname file append nondir_replacement
+# Compute the dirname of FILE.  If nonempty, add APPEND to the result,
+# otherwise set result to NONDIR_REPLACEMENT.
+func_dirname ()
+{
+    func_dirname_result=`$ECHO "${1}" | $SED "$dirname"`
+    if test "X$func_dirname_result" = "X${1}"; then
+      func_dirname_result="${3}"
+    else
+      func_dirname_result="$func_dirname_result${2}"
+    fi
+} # func_dirname may be replaced by extended shell implementation
+
+
+# func_basename file
+func_basename ()
+{
+    func_basename_result=`$ECHO "${1}" | $SED "$basename"`
+} # func_basename may be replaced by extended shell implementation
+
+
+# func_dirname_and_basename file append nondir_replacement
+# perform func_basename and func_dirname in a single function
+# call:
+#   dirname:  Compute the dirname of FILE.  If nonempty,
+#             add APPEND to the result, otherwise set result
+#             to NONDIR_REPLACEMENT.
+#             value returned in "$func_dirname_result"
+#   basename: Compute filename of FILE.
+#             value retuned in "$func_basename_result"
+# Implementation must be kept synchronized with func_dirname
+# and func_basename. For efficiency, we do not delegate to
+# those functions but instead duplicate the functionality here.
+func_dirname_and_basename ()
+{
+    # Extract subdirectory from the argument.
+    func_dirname_result=`$ECHO "${1}" | $SED -e "$dirname"`
+    if test "X$func_dirname_result" = "X${1}"; then
+      func_dirname_result="${3}"
+    else
+      func_dirname_result="$func_dirname_result${2}"
+    fi
+    func_basename_result=`$ECHO "${1}" | $SED -e "$basename"`
+} # func_dirname_and_basename may be replaced by extended shell implementation
+
+
+# func_stripname prefix suffix name
+# strip PREFIX and SUFFIX off of NAME.
+# PREFIX and SUFFIX must not contain globbing or regex special
+# characters, hashes, percent signs, but SUFFIX may contain a leading
+# dot (in which case that matches only a dot).
+# func_strip_suffix prefix name
+func_stripname ()
+{
+    case ${2} in
+      .*) func_stripname_result=`$ECHO "${3}" | $SED "s%^${1}%%; s%\\\\${2}\$%%"`;;
+      *)  func_stripname_result=`$ECHO "${3}" | $SED "s%^${1}%%; s%${2}\$%%"`;;
+    esac
+} # func_stripname may be replaced by extended shell implementation
+
+
+# These SED scripts presuppose an absolute path with a trailing slash.
+pathcar='s,^/\([^/]*\).*$,\1,'
+pathcdr='s,^/[^/]*,,'
+removedotparts=':dotsl
+		s@/\./@/@g
+		t dotsl
+		s,/\.$,/,'
+collapseslashes='s@/\{1,\}@/@g'
+finalslash='s,/*$,/,'
+
+# func_normal_abspath PATH
+# Remove doubled-up and trailing slashes, "." path components,
+# and cancel out any ".." path components in PATH after making
+# it an absolute path.
+#             value returned in "$func_normal_abspath_result"
+func_normal_abspath ()
+{
+  # Start from root dir and reassemble the path.
+  func_normal_abspath_result=
+  func_normal_abspath_tpath=$1
+  func_normal_abspath_altnamespace=
+  case $func_normal_abspath_tpath in
+    "")
+      # Empty path, that just means $cwd.
+      func_stripname '' '/' "`pwd`"
+      func_normal_abspath_result=$func_stripname_result
+      return
+    ;;
+    # The next three entries are used to spot a run of precisely
+    # two leading slashes without using negated character classes;
+    # we take advantage of case's first-match behaviour.
+    ///*)
+      # Unusual form of absolute path, do nothing.
+    ;;
+    //*)
+      # Not necessarily an ordinary path; POSIX reserves leading '//'
+      # and for example Cygwin uses it to access remote file shares
+      # over CIFS/SMB, so we conserve a leading double slash if found.
+      func_normal_abspath_altnamespace=/
+    ;;
+    /*)
+      # Absolute path, do nothing.
+    ;;
+    *)
+      # Relative path, prepend $cwd.
+      func_normal_abspath_tpath=`pwd`/$func_normal_abspath_tpath
+    ;;
+  esac
+  # Cancel out all the simple stuff to save iterations.  We also want
+  # the path to end with a slash for ease of parsing, so make sure
+  # there is one (and only one) here.
+  func_normal_abspath_tpath=`$ECHO "$func_normal_abspath_tpath" | $SED \
+        -e "$removedotparts" -e "$collapseslashes" -e "$finalslash"`
+  while :; do
+    # Processed it all yet?
+    if test "$func_normal_abspath_tpath" = / ; then
+      # If we ascended to the root using ".." the result may be empty now.
+      if test -z "$func_normal_abspath_result" ; then
+        func_normal_abspath_result=/
+      fi
+      break
+    fi
+    func_normal_abspath_tcomponent=`$ECHO "$func_normal_abspath_tpath" | $SED \
+        -e "$pathcar"`
+    func_normal_abspath_tpath=`$ECHO "$func_normal_abspath_tpath" | $SED \
+        -e "$pathcdr"`
+    # Figure out what to do with it
+    case $func_normal_abspath_tcomponent in
+      "")
+        # Trailing empty path component, ignore it.
+      ;;
+      ..)
+        # Parent dir; strip last assembled component from result.
+        func_dirname "$func_normal_abspath_result"
+        func_normal_abspath_result=$func_dirname_result
+      ;;
+      *)
+        # Actual path component, append it.
+        func_normal_abspath_result=$func_normal_abspath_result/$func_normal_abspath_tcomponent
+      ;;
+    esac
+  done
+  # Restore leading double-slash if one was found on entry.
+  func_normal_abspath_result=$func_normal_abspath_altnamespace$func_normal_abspath_result
+}
+
+# func_relative_path SRCDIR DSTDIR
+# generates a relative path from SRCDIR to DSTDIR, with a trailing
+# slash if non-empty, suitable for immediately appending a filename
+# without needing to append a separator.
+#             value returned in "$func_relative_path_result"
+func_relative_path ()
+{
+  func_relative_path_result=
+  func_normal_abspath "$1"
+  func_relative_path_tlibdir=$func_normal_abspath_result
+  func_normal_abspath "$2"
+  func_relative_path_tbindir=$func_normal_abspath_result
+
+  # Ascend the tree starting from libdir
+  while :; do
+    # check if we have found a prefix of bindir
+    case $func_relative_path_tbindir in
+      $func_relative_path_tlibdir)
+        # found an exact match
+        func_relative_path_tcancelled=
+        break
+        ;;
+      $func_relative_path_tlibdir*)
+        # found a matching prefix
+        func_stripname "$func_relative_path_tlibdir" '' "$func_relative_path_tbindir"
+        func_relative_path_tcancelled=$func_stripname_result
+        if test -z "$func_relative_path_result"; then
+          func_relative_path_result=.
+        fi
+        break
+        ;;
+      *)
+        func_dirname $func_relative_path_tlibdir
+        func_relative_path_tlibdir=${func_dirname_result}
+        if test "x$func_relative_path_tlibdir" = x ; then
+          # Have to descend all the way to the root!
+          func_relative_path_result=../$func_relative_path_result
+          func_relative_path_tcancelled=$func_relative_path_tbindir
+          break
+        fi
+        func_relative_path_result=../$func_relative_path_result
+        ;;
+    esac
+  done
+
+  # Now calculate path; take care to avoid doubling-up slashes.
+  func_stripname '' '/' "$func_relative_path_result"
+  func_relative_path_result=$func_stripname_result
+  func_stripname '/' '/' "$func_relative_path_tcancelled"
+  if test "x$func_stripname_result" != x ; then
+    func_relative_path_result=${func_relative_path_result}/${func_stripname_result}
+  fi
+
+  # Normalisation. If bindir is libdir, return empty string,
+  # else relative path ending with a slash; either way, target
+  # file name can be directly appended.
+  if test ! -z "$func_relative_path_result"; then
+    func_stripname './' '' "$func_relative_path_result/"
+    func_relative_path_result=$func_stripname_result
+  fi
+}
+
+# The name of this program:
+func_dirname_and_basename "$progpath"
+progname=$func_basename_result
+
+# Make sure we have an absolute path for reexecution:
+case $progpath in
+  [\\/]*|[A-Za-z]:\\*) ;;
+  *[\\/]*)
+     progdir=$func_dirname_result
+     progdir=`cd "$progdir" && pwd`
+     progpath="$progdir/$progname"
+     ;;
+  *)
+     save_IFS="$IFS"
+     IFS=${PATH_SEPARATOR-:}
+     for progdir in $PATH; do
+       IFS="$save_IFS"
+       test -x "$progdir/$progname" && break
+     done
+     IFS="$save_IFS"
+     test -n "$progdir" || progdir=`pwd`
+     progpath="$progdir/$progname"
+     ;;
+esac
+
+# Sed substitution that helps us do robust quoting.  It backslashifies
+# metacharacters that are still active within double-quoted strings.
+Xsed="${SED}"' -e 1s/^X//'
+sed_quote_subst='s/\([`"$\\]\)/\\\1/g'
+
+# Same as above, but do not quote variable references.
+double_quote_subst='s/\(["`\\]\)/\\\1/g'
+
+# Sed substitution that turns a string into a regex matching for the
+# string literally.
+sed_make_literal_regex='s,[].[^$\\*\/],\\&,g'
+
+# Sed substitution that converts a w32 file name or path
+# which contains forward slashes, into one that contains
+# (escaped) backslashes.  A very naive implementation.
+lt_sed_naive_backslashify='s|\\\\*|\\|g;s|/|\\|g;s|\\|\\\\|g'
+
+# Re-`\' parameter expansions in output of double_quote_subst that were
+# `\'-ed in input to the same.  If an odd number of `\' preceded a '$'
+# in input to double_quote_subst, that '$' was protected from expansion.
+# Since each input `\' is now two `\'s, look for any number of runs of
+# four `\'s followed by two `\'s and then a '$'.  `\' that '$'.
+bs='\\'
+bs2='\\\\'
+bs4='\\\\\\\\'
+dollar='\$'
+sed_double_backslash="\
+  s/$bs4/&\\
+/g
+  s/^$bs2$dollar/$bs&/
+  s/\\([^$bs]\\)$bs2$dollar/\\1$bs2$bs$dollar/g
+  s/\n//g"
+
+# Standard options:
+opt_dry_run=false
+opt_help=false
+opt_quiet=false
+opt_verbose=false
+opt_warning=:
+
+# func_echo arg...
+# Echo program name prefixed message, along with the current mode
+# name if it has been set yet.
+func_echo ()
+{
+    $ECHO "$progname: ${opt_mode+$opt_mode: }$*"
+}
+
+# func_verbose arg...
+# Echo program name prefixed message in verbose mode only.
+func_verbose ()
+{
+    $opt_verbose && func_echo ${1+"$@"}
+
+    # A bug in bash halts the script if the last line of a function
+    # fails when set -e is in force, so we need another command to
+    # work around that:
+    :
+}
+
+# func_echo_all arg...
+# Invoke $ECHO with all args, space-separated.
+func_echo_all ()
+{
+    $ECHO "$*"
+}
+
+# func_error arg...
+# Echo program name prefixed message to standard error.
+func_error ()
+{
+    $ECHO "$progname: ${opt_mode+$opt_mode: }"${1+"$@"} 1>&2
+}
+
+# func_warning arg...
+# Echo program name prefixed warning message to standard error.
+func_warning ()
+{
+    $opt_warning && $ECHO "$progname: ${opt_mode+$opt_mode: }warning: "${1+"$@"} 1>&2
+
+    # bash bug again:
+    :
+}
+
+# func_fatal_error arg...
+# Echo program name prefixed message to standard error, and exit.
+func_fatal_error ()
+{
+    func_error ${1+"$@"}
+    exit $EXIT_FAILURE
+}
+
+# func_fatal_help arg...
+# Echo program name prefixed message to standard error, followed by
+# a help hint, and exit.
+func_fatal_help ()
+{
+    func_error ${1+"$@"}
+    func_fatal_error "$help"
+}
+help="Try \`$progname --help' for more information."  ## default
+
+
+# func_grep expression filename
+# Check whether EXPRESSION matches any line of FILENAME, without output.
+func_grep ()
+{
+    $GREP "$1" "$2" >/dev/null 2>&1
+}
+
+
+# func_mkdir_p directory-path
+# Make sure the entire path to DIRECTORY-PATH is available.
+func_mkdir_p ()
+{
+    my_directory_path="$1"
+    my_dir_list=
+
+    if test -n "$my_directory_path" && test "$opt_dry_run" != ":"; then
+
+      # Protect directory names starting with `-'
+      case $my_directory_path in
+        -*) my_directory_path="./$my_directory_path" ;;
+      esac
+
+      # While some portion of DIR does not yet exist...
+      while test ! -d "$my_directory_path"; do
+        # ...make a list in topmost first order.  Use a colon delimited
+	# list incase some portion of path contains whitespace.
+        my_dir_list="$my_directory_path:$my_dir_list"
+
+        # If the last portion added has no slash in it, the list is done
+        case $my_directory_path in */*) ;; *) break ;; esac
+
+        # ...otherwise throw away the child directory and loop
+        my_directory_path=`$ECHO "$my_directory_path" | $SED -e "$dirname"`
+      done
+      my_dir_list=`$ECHO "$my_dir_list" | $SED 's,:*$,,'`
+
+      save_mkdir_p_IFS="$IFS"; IFS=':'
+      for my_dir in $my_dir_list; do
+	IFS="$save_mkdir_p_IFS"
+        # mkdir can fail with a `File exist' error if two processes
+        # try to create one of the directories concurrently.  Don't
+        # stop in that case!
+        $MKDIR "$my_dir" 2>/dev/null || :
+      done
+      IFS="$save_mkdir_p_IFS"
+
+      # Bail out if we (or some other process) failed to create a directory.
+      test -d "$my_directory_path" || \
+        func_fatal_error "Failed to create \`$1'"
+    fi
+}
+
+
+# func_mktempdir [string]
+# Make a temporary directory that won't clash with other running
+# libtool processes, and avoids race conditions if possible.  If
+# given, STRING is the basename for that directory.
+func_mktempdir ()
+{
+    my_template="${TMPDIR-/tmp}/${1-$progname}"
+
+    if test "$opt_dry_run" = ":"; then
+      # Return a directory name, but don't create it in dry-run mode
+      my_tmpdir="${my_template}-$$"
+    else
+
+      # If mktemp works, use that first and foremost
+      my_tmpdir=`mktemp -d "${my_template}-XXXXXXXX" 2>/dev/null`
+
+      if test ! -d "$my_tmpdir"; then
+        # Failing that, at least try and use $RANDOM to avoid a race
+        my_tmpdir="${my_template}-${RANDOM-0}$$"
+
+        save_mktempdir_umask=`umask`
+        umask 0077
+        $MKDIR "$my_tmpdir"
+        umask $save_mktempdir_umask
+      fi
+
+      # If we're not in dry-run mode, bomb out on failure
+      test -d "$my_tmpdir" || \
+        func_fatal_error "cannot create temporary directory \`$my_tmpdir'"
+    fi
+
+    $ECHO "$my_tmpdir"
+}
+
+
+# func_quote_for_eval arg
+# Aesthetically quote ARG to be evaled later.
+# This function returns two values: FUNC_QUOTE_FOR_EVAL_RESULT
+# is double-quoted, suitable for a subsequent eval, whereas
+# FUNC_QUOTE_FOR_EVAL_UNQUOTED_RESULT has merely all characters
+# which are still active within double quotes backslashified.
+func_quote_for_eval ()
+{
+    case $1 in
+      *[\\\`\"\$]*)
+	func_quote_for_eval_unquoted_result=`$ECHO "$1" | $SED "$sed_quote_subst"` ;;
+      *)
+        func_quote_for_eval_unquoted_result="$1" ;;
+    esac
+
+    case $func_quote_for_eval_unquoted_result in
+      # Double-quote args containing shell metacharacters to delay
+      # word splitting, command substitution and and variable
+      # expansion for a subsequent eval.
+      # Many Bourne shells cannot handle close brackets correctly
+      # in scan sets, so we specify it separately.
+      *[\[\~\#\^\&\*\(\)\{\}\|\;\<\>\?\'\ \	]*|*]*|"")
+        func_quote_for_eval_result="\"$func_quote_for_eval_unquoted_result\""
+        ;;
+      *)
+        func_quote_for_eval_result="$func_quote_for_eval_unquoted_result"
+    esac
+}
+
+
+# func_quote_for_expand arg
+# Aesthetically quote ARG to be evaled later; same as above,
+# but do not quote variable references.
+func_quote_for_expand ()
+{
+    case $1 in
+      *[\\\`\"]*)
+	my_arg=`$ECHO "$1" | $SED \
+	    -e "$double_quote_subst" -e "$sed_double_backslash"` ;;
+      *)
+        my_arg="$1" ;;
+    esac
+
+    case $my_arg in
+      # Double-quote args containing shell metacharacters to delay
+      # word splitting and command substitution for a subsequent eval.
+      # Many Bourne shells cannot handle close brackets correctly
+      # in scan sets, so we specify it separately.
+      *[\[\~\#\^\&\*\(\)\{\}\|\;\<\>\?\'\ \	]*|*]*|"")
+        my_arg="\"$my_arg\""
+        ;;
+    esac
+
+    func_quote_for_expand_result="$my_arg"
+}
+
+
+# func_show_eval cmd [fail_exp]
+# Unless opt_silent is true, then output CMD.  Then, if opt_dryrun is
+# not true, evaluate CMD.  If the evaluation of CMD fails, and FAIL_EXP
+# is given, then evaluate it.
+func_show_eval ()
+{
+    my_cmd="$1"
+    my_fail_exp="${2-:}"
+
+    ${opt_silent-false} || {
+      func_quote_for_expand "$my_cmd"
+      eval "func_echo $func_quote_for_expand_result"
+    }
+
+    if ${opt_dry_run-false}; then :; else
+      eval "$my_cmd"
+      my_status=$?
+      if test "$my_status" -eq 0; then :; else
+	eval "(exit $my_status); $my_fail_exp"
+      fi
+    fi
+}
+
+
+# func_show_eval_locale cmd [fail_exp]
+# Unless opt_silent is true, then output CMD.  Then, if opt_dryrun is
+# not true, evaluate CMD.  If the evaluation of CMD fails, and FAIL_EXP
+# is given, then evaluate it.  Use the saved locale for evaluation.
+func_show_eval_locale ()
+{
+    my_cmd="$1"
+    my_fail_exp="${2-:}"
+
+    ${opt_silent-false} || {
+      func_quote_for_expand "$my_cmd"
+      eval "func_echo $func_quote_for_expand_result"
+    }
+
+    if ${opt_dry_run-false}; then :; else
+      eval "$lt_user_locale
+	    $my_cmd"
+      my_status=$?
+      eval "$lt_safe_locale"
+      if test "$my_status" -eq 0; then :; else
+	eval "(exit $my_status); $my_fail_exp"
+      fi
+    fi
+}
+
+# func_tr_sh
+# Turn $1 into a string suitable for a shell variable name.
+# Result is stored in $func_tr_sh_result.  All characters
+# not in the set a-zA-Z0-9_ are replaced with '_'. Further,
+# if $1 begins with a digit, a '_' is prepended as well.
+func_tr_sh ()
+{
+  case $1 in
+  [0-9]* | *[!a-zA-Z0-9_]*)
+    func_tr_sh_result=`$ECHO "$1" | $SED 's/^\([0-9]\)/_\1/; s/[^a-zA-Z0-9_]/_/g'`
+    ;;
+  * )
+    func_tr_sh_result=$1
+    ;;
+  esac
+}
+
+
+# func_version
+# Echo version message to standard output and exit.
+func_version ()
+{
+    $opt_debug
+
+    $SED -n '/(C)/!b go
+	:more
+	/\./!{
+	  N
+	  s/\n# / /
+	  b more
+	}
+	:go
+	/^# '$PROGRAM' (GNU /,/# warranty; / {
+        s/^# //
+	s/^# *$//
+        s/\((C)\)[ 0-9,-]*\( [1-9][0-9]*\)/\1\2/
+        p
+     }' < "$progpath"
+     exit $?
+}
+
+# func_usage
+# Echo short help message to standard output and exit.
+func_usage ()
+{
+    $opt_debug
+
+    $SED -n '/^# Usage:/,/^#  *.*--help/ {
+        s/^# //
+	s/^# *$//
+	s/\$progname/'$progname'/
+	p
+    }' < "$progpath"
+    echo
+    $ECHO "run \`$progname --help | more' for full usage"
+    exit $?
+}
+
+# func_help [NOEXIT]
+# Echo long help message to standard output and exit,
+# unless 'noexit' is passed as argument.
+func_help ()
+{
+    $opt_debug
+
+    $SED -n '/^# Usage:/,/# Report bugs to/ {
+	:print
+        s/^# //
+	s/^# *$//
+	s*\$progname*'$progname'*
+	s*\$host*'"$host"'*
+	s*\$SHELL*'"$SHELL"'*
+	s*\$LTCC*'"$LTCC"'*
+	s*\$LTCFLAGS*'"$LTCFLAGS"'*
+	s*\$LD*'"$LD"'*
+	s/\$with_gnu_ld/'"$with_gnu_ld"'/
+	s/\$automake_version/'"`(${AUTOMAKE-automake} --version) 2>/dev/null |$SED 1q`"'/
+	s/\$autoconf_version/'"`(${AUTOCONF-autoconf} --version) 2>/dev/null |$SED 1q`"'/
+	p
+	d
+     }
+     /^# .* home page:/b print
+     /^# General help using/b print
+     ' < "$progpath"
+    ret=$?
+    if test -z "$1"; then
+      exit $ret
+    fi
+}
+
+# func_missing_arg argname
+# Echo program name prefixed message to standard error and set global
+# exit_cmd.
+func_missing_arg ()
+{
+    $opt_debug
+
+    func_error "missing argument for $1."
+    exit_cmd=exit
+}
+
+
+# func_split_short_opt shortopt
+# Set func_split_short_opt_name and func_split_short_opt_arg shell
+# variables after splitting SHORTOPT after the 2nd character.
+func_split_short_opt ()
+{
+    my_sed_short_opt='1s/^\(..\).*$/\1/;q'
+    my_sed_short_rest='1s/^..\(.*\)$/\1/;q'
+
+    func_split_short_opt_name=`$ECHO "$1" | $SED "$my_sed_short_opt"`
+    func_split_short_opt_arg=`$ECHO "$1" | $SED "$my_sed_short_rest"`
+} # func_split_short_opt may be replaced by extended shell implementation
+
+
+# func_split_long_opt longopt
+# Set func_split_long_opt_name and func_split_long_opt_arg shell
+# variables after splitting LONGOPT at the `=' sign.
+func_split_long_opt ()
+{
+    my_sed_long_opt='1s/^\(--[^=]*\)=.*/\1/;q'
+    my_sed_long_arg='1s/^--[^=]*=//'
+
+    func_split_long_opt_name=`$ECHO "$1" | $SED "$my_sed_long_opt"`
+    func_split_long_opt_arg=`$ECHO "$1" | $SED "$my_sed_long_arg"`
+} # func_split_long_opt may be replaced by extended shell implementation
+
+exit_cmd=:
+
+
+
+
+
+magic="%%%MAGIC variable%%%"
+magic_exe="%%%MAGIC EXE variable%%%"
+
+# Global variables.
+nonopt=
+preserve_args=
+lo2o="s/\\.lo\$/.${objext}/"
+o2lo="s/\\.${objext}\$/.lo/"
+extracted_archives=
+extracted_serial=0
+
+# If this variable is set in any of the actions, the command in it
+# will be execed at the end.  This prevents here-documents from being
+# left over by shells.
+exec_cmd=
+
+# func_append var value
+# Append VALUE to the end of shell variable VAR.
+func_append ()
+{
+    eval "${1}=\$${1}\${2}"
+} # func_append may be replaced by extended shell implementation
+
+# func_append_quoted var value
+# Quote VALUE and append to the end of shell variable VAR, separated
+# by a space.
+func_append_quoted ()
+{
+    func_quote_for_eval "${2}"
+    eval "${1}=\$${1}\\ \$func_quote_for_eval_result"
+} # func_append_quoted may be replaced by extended shell implementation
+
+
+# func_arith arithmetic-term...
+func_arith ()
+{
+    func_arith_result=`expr "${@}"`
+} # func_arith may be replaced by extended shell implementation
+
+
+# func_len string
+# STRING may not start with a hyphen.
+func_len ()
+{
+    func_len_result=`expr "${1}" : ".*" 2>/dev/null || echo $max_cmd_len`
+} # func_len may be replaced by extended shell implementation
+
+
+# func_lo2o object
+func_lo2o ()
+{
+    func_lo2o_result=`$ECHO "${1}" | $SED "$lo2o"`
+} # func_lo2o may be replaced by extended shell implementation
+
+
+# func_xform libobj-or-source
+func_xform ()
+{
+    func_xform_result=`$ECHO "${1}" | $SED 's/\.[^.]*$/.lo/'`
+} # func_xform may be replaced by extended shell implementation
+
+
+# func_fatal_configuration arg...
+# Echo program name prefixed message to standard error, followed by
+# a configuration failure hint, and exit.
+func_fatal_configuration ()
+{
+    func_error ${1+"$@"}
+    func_error "See the $PACKAGE documentation for more information."
+    func_fatal_error "Fatal configuration error."
+}
+
+
+# func_config
+# Display the configuration for all the tags in this script.
+func_config ()
+{
+    re_begincf='^# ### BEGIN LIBTOOL'
+    re_endcf='^# ### END LIBTOOL'
+
+    # Default configuration.
+    $SED "1,/$re_begincf CONFIG/d;/$re_endcf CONFIG/,\$d" < "$progpath"
+
+    # Now print the configurations for the tags.
+    for tagname in $taglist; do
+      $SED -n "/$re_begincf TAG CONFIG: $tagname\$/,/$re_endcf TAG CONFIG: $tagname\$/p" < "$progpath"
+    done
+
+    exit $?
+}
+
+# func_features
+# Display the features supported by this script.
+func_features ()
+{
+    echo "host: $host"
+    if test "$build_libtool_libs" = yes; then
+      echo "enable shared libraries"
+    else
+      echo "disable shared libraries"
+    fi
+    if test "$build_old_libs" = yes; then
+      echo "enable static libraries"
+    else
+      echo "disable static libraries"
+    fi
+
+    exit $?
+}
+
+# func_enable_tag tagname
+# Verify that TAGNAME is valid, and either flag an error and exit, or
+# enable the TAGNAME tag.  We also add TAGNAME to the global $taglist
+# variable here.
+func_enable_tag ()
+{
+  # Global variable:
+  tagname="$1"
+
+  re_begincf="^# ### BEGIN LIBTOOL TAG CONFIG: $tagname\$"
+  re_endcf="^# ### END LIBTOOL TAG CONFIG: $tagname\$"
+  sed_extractcf="/$re_begincf/,/$re_endcf/p"
+
+  # Validate tagname.
+  case $tagname in
+    *[!-_A-Za-z0-9,/]*)
+      func_fatal_error "invalid tag name: $tagname"
+      ;;
+  esac
+
+  # Don't test for the "default" C tag, as we know it's
+  # there but not specially marked.
+  case $tagname in
+    CC) ;;
+    *)
+      if $GREP "$re_begincf" "$progpath" >/dev/null 2>&1; then
+	taglist="$taglist $tagname"
+
+	# Evaluate the configuration.  Be careful to quote the path
+	# and the sed script, to avoid splitting on whitespace, but
+	# also don't use non-portable quotes within backquotes within
+	# quotes we have to do it in 2 steps:
+	extractedcf=`$SED -n -e "$sed_extractcf" < "$progpath"`
+	eval "$extractedcf"
+      else
+	func_error "ignoring unknown tag $tagname"
+      fi
+      ;;
+  esac
+}
+
+# func_check_version_match
+# Ensure that we are using m4 macros, and libtool script from the same
+# release of libtool.
+func_check_version_match ()
+{
+  if test "$package_revision" != "$macro_revision"; then
+    if test "$VERSION" != "$macro_version"; then
+      if test -z "$macro_version"; then
+        cat >&2 <<_LT_EOF
+$progname: Version mismatch error.  This is $PACKAGE $VERSION, but the
+$progname: definition of this LT_INIT comes from an older release.
+$progname: You should recreate aclocal.m4 with macros from $PACKAGE $VERSION
+$progname: and run autoconf again.
+_LT_EOF
+      else
+        cat >&2 <<_LT_EOF
+$progname: Version mismatch error.  This is $PACKAGE $VERSION, but the
+$progname: definition of this LT_INIT comes from $PACKAGE $macro_version.
+$progname: You should recreate aclocal.m4 with macros from $PACKAGE $VERSION
+$progname: and run autoconf again.
+_LT_EOF
+      fi
+    else
+      cat >&2 <<_LT_EOF
+$progname: Version mismatch error.  This is $PACKAGE $VERSION, revision $package_revision,
+$progname: but the definition of this LT_INIT comes from revision $macro_revision.
+$progname: You should recreate aclocal.m4 with macros from revision $package_revision
+$progname: of $PACKAGE $VERSION and run autoconf again.
+_LT_EOF
+    fi
+
+    exit $EXIT_MISMATCH
+  fi
+}
+
+
+# Shorthand for --mode=foo, only valid as the first argument
+case $1 in
+clean|clea|cle|cl)
+  shift; set dummy --mode clean ${1+"$@"}; shift
+  ;;
+compile|compil|compi|comp|com|co|c)
+  shift; set dummy --mode compile ${1+"$@"}; shift
+  ;;
+execute|execut|execu|exec|exe|ex|e)
+  shift; set dummy --mode execute ${1+"$@"}; shift
+  ;;
+finish|finis|fini|fin|fi|f)
+  shift; set dummy --mode finish ${1+"$@"}; shift
+  ;;
+install|instal|insta|inst|ins|in|i)
+  shift; set dummy --mode install ${1+"$@"}; shift
+  ;;
+link|lin|li|l)
+  shift; set dummy --mode link ${1+"$@"}; shift
+  ;;
+uninstall|uninstal|uninsta|uninst|unins|unin|uni|un|u)
+  shift; set dummy --mode uninstall ${1+"$@"}; shift
+  ;;
+esac
+
+
+
+# Option defaults:
+opt_debug=:
+opt_dry_run=false
+opt_config=false
+opt_preserve_dup_deps=false
+opt_features=false
+opt_finish=false
+opt_help=false
+opt_help_all=false
+opt_silent=:
+opt_warning=:
+opt_verbose=:
+opt_silent=false
+opt_verbose=false
+
+
+# Parse options once, thoroughly.  This comes as soon as possible in the
+# script to make things like `--version' happen as quickly as we can.
+{
+  # this just eases exit handling
+  while test $# -gt 0; do
+    opt="$1"
+    shift
+    case $opt in
+      --debug|-x)	opt_debug='set -x'
+			func_echo "enabling shell trace mode"
+			$opt_debug
+			;;
+      --dry-run|--dryrun|-n)
+			opt_dry_run=:
+			;;
+      --config)
+			opt_config=:
+func_config
+			;;
+      --dlopen|-dlopen)
+			optarg="$1"
+			opt_dlopen="${opt_dlopen+$opt_dlopen
+}$optarg"
+			shift
+			;;
+      --preserve-dup-deps)
+			opt_preserve_dup_deps=:
+			;;
+      --features)
+			opt_features=:
+func_features
+			;;
+      --finish)
+			opt_finish=:
+set dummy --mode finish ${1+"$@"}; shift
+			;;
+      --help)
+			opt_help=:
+			;;
+      --help-all)
+			opt_help_all=:
+opt_help=': help-all'
+			;;
+      --mode)
+			test $# = 0 && func_missing_arg $opt && break
+			optarg="$1"
+			opt_mode="$optarg"
+case $optarg in
+  # Valid mode arguments:
+  clean|compile|execute|finish|install|link|relink|uninstall) ;;
+
+  # Catch anything else as an error
+  *) func_error "invalid argument for $opt"
+     exit_cmd=exit
+     break
+     ;;
+esac
+			shift
+			;;
+      --no-silent|--no-quiet)
+			opt_silent=false
+func_append preserve_args " $opt"
+			;;
+      --no-warning|--no-warn)
+			opt_warning=false
+func_append preserve_args " $opt"
+			;;
+      --no-verbose)
+			opt_verbose=false
+func_append preserve_args " $opt"
+			;;
+      --silent|--quiet)
+			opt_silent=:
+func_append preserve_args " $opt"
+        opt_verbose=false
+			;;
+      --verbose|-v)
+			opt_verbose=:
+func_append preserve_args " $opt"
+opt_silent=false
+			;;
+      --tag)
+			test $# = 0 && func_missing_arg $opt && break
+			optarg="$1"
+			opt_tag="$optarg"
+func_append preserve_args " $opt $optarg"
+func_enable_tag "$optarg"
+			shift
+			;;
+
+      -\?|-h)		func_usage				;;
+      --help)		func_help				;;
+      --version)	func_version				;;
+
+      # Separate optargs to long options:
+      --*=*)
+			func_split_long_opt "$opt"
+			set dummy "$func_split_long_opt_name" "$func_split_long_opt_arg" ${1+"$@"}
+			shift
+			;;
+
+      # Separate non-argument short options:
+      -\?*|-h*|-n*|-v*)
+			func_split_short_opt "$opt"
+			set dummy "$func_split_short_opt_name" "-$func_split_short_opt_arg" ${1+"$@"}
+			shift
+			;;
+
+      --)		break					;;
+      -*)		func_fatal_help "unrecognized option \`$opt'" ;;
+      *)		set dummy "$opt" ${1+"$@"};	shift; break  ;;
+    esac
+  done
+
+  # Validate options:
+
+  # save first non-option argument
+  if test "$#" -gt 0; then
+    nonopt="$opt"
+    shift
+  fi
+
+  # preserve --debug
+  test "$opt_debug" = : || func_append preserve_args " --debug"
+
+  case $host in
+    *cygwin* | *mingw* | *pw32* | *cegcc*)
+      # don't eliminate duplications in $postdeps and $predeps
+      opt_duplicate_compiler_generated_deps=:
+      ;;
+    *)
+      opt_duplicate_compiler_generated_deps=$opt_preserve_dup_deps
+      ;;
+  esac
+
+  $opt_help || {
+    # Sanity checks first:
+    func_check_version_match
+
+    if test "$build_libtool_libs" != yes && test "$build_old_libs" != yes; then
+      func_fatal_configuration "not configured to build any kind of library"
+    fi
+
+    # Darwin sucks
+    eval std_shrext=\"$shrext_cmds\"
+
+    # Only execute mode is allowed to have -dlopen flags.
+    if test -n "$opt_dlopen" && test "$opt_mode" != execute; then
+      func_error "unrecognized option \`-dlopen'"
+      $ECHO "$help" 1>&2
+      exit $EXIT_FAILURE
+    fi
+
+    # Change the help message to a mode-specific one.
+    generic_help="$help"
+    help="Try \`$progname --help --mode=$opt_mode' for more information."
+  }
+
+
+  # Bail if the options were screwed
+  $exit_cmd $EXIT_FAILURE
+}
+
+
+
+
+## ----------- ##
+##    Main.    ##
+## ----------- ##
+
+# func_lalib_p file
+# True iff FILE is a libtool `.la' library or `.lo' object file.
+# This function is only a basic sanity check; it will hardly flush out
+# determined imposters.
+func_lalib_p ()
+{
+    test -f "$1" &&
+      $SED -e 4q "$1" 2>/dev/null \
+        | $GREP "^# Generated by .*$PACKAGE" > /dev/null 2>&1
+}
+
+# func_lalib_unsafe_p file
+# True iff FILE is a libtool `.la' library or `.lo' object file.
+# This function implements the same check as func_lalib_p without
+# resorting to external programs.  To this end, it redirects stdin and
+# closes it afterwards, without saving the original file descriptor.
+# As a safety measure, use it only where a negative result would be
+# fatal anyway.  Works if `file' does not exist.
+func_lalib_unsafe_p ()
+{
+    lalib_p=no
+    if test -f "$1" && test -r "$1" && exec 5<&0 <"$1"; then
+	for lalib_p_l in 1 2 3 4
+	do
+	    read lalib_p_line
+	    case "$lalib_p_line" in
+		\#\ Generated\ by\ *$PACKAGE* ) lalib_p=yes; break;;
+	    esac
+	done
+	exec 0<&5 5<&-
+    fi
+    test "$lalib_p" = yes
+}
+
+# func_ltwrapper_script_p file
+# True iff FILE is a libtool wrapper script
+# This function is only a basic sanity check; it will hardly flush out
+# determined imposters.
+func_ltwrapper_script_p ()
+{
+    func_lalib_p "$1"
+}
+
+# func_ltwrapper_executable_p file
+# True iff FILE is a libtool wrapper executable
+# This function is only a basic sanity check; it will hardly flush out
+# determined imposters.
+func_ltwrapper_executable_p ()
+{
+    func_ltwrapper_exec_suffix=
+    case $1 in
+    *.exe) ;;
+    *) func_ltwrapper_exec_suffix=.exe ;;
+    esac
+    $GREP "$magic_exe" "$1$func_ltwrapper_exec_suffix" >/dev/null 2>&1
+}
+
+# func_ltwrapper_scriptname file
+# Assumes file is an ltwrapper_executable
+# uses $file to determine the appropriate filename for a
+# temporary ltwrapper_script.
+func_ltwrapper_scriptname ()
+{
+    func_dirname_and_basename "$1" "" "."
+    func_stripname '' '.exe' "$func_basename_result"
+    func_ltwrapper_scriptname_result="$func_dirname_result/$objdir/${func_stripname_result}_ltshwrapper"
+}
+
+# func_ltwrapper_p file
+# True iff FILE is a libtool wrapper script or wrapper executable
+# This function is only a basic sanity check; it will hardly flush out
+# determined imposters.
+func_ltwrapper_p ()
+{
+    func_ltwrapper_script_p "$1" || func_ltwrapper_executable_p "$1"
+}
+
+
+# func_execute_cmds commands fail_cmd
+# Execute tilde-delimited COMMANDS.
+# If FAIL_CMD is given, eval that upon failure.
+# FAIL_CMD may read-access the current command in variable CMD!
+func_execute_cmds ()
+{
+    $opt_debug
+    save_ifs=$IFS; IFS='~'
+    for cmd in $1; do
+      IFS=$save_ifs
+      eval cmd=\"$cmd\"
+      func_show_eval "$cmd" "${2-:}"
+    done
+    IFS=$save_ifs
+}
+
+
+# func_source file
+# Source FILE, adding directory component if necessary.
+# Note that it is not necessary on cygwin/mingw to append a dot to
+# FILE even if both FILE and FILE.exe exist: automatic-append-.exe
+# behavior happens only for exec(3), not for open(2)!  Also, sourcing
+# `FILE.' does not work on cygwin managed mounts.
+func_source ()
+{
+    $opt_debug
+    case $1 in
+    */* | *\\*)	. "$1" ;;
+    *)		. "./$1" ;;
+    esac
+}
+
+
+# func_resolve_sysroot PATH
+# Replace a leading = in PATH with a sysroot.  Store the result into
+# func_resolve_sysroot_result
+func_resolve_sysroot ()
+{
+  func_resolve_sysroot_result=$1
+  case $func_resolve_sysroot_result in
+  =*)
+    func_stripname '=' '' "$func_resolve_sysroot_result"
+    func_resolve_sysroot_result=$lt_sysroot$func_stripname_result
+    ;;
+  esac
+}
+
+# func_replace_sysroot PATH
+# If PATH begins with the sysroot, replace it with = and
+# store the result into func_replace_sysroot_result.
+func_replace_sysroot ()
+{
+  case "$lt_sysroot:$1" in
+  ?*:"$lt_sysroot"*)
+    func_stripname "$lt_sysroot" '' "$1"
+    func_replace_sysroot_result="=$func_stripname_result"
+    ;;
+  *)
+    # Including no sysroot.
+    func_replace_sysroot_result=$1
+    ;;
+  esac
+}
+
+# func_infer_tag arg
+# Infer tagged configuration to use if any are available and
+# if one wasn't chosen via the "--tag" command line option.
+# Only attempt this if the compiler in the base compile
+# command doesn't match the default compiler.
+# arg is usually of the form 'gcc ...'
+func_infer_tag ()
+{
+    $opt_debug
+    if test -n "$available_tags" && test -z "$tagname"; then
+      CC_quoted=
+      for arg in $CC; do
+	func_append_quoted CC_quoted "$arg"
+      done
+      CC_expanded=`func_echo_all $CC`
+      CC_quoted_expanded=`func_echo_all $CC_quoted`
+      case $@ in
+      # Blanks in the command may have been stripped by the calling shell,
+      # but not from the CC environment variable when configure was run.
+      " $CC "* | "$CC "* | " $CC_expanded "* | "$CC_expanded "* | \
+      " $CC_quoted"* | "$CC_quoted "* | " $CC_quoted_expanded "* | "$CC_quoted_expanded "*) ;;
+      # Blanks at the start of $base_compile will cause this to fail
+      # if we don't check for them as well.
+      *)
+	for z in $available_tags; do
+	  if $GREP "^# ### BEGIN LIBTOOL TAG CONFIG: $z$" < "$progpath" > /dev/null; then
+	    # Evaluate the configuration.
+	    eval "`${SED} -n -e '/^# ### BEGIN LIBTOOL TAG CONFIG: '$z'$/,/^# ### END LIBTOOL TAG CONFIG: '$z'$/p' < $progpath`"
+	    CC_quoted=
+	    for arg in $CC; do
+	      # Double-quote args containing other shell metacharacters.
+	      func_append_quoted CC_quoted "$arg"
+	    done
+	    CC_expanded=`func_echo_all $CC`
+	    CC_quoted_expanded=`func_echo_all $CC_quoted`
+	    case "$@ " in
+	    " $CC "* | "$CC "* | " $CC_expanded "* | "$CC_expanded "* | \
+	    " $CC_quoted"* | "$CC_quoted "* | " $CC_quoted_expanded "* | "$CC_quoted_expanded "*)
+	      # The compiler in the base compile command matches
+	      # the one in the tagged configuration.
+	      # Assume this is the tagged configuration we want.
+	      tagname=$z
+	      break
+	      ;;
+	    esac
+	  fi
+	done
+	# If $tagname still isn't set, then no tagged configuration
+	# was found and let the user know that the "--tag" command
+	# line option must be used.
+	if test -z "$tagname"; then
+	  func_echo "unable to infer tagged configuration"
+	  func_fatal_error "specify a tag with \`--tag'"
+#	else
+#	  func_verbose "using $tagname tagged configuration"
+	fi
+	;;
+      esac
+    fi
+}
+
+
+
+# func_write_libtool_object output_name pic_name nonpic_name
+# Create a libtool object file (analogous to a ".la" file),
+# but don't create it if we're doing a dry run.
+func_write_libtool_object ()
+{
+    write_libobj=${1}
+    if test "$build_libtool_libs" = yes; then
+      write_lobj=\'${2}\'
+    else
+      write_lobj=none
+    fi
+
+    if test "$build_old_libs" = yes; then
+      write_oldobj=\'${3}\'
+    else
+      write_oldobj=none
+    fi
+
+    $opt_dry_run || {
+      cat >${write_libobj}T <<EOF
+# $write_libobj - a libtool object file
+# Generated by $PROGRAM (GNU $PACKAGE$TIMESTAMP) $VERSION
+#
+# Please DO NOT delete this file!
+# It is necessary for linking the library.
+
+# Name of the PIC object.
+pic_object=$write_lobj
+
+# Name of the non-PIC object
+non_pic_object=$write_oldobj
+
+EOF
+      $MV "${write_libobj}T" "${write_libobj}"
+    }
+}
+
+
+##################################################
+# FILE NAME AND PATH CONVERSION HELPER FUNCTIONS #
+##################################################
+
+# func_convert_core_file_wine_to_w32 ARG
+# Helper function used by file name conversion functions when $build is *nix,
+# and $host is mingw, cygwin, or some other w32 environment. Relies on a
+# correctly configured wine environment available, with the winepath program
+# in $build's $PATH.
+#
+# ARG is the $build file name to be converted to w32 format.
+# Result is available in $func_convert_core_file_wine_to_w32_result, and will
+# be empty on error (or when ARG is empty)
+func_convert_core_file_wine_to_w32 ()
+{
+  $opt_debug
+  func_convert_core_file_wine_to_w32_result="$1"
+  if test -n "$1"; then
+    # Unfortunately, winepath does not exit with a non-zero error code, so we
+    # are forced to check the contents of stdout. On the other hand, if the
+    # command is not found, the shell will set an exit code of 127 and print
+    # *an error message* to stdout. So we must check for both error code of
+    # zero AND non-empty stdout, which explains the odd construction:
+    func_convert_core_file_wine_to_w32_tmp=`winepath -w "$1" 2>/dev/null`
+    if test "$?" -eq 0 && test -n "${func_convert_core_file_wine_to_w32_tmp}"; then
+      func_convert_core_file_wine_to_w32_result=`$ECHO "$func_convert_core_file_wine_to_w32_tmp" |
+        $SED -e "$lt_sed_naive_backslashify"`
+    else
+      func_convert_core_file_wine_to_w32_result=
+    fi
+  fi
+}
+# end: func_convert_core_file_wine_to_w32
+
+
+# func_convert_core_path_wine_to_w32 ARG
+# Helper function used by path conversion functions when $build is *nix, and
+# $host is mingw, cygwin, or some other w32 environment. Relies on a correctly
+# configured wine environment available, with the winepath program in $build's
+# $PATH. Assumes ARG has no leading or trailing path separator characters.
+#
+# ARG is path to be converted from $build format to win32.
+# Result is available in $func_convert_core_path_wine_to_w32_result.
+# Unconvertible file (directory) names in ARG are skipped; if no directory names
+# are convertible, then the result may be empty.
+func_convert_core_path_wine_to_w32 ()
+{
+  $opt_debug
+  # unfortunately, winepath doesn't convert paths, only file names
+  func_convert_core_path_wine_to_w32_result=""
+  if test -n "$1"; then
+    oldIFS=$IFS
+    IFS=:
+    for func_convert_core_path_wine_to_w32_f in $1; do
+      IFS=$oldIFS
+      func_convert_core_file_wine_to_w32 "$func_convert_core_path_wine_to_w32_f"
+      if test -n "$func_convert_core_file_wine_to_w32_result" ; then
+        if test -z "$func_convert_core_path_wine_to_w32_result"; then
+          func_convert_core_path_wine_to_w32_result="$func_convert_core_file_wine_to_w32_result"
+        else
+          func_append func_convert_core_path_wine_to_w32_result ";$func_convert_core_file_wine_to_w32_result"
+        fi
+      fi
+    done
+    IFS=$oldIFS
+  fi
+}
+# end: func_convert_core_path_wine_to_w32
+
+
+# func_cygpath ARGS...
+# Wrapper around calling the cygpath program via LT_CYGPATH. This is used when
+# when (1) $build is *nix and Cygwin is hosted via a wine environment; or (2)
+# $build is MSYS and $host is Cygwin, or (3) $build is Cygwin. In case (1) or
+# (2), returns the Cygwin file name or path in func_cygpath_result (input
+# file name or path is assumed to be in w32 format, as previously converted
+# from $build's *nix or MSYS format). In case (3), returns the w32 file name
+# or path in func_cygpath_result (input file name or path is assumed to be in
+# Cygwin format). Returns an empty string on error.
+#
+# ARGS are passed to cygpath, with the last one being the file name or path to
+# be converted.
+#
+# Specify the absolute *nix (or w32) name to cygpath in the LT_CYGPATH
+# environment variable; do not put it in $PATH.
+func_cygpath ()
+{
+  $opt_debug
+  if test -n "$LT_CYGPATH" && test -f "$LT_CYGPATH"; then
+    func_cygpath_result=`$LT_CYGPATH "$@" 2>/dev/null`
+    if test "$?" -ne 0; then
+      # on failure, ensure result is empty
+      func_cygpath_result=
+    fi
+  else
+    func_cygpath_result=
+    func_error "LT_CYGPATH is empty or specifies non-existent file: \`$LT_CYGPATH'"
+  fi
+}
+#end: func_cygpath
+
+
+# func_convert_core_msys_to_w32 ARG
+# Convert file name or path ARG from MSYS format to w32 format.  Return
+# result in func_convert_core_msys_to_w32_result.
+func_convert_core_msys_to_w32 ()
+{
+  $opt_debug
+  # awkward: cmd appends spaces to result
+  func_convert_core_msys_to_w32_result=`( cmd //c echo "$1" ) 2>/dev/null |
+    $SED -e 's/[ ]*$//' -e "$lt_sed_naive_backslashify"`
+}
+#end: func_convert_core_msys_to_w32
+
+
+# func_convert_file_check ARG1 ARG2
+# Verify that ARG1 (a file name in $build format) was converted to $host
+# format in ARG2. Otherwise, emit an error message, but continue (resetting
+# func_to_host_file_result to ARG1).
+func_convert_file_check ()
+{
+  $opt_debug
+  if test -z "$2" && test -n "$1" ; then
+    func_error "Could not determine host file name corresponding to"
+    func_error "  \`$1'"
+    func_error "Continuing, but uninstalled executables may not work."
+    # Fallback:
+    func_to_host_file_result="$1"
+  fi
+}
+# end func_convert_file_check
+
+
+# func_convert_path_check FROM_PATHSEP TO_PATHSEP FROM_PATH TO_PATH
+# Verify that FROM_PATH (a path in $build format) was converted to $host
+# format in TO_PATH. Otherwise, emit an error message, but continue, resetting
+# func_to_host_file_result to a simplistic fallback value (see below).
+func_convert_path_check ()
+{
+  $opt_debug
+  if test -z "$4" && test -n "$3"; then
+    func_error "Could not determine the host path corresponding to"
+    func_error "  \`$3'"
+    func_error "Continuing, but uninstalled executables may not work."
+    # Fallback.  This is a deliberately simplistic "conversion" and
+    # should not be "improved".  See libtool.info.
+    if test "x$1" != "x$2"; then
+      lt_replace_pathsep_chars="s|$1|$2|g"
+      func_to_host_path_result=`echo "$3" |
+        $SED -e "$lt_replace_pathsep_chars"`
+    else
+      func_to_host_path_result="$3"
+    fi
+  fi
+}
+# end func_convert_path_check
+
+
+# func_convert_path_front_back_pathsep FRONTPAT BACKPAT REPL ORIG
+# Modifies func_to_host_path_result by prepending REPL if ORIG matches FRONTPAT
+# and appending REPL if ORIG matches BACKPAT.
+func_convert_path_front_back_pathsep ()
+{
+  $opt_debug
+  case $4 in
+  $1 ) func_to_host_path_result="$3$func_to_host_path_result"
+    ;;
+  esac
+  case $4 in
+  $2 ) func_append func_to_host_path_result "$3"
+    ;;
+  esac
+}
+# end func_convert_path_front_back_pathsep
+
+
+##################################################
+# $build to $host FILE NAME CONVERSION FUNCTIONS #
+##################################################
+# invoked via `$to_host_file_cmd ARG'
+#
+# In each case, ARG is the path to be converted from $build to $host format.
+# Result will be available in $func_to_host_file_result.
+
+
+# func_to_host_file ARG
+# Converts the file name ARG from $build format to $host format. Return result
+# in func_to_host_file_result.
+func_to_host_file ()
+{
+  $opt_debug
+  $to_host_file_cmd "$1"
+}
+# end func_to_host_file
+
+
+# func_to_tool_file ARG LAZY
+# converts the file name ARG from $build format to toolchain format. Return
+# result in func_to_tool_file_result.  If the conversion in use is listed
+# in (the comma separated) LAZY, no conversion takes place.
+func_to_tool_file ()
+{
+  $opt_debug
+  case ,$2, in
+    *,"$to_tool_file_cmd",*)
+      func_to_tool_file_result=$1
+      ;;
+    *)
+      $to_tool_file_cmd "$1"
+      func_to_tool_file_result=$func_to_host_file_result
+      ;;
+  esac
+}
+# end func_to_tool_file
+
+
+# func_convert_file_noop ARG
+# Copy ARG to func_to_host_file_result.
+func_convert_file_noop ()
+{
+  func_to_host_file_result="$1"
+}
+# end func_convert_file_noop
+
+
+# func_convert_file_msys_to_w32 ARG
+# Convert file name ARG from (mingw) MSYS to (mingw) w32 format; automatic
+# conversion to w32 is not available inside the cwrapper.  Returns result in
+# func_to_host_file_result.
+func_convert_file_msys_to_w32 ()
+{
+  $opt_debug
+  func_to_host_file_result="$1"
+  if test -n "$1"; then
+    func_convert_core_msys_to_w32 "$1"
+    func_to_host_file_result="$func_convert_core_msys_to_w32_result"
+  fi
+  func_convert_file_check "$1" "$func_to_host_file_result"
+}
+# end func_convert_file_msys_to_w32
+
+
+# func_convert_file_cygwin_to_w32 ARG
+# Convert file name ARG from Cygwin to w32 format.  Returns result in
+# func_to_host_file_result.
+func_convert_file_cygwin_to_w32 ()
+{
+  $opt_debug
+  func_to_host_file_result="$1"
+  if test -n "$1"; then
+    # because $build is cygwin, we call "the" cygpath in $PATH; no need to use
+    # LT_CYGPATH in this case.
+    func_to_host_file_result=`cygpath -m "$1"`
+  fi
+  func_convert_file_check "$1" "$func_to_host_file_result"
+}
+# end func_convert_file_cygwin_to_w32
+
+
+# func_convert_file_nix_to_w32 ARG
+# Convert file name ARG from *nix to w32 format.  Requires a wine environment
+# and a working winepath. Returns result in func_to_host_file_result.
+func_convert_file_nix_to_w32 ()
+{
+  $opt_debug
+  func_to_host_file_result="$1"
+  if test -n "$1"; then
+    func_convert_core_file_wine_to_w32 "$1"
+    func_to_host_file_result="$func_convert_core_file_wine_to_w32_result"
+  fi
+  func_convert_file_check "$1" "$func_to_host_file_result"
+}
+# end func_convert_file_nix_to_w32
+
+
+# func_convert_file_msys_to_cygwin ARG
+# Convert file name ARG from MSYS to Cygwin format.  Requires LT_CYGPATH set.
+# Returns result in func_to_host_file_result.
+func_convert_file_msys_to_cygwin ()
+{
+  $opt_debug
+  func_to_host_file_result="$1"
+  if test -n "$1"; then
+    func_convert_core_msys_to_w32 "$1"
+    func_cygpath -u "$func_convert_core_msys_to_w32_result"
+    func_to_host_file_result="$func_cygpath_result"
+  fi
+  func_convert_file_check "$1" "$func_to_host_file_result"
+}
+# end func_convert_file_msys_to_cygwin
+
+
+# func_convert_file_nix_to_cygwin ARG
+# Convert file name ARG from *nix to Cygwin format.  Requires Cygwin installed
+# in a wine environment, working winepath, and LT_CYGPATH set.  Returns result
+# in func_to_host_file_result.
+func_convert_file_nix_to_cygwin ()
+{
+  $opt_debug
+  func_to_host_file_result="$1"
+  if test -n "$1"; then
+    # convert from *nix to w32, then use cygpath to convert from w32 to cygwin.
+    func_convert_core_file_wine_to_w32 "$1"
+    func_cygpath -u "$func_convert_core_file_wine_to_w32_result"
+    func_to_host_file_result="$func_cygpath_result"
+  fi
+  func_convert_file_check "$1" "$func_to_host_file_result"
+}
+# end func_convert_file_nix_to_cygwin
+
+
+#############################################
+# $build to $host PATH CONVERSION FUNCTIONS #
+#############################################
+# invoked via `$to_host_path_cmd ARG'
+#
+# In each case, ARG is the path to be converted from $build to $host format.
+# The result will be available in $func_to_host_path_result.
+#
+# Path separators are also converted from $build format to $host format.  If
+# ARG begins or ends with a path separator character, it is preserved (but
+# converted to $host format) on output.
+#
+# All path conversion functions are named using the following convention:
+#   file name conversion function    : func_convert_file_X_to_Y ()
+#   path conversion function         : func_convert_path_X_to_Y ()
+# where, for any given $build/$host combination the 'X_to_Y' value is the
+# same.  If conversion functions are added for new $build/$host combinations,
+# the two new functions must follow this pattern, or func_init_to_host_path_cmd
+# will break.
+
+
+# func_init_to_host_path_cmd
+# Ensures that function "pointer" variable $to_host_path_cmd is set to the
+# appropriate value, based on the value of $to_host_file_cmd.
+to_host_path_cmd=
+func_init_to_host_path_cmd ()
+{
+  $opt_debug
+  if test -z "$to_host_path_cmd"; then
+    func_stripname 'func_convert_file_' '' "$to_host_file_cmd"
+    to_host_path_cmd="func_convert_path_${func_stripname_result}"
+  fi
+}
+
+
+# func_to_host_path ARG
+# Converts the path ARG from $build format to $host format. Return result
+# in func_to_host_path_result.
+func_to_host_path ()
+{
+  $opt_debug
+  func_init_to_host_path_cmd
+  $to_host_path_cmd "$1"
+}
+# end func_to_host_path
+
+
+# func_convert_path_noop ARG
+# Copy ARG to func_to_host_path_result.
+func_convert_path_noop ()
+{
+  func_to_host_path_result="$1"
+}
+# end func_convert_path_noop
+
+
+# func_convert_path_msys_to_w32 ARG
+# Convert path ARG from (mingw) MSYS to (mingw) w32 format; automatic
+# conversion to w32 is not available inside the cwrapper.  Returns result in
+# func_to_host_path_result.
+func_convert_path_msys_to_w32 ()
+{
+  $opt_debug
+  func_to_host_path_result="$1"
+  if test -n "$1"; then
+    # Remove leading and trailing path separator characters from ARG.  MSYS
+    # behavior is inconsistent here; cygpath turns them into '.;' and ';.';
+    # and winepath ignores them completely.
+    func_stripname : : "$1"
+    func_to_host_path_tmp1=$func_stripname_result
+    func_convert_core_msys_to_w32 "$func_to_host_path_tmp1"
+    func_to_host_path_result="$func_convert_core_msys_to_w32_result"
+    func_convert_path_check : ";" \
+      "$func_to_host_path_tmp1" "$func_to_host_path_result"
+    func_convert_path_front_back_pathsep ":*" "*:" ";" "$1"
+  fi
+}
+# end func_convert_path_msys_to_w32
+
+
+# func_convert_path_cygwin_to_w32 ARG
+# Convert path ARG from Cygwin to w32 format.  Returns result in
+# func_to_host_file_result.
+func_convert_path_cygwin_to_w32 ()
+{
+  $opt_debug
+  func_to_host_path_result="$1"
+  if test -n "$1"; then
+    # See func_convert_path_msys_to_w32:
+    func_stripname : : "$1"
+    func_to_host_path_tmp1=$func_stripname_result
+    func_to_host_path_result=`cygpath -m -p "$func_to_host_path_tmp1"`
+    func_convert_path_check : ";" \
+      "$func_to_host_path_tmp1" "$func_to_host_path_result"
+    func_convert_path_front_back_pathsep ":*" "*:" ";" "$1"
+  fi
+}
+# end func_convert_path_cygwin_to_w32
+
+
+# func_convert_path_nix_to_w32 ARG
+# Convert path ARG from *nix to w32 format.  Requires a wine environment and
+# a working winepath.  Returns result in func_to_host_file_result.
+func_convert_path_nix_to_w32 ()
+{
+  $opt_debug
+  func_to_host_path_result="$1"
+  if test -n "$1"; then
+    # See func_convert_path_msys_to_w32:
+    func_stripname : : "$1"
+    func_to_host_path_tmp1=$func_stripname_result
+    func_convert_core_path_wine_to_w32 "$func_to_host_path_tmp1"
+    func_to_host_path_result="$func_convert_core_path_wine_to_w32_result"
+    func_convert_path_check : ";" \
+      "$func_to_host_path_tmp1" "$func_to_host_path_result"
+    func_convert_path_front_back_pathsep ":*" "*:" ";" "$1"
+  fi
+}
+# end func_convert_path_nix_to_w32
+
+
+# func_convert_path_msys_to_cygwin ARG
+# Convert path ARG from MSYS to Cygwin format.  Requires LT_CYGPATH set.
+# Returns result in func_to_host_file_result.
+func_convert_path_msys_to_cygwin ()
+{
+  $opt_debug
+  func_to_host_path_result="$1"
+  if test -n "$1"; then
+    # See func_convert_path_msys_to_w32:
+    func_stripname : : "$1"
+    func_to_host_path_tmp1=$func_stripname_result
+    func_convert_core_msys_to_w32 "$func_to_host_path_tmp1"
+    func_cygpath -u -p "$func_convert_core_msys_to_w32_result"
+    func_to_host_path_result="$func_cygpath_result"
+    func_convert_path_check : : \
+      "$func_to_host_path_tmp1" "$func_to_host_path_result"
+    func_convert_path_front_back_pathsep ":*" "*:" : "$1"
+  fi
+}
+# end func_convert_path_msys_to_cygwin
+
+
+# func_convert_path_nix_to_cygwin ARG
+# Convert path ARG from *nix to Cygwin format.  Requires Cygwin installed in a
+# a wine environment, working winepath, and LT_CYGPATH set.  Returns result in
+# func_to_host_file_result.
+func_convert_path_nix_to_cygwin ()
+{
+  $opt_debug
+  func_to_host_path_result="$1"
+  if test -n "$1"; then
+    # Remove leading and trailing path separator characters from
+    # ARG. msys behavior is inconsistent here, cygpath turns them
+    # into '.;' and ';.', and winepath ignores them completely.
+    func_stripname : : "$1"
+    func_to_host_path_tmp1=$func_stripname_result
+    func_convert_core_path_wine_to_w32 "$func_to_host_path_tmp1"
+    func_cygpath -u -p "$func_convert_core_path_wine_to_w32_result"
+    func_to_host_path_result="$func_cygpath_result"
+    func_convert_path_check : : \
+      "$func_to_host_path_tmp1" "$func_to_host_path_result"
+    func_convert_path_front_back_pathsep ":*" "*:" : "$1"
+  fi
+}
+# end func_convert_path_nix_to_cygwin
+
+
+# func_mode_compile arg...
+func_mode_compile ()
+{
+    $opt_debug
+    # Get the compilation command and the source file.
+    base_compile=
+    srcfile="$nonopt"  #  always keep a non-empty value in "srcfile"
+    suppress_opt=yes
+    suppress_output=
+    arg_mode=normal
+    libobj=
+    later=
+    pie_flag=
+
+    for arg
+    do
+      case $arg_mode in
+      arg  )
+	# do not "continue".  Instead, add this to base_compile
+	lastarg="$arg"
+	arg_mode=normal
+	;;
+
+      target )
+	libobj="$arg"
+	arg_mode=normal
+	continue
+	;;
+
+      normal )
+	# Accept any command-line options.
+	case $arg in
+	-o)
+	  test -n "$libobj" && \
+	    func_fatal_error "you cannot specify \`-o' more than once"
+	  arg_mode=target
+	  continue
+	  ;;
+
+	-pie | -fpie | -fPIE)
+          func_append pie_flag " $arg"
+	  continue
+	  ;;
+
+	-shared | -static | -prefer-pic | -prefer-non-pic)
+	  func_append later " $arg"
+	  continue
+	  ;;
+
+	-no-suppress)
+	  suppress_opt=no
+	  continue
+	  ;;
+
+	-Xcompiler)
+	  arg_mode=arg  #  the next one goes into the "base_compile" arg list
+	  continue      #  The current "srcfile" will either be retained or
+	  ;;            #  replaced later.  I would guess that would be a bug.
+
+	-Wc,*)
+	  func_stripname '-Wc,' '' "$arg"
+	  args=$func_stripname_result
+	  lastarg=
+	  save_ifs="$IFS"; IFS=','
+	  for arg in $args; do
+	    IFS="$save_ifs"
+	    func_append_quoted lastarg "$arg"
+	  done
+	  IFS="$save_ifs"
+	  func_stripname ' ' '' "$lastarg"
+	  lastarg=$func_stripname_result
+
+	  # Add the arguments to base_compile.
+	  func_append base_compile " $lastarg"
+	  continue
+	  ;;
+
+	*)
+	  # Accept the current argument as the source file.
+	  # The previous "srcfile" becomes the current argument.
+	  #
+	  lastarg="$srcfile"
+	  srcfile="$arg"
+	  ;;
+	esac  #  case $arg
+	;;
+      esac    #  case $arg_mode
+
+      # Aesthetically quote the previous argument.
+      func_append_quoted base_compile "$lastarg"
+    done # for arg
+
+    case $arg_mode in
+    arg)
+      func_fatal_error "you must specify an argument for -Xcompile"
+      ;;
+    target)
+      func_fatal_error "you must specify a target with \`-o'"
+      ;;
+    *)
+      # Get the name of the library object.
+      test -z "$libobj" && {
+	func_basename "$srcfile"
+	libobj="$func_basename_result"
+      }
+      ;;
+    esac
+
+    # Recognize several different file suffixes.
+    # If the user specifies -o file.o, it is replaced with file.lo
+    case $libobj in
+    *.[cCFSifmso] | \
+    *.ada | *.adb | *.ads | *.asm | \
+    *.c++ | *.cc | *.ii | *.class | *.cpp | *.cxx | \
+    *.[fF][09]? | *.for | *.java | *.go | *.obj | *.sx | *.cu | *.cup)
+      func_xform "$libobj"
+      libobj=$func_xform_result
+      ;;
+    esac
+
+    case $libobj in
+    *.lo) func_lo2o "$libobj"; obj=$func_lo2o_result ;;
+    *)
+      func_fatal_error "cannot determine name of library object from \`$libobj'"
+      ;;
+    esac
+
+    func_infer_tag $base_compile
+
+    for arg in $later; do
+      case $arg in
+      -shared)
+	test "$build_libtool_libs" != yes && \
+	  func_fatal_configuration "can not build a shared library"
+	build_old_libs=no
+	continue
+	;;
+
+      -static)
+	build_libtool_libs=no
+	build_old_libs=yes
+	continue
+	;;
+
+      -prefer-pic)
+	pic_mode=yes
+	continue
+	;;
+
+      -prefer-non-pic)
+	pic_mode=no
+	continue
+	;;
+      esac
+    done
+
+    func_quote_for_eval "$libobj"
+    test "X$libobj" != "X$func_quote_for_eval_result" \
+      && $ECHO "X$libobj" | $GREP '[]~#^*{};<>?"'"'"'	 &()|`$[]' \
+      && func_warning "libobj name \`$libobj' may not contain shell special characters."
+    func_dirname_and_basename "$obj" "/" ""
+    objname="$func_basename_result"
+    xdir="$func_dirname_result"
+    lobj=${xdir}$objdir/$objname
+
+    test -z "$base_compile" && \
+      func_fatal_help "you must specify a compilation command"
+
+    # Delete any leftover library objects.
+    if test "$build_old_libs" = yes; then
+      removelist="$obj $lobj $libobj ${libobj}T"
+    else
+      removelist="$lobj $libobj ${libobj}T"
+    fi
+
+    # On Cygwin there's no "real" PIC flag so we must build both object types
+    case $host_os in
+    cygwin* | mingw* | pw32* | os2* | cegcc*)
+      pic_mode=default
+      ;;
+    esac
+    if test "$pic_mode" = no && test "$deplibs_check_method" != pass_all; then
+      # non-PIC code in shared libraries is not supported
+      pic_mode=default
+    fi
+
+    # Calculate the filename of the output object if compiler does
+    # not support -o with -c
+    if test "$compiler_c_o" = no; then
+      output_obj=`$ECHO "$srcfile" | $SED 's%^.*/%%; s%\.[^.]*$%%'`.${objext}
+      lockfile="$output_obj.lock"
+    else
+      output_obj=
+      need_locks=no
+      lockfile=
+    fi
+
+    # Lock this critical section if it is needed
+    # We use this script file to make the link, it avoids creating a new file
+    if test "$need_locks" = yes; then
+      until $opt_dry_run || ln "$progpath" "$lockfile" 2>/dev/null; do
+	func_echo "Waiting for $lockfile to be removed"
+	sleep 2
+      done
+    elif test "$need_locks" = warn; then
+      if test -f "$lockfile"; then
+	$ECHO "\
+*** ERROR, $lockfile exists and contains:
+`cat $lockfile 2>/dev/null`
+
+This indicates that another process is trying to use the same
+temporary object file, and libtool could not work around it because
+your compiler does not support \`-c' and \`-o' together.  If you
+repeat this compilation, it may succeed, by chance, but you had better
+avoid parallel builds (make -j) in this platform, or get a better
+compiler."
+
+	$opt_dry_run || $RM $removelist
+	exit $EXIT_FAILURE
+      fi
+      func_append removelist " $output_obj"
+      $ECHO "$srcfile" > "$lockfile"
+    fi
+
+    $opt_dry_run || $RM $removelist
+    func_append removelist " $lockfile"
+    trap '$opt_dry_run || $RM $removelist; exit $EXIT_FAILURE' 1 2 15
+
+    func_to_tool_file "$srcfile" func_convert_file_msys_to_w32
+    srcfile=$func_to_tool_file_result
+    func_quote_for_eval "$srcfile"
+    qsrcfile=$func_quote_for_eval_result
+
+    # Only build a PIC object if we are building libtool libraries.
+    if test "$build_libtool_libs" = yes; then
+      # Without this assignment, base_compile gets emptied.
+      fbsd_hideous_sh_bug=$base_compile
+
+      if test "$pic_mode" != no; then
+	command="$base_compile $qsrcfile $pic_flag"
+      else
+	# Don't build PIC code
+	command="$base_compile $qsrcfile"
+      fi
+
+      func_mkdir_p "$xdir$objdir"
+
+      if test -z "$output_obj"; then
+	# Place PIC objects in $objdir
+	func_append command " -o $lobj"
+      fi
+
+      func_show_eval_locale "$command"	\
+          'test -n "$output_obj" && $RM $removelist; exit $EXIT_FAILURE'
+
+      if test "$need_locks" = warn &&
+	 test "X`cat $lockfile 2>/dev/null`" != "X$srcfile"; then
+	$ECHO "\
+*** ERROR, $lockfile contains:
+`cat $lockfile 2>/dev/null`
+
+but it should contain:
+$srcfile
+
+This indicates that another process is trying to use the same
+temporary object file, and libtool could not work around it because
+your compiler does not support \`-c' and \`-o' together.  If you
+repeat this compilation, it may succeed, by chance, but you had better
+avoid parallel builds (make -j) in this platform, or get a better
+compiler."
+
+	$opt_dry_run || $RM $removelist
+	exit $EXIT_FAILURE
+      fi
+
+      # Just move the object if needed, then go on to compile the next one
+      if test -n "$output_obj" && test "X$output_obj" != "X$lobj"; then
+	func_show_eval '$MV "$output_obj" "$lobj"' \
+	  'error=$?; $opt_dry_run || $RM $removelist; exit $error'
+      fi
+
+      # Allow error messages only from the first compilation.
+      if test "$suppress_opt" = yes; then
+	suppress_output=' >/dev/null 2>&1'
+      fi
+    fi
+
+    # Only build a position-dependent object if we build old libraries.
+    if test "$build_old_libs" = yes; then
+      if test "$pic_mode" != yes; then
+	# Don't build PIC code
+	command="$base_compile $qsrcfile$pie_flag"
+      else
+	command="$base_compile $qsrcfile $pic_flag"
+      fi
+      if test "$compiler_c_o" = yes; then
+	func_append command " -o $obj"
+      fi
+
+      # Suppress compiler output if we already did a PIC compilation.
+      func_append command "$suppress_output"
+      func_show_eval_locale "$command" \
+        '$opt_dry_run || $RM $removelist; exit $EXIT_FAILURE'
+
+      if test "$need_locks" = warn &&
+	 test "X`cat $lockfile 2>/dev/null`" != "X$srcfile"; then
+	$ECHO "\
+*** ERROR, $lockfile contains:
+`cat $lockfile 2>/dev/null`
+
+but it should contain:
+$srcfile
+
+This indicates that another process is trying to use the same
+temporary object file, and libtool could not work around it because
+your compiler does not support \`-c' and \`-o' together.  If you
+repeat this compilation, it may succeed, by chance, but you had better
+avoid parallel builds (make -j) in this platform, or get a better
+compiler."
+
+	$opt_dry_run || $RM $removelist
+	exit $EXIT_FAILURE
+      fi
+
+      # Just move the object if needed
+      if test -n "$output_obj" && test "X$output_obj" != "X$obj"; then
+	func_show_eval '$MV "$output_obj" "$obj"' \
+	  'error=$?; $opt_dry_run || $RM $removelist; exit $error'
+      fi
+    fi
+
+    $opt_dry_run || {
+      func_write_libtool_object "$libobj" "$objdir/$objname" "$objname"
+
+      # Unlock the critical section if it was locked
+      if test "$need_locks" != no; then
+	removelist=$lockfile
+        $RM "$lockfile"
+      fi
+    }
+
+    exit $EXIT_SUCCESS
+}
+
+$opt_help || {
+  test "$opt_mode" = compile && func_mode_compile ${1+"$@"}
+}
+
+func_mode_help ()
+{
+    # We need to display help for each of the modes.
+    case $opt_mode in
+      "")
+        # Generic help is extracted from the usage comments
+        # at the start of this file.
+        func_help
+        ;;
+
+      clean)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=clean RM [RM-OPTION]... FILE...
+
+Remove files from the build directory.
+
+RM is the name of the program to use to delete files associated with each FILE
+(typically \`/bin/rm').  RM-OPTIONS are options (such as \`-f') to be passed
+to RM.
+
+If FILE is a libtool library, object or program, all the files associated
+with it are deleted. Otherwise, only FILE itself is deleted using RM."
+        ;;
+
+      compile)
+      $ECHO \
+"Usage: $progname [OPTION]... --mode=compile COMPILE-COMMAND... SOURCEFILE
+
+Compile a source file into a libtool library object.
+
+This mode accepts the following additional options:
+
+  -o OUTPUT-FILE    set the output file name to OUTPUT-FILE
+  -no-suppress      do not suppress compiler output for multiple passes
+  -prefer-pic       try to build PIC objects only
+  -prefer-non-pic   try to build non-PIC objects only
+  -shared           do not build a \`.o' file suitable for static linking
+  -static           only build a \`.o' file suitable for static linking
+  -Wc,FLAG          pass FLAG directly to the compiler
+
+COMPILE-COMMAND is a command to be used in creating a \`standard' object file
+from the given SOURCEFILE.
+
+The output file name is determined by removing the directory component from
+SOURCEFILE, then substituting the C source code suffix \`.c' with the
+library object suffix, \`.lo'."
+        ;;
+
+      execute)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=execute COMMAND [ARGS]...
+
+Automatically set library path, then run a program.
+
+This mode accepts the following additional options:
+
+  -dlopen FILE      add the directory containing FILE to the library path
+
+This mode sets the library path environment variable according to \`-dlopen'
+flags.
+
+If any of the ARGS are libtool executable wrappers, then they are translated
+into their corresponding uninstalled binary, and any of their required library
+directories are added to the library path.
+
+Then, COMMAND is executed, with ARGS as arguments."
+        ;;
+
+      finish)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=finish [LIBDIR]...
+
+Complete the installation of libtool libraries.
+
+Each LIBDIR is a directory that contains libtool libraries.
+
+The commands that this mode executes may require superuser privileges.  Use
+the \`--dry-run' option if you just want to see what would be executed."
+        ;;
+
+      install)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=install INSTALL-COMMAND...
+
+Install executables or libraries.
+
+INSTALL-COMMAND is the installation command.  The first component should be
+either the \`install' or \`cp' program.
+
+The following components of INSTALL-COMMAND are treated specially:
+
+  -inst-prefix-dir PREFIX-DIR  Use PREFIX-DIR as a staging area for installation
+
+The rest of the components are interpreted as arguments to that command (only
+BSD-compatible install options are recognized)."
+        ;;
+
+      link)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=link LINK-COMMAND...
+
+Link object files or libraries together to form another library, or to
+create an executable program.
+
+LINK-COMMAND is a command using the C compiler that you would use to create
+a program from several object files.
+
+The following components of LINK-COMMAND are treated specially:
+
+  -all-static       do not do any dynamic linking at all
+  -avoid-version    do not add a version suffix if possible
+  -bindir BINDIR    specify path to binaries directory (for systems where
+                    libraries must be found in the PATH setting at runtime)
+  -dlopen FILE      \`-dlpreopen' FILE if it cannot be dlopened at runtime
+  -dlpreopen FILE   link in FILE and add its symbols to lt_preloaded_symbols
+  -export-dynamic   allow symbols from OUTPUT-FILE to be resolved with dlsym(3)
+  -export-symbols SYMFILE
+                    try to export only the symbols listed in SYMFILE
+  -export-symbols-regex REGEX
+                    try to export only the symbols matching REGEX
+  -LLIBDIR          search LIBDIR for required installed libraries
+  -lNAME            OUTPUT-FILE requires the installed library libNAME
+  -module           build a library that can dlopened
+  -no-fast-install  disable the fast-install mode
+  -no-install       link a not-installable executable
+  -no-undefined     declare that a library does not refer to external symbols
+  -o OUTPUT-FILE    create OUTPUT-FILE from the specified objects
+  -objectlist FILE  Use a list of object files found in FILE to specify objects
+  -precious-files-regex REGEX
+                    don't remove output files matching REGEX
+  -release RELEASE  specify package release information
+  -rpath LIBDIR     the created library will eventually be installed in LIBDIR
+  -R[ ]LIBDIR       add LIBDIR to the runtime path of programs and libraries
+  -shared           only do dynamic linking of libtool libraries
+  -shrext SUFFIX    override the standard shared library file extension
+  -static           do not do any dynamic linking of uninstalled libtool libraries
+  -static-libtool-libs
+                    do not do any dynamic linking of libtool libraries
+  -version-info CURRENT[:REVISION[:AGE]]
+                    specify library version info [each variable defaults to 0]
+  -weak LIBNAME     declare that the target provides the LIBNAME interface
+  -Wc,FLAG
+  -Xcompiler FLAG   pass linker-specific FLAG directly to the compiler
+  -Wl,FLAG
+  -Xlinker FLAG     pass linker-specific FLAG directly to the linker
+  -XCClinker FLAG   pass link-specific FLAG to the compiler driver (CC)
+
+All other options (arguments beginning with \`-') are ignored.
+
+Every other argument is treated as a filename.  Files ending in \`.la' are
+treated as uninstalled libtool libraries, other files are standard or library
+object files.
+
+If the OUTPUT-FILE ends in \`.la', then a libtool library is created,
+only library objects (\`.lo' files) may be specified, and \`-rpath' is
+required, except when creating a convenience library.
+
+If OUTPUT-FILE ends in \`.a' or \`.lib', then a standard library is created
+using \`ar' and \`ranlib', or on Windows using \`lib'.
+
+If OUTPUT-FILE ends in \`.lo' or \`.${objext}', then a reloadable object file
+is created, otherwise an executable program is created."
+        ;;
+
+      uninstall)
+        $ECHO \
+"Usage: $progname [OPTION]... --mode=uninstall RM [RM-OPTION]... FILE...
+
+Remove libraries from an installation directory.
+
+RM is the name of the program to use to delete files associated with each FILE
+(typically \`/bin/rm').  RM-OPTIONS are options (such as \`-f') to be passed
+to RM.
+
+If FILE is a libtool library, all the files associated with it are deleted.
+Otherwise, only FILE itself is deleted using RM."
+        ;;
+
+      *)
+        func_fatal_help "invalid operation mode \`$opt_mode'"
+        ;;
+    esac
+
+    echo
+    $ECHO "Try \`$progname --help' for more information about other modes."
+}
+
+# Now that we've collected a possible --mode arg, show help if necessary
+if $opt_help; then
+  if test "$opt_help" = :; then
+    func_mode_help
+  else
+    {
+      func_help noexit
+      for opt_mode in compile link execute install finish uninstall clean; do
+	func_mode_help
+      done
+    } | sed -n '1p; 2,$s/^Usage:/  or: /p'
+    {
+      func_help noexit
+      for opt_mode in compile link execute install finish uninstall clean; do
+	echo
+	func_mode_help
+      done
+    } |
+    sed '1d
+      /^When reporting/,/^Report/{
+	H
+	d
+      }
+      $x
+      /information about other modes/d
+      /more detailed .*MODE/d
+      s/^Usage:.*--mode=\([^ ]*\) .*/Description of \1 mode:/'
+  fi
+  exit $?
+fi
+
+
+# func_mode_execute arg...
+func_mode_execute ()
+{
+    $opt_debug
+    # The first argument is the command name.
+    cmd="$nonopt"
+    test -z "$cmd" && \
+      func_fatal_help "you must specify a COMMAND"
+
+    # Handle -dlopen flags immediately.
+    for file in $opt_dlopen; do
+      test -f "$file" \
+	|| func_fatal_help "\`$file' is not a file"
+
+      dir=
+      case $file in
+      *.la)
+	func_resolve_sysroot "$file"
+	file=$func_resolve_sysroot_result
+
+	# Check to see that this really is a libtool archive.
+	func_lalib_unsafe_p "$file" \
+	  || func_fatal_help "\`$lib' is not a valid libtool archive"
+
+	# Read the libtool library.
+	dlname=
+	library_names=
+	func_source "$file"
+
+	# Skip this library if it cannot be dlopened.
+	if test -z "$dlname"; then
+	  # Warn if it was a shared library.
+	  test -n "$library_names" && \
+	    func_warning "\`$file' was not linked with \`-export-dynamic'"
+	  continue
+	fi
+
+	func_dirname "$file" "" "."
+	dir="$func_dirname_result"
+
+	if test -f "$dir/$objdir/$dlname"; then
+	  func_append dir "/$objdir"
+	else
+	  if test ! -f "$dir/$dlname"; then
+	    func_fatal_error "cannot find \`$dlname' in \`$dir' or \`$dir/$objdir'"
+	  fi
+	fi
+	;;
+
+      *.lo)
+	# Just add the directory containing the .lo file.
+	func_dirname "$file" "" "."
+	dir="$func_dirname_result"
+	;;
+
+      *)
+	func_warning "\`-dlopen' is ignored for non-libtool libraries and objects"
+	continue
+	;;
+      esac
+
+      # Get the absolute pathname.
+      absdir=`cd "$dir" && pwd`
+      test -n "$absdir" && dir="$absdir"
+
+      # Now add the directory to shlibpath_var.
+      if eval "test -z \"\$$shlibpath_var\""; then
+	eval "$shlibpath_var=\"\$dir\""
+      else
+	eval "$shlibpath_var=\"\$dir:\$$shlibpath_var\""
+      fi
+    done
+
+    # This variable tells wrapper scripts just to set shlibpath_var
+    # rather than running their programs.
+    libtool_execute_magic="$magic"
+
+    # Check if any of the arguments is a wrapper script.
+    args=
+    for file
+    do
+      case $file in
+      -* | *.la | *.lo ) ;;
+      *)
+	# Do a test to see if this is really a libtool program.
+	if func_ltwrapper_script_p "$file"; then
+	  func_source "$file"
+	  # Transform arg to wrapped name.
+	  file="$progdir/$program"
+	elif func_ltwrapper_executable_p "$file"; then
+	  func_ltwrapper_scriptname "$file"
+	  func_source "$func_ltwrapper_scriptname_result"
+	  # Transform arg to wrapped name.
+	  file="$progdir/$program"
+	fi
+	;;
+      esac
+      # Quote arguments (to preserve shell metacharacters).
+      func_append_quoted args "$file"
+    done
+
+    if test "X$opt_dry_run" = Xfalse; then
+      if test -n "$shlibpath_var"; then
+	# Export the shlibpath_var.
+	eval "export $shlibpath_var"
+      fi
+
+      # Restore saved environment variables
+      for lt_var in LANG LANGUAGE LC_ALL LC_CTYPE LC_COLLATE LC_MESSAGES
+      do
+	eval "if test \"\${save_$lt_var+set}\" = set; then
+                $lt_var=\$save_$lt_var; export $lt_var
+	      else
+		$lt_unset $lt_var
+	      fi"
+      done
+
+      # Now prepare to actually exec the command.
+      exec_cmd="\$cmd$args"
+    else
+      # Display what would be done.
+      if test -n "$shlibpath_var"; then
+	eval "\$ECHO \"\$shlibpath_var=\$$shlibpath_var\""
+	echo "export $shlibpath_var"
+      fi
+      $ECHO "$cmd$args"
+      exit $EXIT_SUCCESS
+    fi
+}
+
+test "$opt_mode" = execute && func_mode_execute ${1+"$@"}
+
+
+# func_mode_finish arg...
+func_mode_finish ()
+{
+    $opt_debug
+    libs=
+    libdirs=
+    admincmds=
+
+    for opt in "$nonopt" ${1+"$@"}
+    do
+      if test -d "$opt"; then
+	func_append libdirs " $opt"
+
+      elif test -f "$opt"; then
+	if func_lalib_unsafe_p "$opt"; then
+	  func_append libs " $opt"
+	else
+	  func_warning "\`$opt' is not a valid libtool archive"
+	fi
+
+      else
+	func_fatal_error "invalid argument \`$opt'"
+      fi
+    done
+
+    if test -n "$libs"; then
+      if test -n "$lt_sysroot"; then
+        sysroot_regex=`$ECHO "$lt_sysroot" | $SED "$sed_make_literal_regex"`
+        sysroot_cmd="s/\([ ']\)$sysroot_regex/\1/g;"
+      else
+        sysroot_cmd=
+      fi
+
+      # Remove sysroot references
+      if $opt_dry_run; then
+        for lib in $libs; do
+          echo "removing references to $lt_sysroot and \`=' prefixes from $lib"
+        done
+      else
+        tmpdir=`func_mktempdir`
+        for lib in $libs; do
+	  sed -e "${sysroot_cmd} s/\([ ']-[LR]\)=/\1/g; s/\([ ']\)=/\1/g" $lib \
+	    > $tmpdir/tmp-la
+	  mv -f $tmpdir/tmp-la $lib
+	done
+        ${RM}r "$tmpdir"
+      fi
+    fi
+
+    if test -n "$finish_cmds$finish_eval" && test -n "$libdirs"; then
+      for libdir in $libdirs; do
+	if test -n "$finish_cmds"; then
+	  # Do each command in the finish commands.
+	  func_execute_cmds "$finish_cmds" 'admincmds="$admincmds
+'"$cmd"'"'
+	fi
+	if test -n "$finish_eval"; then
+	  # Do the single finish_eval.
+	  eval cmds=\"$finish_eval\"
+	  $opt_dry_run || eval "$cmds" || func_append admincmds "
+       $cmds"
+	fi
+      done
+    fi
+
+    # Exit here if they wanted silent mode.
+    $opt_silent && exit $EXIT_SUCCESS
+
+    if test -n "$finish_cmds$finish_eval" && test -n "$libdirs"; then
+      echo "----------------------------------------------------------------------"
+      echo "Libraries have been installed in:"
+      for libdir in $libdirs; do
+	$ECHO "   $libdir"
+      done
+      echo
+      echo "If you ever happen to want to link against installed libraries"
+      echo "in a given directory, LIBDIR, you must either use libtool, and"
+      echo "specify the full pathname of the library, or use the \`-LLIBDIR'"
+      echo "flag during linking and do at least one of the following:"
+      if test -n "$shlibpath_var"; then
+	echo "   - add LIBDIR to the \`$shlibpath_var' environment variable"
+	echo "     during execution"
+      fi
+      if test -n "$runpath_var"; then
+	echo "   - add LIBDIR to the \`$runpath_var' environment variable"
+	echo "     during linking"
+      fi
+      if test -n "$hardcode_libdir_flag_spec"; then
+	libdir=LIBDIR
+	eval flag=\"$hardcode_libdir_flag_spec\"
+
+	$ECHO "   - use the \`$flag' linker flag"
+      fi
+      if test -n "$admincmds"; then
+	$ECHO "   - have your system administrator run these commands:$admincmds"
+      fi
+      if test -f /etc/ld.so.conf; then
+	echo "   - have your system administrator add LIBDIR to \`/etc/ld.so.conf'"
+      fi
+      echo
+
+      echo "See any operating system documentation about shared libraries for"
+      case $host in
+	solaris2.[6789]|solaris2.1[0-9])
+	  echo "more information, such as the ld(1), crle(1) and ld.so(8) manual"
+	  echo "pages."
+	  ;;
+	*)
+	  echo "more information, such as the ld(1) and ld.so(8) manual pages."
+	  ;;
+      esac
+      echo "----------------------------------------------------------------------"
+    fi
+    exit $EXIT_SUCCESS
+}
+
+test "$opt_mode" = finish && func_mode_finish ${1+"$@"}
+
+
+# func_mode_install arg...
+func_mode_install ()
+{
+    $opt_debug
+    # There may be an optional sh(1) argument at the beginning of
+    # install_prog (especially on Windows NT).
+    if test "$nonopt" = "$SHELL" || test "$nonopt" = /bin/sh ||
+       # Allow the use of GNU shtool's install command.
+       case $nonopt in *shtool*) :;; *) false;; esac; then
+      # Aesthetically quote it.
+      func_quote_for_eval "$nonopt"
+      install_prog="$func_quote_for_eval_result "
+      arg=$1
+      shift
+    else
+      install_prog=
+      arg=$nonopt
+    fi
+
+    # The real first argument should be the name of the installation program.
+    # Aesthetically quote it.
+    func_quote_for_eval "$arg"
+    func_append install_prog "$func_quote_for_eval_result"
+    install_shared_prog=$install_prog
+    case " $install_prog " in
+      *[\\\ /]cp\ *) install_cp=: ;;
+      *) install_cp=false ;;
+    esac
+
+    # We need to accept at least all the BSD install flags.
+    dest=
+    files=
+    opts=
+    prev=
+    install_type=
+    isdir=no
+    stripme=
+    no_mode=:
+    for arg
+    do
+      arg2=
+      if test -n "$dest"; then
+	func_append files " $dest"
+	dest=$arg
+	continue
+      fi
+
+      case $arg in
+      -d) isdir=yes ;;
+      -f)
+	if $install_cp; then :; else
+	  prev=$arg
+	fi
+	;;
+      -g | -m | -o)
+	prev=$arg
+	;;
+      -s)
+	stripme=" -s"
+	continue
+	;;
+      -*)
+	;;
+      *)
+	# If the previous option needed an argument, then skip it.
+	if test -n "$prev"; then
+	  if test "x$prev" = x-m && test -n "$install_override_mode"; then
+	    arg2=$install_override_mode
+	    no_mode=false
+	  fi
+	  prev=
+	else
+	  dest=$arg
+	  continue
+	fi
+	;;
+      esac
+
+      # Aesthetically quote the argument.
+      func_quote_for_eval "$arg"
+      func_append install_prog " $func_quote_for_eval_result"
+      if test -n "$arg2"; then
+	func_quote_for_eval "$arg2"
+      fi
+      func_append install_shared_prog " $func_quote_for_eval_result"
+    done
+
+    test -z "$install_prog" && \
+      func_fatal_help "you must specify an install program"
+
+    test -n "$prev" && \
+      func_fatal_help "the \`$prev' option requires an argument"
+
+    if test -n "$install_override_mode" && $no_mode; then
+      if $install_cp; then :; else
+	func_quote_for_eval "$install_override_mode"
+	func_append install_shared_prog " -m $func_quote_for_eval_result"
+      fi
+    fi
+
+    if test -z "$files"; then
+      if test -z "$dest"; then
+	func_fatal_help "no file or destination specified"
+      else
+	func_fatal_help "you must specify a destination"
+      fi
+    fi
+
+    # Strip any trailing slash from the destination.
+    func_stripname '' '/' "$dest"
+    dest=$func_stripname_result
+
+    # Check to see that the destination is a directory.
+    test -d "$dest" && isdir=yes
+    if test "$isdir" = yes; then
+      destdir="$dest"
+      destname=
+    else
+      func_dirname_and_basename "$dest" "" "."
+      destdir="$func_dirname_result"
+      destname="$func_basename_result"
+
+      # Not a directory, so check to see that there is only one file specified.
+      set dummy $files; shift
+      test "$#" -gt 1 && \
+	func_fatal_help "\`$dest' is not a directory"
+    fi
+    case $destdir in
+    [\\/]* | [A-Za-z]:[\\/]*) ;;
+    *)
+      for file in $files; do
+	case $file in
+	*.lo) ;;
+	*)
+	  func_fatal_help "\`$destdir' must be an absolute directory name"
+	  ;;
+	esac
+      done
+      ;;
+    esac
+
+    # This variable tells wrapper scripts just to set variables rather
+    # than running their programs.
+    libtool_install_magic="$magic"
+
+    staticlibs=
+    future_libdirs=
+    current_libdirs=
+    for file in $files; do
+
+      # Do each installation.
+      case $file in
+      *.$libext)
+	# Do the static libraries later.
+	func_append staticlibs " $file"
+	;;
+
+      *.la)
+	func_resolve_sysroot "$file"
+	file=$func_resolve_sysroot_result
+
+	# Check to see that this really is a libtool archive.
+	func_lalib_unsafe_p "$file" \
+	  || func_fatal_help "\`$file' is not a valid libtool archive"
+
+	library_names=
+	old_library=
+	relink_command=
+	func_source "$file"
+
+	# Add the libdir to current_libdirs if it is the destination.
+	if test "X$destdir" = "X$libdir"; then
+	  case "$current_libdirs " in
+	  *" $libdir "*) ;;
+	  *) func_append current_libdirs " $libdir" ;;
+	  esac
+	else
+	  # Note the libdir as a future libdir.
+	  case "$future_libdirs " in
+	  *" $libdir "*) ;;
+	  *) func_append future_libdirs " $libdir" ;;
+	  esac
+	fi
+
+	func_dirname "$file" "/" ""
+	dir="$func_dirname_result"
+	func_append dir "$objdir"
+
+	if test -n "$relink_command"; then
+	  # Determine the prefix the user has applied to our future dir.
+	  inst_prefix_dir=`$ECHO "$destdir" | $SED -e "s%$libdir\$%%"`
+
+	  # Don't allow the user to place us outside of our expected
+	  # location b/c this prevents finding dependent libraries that
+	  # are installed to the same prefix.
+	  # At present, this check doesn't affect windows .dll's that
+	  # are installed into $libdir/../bin (currently, that works fine)
+	  # but it's something to keep an eye on.
+	  test "$inst_prefix_dir" = "$destdir" && \
+	    func_fatal_error "error: cannot install \`$file' to a directory not ending in $libdir"
+
+	  if test -n "$inst_prefix_dir"; then
+	    # Stick the inst_prefix_dir data into the link command.
+	    relink_command=`$ECHO "$relink_command" | $SED "s%@inst_prefix_dir@%-inst-prefix-dir $inst_prefix_dir%"`
+	  else
+	    relink_command=`$ECHO "$relink_command" | $SED "s%@inst_prefix_dir@%%"`
+	  fi
+
+	  func_warning "relinking \`$file'"
+	  func_show_eval "$relink_command" \
+	    'func_fatal_error "error: relink \`$file'\'' with the above command before installing it"'
+	fi
+
+	# See the names of the shared library.
+	set dummy $library_names; shift
+	if test -n "$1"; then
+	  realname="$1"
+	  shift
+
+	  srcname="$realname"
+	  test -n "$relink_command" && srcname="$realname"T
+
+	  # Install the shared library and build the symlinks.
+	  func_show_eval "$install_shared_prog $dir/$srcname $destdir/$realname" \
+	      'exit $?'
+	  tstripme="$stripme"
+	  case $host_os in
+	  cygwin* | mingw* | pw32* | cegcc*)
+	    case $realname in
+	    *.dll.a)
+	      tstripme=""
+	      ;;
+	    esac
+	    ;;
+	  esac
+	  if test -n "$tstripme" && test -n "$striplib"; then
+	    func_show_eval "$striplib $destdir/$realname" 'exit $?'
+	  fi
+
+	  if test "$#" -gt 0; then
+	    # Delete the old symlinks, and create new ones.
+	    # Try `ln -sf' first, because the `ln' binary might depend on
+	    # the symlink we replace!  Solaris /bin/ln does not understand -f,
+	    # so we also need to try rm && ln -s.
+	    for linkname
+	    do
+	      test "$linkname" != "$realname" \
+		&& func_show_eval "(cd $destdir && { $LN_S -f $realname $linkname || { $RM $linkname && $LN_S $realname $linkname; }; })"
+	    done
+	  fi
+
+	  # Do each command in the postinstall commands.
+	  lib="$destdir/$realname"
+	  func_execute_cmds "$postinstall_cmds" 'exit $?'
+	fi
+
+	# Install the pseudo-library for information purposes.
+	func_basename "$file"
+	name="$func_basename_result"
+	instname="$dir/$name"i
+	func_show_eval "$install_prog $instname $destdir/$name" 'exit $?'
+
+	# Maybe install the static library, too.
+	test -n "$old_library" && func_append staticlibs " $dir/$old_library"
+	;;
+
+      *.lo)
+	# Install (i.e. copy) a libtool object.
+
+	# Figure out destination file name, if it wasn't already specified.
+	if test -n "$destname"; then
+	  destfile="$destdir/$destname"
+	else
+	  func_basename "$file"
+	  destfile="$func_basename_result"
+	  destfile="$destdir/$destfile"
+	fi
+
+	# Deduce the name of the destination old-style object file.
+	case $destfile in
+	*.lo)
+	  func_lo2o "$destfile"
+	  staticdest=$func_lo2o_result
+	  ;;
+	*.$objext)
+	  staticdest="$destfile"
+	  destfile=
+	  ;;
+	*)
+	  func_fatal_help "cannot copy a libtool object to \`$destfile'"
+	  ;;
+	esac
+
+	# Install the libtool object if requested.
+	test -n "$destfile" && \
+	  func_show_eval "$install_prog $file $destfile" 'exit $?'
+
+	# Install the old object if enabled.
+	if test "$build_old_libs" = yes; then
+	  # Deduce the name of the old-style object file.
+	  func_lo2o "$file"
+	  staticobj=$func_lo2o_result
+	  func_show_eval "$install_prog \$staticobj \$staticdest" 'exit $?'
+	fi
+	exit $EXIT_SUCCESS
+	;;
+
+      *)
+	# Figure out destination file name, if it wasn't already specified.
+	if test -n "$destname"; then
+	  destfile="$destdir/$destname"
+	else
+	  func_basename "$file"
+	  destfile="$func_basename_result"
+	  destfile="$destdir/$destfile"
+	fi
+
+	# If the file is missing, and there is a .exe on the end, strip it
+	# because it is most likely a libtool script we actually want to
+	# install
+	stripped_ext=""
+	case $file in
+	  *.exe)
+	    if test ! -f "$file"; then
+	      func_stripname '' '.exe' "$file"
+	      file=$func_stripname_result
+	      stripped_ext=".exe"
+	    fi
+	    ;;
+	esac
+
+	# Do a test to see if this is really a libtool program.
+	case $host in
+	*cygwin* | *mingw*)
+	    if func_ltwrapper_executable_p "$file"; then
+	      func_ltwrapper_scriptname "$file"
+	      wrapper=$func_ltwrapper_scriptname_result
+	    else
+	      func_stripname '' '.exe' "$file"
+	      wrapper=$func_stripname_result
+	    fi
+	    ;;
+	*)
+	    wrapper=$file
+	    ;;
+	esac
+	if func_ltwrapper_script_p "$wrapper"; then
+	  notinst_deplibs=
+	  relink_command=
+
+	  func_source "$wrapper"
+
+	  # Check the variables that should have been set.
+	  test -z "$generated_by_libtool_version" && \
+	    func_fatal_error "invalid libtool wrapper script \`$wrapper'"
+
+	  finalize=yes
+	  for lib in $notinst_deplibs; do
+	    # Check to see that each library is installed.
+	    libdir=
+	    if test -f "$lib"; then
+	      func_source "$lib"
+	    fi
+	    libfile="$libdir/"`$ECHO "$lib" | $SED 's%^.*/%%g'` ### testsuite: skip nested quoting test
+	    if test -n "$libdir" && test ! -f "$libfile"; then
+	      func_warning "\`$lib' has not been installed in \`$libdir'"
+	      finalize=no
+	    fi
+	  done
+
+	  relink_command=
+	  func_source "$wrapper"
+
+	  outputname=
+	  if test "$fast_install" = no && test -n "$relink_command"; then
+	    $opt_dry_run || {
+	      if test "$finalize" = yes; then
+	        tmpdir=`func_mktempdir`
+		func_basename "$file$stripped_ext"
+		file="$func_basename_result"
+	        outputname="$tmpdir/$file"
+	        # Replace the output file specification.
+	        relink_command=`$ECHO "$relink_command" | $SED 's%@OUTPUT@%'"$outputname"'%g'`
+
+	        $opt_silent || {
+	          func_quote_for_expand "$relink_command"
+		  eval "func_echo $func_quote_for_expand_result"
+	        }
+	        if eval "$relink_command"; then :
+	          else
+		  func_error "error: relink \`$file' with the above command before installing it"
+		  $opt_dry_run || ${RM}r "$tmpdir"
+		  continue
+	        fi
+	        file="$outputname"
+	      else
+	        func_warning "cannot relink \`$file'"
+	      fi
+	    }
+	  else
+	    # Install the binary that we compiled earlier.
+	    file=`$ECHO "$file$stripped_ext" | $SED "s%\([^/]*\)$%$objdir/\1%"`
+	  fi
+	fi
+
+	# remove .exe since cygwin /usr/bin/install will append another
+	# one anyway
+	case $install_prog,$host in
+	*/usr/bin/install*,*cygwin*)
+	  case $file:$destfile in
+	  *.exe:*.exe)
+	    # this is ok
+	    ;;
+	  *.exe:*)
+	    destfile=$destfile.exe
+	    ;;
+	  *:*.exe)
+	    func_stripname '' '.exe' "$destfile"
+	    destfile=$func_stripname_result
+	    ;;
+	  esac
+	  ;;
+	esac
+	func_show_eval "$install_prog\$stripme \$file \$destfile" 'exit $?'
+	$opt_dry_run || if test -n "$outputname"; then
+	  ${RM}r "$tmpdir"
+	fi
+	;;
+      esac
+    done
+
+    for file in $staticlibs; do
+      func_basename "$file"
+      name="$func_basename_result"
+
+      # Set up the ranlib parameters.
+      oldlib="$destdir/$name"
+      func_to_tool_file "$oldlib" func_convert_file_msys_to_w32
+      tool_oldlib=$func_to_tool_file_result
+
+      func_show_eval "$install_prog \$file \$oldlib" 'exit $?'
+
+      if test -n "$stripme" && test -n "$old_striplib"; then
+	func_show_eval "$old_striplib $tool_oldlib" 'exit $?'
+      fi
+
+      # Do each command in the postinstall commands.
+      func_execute_cmds "$old_postinstall_cmds" 'exit $?'
+    done
+
+    test -n "$future_libdirs" && \
+      func_warning "remember to run \`$progname --finish$future_libdirs'"
+
+    if test -n "$current_libdirs"; then
+      # Maybe just do a dry run.
+      $opt_dry_run && current_libdirs=" -n$current_libdirs"
+      exec_cmd='$SHELL $progpath $preserve_args --finish$current_libdirs'
+    else
+      exit $EXIT_SUCCESS
+    fi
+}
+
+test "$opt_mode" = install && func_mode_install ${1+"$@"}
+
+
+# func_generate_dlsyms outputname originator pic_p
+# Extract symbols from dlprefiles and create ${outputname}S.o with
+# a dlpreopen symbol table.
+func_generate_dlsyms ()
+{
+    $opt_debug
+    my_outputname="$1"
+    my_originator="$2"
+    my_pic_p="${3-no}"
+    my_prefix=`$ECHO "$my_originator" | sed 's%[^a-zA-Z0-9]%_%g'`
+    my_dlsyms=
+
+    if test -n "$dlfiles$dlprefiles" || test "$dlself" != no; then
+      if test -n "$NM" && test -n "$global_symbol_pipe"; then
+	my_dlsyms="${my_outputname}S.c"
+      else
+	func_error "not configured to extract global symbols from dlpreopened files"
+      fi
+    fi
+
+    if test -n "$my_dlsyms"; then
+      case $my_dlsyms in
+      "") ;;
+      *.c)
+	# Discover the nlist of each of the dlfiles.
+	nlist="$output_objdir/${my_outputname}.nm"
+
+	func_show_eval "$RM $nlist ${nlist}S ${nlist}T"
+
+	# Parse the name list into a source file.
+	func_verbose "creating $output_objdir/$my_dlsyms"
+
+	$opt_dry_run || $ECHO > "$output_objdir/$my_dlsyms" "\
+/* $my_dlsyms - symbol resolution table for \`$my_outputname' dlsym emulation. */
+/* Generated by $PROGRAM (GNU $PACKAGE$TIMESTAMP) $VERSION */
+
+#ifdef __cplusplus
+extern \"C\" {
+#endif
+
+#if defined(__GNUC__) && (((__GNUC__ == 4) && (__GNUC_MINOR__ >= 4)) || (__GNUC__ > 4))
+#pragma GCC diagnostic ignored \"-Wstrict-prototypes\"
+#endif
+
+/* Keep this code in sync between libtool.m4, ltmain, lt_system.h, and tests.  */
+#if defined(_WIN32) || defined(__CYGWIN__) || defined(_WIN32_WCE)
+/* DATA imports from DLLs on WIN32 con't be const, because runtime
+   relocations are performed -- see ld's documentation on pseudo-relocs.  */
+# define LT_DLSYM_CONST
+#elif defined(__osf__)
+/* This system does not cope well with relocations in const data.  */
+# define LT_DLSYM_CONST
+#else
+# define LT_DLSYM_CONST const
+#endif
+
+/* External symbol declarations for the compiler. */\
+"
+
+	if test "$dlself" = yes; then
+	  func_verbose "generating symbol list for \`$output'"
+
+	  $opt_dry_run || echo ': @PROGRAM@ ' > "$nlist"
+
+	  # Add our own program objects to the symbol list.
+	  progfiles=`$ECHO "$objs$old_deplibs" | $SP2NL | $SED "$lo2o" | $NL2SP`
+	  for progfile in $progfiles; do
+	    func_to_tool_file "$progfile" func_convert_file_msys_to_w32
+	    func_verbose "extracting global C symbols from \`$func_to_tool_file_result'"
+	    $opt_dry_run || eval "$NM $func_to_tool_file_result | $global_symbol_pipe >> '$nlist'"
+	  done
+
+	  if test -n "$exclude_expsyms"; then
+	    $opt_dry_run || {
+	      eval '$EGREP -v " ($exclude_expsyms)$" "$nlist" > "$nlist"T'
+	      eval '$MV "$nlist"T "$nlist"'
+	    }
+	  fi
+
+	  if test -n "$export_symbols_regex"; then
+	    $opt_dry_run || {
+	      eval '$EGREP -e "$export_symbols_regex" "$nlist" > "$nlist"T'
+	      eval '$MV "$nlist"T "$nlist"'
+	    }
+	  fi
+
+	  # Prepare the list of exported symbols
+	  if test -z "$export_symbols"; then
+	    export_symbols="$output_objdir/$outputname.exp"
+	    $opt_dry_run || {
+	      $RM $export_symbols
+	      eval "${SED} -n -e '/^: @PROGRAM@ $/d' -e 's/^.* \(.*\)$/\1/p' "'< "$nlist" > "$export_symbols"'
+	      case $host in
+	      *cygwin* | *mingw* | *cegcc* )
+                eval "echo EXPORTS "'> "$output_objdir/$outputname.def"'
+                eval 'cat "$export_symbols" >> "$output_objdir/$outputname.def"'
+	        ;;
+	      esac
+	    }
+	  else
+	    $opt_dry_run || {
+	      eval "${SED} -e 's/\([].[*^$]\)/\\\\\1/g' -e 's/^/ /' -e 's/$/$/'"' < "$export_symbols" > "$output_objdir/$outputname.exp"'
+	      eval '$GREP -f "$output_objdir/$outputname.exp" < "$nlist" > "$nlist"T'
+	      eval '$MV "$nlist"T "$nlist"'
+	      case $host in
+	        *cygwin* | *mingw* | *cegcc* )
+	          eval "echo EXPORTS "'> "$output_objdir/$outputname.def"'
+	          eval 'cat "$nlist" >> "$output_objdir/$outputname.def"'
+	          ;;
+	      esac
+	    }
+	  fi
+	fi
+
+	for dlprefile in $dlprefiles; do
+	  func_verbose "extracting global C symbols from \`$dlprefile'"
+	  func_basename "$dlprefile"
+	  name="$func_basename_result"
+          case $host in
+	    *cygwin* | *mingw* | *cegcc* )
+	      # if an import library, we need to obtain dlname
+	      if func_win32_import_lib_p "$dlprefile"; then
+	        func_tr_sh "$dlprefile"
+	        eval "curr_lafile=\$libfile_$func_tr_sh_result"
+	        dlprefile_dlbasename=""
+	        if test -n "$curr_lafile" && func_lalib_p "$curr_lafile"; then
+	          # Use subshell, to avoid clobbering current variable values
+	          dlprefile_dlname=`source "$curr_lafile" && echo "$dlname"`
+	          if test -n "$dlprefile_dlname" ; then
+	            func_basename "$dlprefile_dlname"
+	            dlprefile_dlbasename="$func_basename_result"
+	          else
+	            # no lafile. user explicitly requested -dlpreopen <import library>.
+	            $sharedlib_from_linklib_cmd "$dlprefile"
+	            dlprefile_dlbasename=$sharedlib_from_linklib_result
+	          fi
+	        fi
+	        $opt_dry_run || {
+	          if test -n "$dlprefile_dlbasename" ; then
+	            eval '$ECHO ": $dlprefile_dlbasename" >> "$nlist"'
+	          else
+	            func_warning "Could not compute DLL name from $name"
+	            eval '$ECHO ": $name " >> "$nlist"'
+	          fi
+	          func_to_tool_file "$dlprefile" func_convert_file_msys_to_w32
+	          eval "$NM \"$func_to_tool_file_result\" 2>/dev/null | $global_symbol_pipe |
+	            $SED -e '/I __imp/d' -e 's/I __nm_/D /;s/_nm__//' >> '$nlist'"
+	        }
+	      else # not an import lib
+	        $opt_dry_run || {
+	          eval '$ECHO ": $name " >> "$nlist"'
+	          func_to_tool_file "$dlprefile" func_convert_file_msys_to_w32
+	          eval "$NM \"$func_to_tool_file_result\" 2>/dev/null | $global_symbol_pipe >> '$nlist'"
+	        }
+	      fi
+	    ;;
+	    *)
+	      $opt_dry_run || {
+	        eval '$ECHO ": $name " >> "$nlist"'
+	        func_to_tool_file "$dlprefile" func_convert_file_msys_to_w32
+	        eval "$NM \"$func_to_tool_file_result\" 2>/dev/null | $global_symbol_pipe >> '$nlist'"
+	      }
+	    ;;
+          esac
+	done
+
+	$opt_dry_run || {
+	  # Make sure we have at least an empty file.
+	  test -f "$nlist" || : > "$nlist"
+
+	  if test -n "$exclude_expsyms"; then
+	    $EGREP -v " ($exclude_expsyms)$" "$nlist" > "$nlist"T
+	    $MV "$nlist"T "$nlist"
+	  fi
+
+	  # Try sorting and uniquifying the output.
+	  if $GREP -v "^: " < "$nlist" |
+	      if sort -k 3 </dev/null >/dev/null 2>&1; then
+		sort -k 3
+	      else
+		sort +2
+	      fi |
+	      uniq > "$nlist"S; then
+	    :
+	  else
+	    $GREP -v "^: " < "$nlist" > "$nlist"S
+	  fi
+
+	  if test -f "$nlist"S; then
+	    eval "$global_symbol_to_cdecl"' < "$nlist"S >> "$output_objdir/$my_dlsyms"'
+	  else
+	    echo '/* NONE */' >> "$output_objdir/$my_dlsyms"
+	  fi
+
+	  echo >> "$output_objdir/$my_dlsyms" "\
+
+/* The mapping between symbol names and symbols.  */
+typedef struct {
+  const char *name;
+  void *address;
+} lt_dlsymlist;
+extern LT_DLSYM_CONST lt_dlsymlist
+lt_${my_prefix}_LTX_preloaded_symbols[];
+LT_DLSYM_CONST lt_dlsymlist
+lt_${my_prefix}_LTX_preloaded_symbols[] =
+{\
+  { \"$my_originator\", (void *) 0 },"
+
+	  case $need_lib_prefix in
+	  no)
+	    eval "$global_symbol_to_c_name_address" < "$nlist" >> "$output_objdir/$my_dlsyms"
+	    ;;
+	  *)
+	    eval "$global_symbol_to_c_name_address_lib_prefix" < "$nlist" >> "$output_objdir/$my_dlsyms"
+	    ;;
+	  esac
+	  echo >> "$output_objdir/$my_dlsyms" "\
+  {0, (void *) 0}
+};
+
+/* This works around a problem in FreeBSD linker */
+#ifdef FREEBSD_WORKAROUND
+static const void *lt_preloaded_setup() {
+  return lt_${my_prefix}_LTX_preloaded_symbols;
+}
+#endif
+
+#ifdef __cplusplus
+}
+#endif\
+"
+	} # !$opt_dry_run
+
+	pic_flag_for_symtable=
+	case "$compile_command " in
+	*" -static "*) ;;
+	*)
+	  case $host in
+	  # compiling the symbol table file with pic_flag works around
+	  # a FreeBSD bug that causes programs to crash when -lm is
+	  # linked before any other PIC object.  But we must not use
+	  # pic_flag when linking with -static.  The problem exists in
+	  # FreeBSD 2.2.6 and is fixed in FreeBSD 3.1.
+	  *-*-freebsd2.*|*-*-freebsd3.0*|*-*-freebsdelf3.0*)
+	    pic_flag_for_symtable=" $pic_flag -DFREEBSD_WORKAROUND" ;;
+	  *-*-hpux*)
+	    pic_flag_for_symtable=" $pic_flag"  ;;
+	  *)
+	    if test "X$my_pic_p" != Xno; then
+	      pic_flag_for_symtable=" $pic_flag"
+	    fi
+	    ;;
+	  esac
+	  ;;
+	esac
+	symtab_cflags=
+	for arg in $LTCFLAGS; do
+	  case $arg in
+	  -pie | -fpie | -fPIE) ;;
+	  *) func_append symtab_cflags " $arg" ;;
+	  esac
+	done
+
+	# Now compile the dynamic symbol file.
+	func_show_eval '(cd $output_objdir && $LTCC$symtab_cflags -c$no_builtin_flag$pic_flag_for_symtable "$my_dlsyms")' 'exit $?'
+
+	# Clean up the generated files.
+	func_show_eval '$RM "$output_objdir/$my_dlsyms" "$nlist" "${nlist}S" "${nlist}T"'
+
+	# Transform the symbol file into the correct name.
+	symfileobj="$output_objdir/${my_outputname}S.$objext"
+	case $host in
+	*cygwin* | *mingw* | *cegcc* )
+	  if test -f "$output_objdir/$my_outputname.def"; then
+	    compile_command=`$ECHO "$compile_command" | $SED "s%@SYMFILE@%$output_objdir/$my_outputname.def $symfileobj%"`
+	    finalize_command=`$ECHO "$finalize_command" | $SED "s%@SYMFILE@%$output_objdir/$my_outputname.def $symfileobj%"`
+	  else
+	    compile_command=`$ECHO "$compile_command" | $SED "s%@SYMFILE@%$symfileobj%"`
+	    finalize_command=`$ECHO "$finalize_command" | $SED "s%@SYMFILE@%$symfileobj%"`
+	  fi
+	  ;;
+	*)
+	  compile_command=`$ECHO "$compile_command" | $SED "s%@SYMFILE@%$symfileobj%"`
+	  finalize_command=`$ECHO "$finalize_command" | $SED "s%@SYMFILE@%$symfileobj%"`
+	  ;;
+	esac
+	;;
+      *)
+	func_fatal_error "unknown suffix for \`$my_dlsyms'"
+	;;
+      esac
+    else
+      # We keep going just in case the user didn't refer to
+      # lt_preloaded_symbols.  The linker will fail if global_symbol_pipe
+      # really was required.
+
+      # Nullify the symbol file.
+      compile_command=`$ECHO "$compile_command" | $SED "s% @SYMFILE@%%"`
+      finalize_command=`$ECHO "$finalize_command" | $SED "s% @SYMFILE@%%"`
+    fi
+}
+
+# func_win32_libid arg
+# return the library type of file 'arg'
+#
+# Need a lot of goo to handle *both* DLLs and import libs
+# Has to be a shell function in order to 'eat' the argument
+# that is supplied when $file_magic_command is called.
+# Despite the name, also deal with 64 bit binaries.
+func_win32_libid ()
+{
+  $opt_debug
+  win32_libid_type="unknown"
+  win32_fileres=`file -L $1 2>/dev/null`
+  case $win32_fileres in
+  *ar\ archive\ import\ library*) # definitely import
+    win32_libid_type="x86 archive import"
+    ;;
+  *ar\ archive*) # could be an import, or static
+    # Keep the egrep pattern in sync with the one in _LT_CHECK_MAGIC_METHOD.
+    if eval $OBJDUMP -f $1 | $SED -e '10q' 2>/dev/null |
+       $EGREP 'file format (pei*-i386(.*architecture: i386)?|pe-arm-wince|pe-x86-64)' >/dev/null; then
+      func_to_tool_file "$1" func_convert_file_msys_to_w32
+      win32_nmres=`eval $NM -f posix -A \"$func_to_tool_file_result\" |
+	$SED -n -e '
+	    1,100{
+		/ I /{
+		    s,.*,import,
+		    p
+		    q
+		}
+	    }'`
+      case $win32_nmres in
+      import*)  win32_libid_type="x86 archive import";;
+      *)        win32_libid_type="x86 archive static";;
+      esac
+    fi
+    ;;
+  *DLL*)
+    win32_libid_type="x86 DLL"
+    ;;
+  *executable*) # but shell scripts are "executable" too...
+    case $win32_fileres in
+    *MS\ Windows\ PE\ Intel*)
+      win32_libid_type="x86 DLL"
+      ;;
+    esac
+    ;;
+  esac
+  $ECHO "$win32_libid_type"
+}
+
+# func_cygming_dll_for_implib ARG
+#
+# Platform-specific function to extract the
+# name of the DLL associated with the specified
+# import library ARG.
+# Invoked by eval'ing the libtool variable
+#    $sharedlib_from_linklib_cmd
+# Result is available in the variable
+#    $sharedlib_from_linklib_result
+func_cygming_dll_for_implib ()
+{
+  $opt_debug
+  sharedlib_from_linklib_result=`$DLLTOOL --identify-strict --identify "$1"`
+}
+
+# func_cygming_dll_for_implib_fallback_core SECTION_NAME LIBNAMEs
+#
+# The is the core of a fallback implementation of a
+# platform-specific function to extract the name of the
+# DLL associated with the specified import library LIBNAME.
+#
+# SECTION_NAME is either .idata$6 or .idata$7, depending
+# on the platform and compiler that created the implib.
+#
+# Echos the name of the DLL associated with the
+# specified import library.
+func_cygming_dll_for_implib_fallback_core ()
+{
+  $opt_debug
+  match_literal=`$ECHO "$1" | $SED "$sed_make_literal_regex"`
+  $OBJDUMP -s --section "$1" "$2" 2>/dev/null |
+    $SED '/^Contents of section '"$match_literal"':/{
+      # Place marker at beginning of archive member dllname section
+      s/.*/====MARK====/
+      p
+      d
+    }
+    # These lines can sometimes be longer than 43 characters, but
+    # are always uninteresting
+    /:[	 ]*file format pe[i]\{,1\}-/d
+    /^In archive [^:]*:/d
+    # Ensure marker is printed
+    /^====MARK====/p
+    # Remove all lines with less than 43 characters
+    /^.\{43\}/!d
+    # From remaining lines, remove first 43 characters
+    s/^.\{43\}//' |
+    $SED -n '
+      # Join marker and all lines until next marker into a single line
+      /^====MARK====/ b para
+      H
+      $ b para
+      b
+      :para
+      x
+      s/\n//g
+      # Remove the marker
+      s/^====MARK====//
+      # Remove trailing dots and whitespace
+      s/[\. \t]*$//
+      # Print
+      /./p' |
+    # we now have a list, one entry per line, of the stringified
+    # contents of the appropriate section of all members of the
+    # archive which possess that section. Heuristic: eliminate
+    # all those which have a first or second character that is
+    # a '.' (that is, objdump's representation of an unprintable
+    # character.) This should work for all archives with less than
+    # 0x302f exports -- but will fail for DLLs whose name actually
+    # begins with a literal '.' or a single character followed by
+    # a '.'.
+    #
+    # Of those that remain, print the first one.
+    $SED -e '/^\./d;/^.\./d;q'
+}
+
+# func_cygming_gnu_implib_p ARG
+# This predicate returns with zero status (TRUE) if
+# ARG is a GNU/binutils-style import library. Returns
+# with nonzero status (FALSE) otherwise.
+func_cygming_gnu_implib_p ()
+{
+  $opt_debug
+  func_to_tool_file "$1" func_convert_file_msys_to_w32
+  func_cygming_gnu_implib_tmp=`$NM "$func_to_tool_file_result" | eval "$global_symbol_pipe" | $EGREP ' (_head_[A-Za-z0-9_]+_[ad]l*|[A-Za-z0-9_]+_[ad]l*_iname)$'`
+  test -n "$func_cygming_gnu_implib_tmp"
+}
+
+# func_cygming_ms_implib_p ARG
+# This predicate returns with zero status (TRUE) if
+# ARG is an MS-style import library. Returns
+# with nonzero status (FALSE) otherwise.
+func_cygming_ms_implib_p ()
+{
+  $opt_debug
+  func_to_tool_file "$1" func_convert_file_msys_to_w32
+  func_cygming_ms_implib_tmp=`$NM "$func_to_tool_file_result" | eval "$global_symbol_pipe" | $GREP '_NULL_IMPORT_DESCRIPTOR'`
+  test -n "$func_cygming_ms_implib_tmp"
+}
+
+# func_cygming_dll_for_implib_fallback ARG
+# Platform-specific function to extract the
+# name of the DLL associated with the specified
+# import library ARG.
+#
+# This fallback implementation is for use when $DLLTOOL
+# does not support the --identify-strict option.
+# Invoked by eval'ing the libtool variable
+#    $sharedlib_from_linklib_cmd
+# Result is available in the variable
+#    $sharedlib_from_linklib_result
+func_cygming_dll_for_implib_fallback ()
+{
+  $opt_debug
+  if func_cygming_gnu_implib_p "$1" ; then
+    # binutils import library
+    sharedlib_from_linklib_result=`func_cygming_dll_for_implib_fallback_core '.idata$7' "$1"`
+  elif func_cygming_ms_implib_p "$1" ; then
+    # ms-generated import library
+    sharedlib_from_linklib_result=`func_cygming_dll_for_implib_fallback_core '.idata$6' "$1"`
+  else
+    # unknown
+    sharedlib_from_linklib_result=""
+  fi
+}
+
+
+# func_extract_an_archive dir oldlib
+func_extract_an_archive ()
+{
+    $opt_debug
+    f_ex_an_ar_dir="$1"; shift
+    f_ex_an_ar_oldlib="$1"
+    if test "$lock_old_archive_extraction" = yes; then
+      lockfile=$f_ex_an_ar_oldlib.lock
+      until $opt_dry_run || ln "$progpath" "$lockfile" 2>/dev/null; do
+	func_echo "Waiting for $lockfile to be removed"
+	sleep 2
+      done
+    fi
+    func_show_eval "(cd \$f_ex_an_ar_dir && $AR x \"\$f_ex_an_ar_oldlib\")" \
+		   'stat=$?; rm -f "$lockfile"; exit $stat'
+    if test "$lock_old_archive_extraction" = yes; then
+      $opt_dry_run || rm -f "$lockfile"
+    fi
+    if ($AR t "$f_ex_an_ar_oldlib" | sort | sort -uc >/dev/null 2>&1); then
+     :
+    else
+      func_fatal_error "object name conflicts in archive: $f_ex_an_ar_dir/$f_ex_an_ar_oldlib"
+    fi
+}
+
+
+# func_extract_archives gentop oldlib ...
+func_extract_archives ()
+{
+    $opt_debug
+    my_gentop="$1"; shift
+    my_oldlibs=${1+"$@"}
+    my_oldobjs=""
+    my_xlib=""
+    my_xabs=""
+    my_xdir=""
+
+    for my_xlib in $my_oldlibs; do
+      # Extract the objects.
+      case $my_xlib in
+	[\\/]* | [A-Za-z]:[\\/]*) my_xabs="$my_xlib" ;;
+	*) my_xabs=`pwd`"/$my_xlib" ;;
+      esac
+      func_basename "$my_xlib"
+      my_xlib="$func_basename_result"
+      my_xlib_u=$my_xlib
+      while :; do
+        case " $extracted_archives " in
+	*" $my_xlib_u "*)
+	  func_arith $extracted_serial + 1
+	  extracted_serial=$func_arith_result
+	  my_xlib_u=lt$extracted_serial-$my_xlib ;;
+	*) break ;;
+	esac
+      done
+      extracted_archives="$extracted_archives $my_xlib_u"
+      my_xdir="$my_gentop/$my_xlib_u"
+
+      func_mkdir_p "$my_xdir"
+
+      case $host in
+      *-darwin*)
+	func_verbose "Extracting $my_xabs"
+	# Do not bother doing anything if just a dry run
+	$opt_dry_run || {
+	  darwin_orig_dir=`pwd`
+	  cd $my_xdir || exit $?
+	  darwin_archive=$my_xabs
+	  darwin_curdir=`pwd`
+	  darwin_base_archive=`basename "$darwin_archive"`
+	  darwin_arches=`$LIPO -info "$darwin_archive" 2>/dev/null | $GREP Architectures 2>/dev/null || true`
+	  if test -n "$darwin_arches"; then
+	    darwin_arches=`$ECHO "$darwin_arches" | $SED -e 's/.*are://'`
+	    darwin_arch=
+	    func_verbose "$darwin_base_archive has multiple architectures $darwin_arches"
+	    for darwin_arch in  $darwin_arches ; do
+	      func_mkdir_p "unfat-$$/${darwin_base_archive}-${darwin_arch}"
+	      $LIPO -thin $darwin_arch -output "unfat-$$/${darwin_base_archive}-${darwin_arch}/${darwin_base_archive}" "${darwin_archive}"
+	      cd "unfat-$$/${darwin_base_archive}-${darwin_arch}"
+	      func_extract_an_archive "`pwd`" "${darwin_base_archive}"
+	      cd "$darwin_curdir"
+	      $RM "unfat-$$/${darwin_base_archive}-${darwin_arch}/${darwin_base_archive}"
+	    done # $darwin_arches
+            ## Okay now we've a bunch of thin objects, gotta fatten them up :)
+	    darwin_filelist=`find unfat-$$ -type f -name \*.o -print -o -name \*.lo -print | $SED -e "$basename" | sort -u`
+	    darwin_file=
+	    darwin_files=
+	    for darwin_file in $darwin_filelist; do
+	      darwin_files=`find unfat-$$ -name $darwin_file -print | sort | $NL2SP`
+	      $LIPO -create -output "$darwin_file" $darwin_files
+	    done # $darwin_filelist
+	    $RM -rf unfat-$$
+	    cd "$darwin_orig_dir"
+	  else
+	    cd $darwin_orig_dir
+	    func_extract_an_archive "$my_xdir" "$my_xabs"
+	  fi # $darwin_arches
+	} # !$opt_dry_run
+	;;
+      *)
+        func_extract_an_archive "$my_xdir" "$my_xabs"
+	;;
+      esac
+      my_oldobjs="$my_oldobjs "`find $my_xdir -name \*.$objext -print -o -name \*.lo -print | sort | $NL2SP`
+    done
+
+    func_extract_archives_result="$my_oldobjs"
+}
+
+
+# func_emit_wrapper [arg=no]
+#
+# Emit a libtool wrapper script on stdout.
+# Don't directly open a file because we may want to
+# incorporate the script contents within a cygwin/mingw
+# wrapper executable.  Must ONLY be called from within
+# func_mode_link because it depends on a number of variables
+# set therein.
+#
+# ARG is the value that the WRAPPER_SCRIPT_BELONGS_IN_OBJDIR
+# variable will take.  If 'yes', then the emitted script
+# will assume that the directory in which it is stored is
+# the $objdir directory.  This is a cygwin/mingw-specific
+# behavior.
+func_emit_wrapper ()
+{
+	func_emit_wrapper_arg1=${1-no}
+
+	$ECHO "\
+#! $SHELL
+
+# $output - temporary wrapper script for $objdir/$outputname
+# Generated by $PROGRAM (GNU $PACKAGE$TIMESTAMP) $VERSION
+#
+# The $output program cannot be directly executed until all the libtool
+# libraries that it depends on are installed.
+#
+# This wrapper script should never be moved out of the build directory.
+# If it is, it will not operate correctly.
+
+# Sed substitution that helps us do robust quoting.  It backslashifies
+# metacharacters that are still active within double-quoted strings.
+sed_quote_subst='$sed_quote_subst'
+
+# Be Bourne compatible
+if test -n \"\${ZSH_VERSION+set}\" && (emulate sh) >/dev/null 2>&1; then
+  emulate sh
+  NULLCMD=:
+  # Zsh 3.x and 4.x performs word splitting on \${1+\"\$@\"}, which
+  # is contrary to our usage.  Disable this feature.
+  alias -g '\${1+\"\$@\"}'='\"\$@\"'
+  setopt NO_GLOB_SUBST
+else
+  case \`(set -o) 2>/dev/null\` in *posix*) set -o posix;; esac
+fi
+BIN_SH=xpg4; export BIN_SH # for Tru64
+DUALCASE=1; export DUALCASE # for MKS sh
+
+# The HP-UX ksh and POSIX shell print the target directory to stdout
+# if CDPATH is set.
+(unset CDPATH) >/dev/null 2>&1 && unset CDPATH
+
+relink_command=\"$relink_command\"
+
+# This environment variable determines our operation mode.
+if test \"\$libtool_install_magic\" = \"$magic\"; then
+  # install mode needs the following variables:
+  generated_by_libtool_version='$macro_version'
+  notinst_deplibs='$notinst_deplibs'
+else
+  # When we are sourced in execute mode, \$file and \$ECHO are already set.
+  if test \"\$libtool_execute_magic\" != \"$magic\"; then
+    file=\"\$0\""
+
+    qECHO=`$ECHO "$ECHO" | $SED "$sed_quote_subst"`
+    $ECHO "\
+
+# A function that is used when there is no print builtin or printf.
+func_fallback_echo ()
+{
+  eval 'cat <<_LTECHO_EOF
+\$1
+_LTECHO_EOF'
+}
+    ECHO=\"$qECHO\"
+  fi
+
+# Very basic option parsing. These options are (a) specific to
+# the libtool wrapper, (b) are identical between the wrapper
+# /script/ and the wrapper /executable/ which is used only on
+# windows platforms, and (c) all begin with the string "--lt-"
+# (application programs are unlikely to have options which match
+# this pattern).
+#
+# There are only two supported options: --lt-debug and
+# --lt-dump-script. There is, deliberately, no --lt-help.
+#
+# The first argument to this parsing function should be the
+# script's $0 value, followed by "$@".
+lt_option_debug=
+func_parse_lt_options ()
+{
+  lt_script_arg0=\$0
+  shift
+  for lt_opt
+  do
+    case \"\$lt_opt\" in
+    --lt-debug) lt_option_debug=1 ;;
+    --lt-dump-script)
+        lt_dump_D=\`\$ECHO \"X\$lt_script_arg0\" | $SED -e 's/^X//' -e 's%/[^/]*$%%'\`
+        test \"X\$lt_dump_D\" = \"X\$lt_script_arg0\" && lt_dump_D=.
+        lt_dump_F=\`\$ECHO \"X\$lt_script_arg0\" | $SED -e 's/^X//' -e 's%^.*/%%'\`
+        cat \"\$lt_dump_D/\$lt_dump_F\"
+        exit 0
+      ;;
+    --lt-*)
+        \$ECHO \"Unrecognized --lt- option: '\$lt_opt'\" 1>&2
+        exit 1
+      ;;
+    esac
+  done
+
+  # Print the debug banner immediately:
+  if test -n \"\$lt_option_debug\"; then
+    echo \"${outputname}:${output}:\${LINENO}: libtool wrapper (GNU $PACKAGE$TIMESTAMP) $VERSION\" 1>&2
+  fi
+}
+
+# Used when --lt-debug. Prints its arguments to stdout
+# (redirection is the responsibility of the caller)
+func_lt_dump_args ()
+{
+  lt_dump_args_N=1;
+  for lt_arg
+  do
+    \$ECHO \"${outputname}:${output}:\${LINENO}: newargv[\$lt_dump_args_N]: \$lt_arg\"
+    lt_dump_args_N=\`expr \$lt_dump_args_N + 1\`
+  done
+}
+
+# Core function for launching the target application
+func_exec_program_core ()
+{
+"
+  case $host in
+  # Backslashes separate directories on plain windows
+  *-*-mingw | *-*-os2* | *-cegcc*)
+    $ECHO "\
+      if test -n \"\$lt_option_debug\"; then
+        \$ECHO \"${outputname}:${output}:\${LINENO}: newargv[0]: \$progdir\\\\\$program\" 1>&2
+        func_lt_dump_args \${1+\"\$@\"} 1>&2
+      fi
+      exec \"\$progdir\\\\\$program\" \${1+\"\$@\"}
+"
+    ;;
+
+  *)
+    $ECHO "\
+      if test -n \"\$lt_option_debug\"; then
+        \$ECHO \"${outputname}:${output}:\${LINENO}: newargv[0]: \$progdir/\$program\" 1>&2
+        func_lt_dump_args \${1+\"\$@\"} 1>&2
+      fi
+      exec \"\$progdir/\$program\" \${1+\"\$@\"}
+"
+    ;;
+  esac
+  $ECHO "\
+      \$ECHO \"\$0: cannot exec \$program \$*\" 1>&2
+      exit 1
+}
+
+# A function to encapsulate launching the target application
+# Strips options in the --lt-* namespace from \$@ and
+# launches target application with the remaining arguments.
+func_exec_program ()
+{
+  case \" \$* \" in
+  *\\ --lt-*)
+    for lt_wr_arg
+    do
+      case \$lt_wr_arg in
+      --lt-*) ;;
+      *) set x \"\$@\" \"\$lt_wr_arg\"; shift;;
+      esac
+      shift
+    done ;;
+  esac
+  func_exec_program_core \${1+\"\$@\"}
+}
+
+  # Parse options
+  func_parse_lt_options \"\$0\" \${1+\"\$@\"}
+
+  # Find the directory that this script lives in.
+  thisdir=\`\$ECHO \"\$file\" | $SED 's%/[^/]*$%%'\`
+  test \"x\$thisdir\" = \"x\$file\" && thisdir=.
+
+  # Follow symbolic links until we get to the real thisdir.
+  file=\`ls -ld \"\$file\" | $SED -n 's/.*-> //p'\`
+  while test -n \"\$file\"; do
+    destdir=\`\$ECHO \"\$file\" | $SED 's%/[^/]*\$%%'\`
+
+    # If there was a directory component, then change thisdir.
+    if test \"x\$destdir\" != \"x\$file\"; then
+      case \"\$destdir\" in
+      [\\\\/]* | [A-Za-z]:[\\\\/]*) thisdir=\"\$destdir\" ;;
+      *) thisdir=\"\$thisdir/\$destdir\" ;;
+      esac
+    fi
+
+    file=\`\$ECHO \"\$file\" | $SED 's%^.*/%%'\`
+    file=\`ls -ld \"\$thisdir/\$file\" | $SED -n 's/.*-> //p'\`
+  done
+
+  # Usually 'no', except on cygwin/mingw when embedded into
+  # the cwrapper.
+  WRAPPER_SCRIPT_BELONGS_IN_OBJDIR=$func_emit_wrapper_arg1
+  if test \"\$WRAPPER_SCRIPT_BELONGS_IN_OBJDIR\" = \"yes\"; then
+    # special case for '.'
+    if test \"\$thisdir\" = \".\"; then
+      thisdir=\`pwd\`
+    fi
+    # remove .libs from thisdir
+    case \"\$thisdir\" in
+    *[\\\\/]$objdir ) thisdir=\`\$ECHO \"\$thisdir\" | $SED 's%[\\\\/][^\\\\/]*$%%'\` ;;
+    $objdir )   thisdir=. ;;
+    esac
+  fi
+
+  # Try to get the absolute directory name.
+  absdir=\`cd \"\$thisdir\" && pwd\`
+  test -n \"\$absdir\" && thisdir=\"\$absdir\"
+"
+
+	if test "$fast_install" = yes; then
+	  $ECHO "\
+  program=lt-'$outputname'$exeext
+  progdir=\"\$thisdir/$objdir\"
+
+  if test ! -f \"\$progdir/\$program\" ||
+     { file=\`ls -1dt \"\$progdir/\$program\" \"\$progdir/../\$program\" 2>/dev/null | ${SED} 1q\`; \\
+       test \"X\$file\" != \"X\$progdir/\$program\"; }; then
+
+    file=\"\$\$-\$program\"
+
+    if test ! -d \"\$progdir\"; then
+      $MKDIR \"\$progdir\"
+    else
+      $RM \"\$progdir/\$file\"
+    fi"
+
+	  $ECHO "\
+
+    # relink executable if necessary
+    if test -n \"\$relink_command\"; then
+      if relink_command_output=\`eval \$relink_command 2>&1\`; then :
+      else
+	$ECHO \"\$relink_command_output\" >&2
+	$RM \"\$progdir/\$file\"
+	exit 1
+      fi
+    fi
+
+    $MV \"\$progdir/\$file\" \"\$progdir/\$program\" 2>/dev/null ||
+    { $RM \"\$progdir/\$program\";
+      $MV \"\$progdir/\$file\" \"\$progdir/\$program\"; }
+    $RM \"\$progdir/\$file\"
+  fi"
+	else
+	  $ECHO "\
+  program='$outputname'
+  progdir=\"\$thisdir/$objdir\"
+"
+	fi
+
+	$ECHO "\
+
+  if test -f \"\$progdir/\$program\"; then"
+
+	# fixup the dll searchpath if we need to.
+	#
+	# Fix the DLL searchpath if we need to.  Do this before prepending
+	# to shlibpath, because on Windows, both are PATH and uninstalled
+	# libraries must come first.
+	if test -n "$dllsearchpath"; then
+	  $ECHO "\
+    # Add the dll search path components to the executable PATH
+    PATH=$dllsearchpath:\$PATH
+"
+	fi
+
+	# Export our shlibpath_var if we have one.
+	if test "$shlibpath_overrides_runpath" = yes && test -n "$shlibpath_var" && test -n "$temp_rpath"; then
+	  $ECHO "\
+    # Add our own library path to $shlibpath_var
+    $shlibpath_var=\"$temp_rpath\$$shlibpath_var\"
+
+    # Some systems cannot cope with colon-terminated $shlibpath_var
+    # The second colon is a workaround for a bug in BeOS R4 sed
+    $shlibpath_var=\`\$ECHO \"\$$shlibpath_var\" | $SED 's/::*\$//'\`
+
+    export $shlibpath_var
+"
+	fi
+
+	$ECHO "\
+    if test \"\$libtool_execute_magic\" != \"$magic\"; then
+      # Run the actual program with our arguments.
+      func_exec_program \${1+\"\$@\"}
+    fi
+  else
+    # The program doesn't exist.
+    \$ECHO \"\$0: error: \\\`\$progdir/\$program' does not exist\" 1>&2
+    \$ECHO \"This script is just a wrapper for \$program.\" 1>&2
+    \$ECHO \"See the $PACKAGE documentation for more information.\" 1>&2
+    exit 1
+  fi
+fi\
+"
+}
+
+
+# func_emit_cwrapperexe_src
+# emit the source code for a wrapper executable on stdout
+# Must ONLY be called from within func_mode_link because
+# it depends on a number of variable set therein.
+func_emit_cwrapperexe_src ()
+{
+	cat <<EOF
+
+/* $cwrappersource - temporary wrapper executable for $objdir/$outputname
+   Generated by $PROGRAM (GNU $PACKAGE$TIMESTAMP) $VERSION
+
+   The $output program cannot be directly executed until all the libtool
+   libraries that it depends on are installed.
+
+   This wrapper executable should never be moved out of the build directory.
+   If it is, it will not operate correctly.
+*/
+EOF
+	    cat <<"EOF"
+#ifdef _MSC_VER
+# define _CRT_SECURE_NO_DEPRECATE 1
+#endif
+#include <stdio.h>
+#include <stdlib.h>
+#ifdef _MSC_VER
+# include <direct.h>
+# include <process.h>
+# include <io.h>
+#else
+# include <unistd.h>
+# include <stdint.h>
+# ifdef __CYGWIN__
+#  include <io.h>
+# endif
+#endif
+#include <malloc.h>
+#include <stdarg.h>
+#include <assert.h>
+#include <string.h>
+#include <ctype.h>
+#include <errno.h>
+#include <fcntl.h>
+#include <sys/stat.h>
+
+/* declarations of non-ANSI functions */
+#if defined(__MINGW32__)
+# ifdef __STRICT_ANSI__
+int _putenv (const char *);
+# endif
+#elif defined(__CYGWIN__)
+# ifdef __STRICT_ANSI__
+char *realpath (const char *, char *);
+int putenv (char *);
+int setenv (const char *, const char *, int);
+# endif
+/* #elif defined (other platforms) ... */
+#endif
+
+/* portability defines, excluding path handling macros */
+#if defined(_MSC_VER)
+# define setmode _setmode
+# define stat    _stat
+# define chmod   _chmod
+# define getcwd  _getcwd
+# define putenv  _putenv
+# define S_IXUSR _S_IEXEC
+# ifndef _INTPTR_T_DEFINED
+#  define _INTPTR_T_DEFINED
+#  define intptr_t int
+# endif
+#elif defined(__MINGW32__)
+# define setmode _setmode
+# define stat    _stat
+# define chmod   _chmod
+# define getcwd  _getcwd
+# define putenv  _putenv
+#elif defined(__CYGWIN__)
+# define HAVE_SETENV
+# define FOPEN_WB "wb"
+/* #elif defined (other platforms) ... */
+#endif
+
+#if defined(PATH_MAX)
+# define LT_PATHMAX PATH_MAX
+#elif defined(MAXPATHLEN)
+# define LT_PATHMAX MAXPATHLEN
+#else
+# define LT_PATHMAX 1024
+#endif
+
+#ifndef S_IXOTH
+# define S_IXOTH 0
+#endif
+#ifndef S_IXGRP
+# define S_IXGRP 0
+#endif
+
+/* path handling portability macros */
+#ifndef DIR_SEPARATOR
+# define DIR_SEPARATOR '/'
+# define PATH_SEPARATOR ':'
+#endif
+
+#if defined (_WIN32) || defined (__MSDOS__) || defined (__DJGPP__) || \
+  defined (__OS2__)
+# define HAVE_DOS_BASED_FILE_SYSTEM
+# define FOPEN_WB "wb"
+# ifndef DIR_SEPARATOR_2
+#  define DIR_SEPARATOR_2 '\\'
+# endif
+# ifndef PATH_SEPARATOR_2
+#  define PATH_SEPARATOR_2 ';'
+# endif
+#endif
+
+#ifndef DIR_SEPARATOR_2
+# define IS_DIR_SEPARATOR(ch) ((ch) == DIR_SEPARATOR)
+#else /* DIR_SEPARATOR_2 */
+# define IS_DIR_SEPARATOR(ch) \
+	(((ch) == DIR_SEPARATOR) || ((ch) == DIR_SEPARATOR_2))
+#endif /* DIR_SEPARATOR_2 */
+
+#ifndef PATH_SEPARATOR_2
+# define IS_PATH_SEPARATOR(ch) ((ch) == PATH_SEPARATOR)
+#else /* PATH_SEPARATOR_2 */
+# define IS_PATH_SEPARATOR(ch) ((ch) == PATH_SEPARATOR_2)
+#endif /* PATH_SEPARATOR_2 */
+
+#ifndef FOPEN_WB
+# define FOPEN_WB "w"
+#endif
+#ifndef _O_BINARY
+# define _O_BINARY 0
+#endif
+
+#define XMALLOC(type, num)      ((type *) xmalloc ((num) * sizeof(type)))
+#define XFREE(stale) do { \
+  if (stale) { free ((void *) stale); stale = 0; } \
+} while (0)
+
+#if defined(LT_DEBUGWRAPPER)
+static int lt_debug = 1;
+#else
+static int lt_debug = 0;
+#endif
+
+const char *program_name = "libtool-wrapper"; /* in case xstrdup fails */
+
+void *xmalloc (size_t num);
+char *xstrdup (const char *string);
+const char *base_name (const char *name);
+char *find_executable (const char *wrapper);
+char *chase_symlinks (const char *pathspec);
+int make_executable (const char *path);
+int check_executable (const char *path);
+char *strendzap (char *str, const char *pat);
+void lt_debugprintf (const char *file, int line, const char *fmt, ...);
+void lt_fatal (const char *file, int line, const char *message, ...);
+static const char *nonnull (const char *s);
+static const char *nonempty (const char *s);
+void lt_setenv (const char *name, const char *value);
+char *lt_extend_str (const char *orig_value, const char *add, int to_end);
+void lt_update_exe_path (const char *name, const char *value);
+void lt_update_lib_path (const char *name, const char *value);
+char **prepare_spawn (char **argv);
+void lt_dump_script (FILE *f);
+EOF
+
+	    cat <<EOF
+volatile const char * MAGIC_EXE = "$magic_exe";
+const char * LIB_PATH_VARNAME = "$shlibpath_var";
+EOF
+
+	    if test "$shlibpath_overrides_runpath" = yes && test -n "$shlibpath_var" && test -n "$temp_rpath"; then
+              func_to_host_path "$temp_rpath"
+	      cat <<EOF
+const char * LIB_PATH_VALUE   = "$func_to_host_path_result";
+EOF
+	    else
+	      cat <<"EOF"
+const char * LIB_PATH_VALUE   = "";
+EOF
+	    fi
+
+	    if test -n "$dllsearchpath"; then
+              func_to_host_path "$dllsearchpath:"
+	      cat <<EOF
+const char * EXE_PATH_VARNAME = "PATH";
+const char * EXE_PATH_VALUE   = "$func_to_host_path_result";
+EOF
+	    else
+	      cat <<"EOF"
+const char * EXE_PATH_VARNAME = "";
+const char * EXE_PATH_VALUE   = "";
+EOF
+	    fi
+
+	    if test "$fast_install" = yes; then
+	      cat <<EOF
+const char * TARGET_PROGRAM_NAME = "lt-$outputname"; /* hopefully, no .exe */
+EOF
+	    else
+	      cat <<EOF
+const char * TARGET_PROGRAM_NAME = "$outputname"; /* hopefully, no .exe */
+EOF
+	    fi
+
+
+	    cat <<"EOF"
+
+#define LTWRAPPER_OPTION_PREFIX         "--lt-"
+
+static const char *ltwrapper_option_prefix = LTWRAPPER_OPTION_PREFIX;
+static const char *dumpscript_opt       = LTWRAPPER_OPTION_PREFIX "dump-script";
+static const char *debug_opt            = LTWRAPPER_OPTION_PREFIX "debug";
+
+int
+main (int argc, char *argv[])
+{
+  char **newargz;
+  int  newargc;
+  char *tmp_pathspec;
+  char *actual_cwrapper_path;
+  char *actual_cwrapper_name;
+  char *target_name;
+  char *lt_argv_zero;
+  intptr_t rval = 127;
+
+  int i;
+
+  program_name = (char *) xstrdup (base_name (argv[0]));
+  newargz = XMALLOC (char *, argc + 1);
+
+  /* very simple arg parsing; don't want to rely on getopt
+   * also, copy all non cwrapper options to newargz, except
+   * argz[0], which is handled differently
+   */
+  newargc=0;
+  for (i = 1; i < argc; i++)
+    {
+      if (strcmp (argv[i], dumpscript_opt) == 0)
+	{
+EOF
+	    case "$host" in
+	      *mingw* | *cygwin* )
+		# make stdout use "unix" line endings
+		echo "          setmode(1,_O_BINARY);"
+		;;
+	      esac
+
+	    cat <<"EOF"
+	  lt_dump_script (stdout);
+	  return 0;
+	}
+      if (strcmp (argv[i], debug_opt) == 0)
+	{
+          lt_debug = 1;
+          continue;
+	}
+      if (strcmp (argv[i], ltwrapper_option_prefix) == 0)
+        {
+          /* however, if there is an option in the LTWRAPPER_OPTION_PREFIX
+             namespace, but it is not one of the ones we know about and
+             have already dealt with, above (inluding dump-script), then
+             report an error. Otherwise, targets might begin to believe
+             they are allowed to use options in the LTWRAPPER_OPTION_PREFIX
+             namespace. The first time any user complains about this, we'll
+             need to make LTWRAPPER_OPTION_PREFIX a configure-time option
+             or a configure.ac-settable value.
+           */
+          lt_fatal (__FILE__, __LINE__,
+		    "unrecognized %s option: '%s'",
+                    ltwrapper_option_prefix, argv[i]);
+        }
+      /* otherwise ... */
+      newargz[++newargc] = xstrdup (argv[i]);
+    }
+  newargz[++newargc] = NULL;
+
+EOF
+	    cat <<EOF
+  /* The GNU banner must be the first non-error debug message */
+  lt_debugprintf (__FILE__, __LINE__, "libtool wrapper (GNU $PACKAGE$TIMESTAMP) $VERSION\n");
+EOF
+	    cat <<"EOF"
+  lt_debugprintf (__FILE__, __LINE__, "(main) argv[0]: %s\n", argv[0]);
+  lt_debugprintf (__FILE__, __LINE__, "(main) program_name: %s\n", program_name);
+
+  tmp_pathspec = find_executable (argv[0]);
+  if (tmp_pathspec == NULL)
+    lt_fatal (__FILE__, __LINE__, "couldn't find %s", argv[0]);
+  lt_debugprintf (__FILE__, __LINE__,
+                  "(main) found exe (before symlink chase) at: %s\n",
+		  tmp_pathspec);
+
+  actual_cwrapper_path = chase_symlinks (tmp_pathspec);
+  lt_debugprintf (__FILE__, __LINE__,
+                  "(main) found exe (after symlink chase) at: %s\n",
+		  actual_cwrapper_path);
+  XFREE (tmp_pathspec);
+
+  actual_cwrapper_name = xstrdup (base_name (actual_cwrapper_path));
+  strendzap (actual_cwrapper_path, actual_cwrapper_name);
+
+  /* wrapper name transforms */
+  strendzap (actual_cwrapper_name, ".exe");
+  tmp_pathspec = lt_extend_str (actual_cwrapper_name, ".exe", 1);
+  XFREE (actual_cwrapper_name);
+  actual_cwrapper_name = tmp_pathspec;
+  tmp_pathspec = 0;
+
+  /* target_name transforms -- use actual target program name; might have lt- prefix */
+  target_name = xstrdup (base_name (TARGET_PROGRAM_NAME));
+  strendzap (target_name, ".exe");
+  tmp_pathspec = lt_extend_str (target_name, ".exe", 1);
+  XFREE (target_name);
+  target_name = tmp_pathspec;
+  tmp_pathspec = 0;
+
+  lt_debugprintf (__FILE__, __LINE__,
+		  "(main) libtool target name: %s\n",
+		  target_name);
+EOF
+
+	    cat <<EOF
+  newargz[0] =
+    XMALLOC (char, (strlen (actual_cwrapper_path) +
+		    strlen ("$objdir") + 1 + strlen (actual_cwrapper_name) + 1));
+  strcpy (newargz[0], actual_cwrapper_path);
+  strcat (newargz[0], "$objdir");
+  strcat (newargz[0], "/");
+EOF
+
+	    cat <<"EOF"
+  /* stop here, and copy so we don't have to do this twice */
+  tmp_pathspec = xstrdup (newargz[0]);
+
+  /* do NOT want the lt- prefix here, so use actual_cwrapper_name */
+  strcat (newargz[0], actual_cwrapper_name);
+
+  /* DO want the lt- prefix here if it exists, so use target_name */
+  lt_argv_zero = lt_extend_str (tmp_pathspec, target_name, 1);
+  XFREE (tmp_pathspec);
+  tmp_pathspec = NULL;
+EOF
+
+	    case $host_os in
+	      mingw*)
+	    cat <<"EOF"
+  {
+    char* p;
+    while ((p = strchr (newargz[0], '\\')) != NULL)
+      {
+	*p = '/';
+      }
+    while ((p = strchr (lt_argv_zero, '\\')) != NULL)
+      {
+	*p = '/';
+      }
+  }
+EOF
+	    ;;
+	    esac
+
+	    cat <<"EOF"
+  XFREE (target_name);
+  XFREE (actual_cwrapper_path);
+  XFREE (actual_cwrapper_name);
+
+  lt_setenv ("BIN_SH", "xpg4"); /* for Tru64 */
+  lt_setenv ("DUALCASE", "1");  /* for MSK sh */
+  /* Update the DLL searchpath.  EXE_PATH_VALUE ($dllsearchpath) must
+     be prepended before (that is, appear after) LIB_PATH_VALUE ($temp_rpath)
+     because on Windows, both *_VARNAMEs are PATH but uninstalled
+     libraries must come first. */
+  lt_update_exe_path (EXE_PATH_VARNAME, EXE_PATH_VALUE);
+  lt_update_lib_path (LIB_PATH_VARNAME, LIB_PATH_VALUE);
+
+  lt_debugprintf (__FILE__, __LINE__, "(main) lt_argv_zero: %s\n",
+		  nonnull (lt_argv_zero));
+  for (i = 0; i < newargc; i++)
+    {
+      lt_debugprintf (__FILE__, __LINE__, "(main) newargz[%d]: %s\n",
+		      i, nonnull (newargz[i]));
+    }
+
+EOF
+
+	    case $host_os in
+	      mingw*)
+		cat <<"EOF"
+  /* execv doesn't actually work on mingw as expected on unix */
+  newargz = prepare_spawn (newargz);
+  rval = _spawnv (_P_WAIT, lt_argv_zero, (const char * const *) newargz);
+  if (rval == -1)
+    {
+      /* failed to start process */
+      lt_debugprintf (__FILE__, __LINE__,
+		      "(main) failed to launch target \"%s\": %s\n",
+		      lt_argv_zero, nonnull (strerror (errno)));
+      return 127;
+    }
+  return rval;
+EOF
+		;;
+	      *)
+		cat <<"EOF"
+  execv (lt_argv_zero, newargz);
+  return rval; /* =127, but avoids unused variable warning */
+EOF
+		;;
+	    esac
+
+	    cat <<"EOF"
+}
+
+void *
+xmalloc (size_t num)
+{
+  void *p = (void *) malloc (num);
+  if (!p)
+    lt_fatal (__FILE__, __LINE__, "memory exhausted");
+
+  return p;
+}
+
+char *
+xstrdup (const char *string)
+{
+  return string ? strcpy ((char *) xmalloc (strlen (string) + 1),
+			  string) : NULL;
+}
+
+const char *
+base_name (const char *name)
+{
+  const char *base;
+
+#if defined (HAVE_DOS_BASED_FILE_SYSTEM)
+  /* Skip over the disk name in MSDOS pathnames. */
+  if (isalpha ((unsigned char) name[0]) && name[1] == ':')
+    name += 2;
+#endif
+
+  for (base = name; *name; name++)
+    if (IS_DIR_SEPARATOR (*name))
+      base = name + 1;
+  return base;
+}
+
+int
+check_executable (const char *path)
+{
+  struct stat st;
+
+  lt_debugprintf (__FILE__, __LINE__, "(check_executable): %s\n",
+                  nonempty (path));
+  if ((!path) || (!*path))
+    return 0;
+
+  if ((stat (path, &st) >= 0)
+      && (st.st_mode & (S_IXUSR | S_IXGRP | S_IXOTH)))
+    return 1;
+  else
+    return 0;
+}
+
+int
+make_executable (const char *path)
+{
+  int rval = 0;
+  struct stat st;
+
+  lt_debugprintf (__FILE__, __LINE__, "(make_executable): %s\n",
+                  nonempty (path));
+  if ((!path) || (!*path))
+    return 0;
+
+  if (stat (path, &st) >= 0)
+    {
+      rval = chmod (path, st.st_mode | S_IXOTH | S_IXGRP | S_IXUSR);
+    }
+  return rval;
+}
+
+/* Searches for the full path of the wrapper.  Returns
+   newly allocated full path name if found, NULL otherwise
+   Does not chase symlinks, even on platforms that support them.
+*/
+char *
+find_executable (const char *wrapper)
+{
+  int has_slash = 0;
+  const char *p;
+  const char *p_next;
+  /* static buffer for getcwd */
+  char tmp[LT_PATHMAX + 1];
+  int tmp_len;
+  char *concat_name;
+
+  lt_debugprintf (__FILE__, __LINE__, "(find_executable): %s\n",
+                  nonempty (wrapper));
+
+  if ((wrapper == NULL) || (*wrapper == '\0'))
+    return NULL;
+
+  /* Absolute path? */
+#if defined (HAVE_DOS_BASED_FILE_SYSTEM)
+  if (isalpha ((unsigned char) wrapper[0]) && wrapper[1] == ':')
+    {
+      concat_name = xstrdup (wrapper);
+      if (check_executable (concat_name))
+	return concat_name;
+      XFREE (concat_name);
+    }
+  else
+    {
+#endif
+      if (IS_DIR_SEPARATOR (wrapper[0]))
+	{
+	  concat_name = xstrdup (wrapper);
+	  if (check_executable (concat_name))
+	    return concat_name;
+	  XFREE (concat_name);
+	}
+#if defined (HAVE_DOS_BASED_FILE_SYSTEM)
+    }
+#endif
+
+  for (p = wrapper; *p; p++)
+    if (*p == '/')
+      {
+	has_slash = 1;
+	break;
+      }
+  if (!has_slash)
+    {
+      /* no slashes; search PATH */
+      const char *path = getenv ("PATH");
+      if (path != NULL)
+	{
+	  for (p = path; *p; p = p_next)
+	    {
+	      const char *q;
+	      size_t p_len;
+	      for (q = p; *q; q++)
+		if (IS_PATH_SEPARATOR (*q))
+		  break;
+	      p_len = q - p;
+	      p_next = (*q == '\0' ? q : q + 1);
+	      if (p_len == 0)
+		{
+		  /* empty path: current directory */
+		  if (getcwd (tmp, LT_PATHMAX) == NULL)
+		    lt_fatal (__FILE__, __LINE__, "getcwd failed: %s",
+                              nonnull (strerror (errno)));
+		  tmp_len = strlen (tmp);
+		  concat_name =
+		    XMALLOC (char, tmp_len + 1 + strlen (wrapper) + 1);
+		  memcpy (concat_name, tmp, tmp_len);
+		  concat_name[tmp_len] = '/';
+		  strcpy (concat_name + tmp_len + 1, wrapper);
+		}
+	      else
+		{
+		  concat_name =
+		    XMALLOC (char, p_len + 1 + strlen (wrapper) + 1);
+		  memcpy (concat_name, p, p_len);
+		  concat_name[p_len] = '/';
+		  strcpy (concat_name + p_len + 1, wrapper);
+		}
+	      if (check_executable (concat_name))
+		return concat_name;
+	      XFREE (concat_name);
+	    }
+	}
+      /* not found in PATH; assume curdir */
+    }
+  /* Relative path | not found in path: prepend cwd */
+  if (getcwd (tmp, LT_PATHMAX) == NULL)
+    lt_fatal (__FILE__, __LINE__, "getcwd failed: %s",
+              nonnull (strerror (errno)));
+  tmp_len = strlen (tmp);
+  concat_name = XMALLOC (char, tmp_len + 1 + strlen (wrapper) + 1);
+  memcpy (concat_name, tmp, tmp_len);
+  concat_name[tmp_len] = '/';
+  strcpy (concat_name + tmp_len + 1, wrapper);
+
+  if (check_executable (concat_name))
+    return concat_name;
+  XFREE (concat_name);
+  return NULL;
+}
+
+char *
+chase_symlinks (const char *pathspec)
+{
+#ifndef S_ISLNK
+  return xstrdup (pathspec);
+#else
+  char buf[LT_PATHMAX];
+  struct stat s;
+  char *tmp_pathspec = xstrdup (pathspec);
+  char *p;
+  int has_symlinks = 0;
+  while (strlen (tmp_pathspec) && !has_symlinks)
+    {
+      lt_debugprintf (__FILE__, __LINE__,
+		      "checking path component for symlinks: %s\n",
+		      tmp_pathspec);
+      if (lstat (tmp_pathspec, &s) == 0)
+	{
+	  if (S_ISLNK (s.st_mode) != 0)
+	    {
+	      has_symlinks = 1;
+	      break;
+	    }
+
+	  /* search backwards for last DIR_SEPARATOR */
+	  p = tmp_pathspec + strlen (tmp_pathspec) - 1;
+	  while ((p > tmp_pathspec) && (!IS_DIR_SEPARATOR (*p)))
+	    p--;
+	  if ((p == tmp_pathspec) && (!IS_DIR_SEPARATOR (*p)))
+	    {
+	      /* no more DIR_SEPARATORS left */
+	      break;
+	    }
+	  *p = '\0';
+	}
+      else
+	{
+	  lt_fatal (__FILE__, __LINE__,
+		    "error accessing file \"%s\": %s",
+		    tmp_pathspec, nonnull (strerror (errno)));
+	}
+    }
+  XFREE (tmp_pathspec);
+
+  if (!has_symlinks)
+    {
+      return xstrdup (pathspec);
+    }
+
+  tmp_pathspec = realpath (pathspec, buf);
+  if (tmp_pathspec == 0)
+    {
+      lt_fatal (__FILE__, __LINE__,
+		"could not follow symlinks for %s", pathspec);
+    }
+  return xstrdup (tmp_pathspec);
+#endif
+}
+
+char *
+strendzap (char *str, const char *pat)
+{
+  size_t len, patlen;
+
+  assert (str != NULL);
+  assert (pat != NULL);
+
+  len = strlen (str);
+  patlen = strlen (pat);
+
+  if (patlen <= len)
+    {
+      str += len - patlen;
+      if (strcmp (str, pat) == 0)
+	*str = '\0';
+    }
+  return str;
+}
+
+void
+lt_debugprintf (const char *file, int line, const char *fmt, ...)
+{
+  va_list args;
+  if (lt_debug)
+    {
+      (void) fprintf (stderr, "%s:%s:%d: ", program_name, file, line);
+      va_start (args, fmt);
+      (void) vfprintf (stderr, fmt, args);
+      va_end (args);
+    }
+}
+
+static void
+lt_error_core (int exit_status, const char *file,
+	       int line, const char *mode,
+	       const char *message, va_list ap)
+{
+  fprintf (stderr, "%s:%s:%d: %s: ", program_name, file, line, mode);
+  vfprintf (stderr, message, ap);
+  fprintf (stderr, ".\n");
+
+  if (exit_status >= 0)
+    exit (exit_status);
+}
+
+void
+lt_fatal (const char *file, int line, const char *message, ...)
+{
+  va_list ap;
+  va_start (ap, message);
+  lt_error_core (EXIT_FAILURE, file, line, "FATAL", message, ap);
+  va_end (ap);
+}
+
+static const char *
+nonnull (const char *s)
+{
+  return s ? s : "(null)";
+}
+
+static const char *
+nonempty (const char *s)
+{
+  return (s && !*s) ? "(empty)" : nonnull (s);
+}
+
+void
+lt_setenv (const char *name, const char *value)
+{
+  lt_debugprintf (__FILE__, __LINE__,
+		  "(lt_setenv) setting '%s' to '%s'\n",
+                  nonnull (name), nonnull (value));
+  {
+#ifdef HAVE_SETENV
+    /* always make a copy, for consistency with !HAVE_SETENV */
+    char *str = xstrdup (value);
+    setenv (name, str, 1);
+#else
+    int len = strlen (name) + 1 + strlen (value) + 1;
+    char *str = XMALLOC (char, len);
+    sprintf (str, "%s=%s", name, value);
+    if (putenv (str) != EXIT_SUCCESS)
+      {
+        XFREE (str);
+      }
+#endif
+  }
+}
+
+char *
+lt_extend_str (const char *orig_value, const char *add, int to_end)
+{
+  char *new_value;
+  if (orig_value && *orig_value)
+    {
+      int orig_value_len = strlen (orig_value);
+      int add_len = strlen (add);
+      new_value = XMALLOC (char, add_len + orig_value_len + 1);
+      if (to_end)
+        {
+          strcpy (new_value, orig_value);
+          strcpy (new_value + orig_value_len, add);
+        }
+      else
+        {
+          strcpy (new_value, add);
+          strcpy (new_value + add_len, orig_value);
+        }
+    }
+  else
+    {
+      new_value = xstrdup (add);
+    }
+  return new_value;
+}
+
+void
+lt_update_exe_path (const char *name, const char *value)
+{
+  lt_debugprintf (__FILE__, __LINE__,
+		  "(lt_update_exe_path) modifying '%s' by prepending '%s'\n",
+                  nonnull (name), nonnull (value));
+
+  if (name && *name && value && *value)
+    {
+      char *new_value = lt_extend_str (getenv (name), value, 0);
+      /* some systems can't cope with a ':'-terminated path #' */
+      int len = strlen (new_value);
+      while (((len = strlen (new_value)) > 0) && IS_PATH_SEPARATOR (new_value[len-1]))
+        {
+          new_value[len-1] = '\0';
+        }
+      lt_setenv (name, new_value);
+      XFREE (new_value);
+    }
+}
+
+void
+lt_update_lib_path (const char *name, const char *value)
+{
+  lt_debugprintf (__FILE__, __LINE__,
+		  "(lt_update_lib_path) modifying '%s' by prepending '%s'\n",
+                  nonnull (name), nonnull (value));
+
+  if (name && *name && value && *value)
+    {
+      char *new_value = lt_extend_str (getenv (name), value, 0);
+      lt_setenv (name, new_value);
+      XFREE (new_value);
+    }
+}
+
+EOF
+	    case $host_os in
+	      mingw*)
+		cat <<"EOF"
+
+/* Prepares an argument vector before calling spawn().
+   Note that spawn() does not by itself call the command interpreter
+     (getenv ("COMSPEC") != NULL ? getenv ("COMSPEC") :
+      ({ OSVERSIONINFO v; v.dwOSVersionInfoSize = sizeof(OSVERSIONINFO);
+         GetVersionEx(&v);
+         v.dwPlatformId == VER_PLATFORM_WIN32_NT;
+      }) ? "cmd.exe" : "command.com").
+   Instead it simply concatenates the arguments, separated by ' ', and calls
+   CreateProcess().  We must quote the arguments since Win32 CreateProcess()
+   interprets characters like ' ', '\t', '\\', '"' (but not '<' and '>') in a
+   special way:
+   - Space and tab are interpreted as delimiters. They are not treated as
+     delimiters if they are surrounded by double quotes: "...".
+   - Unescaped double quotes are removed from the input. Their only effect is
+     that within double quotes, space and tab are treated like normal
+     characters.
+   - Backslashes not followed by double quotes are not special.
+   - But 2*n+1 backslashes followed by a double quote become
+     n backslashes followed by a double quote (n >= 0):
+       \" -> "
+       \\\" -> \"
+       \\\\\" -> \\"
+ */
+#define SHELL_SPECIAL_CHARS "\"\\ \001\002\003\004\005\006\007\010\011\012\013\014\015\016\017\020\021\022\023\024\025\026\027\030\031\032\033\034\035\036\037"
+#define SHELL_SPACE_CHARS " \001\002\003\004\005\006\007\010\011\012\013\014\015\016\017\020\021\022\023\024\025\026\027\030\031\032\033\034\035\036\037"
+char **
+prepare_spawn (char **argv)
+{
+  size_t argc;
+  char **new_argv;
+  size_t i;
+
+  /* Count number of arguments.  */
+  for (argc = 0; argv[argc] != NULL; argc++)
+    ;
+
+  /* Allocate new argument vector.  */
+  new_argv = XMALLOC (char *, argc + 1);
+
+  /* Put quoted arguments into the new argument vector.  */
+  for (i = 0; i < argc; i++)
+    {
+      const char *string = argv[i];
+
+      if (string[0] == '\0')
+	new_argv[i] = xstrdup ("\"\"");
+      else if (strpbrk (string, SHELL_SPECIAL_CHARS) != NULL)
+	{
+	  int quote_around = (strpbrk (string, SHELL_SPACE_CHARS) != NULL);
+	  size_t length;
+	  unsigned int backslashes;
+	  const char *s;
+	  char *quoted_string;
+	  char *p;
+
+	  length = 0;
+	  backslashes = 0;
+	  if (quote_around)
+	    length++;
+	  for (s = string; *s != '\0'; s++)
+	    {
+	      char c = *s;
+	      if (c == '"')
+		length += backslashes + 1;
+	      length++;
+	      if (c == '\\')
+		backslashes++;
+	      else
+		backslashes = 0;
+	    }
+	  if (quote_around)
+	    length += backslashes + 1;
+
+	  quoted_string = XMALLOC (char, length + 1);
+
+	  p = quoted_string;
+	  backslashes = 0;
+	  if (quote_around)
+	    *p++ = '"';
+	  for (s = string; *s != '\0'; s++)
+	    {
+	      char c = *s;
+	      if (c == '"')
+		{
+		  unsigned int j;
+		  for (j = backslashes + 1; j > 0; j--)
+		    *p++ = '\\';
+		}
+	      *p++ = c;
+	      if (c == '\\')
+		backslashes++;
+	      else
+		backslashes = 0;
+	    }
+	  if (quote_around)
+	    {
+	      unsigned int j;
+	      for (j = backslashes; j > 0; j--)
+		*p++ = '\\';
+	      *p++ = '"';
+	    }
+	  *p = '\0';
+
+	  new_argv[i] = quoted_string;
+	}
+      else
+	new_argv[i] = (char *) string;
+    }
+  new_argv[argc] = NULL;
+
+  return new_argv;
+}
+EOF
+		;;
+	    esac
+
+            cat <<"EOF"
+void lt_dump_script (FILE* f)
+{
+EOF
+	    func_emit_wrapper yes |
+	      $SED -n -e '
+s/^\(.\{79\}\)\(..*\)/\1\
+\2/
+h
+s/\([\\"]\)/\\\1/g
+s/$/\\n/
+s/\([^\n]*\).*/  fputs ("\1", f);/p
+g
+D'
+            cat <<"EOF"
+}
+EOF
+}
+# end: func_emit_cwrapperexe_src
+
+# func_win32_import_lib_p ARG
+# True if ARG is an import lib, as indicated by $file_magic_cmd
+func_win32_import_lib_p ()
+{
+    $opt_debug
+    case `eval $file_magic_cmd \"\$1\" 2>/dev/null | $SED -e 10q` in
+    *import*) : ;;
+    *) false ;;
+    esac
+}
+
+# func_mode_link arg...
+func_mode_link ()
+{
+    $opt_debug
+    case $host in
+    *-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-os2* | *-cegcc*)
+      # It is impossible to link a dll without this setting, and
+      # we shouldn't force the makefile maintainer to figure out
+      # which system we are compiling for in order to pass an extra
+      # flag for every libtool invocation.
+      # allow_undefined=no
+
+      # FIXME: Unfortunately, there are problems with the above when trying
+      # to make a dll which has undefined symbols, in which case not
+      # even a static library is built.  For now, we need to specify
+      # -no-undefined on the libtool link line when we can be certain
+      # that all symbols are satisfied, otherwise we get a static library.
+      allow_undefined=yes
+      ;;
+    *)
+      allow_undefined=yes
+      ;;
+    esac
+    libtool_args=$nonopt
+    base_compile="$nonopt $@"
+    compile_command=$nonopt
+    finalize_command=$nonopt
+
+    compile_rpath=
+    finalize_rpath=
+    compile_shlibpath=
+    finalize_shlibpath=
+    convenience=
+    old_convenience=
+    deplibs=
+    old_deplibs=
+    compiler_flags=
+    linker_flags=
+    dllsearchpath=
+    lib_search_path=`pwd`
+    inst_prefix_dir=
+    new_inherited_linker_flags=
+
+    avoid_version=no
+    bindir=
+    dlfiles=
+    dlprefiles=
+    dlself=no
+    export_dynamic=no
+    export_symbols=
+    export_symbols_regex=
+    generated=
+    libobjs=
+    ltlibs=
+    module=no
+    no_install=no
+    objs=
+    non_pic_objects=
+    precious_files_regex=
+    prefer_static_libs=no
+    preload=no
+    prev=
+    prevarg=
+    release=
+    rpath=
+    xrpath=
+    perm_rpath=
+    temp_rpath=
+    thread_safe=no
+    vinfo=
+    vinfo_number=no
+    weak_libs=
+    single_module="${wl}-single_module"
+    func_infer_tag $base_compile
+
+    # We need to know -static, to get the right output filenames.
+    for arg
+    do
+      case $arg in
+      -shared)
+	test "$build_libtool_libs" != yes && \
+	  func_fatal_configuration "can not build a shared library"
+	build_old_libs=no
+	break
+	;;
+      -all-static | -static | -static-libtool-libs)
+	case $arg in
+	-all-static)
+	  if test "$build_libtool_libs" = yes && test -z "$link_static_flag"; then
+	    func_warning "complete static linking is impossible in this configuration"
+	  fi
+	  if test -n "$link_static_flag"; then
+	    dlopen_self=$dlopen_self_static
+	  fi
+	  prefer_static_libs=yes
+	  ;;
+	-static)
+	  if test -z "$pic_flag" && test -n "$link_static_flag"; then
+	    dlopen_self=$dlopen_self_static
+	  fi
+	  prefer_static_libs=built
+	  ;;
+	-static-libtool-libs)
+	  if test -z "$pic_flag" && test -n "$link_static_flag"; then
+	    dlopen_self=$dlopen_self_static
+	  fi
+	  prefer_static_libs=yes
+	  ;;
+	esac
+	build_libtool_libs=no
+	build_old_libs=yes
+	break
+	;;
+      esac
+    done
+
+    # See if our shared archives depend on static archives.
+    test -n "$old_archive_from_new_cmds" && build_old_libs=yes
+
+    # Go through the arguments, transforming them on the way.
+    while test "$#" -gt 0; do
+      arg="$1"
+      shift
+      func_quote_for_eval "$arg"
+      qarg=$func_quote_for_eval_unquoted_result
+      func_append libtool_args " $func_quote_for_eval_result"
+
+      # If the previous option needs an argument, assign it.
+      if test -n "$prev"; then
+	case $prev in
+	output)
+	  func_append compile_command " @OUTPUT@"
+	  func_append finalize_command " @OUTPUT@"
+	  ;;
+	esac
+
+	case $prev in
+	bindir)
+	  bindir="$arg"
+	  prev=
+	  continue
+	  ;;
+	dlfiles|dlprefiles)
+	  if test "$preload" = no; then
+	    # Add the symbol object into the linking commands.
+	    func_append compile_command " @SYMFILE@"
+	    func_append finalize_command " @SYMFILE@"
+	    preload=yes
+	  fi
+	  case $arg in
+	  *.la | *.lo) ;;  # We handle these cases below.
+	  force)
+	    if test "$dlself" = no; then
+	      dlself=needless
+	      export_dynamic=yes
+	    fi
+	    prev=
+	    continue
+	    ;;
+	  self)
+	    if test "$prev" = dlprefiles; then
+	      dlself=yes
+	    elif test "$prev" = dlfiles && test "$dlopen_self" != yes; then
+	      dlself=yes
+	    else
+	      dlself=needless
+	      export_dynamic=yes
+	    fi
+	    prev=
+	    continue
+	    ;;
+	  *)
+	    if test "$prev" = dlfiles; then
+	      func_append dlfiles " $arg"
+	    else
+	      func_append dlprefiles " $arg"
+	    fi
+	    prev=
+	    continue
+	    ;;
+	  esac
+	  ;;
+	expsyms)
+	  export_symbols="$arg"
+	  test -f "$arg" \
+	    || func_fatal_error "symbol file \`$arg' does not exist"
+	  prev=
+	  continue
+	  ;;
+	expsyms_regex)
+	  export_symbols_regex="$arg"
+	  prev=
+	  continue
+	  ;;
+	framework)
+	  case $host in
+	    *-*-darwin*)
+	      case "$deplibs " in
+		*" $qarg.ltframework "*) ;;
+		*) func_append deplibs " $qarg.ltframework" # this is fixed later
+		   ;;
+	      esac
+	      ;;
+	  esac
+	  prev=
+	  continue
+	  ;;
+	inst_prefix)
+	  inst_prefix_dir="$arg"
+	  prev=
+	  continue
+	  ;;
+	objectlist)
+	  if test -f "$arg"; then
+	    save_arg=$arg
+	    moreargs=
+	    for fil in `cat "$save_arg"`
+	    do
+#	      func_append moreargs " $fil"
+	      arg=$fil
+	      # A libtool-controlled object.
+
+	      # Check to see that this really is a libtool object.
+	      if func_lalib_unsafe_p "$arg"; then
+		pic_object=
+		non_pic_object=
+
+		# Read the .lo file
+		func_source "$arg"
+
+		if test -z "$pic_object" ||
+		   test -z "$non_pic_object" ||
+		   test "$pic_object" = none &&
+		   test "$non_pic_object" = none; then
+		  func_fatal_error "cannot find name of object for \`$arg'"
+		fi
+
+		# Extract subdirectory from the argument.
+		func_dirname "$arg" "/" ""
+		xdir="$func_dirname_result"
+
+		if test "$pic_object" != none; then
+		  # Prepend the subdirectory the object is found in.
+		  pic_object="$xdir$pic_object"
+
+		  if test "$prev" = dlfiles; then
+		    if test "$build_libtool_libs" = yes && test "$dlopen_support" = yes; then
+		      func_append dlfiles " $pic_object"
+		      prev=
+		      continue
+		    else
+		      # If libtool objects are unsupported, then we need to preload.
+		      prev=dlprefiles
+		    fi
+		  fi
+
+		  # CHECK ME:  I think I busted this.  -Ossama
+		  if test "$prev" = dlprefiles; then
+		    # Preload the old-style object.
+		    func_append dlprefiles " $pic_object"
+		    prev=
+		  fi
+
+		  # A PIC object.
+		  func_append libobjs " $pic_object"
+		  arg="$pic_object"
+		fi
+
+		# Non-PIC object.
+		if test "$non_pic_object" != none; then
+		  # Prepend the subdirectory the object is found in.
+		  non_pic_object="$xdir$non_pic_object"
+
+		  # A standard non-PIC object
+		  func_append non_pic_objects " $non_pic_object"
+		  if test -z "$pic_object" || test "$pic_object" = none ; then
+		    arg="$non_pic_object"
+		  fi
+		else
+		  # If the PIC object exists, use it instead.
+		  # $xdir was prepended to $pic_object above.
+		  non_pic_object="$pic_object"
+		  func_append non_pic_objects " $non_pic_object"
+		fi
+	      else
+		# Only an error if not doing a dry-run.
+		if $opt_dry_run; then
+		  # Extract subdirectory from the argument.
+		  func_dirname "$arg" "/" ""
+		  xdir="$func_dirname_result"
+
+		  func_lo2o "$arg"
+		  pic_object=$xdir$objdir/$func_lo2o_result
+		  non_pic_object=$xdir$func_lo2o_result
+		  func_append libobjs " $pic_object"
+		  func_append non_pic_objects " $non_pic_object"
+	        else
+		  func_fatal_error "\`$arg' is not a valid libtool object"
+		fi
+	      fi
+	    done
+	  else
+	    func_fatal_error "link input file \`$arg' does not exist"
+	  fi
+	  arg=$save_arg
+	  prev=
+	  continue
+	  ;;
+	precious_regex)
+	  precious_files_regex="$arg"
+	  prev=
+	  continue
+	  ;;
+	release)
+	  release="-$arg"
+	  prev=
+	  continue
+	  ;;
+	rpath | xrpath)
+	  # We need an absolute path.
+	  case $arg in
+	  [\\/]* | [A-Za-z]:[\\/]*) ;;
+	  *)
+	    func_fatal_error "only absolute run-paths are allowed"
+	    ;;
+	  esac
+	  if test "$prev" = rpath; then
+	    case "$rpath " in
+	    *" $arg "*) ;;
+	    *) func_append rpath " $arg" ;;
+	    esac
+	  else
+	    case "$xrpath " in
+	    *" $arg "*) ;;
+	    *) func_append xrpath " $arg" ;;
+	    esac
+	  fi
+	  prev=
+	  continue
+	  ;;
+	shrext)
+	  shrext_cmds="$arg"
+	  prev=
+	  continue
+	  ;;
+	weak)
+	  func_append weak_libs " $arg"
+	  prev=
+	  continue
+	  ;;
+	xcclinker)
+	  func_append linker_flags " $qarg"
+	  func_append compiler_flags " $qarg"
+	  prev=
+	  func_append compile_command " $qarg"
+	  func_append finalize_command " $qarg"
+	  continue
+	  ;;
+	xcompiler)
+	  func_append compiler_flags " $qarg"
+	  prev=
+	  func_append compile_command " $qarg"
+	  func_append finalize_command " $qarg"
+	  continue
+	  ;;
+	xlinker)
+	  func_append linker_flags " $qarg"
+	  func_append compiler_flags " $wl$qarg"
+	  prev=
+	  func_append compile_command " $wl$qarg"
+	  func_append finalize_command " $wl$qarg"
+	  continue
+	  ;;
+	*)
+	  eval "$prev=\"\$arg\""
+	  prev=
+	  continue
+	  ;;
+	esac
+      fi # test -n "$prev"
+
+      prevarg="$arg"
+
+      case $arg in
+      -all-static)
+	if test -n "$link_static_flag"; then
+	  # See comment for -static flag below, for more details.
+	  func_append compile_command " $link_static_flag"
+	  func_append finalize_command " $link_static_flag"
+	fi
+	continue
+	;;
+
+      -allow-undefined)
+	# FIXME: remove this flag sometime in the future.
+	func_fatal_error "\`-allow-undefined' must not be used because it is the default"
+	;;
+
+      -avoid-version)
+	avoid_version=yes
+	continue
+	;;
+
+      -bindir)
+	prev=bindir
+	continue
+	;;
+
+      -dlopen)
+	prev=dlfiles
+	continue
+	;;
+
+      -dlpreopen)
+	prev=dlprefiles
+	continue
+	;;
+
+      -export-dynamic)
+	export_dynamic=yes
+	continue
+	;;
+
+      -export-symbols | -export-symbols-regex)
+	if test -n "$export_symbols" || test -n "$export_symbols_regex"; then
+	  func_fatal_error "more than one -exported-symbols argument is not allowed"
+	fi
+	if test "X$arg" = "X-export-symbols"; then
+	  prev=expsyms
+	else
+	  prev=expsyms_regex
+	fi
+	continue
+	;;
+
+      -framework)
+	prev=framework
+	continue
+	;;
+
+      -inst-prefix-dir)
+	prev=inst_prefix
+	continue
+	;;
+
+      # The native IRIX linker understands -LANG:*, -LIST:* and -LNO:*
+      # so, if we see these flags be careful not to treat them like -L
+      -L[A-Z][A-Z]*:*)
+	case $with_gcc/$host in
+	no/*-*-irix* | /*-*-irix*)
+	  func_append compile_command " $arg"
+	  func_append finalize_command " $arg"
+	  ;;
+	esac
+	continue
+	;;
+
+      -L*)
+	func_stripname "-L" '' "$arg"
+	if test -z "$func_stripname_result"; then
+	  if test "$#" -gt 0; then
+	    func_fatal_error "require no space between \`-L' and \`$1'"
+	  else
+	    func_fatal_error "need path for \`-L' option"
+	  fi
+	fi
+	func_resolve_sysroot "$func_stripname_result"
+	dir=$func_resolve_sysroot_result
+	# We need an absolute path.
+	case $dir in
+	[\\/]* | [A-Za-z]:[\\/]*) ;;
+	*)
+	  absdir=`cd "$dir" && pwd`
+	  test -z "$absdir" && \
+	    func_fatal_error "cannot determine absolute directory name of \`$dir'"
+	  dir="$absdir"
+	  ;;
+	esac
+	case "$deplibs " in
+	*" -L$dir "* | *" $arg "*)
+	  # Will only happen for absolute or sysroot arguments
+	  ;;
+	*)
+	  # Preserve sysroot, but never include relative directories
+	  case $dir in
+	    [\\/]* | [A-Za-z]:[\\/]* | =*) func_append deplibs " $arg" ;;
+	    *) func_append deplibs " -L$dir" ;;
+	  esac
+	  func_append lib_search_path " $dir"
+	  ;;
+	esac
+	case $host in
+	*-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-os2* | *-cegcc*)
+	  testbindir=`$ECHO "$dir" | $SED 's*/lib$*/bin*'`
+	  case :$dllsearchpath: in
+	  *":$dir:"*) ;;
+	  ::) dllsearchpath=$dir;;
+	  *) func_append dllsearchpath ":$dir";;
+	  esac
+	  case :$dllsearchpath: in
+	  *":$testbindir:"*) ;;
+	  ::) dllsearchpath=$testbindir;;
+	  *) func_append dllsearchpath ":$testbindir";;
+	  esac
+	  ;;
+	esac
+	continue
+	;;
+
+      -l*)
+	if test "X$arg" = "X-lc" || test "X$arg" = "X-lm"; then
+	  case $host in
+	  *-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-beos* | *-cegcc* | *-*-haiku*)
+	    # These systems don't actually have a C or math library (as such)
+	    continue
+	    ;;
+	  *-*-os2*)
+	    # These systems don't actually have a C library (as such)
+	    test "X$arg" = "X-lc" && continue
+	    ;;
+	  *-*-openbsd* | *-*-freebsd* | *-*-dragonfly*)
+	    # Do not include libc due to us having libc/libc_r.
+	    test "X$arg" = "X-lc" && continue
+	    ;;
+	  *-*-rhapsody* | *-*-darwin1.[012])
+	    # Rhapsody C and math libraries are in the System framework
+	    func_append deplibs " System.ltframework"
+	    continue
+	    ;;
+	  *-*-sco3.2v5* | *-*-sco5v6*)
+	    # Causes problems with __ctype
+	    test "X$arg" = "X-lc" && continue
+	    ;;
+	  *-*-sysv4.2uw2* | *-*-sysv5* | *-*-unixware* | *-*-OpenUNIX*)
+	    # Compiler inserts libc in the correct place for threads to work
+	    test "X$arg" = "X-lc" && continue
+	    ;;
+	  esac
+	elif test "X$arg" = "X-lc_r"; then
+	 case $host in
+	 *-*-openbsd* | *-*-freebsd* | *-*-dragonfly*)
+	   # Do not include libc_r directly, use -pthread flag.
+	   continue
+	   ;;
+	 esac
+	fi
+	func_append deplibs " $arg"
+	continue
+	;;
+
+      -module)
+	module=yes
+	continue
+	;;
+
+      # Tru64 UNIX uses -model [arg] to determine the layout of C++
+      # classes, name mangling, and exception handling.
+      # Darwin uses the -arch flag to determine output architecture.
+      -model|-arch|-isysroot|--sysroot)
+	func_append compiler_flags " $arg"
+	func_append compile_command " $arg"
+	func_append finalize_command " $arg"
+	prev=xcompiler
+	continue
+	;;
+
+      -mt|-mthreads|-kthread|-Kthread|-pthread|-pthreads|--thread-safe \
+      |-threads|-fopenmp|-openmp|-mp|-xopenmp|-omp|-qsmp=*)
+	func_append compiler_flags " $arg"
+	func_append compile_command " $arg"
+	func_append finalize_command " $arg"
+	case "$new_inherited_linker_flags " in
+	    *" $arg "*) ;;
+	    * ) func_append new_inherited_linker_flags " $arg" ;;
+	esac
+	continue
+	;;
+
+      -multi_module)
+	single_module="${wl}-multi_module"
+	continue
+	;;
+
+      -no-fast-install)
+	fast_install=no
+	continue
+	;;
+
+      -no-install)
+	case $host in
+	*-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-os2* | *-*-darwin* | *-cegcc*)
+	  # The PATH hackery in wrapper scripts is required on Windows
+	  # and Darwin in order for the loader to find any dlls it needs.
+	  func_warning "\`-no-install' is ignored for $host"
+	  func_warning "assuming \`-no-fast-install' instead"
+	  fast_install=no
+	  ;;
+	*) no_install=yes ;;
+	esac
+	continue
+	;;
+
+      -no-undefined)
+	allow_undefined=no
+	continue
+	;;
+
+      -objectlist)
+	prev=objectlist
+	continue
+	;;
+
+      -o) prev=output ;;
+
+      -precious-files-regex)
+	prev=precious_regex
+	continue
+	;;
+
+      -release)
+	prev=release
+	continue
+	;;
+
+      -rpath)
+	prev=rpath
+	continue
+	;;
+
+      -R)
+	prev=xrpath
+	continue
+	;;
+
+      -R*)
+	func_stripname '-R' '' "$arg"
+	dir=$func_stripname_result
+	# We need an absolute path.
+	case $dir in
+	[\\/]* | [A-Za-z]:[\\/]*) ;;
+	=*)
+	  func_stripname '=' '' "$dir"
+	  dir=$lt_sysroot$func_stripname_result
+	  ;;
+	*)
+	  func_fatal_error "only absolute run-paths are allowed"
+	  ;;
+	esac
+	case "$xrpath " in
+	*" $dir "*) ;;
+	*) func_append xrpath " $dir" ;;
+	esac
+	continue
+	;;
+
+      -shared)
+	# The effects of -shared are defined in a previous loop.
+	continue
+	;;
+
+      -shrext)
+	prev=shrext
+	continue
+	;;
+
+      -static | -static-libtool-libs)
+	# The effects of -static are defined in a previous loop.
+	# We used to do the same as -all-static on platforms that
+	# didn't have a PIC flag, but the assumption that the effects
+	# would be equivalent was wrong.  It would break on at least
+	# Digital Unix and AIX.
+	continue
+	;;
+
+      -thread-safe)
+	thread_safe=yes
+	continue
+	;;
+
+      -version-info)
+	prev=vinfo
+	continue
+	;;
+
+      -version-number)
+	prev=vinfo
+	vinfo_number=yes
+	continue
+	;;
+
+      -weak)
+        prev=weak
+	continue
+	;;
+
+      -Wc,*)
+	func_stripname '-Wc,' '' "$arg"
+	args=$func_stripname_result
+	arg=
+	save_ifs="$IFS"; IFS=','
+	for flag in $args; do
+	  IFS="$save_ifs"
+          func_quote_for_eval "$flag"
+	  func_append arg " $func_quote_for_eval_result"
+	  func_append compiler_flags " $func_quote_for_eval_result"
+	done
+	IFS="$save_ifs"
+	func_stripname ' ' '' "$arg"
+	arg=$func_stripname_result
+	;;
+
+      -Wl,*)
+	func_stripname '-Wl,' '' "$arg"
+	args=$func_stripname_result
+	arg=
+	save_ifs="$IFS"; IFS=','
+	for flag in $args; do
+	  IFS="$save_ifs"
+          func_quote_for_eval "$flag"
+	  func_append arg " $wl$func_quote_for_eval_result"
+	  func_append compiler_flags " $wl$func_quote_for_eval_result"
+	  func_append linker_flags " $func_quote_for_eval_result"
+	done
+	IFS="$save_ifs"
+	func_stripname ' ' '' "$arg"
+	arg=$func_stripname_result
+	;;
+
+      -Xcompiler)
+	prev=xcompiler
+	continue
+	;;
+
+      -Xlinker)
+	prev=xlinker
+	continue
+	;;
+
+      -XCClinker)
+	prev=xcclinker
+	continue
+	;;
+
+      # -msg_* for osf cc
+      -msg_*)
+	func_quote_for_eval "$arg"
+	arg="$func_quote_for_eval_result"
+	;;
+
+      # Flags to be passed through unchanged, with rationale:
+      # -64, -mips[0-9]      enable 64-bit mode for the SGI compiler
+      # -r[0-9][0-9]*        specify processor for the SGI compiler
+      # -xarch=*, -xtarget=* enable 64-bit mode for the Sun compiler
+      # +DA*, +DD*           enable 64-bit mode for the HP compiler
+      # -q*                  compiler args for the IBM compiler
+      # -m*, -t[45]*, -txscale* architecture-specific flags for GCC
+      # -F/path              path to uninstalled frameworks, gcc on darwin
+      # -p, -pg, --coverage, -fprofile-*  profiling flags for GCC
+      # @file                GCC response files
+      # -tp=*                Portland pgcc target processor selection
+      # --sysroot=*          for sysroot support
+      # -O*, -flto*, -fwhopr*, -fuse-linker-plugin GCC link-time optimization
+      -64|-mips[0-9]|-r[0-9][0-9]*|-xarch=*|-xtarget=*|+DA*|+DD*|-q*|-m*| \
+      -t[45]*|-txscale*|-p|-pg|--coverage|-fprofile-*|-F*|@*|-tp=*|--sysroot=*| \
+      -O*|-flto*|-fwhopr*|-fuse-linker-plugin)
+        func_quote_for_eval "$arg"
+	arg="$func_quote_for_eval_result"
+        func_append compile_command " $arg"
+        func_append finalize_command " $arg"
+        func_append compiler_flags " $arg"
+        continue
+        ;;
+
+      # Some other compiler flag.
+      -* | +*)
+        func_quote_for_eval "$arg"
+	arg="$func_quote_for_eval_result"
+	;;
+
+      *.$objext)
+	# A standard object.
+	func_append objs " $arg"
+	;;
+
+      *.lo)
+	# A libtool-controlled object.
+
+	# Check to see that this really is a libtool object.
+	if func_lalib_unsafe_p "$arg"; then
+	  pic_object=
+	  non_pic_object=
+
+	  # Read the .lo file
+	  func_source "$arg"
+
+	  if test -z "$pic_object" ||
+	     test -z "$non_pic_object" ||
+	     test "$pic_object" = none &&
+	     test "$non_pic_object" = none; then
+	    func_fatal_error "cannot find name of object for \`$arg'"
+	  fi
+
+	  # Extract subdirectory from the argument.
+	  func_dirname "$arg" "/" ""
+	  xdir="$func_dirname_result"
+
+	  if test "$pic_object" != none; then
+	    # Prepend the subdirectory the object is found in.
+	    pic_object="$xdir$pic_object"
+
+	    if test "$prev" = dlfiles; then
+	      if test "$build_libtool_libs" = yes && test "$dlopen_support" = yes; then
+		func_append dlfiles " $pic_object"
+		prev=
+		continue
+	      else
+		# If libtool objects are unsupported, then we need to preload.
+		prev=dlprefiles
+	      fi
+	    fi
+
+	    # CHECK ME:  I think I busted this.  -Ossama
+	    if test "$prev" = dlprefiles; then
+	      # Preload the old-style object.
+	      func_append dlprefiles " $pic_object"
+	      prev=
+	    fi
+
+	    # A PIC object.
+	    func_append libobjs " $pic_object"
+	    arg="$pic_object"
+	  fi
+
+	  # Non-PIC object.
+	  if test "$non_pic_object" != none; then
+	    # Prepend the subdirectory the object is found in.
+	    non_pic_object="$xdir$non_pic_object"
+
+	    # A standard non-PIC object
+	    func_append non_pic_objects " $non_pic_object"
+	    if test -z "$pic_object" || test "$pic_object" = none ; then
+	      arg="$non_pic_object"
+	    fi
+	  else
+	    # If the PIC object exists, use it instead.
+	    # $xdir was prepended to $pic_object above.
+	    non_pic_object="$pic_object"
+	    func_append non_pic_objects " $non_pic_object"
+	  fi
+	else
+	  # Only an error if not doing a dry-run.
+	  if $opt_dry_run; then
+	    # Extract subdirectory from the argument.
+	    func_dirname "$arg" "/" ""
+	    xdir="$func_dirname_result"
+
+	    func_lo2o "$arg"
+	    pic_object=$xdir$objdir/$func_lo2o_result
+	    non_pic_object=$xdir$func_lo2o_result
+	    func_append libobjs " $pic_object"
+	    func_append non_pic_objects " $non_pic_object"
+	  else
+	    func_fatal_error "\`$arg' is not a valid libtool object"
+	  fi
+	fi
+	;;
+
+      *.$libext)
+	# An archive.
+	func_append deplibs " $arg"
+	func_append old_deplibs " $arg"
+	continue
+	;;
+
+      *.la)
+	# A libtool-controlled library.
+
+	func_resolve_sysroot "$arg"
+	if test "$prev" = dlfiles; then
+	  # This library was specified with -dlopen.
+	  func_append dlfiles " $func_resolve_sysroot_result"
+	  prev=
+	elif test "$prev" = dlprefiles; then
+	  # The library was specified with -dlpreopen.
+	  func_append dlprefiles " $func_resolve_sysroot_result"
+	  prev=
+	else
+	  func_append deplibs " $func_resolve_sysroot_result"
+	fi
+	continue
+	;;
+
+      # Some other compiler argument.
+      *)
+	# Unknown arguments in both finalize_command and compile_command need
+	# to be aesthetically quoted because they are evaled later.
+	func_quote_for_eval "$arg"
+	arg="$func_quote_for_eval_result"
+	;;
+      esac # arg
+
+      # Now actually substitute the argument into the commands.
+      if test -n "$arg"; then
+	func_append compile_command " $arg"
+	func_append finalize_command " $arg"
+      fi
+    done # argument parsing loop
+
+    test -n "$prev" && \
+      func_fatal_help "the \`$prevarg' option requires an argument"
+
+    if test "$export_dynamic" = yes && test -n "$export_dynamic_flag_spec"; then
+      eval arg=\"$export_dynamic_flag_spec\"
+      func_append compile_command " $arg"
+      func_append finalize_command " $arg"
+    fi
+
+    oldlibs=
+    # calculate the name of the file, without its directory
+    func_basename "$output"
+    outputname="$func_basename_result"
+    libobjs_save="$libobjs"
+
+    if test -n "$shlibpath_var"; then
+      # get the directories listed in $shlibpath_var
+      eval shlib_search_path=\`\$ECHO \"\${$shlibpath_var}\" \| \$SED \'s/:/ /g\'\`
+    else
+      shlib_search_path=
+    fi
+    eval sys_lib_search_path=\"$sys_lib_search_path_spec\"
+    eval sys_lib_dlsearch_path=\"$sys_lib_dlsearch_path_spec\"
+
+    func_dirname "$output" "/" ""
+    output_objdir="$func_dirname_result$objdir"
+    func_to_tool_file "$output_objdir/"
+    tool_output_objdir=$func_to_tool_file_result
+    # Create the object directory.
+    func_mkdir_p "$output_objdir"
+
+    # Determine the type of output
+    case $output in
+    "")
+      func_fatal_help "you must specify an output file"
+      ;;
+    *.$libext) linkmode=oldlib ;;
+    *.lo | *.$objext) linkmode=obj ;;
+    *.la) linkmode=lib ;;
+    *) linkmode=prog ;; # Anything else should be a program.
+    esac
+
+    specialdeplibs=
+
+    libs=
+    # Find all interdependent deplibs by searching for libraries
+    # that are linked more than once (e.g. -la -lb -la)
+    for deplib in $deplibs; do
+      if $opt_preserve_dup_deps ; then
+	case "$libs " in
+	*" $deplib "*) func_append specialdeplibs " $deplib" ;;
+	esac
+      fi
+      func_append libs " $deplib"
+    done
+
+    if test "$linkmode" = lib; then
+      libs="$predeps $libs $compiler_lib_search_path $postdeps"
+
+      # Compute libraries that are listed more than once in $predeps
+      # $postdeps and mark them as special (i.e., whose duplicates are
+      # not to be eliminated).
+      pre_post_deps=
+      if $opt_duplicate_compiler_generated_deps; then
+	for pre_post_dep in $predeps $postdeps; do
+	  case "$pre_post_deps " in
+	  *" $pre_post_dep "*) func_append specialdeplibs " $pre_post_deps" ;;
+	  esac
+	  func_append pre_post_deps " $pre_post_dep"
+	done
+      fi
+      pre_post_deps=
+    fi
+
+    deplibs=
+    newdependency_libs=
+    newlib_search_path=
+    need_relink=no # whether we're linking any uninstalled libtool libraries
+    notinst_deplibs= # not-installed libtool libraries
+    notinst_path= # paths that contain not-installed libtool libraries
+
+    case $linkmode in
+    lib)
+	passes="conv dlpreopen link"
+	for file in $dlfiles $dlprefiles; do
+	  case $file in
+	  *.la) ;;
+	  *)
+	    func_fatal_help "libraries can \`-dlopen' only libtool libraries: $file"
+	    ;;
+	  esac
+	done
+	;;
+    prog)
+	compile_deplibs=
+	finalize_deplibs=
+	alldeplibs=no
+	newdlfiles=
+	newdlprefiles=
+	passes="conv scan dlopen dlpreopen link"
+	;;
+    *)  passes="conv"
+	;;
+    esac
+
+    for pass in $passes; do
+      # The preopen pass in lib mode reverses $deplibs; put it back here
+      # so that -L comes before libs that need it for instance...
+      if test "$linkmode,$pass" = "lib,link"; then
+	## FIXME: Find the place where the list is rebuilt in the wrong
+	##        order, and fix it there properly
+        tmp_deplibs=
+	for deplib in $deplibs; do
+	  tmp_deplibs="$deplib $tmp_deplibs"
+	done
+	deplibs="$tmp_deplibs"
+      fi
+
+      if test "$linkmode,$pass" = "lib,link" ||
+	 test "$linkmode,$pass" = "prog,scan"; then
+	libs="$deplibs"
+	deplibs=
+      fi
+      if test "$linkmode" = prog; then
+	case $pass in
+	dlopen) libs="$dlfiles" ;;
+	dlpreopen) libs="$dlprefiles" ;;
+	link)
+	  libs="$deplibs %DEPLIBS%"
+	  test "X$link_all_deplibs" != Xno && libs="$libs $dependency_libs"
+	  ;;
+	esac
+      fi
+      if test "$linkmode,$pass" = "lib,dlpreopen"; then
+	# Collect and forward deplibs of preopened libtool libs
+	for lib in $dlprefiles; do
+	  # Ignore non-libtool-libs
+	  dependency_libs=
+	  func_resolve_sysroot "$lib"
+	  case $lib in
+	  *.la)	func_source "$func_resolve_sysroot_result" ;;
+	  esac
+
+	  # Collect preopened libtool deplibs, except any this library
+	  # has declared as weak libs
+	  for deplib in $dependency_libs; do
+	    func_basename "$deplib"
+            deplib_base=$func_basename_result
+	    case " $weak_libs " in
+	    *" $deplib_base "*) ;;
+	    *) func_append deplibs " $deplib" ;;
+	    esac
+	  done
+	done
+	libs="$dlprefiles"
+      fi
+      if test "$pass" = dlopen; then
+	# Collect dlpreopened libraries
+	save_deplibs="$deplibs"
+	deplibs=
+      fi
+
+      for deplib in $libs; do
+	lib=
+	found=no
+	case $deplib in
+	-mt|-mthreads|-kthread|-Kthread|-pthread|-pthreads|--thread-safe \
+        |-threads|-fopenmp|-openmp|-mp|-xopenmp|-omp|-qsmp=*)
+	  if test "$linkmode,$pass" = "prog,link"; then
+	    compile_deplibs="$deplib $compile_deplibs"
+	    finalize_deplibs="$deplib $finalize_deplibs"
+	  else
+	    func_append compiler_flags " $deplib"
+	    if test "$linkmode" = lib ; then
+		case "$new_inherited_linker_flags " in
+		    *" $deplib "*) ;;
+		    * ) func_append new_inherited_linker_flags " $deplib" ;;
+		esac
+	    fi
+	  fi
+	  continue
+	  ;;
+	-l*)
+	  if test "$linkmode" != lib && test "$linkmode" != prog; then
+	    func_warning "\`-l' is ignored for archives/objects"
+	    continue
+	  fi
+	  func_stripname '-l' '' "$deplib"
+	  name=$func_stripname_result
+	  if test "$linkmode" = lib; then
+	    searchdirs="$newlib_search_path $lib_search_path $compiler_lib_search_dirs $sys_lib_search_path $shlib_search_path"
+	  else
+	    searchdirs="$newlib_search_path $lib_search_path $sys_lib_search_path $shlib_search_path"
+	  fi
+	  for searchdir in $searchdirs; do
+	    for search_ext in .la $std_shrext .so .a; do
+	      # Search the libtool library
+	      lib="$searchdir/lib${name}${search_ext}"
+	      if test -f "$lib"; then
+		if test "$search_ext" = ".la"; then
+		  found=yes
+		else
+		  found=no
+		fi
+		break 2
+	      fi
+	    done
+	  done
+	  if test "$found" != yes; then
+	    # deplib doesn't seem to be a libtool library
+	    if test "$linkmode,$pass" = "prog,link"; then
+	      compile_deplibs="$deplib $compile_deplibs"
+	      finalize_deplibs="$deplib $finalize_deplibs"
+	    else
+	      deplibs="$deplib $deplibs"
+	      test "$linkmode" = lib && newdependency_libs="$deplib $newdependency_libs"
+	    fi
+	    continue
+	  else # deplib is a libtool library
+	    # If $allow_libtool_libs_with_static_runtimes && $deplib is a stdlib,
+	    # We need to do some special things here, and not later.
+	    if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+	      case " $predeps $postdeps " in
+	      *" $deplib "*)
+		if func_lalib_p "$lib"; then
+		  library_names=
+		  old_library=
+		  func_source "$lib"
+		  for l in $old_library $library_names; do
+		    ll="$l"
+		  done
+		  if test "X$ll" = "X$old_library" ; then # only static version available
+		    found=no
+		    func_dirname "$lib" "" "."
+		    ladir="$func_dirname_result"
+		    lib=$ladir/$old_library
+		    if test "$linkmode,$pass" = "prog,link"; then
+		      compile_deplibs="$deplib $compile_deplibs"
+		      finalize_deplibs="$deplib $finalize_deplibs"
+		    else
+		      deplibs="$deplib $deplibs"
+		      test "$linkmode" = lib && newdependency_libs="$deplib $newdependency_libs"
+		    fi
+		    continue
+		  fi
+		fi
+		;;
+	      *) ;;
+	      esac
+	    fi
+	  fi
+	  ;; # -l
+	*.ltframework)
+	  if test "$linkmode,$pass" = "prog,link"; then
+	    compile_deplibs="$deplib $compile_deplibs"
+	    finalize_deplibs="$deplib $finalize_deplibs"
+	  else
+	    deplibs="$deplib $deplibs"
+	    if test "$linkmode" = lib ; then
+		case "$new_inherited_linker_flags " in
+		    *" $deplib "*) ;;
+		    * ) func_append new_inherited_linker_flags " $deplib" ;;
+		esac
+	    fi
+	  fi
+	  continue
+	  ;;
+	-L*)
+	  case $linkmode in
+	  lib)
+	    deplibs="$deplib $deplibs"
+	    test "$pass" = conv && continue
+	    newdependency_libs="$deplib $newdependency_libs"
+	    func_stripname '-L' '' "$deplib"
+	    func_resolve_sysroot "$func_stripname_result"
+	    func_append newlib_search_path " $func_resolve_sysroot_result"
+	    ;;
+	  prog)
+	    if test "$pass" = conv; then
+	      deplibs="$deplib $deplibs"
+	      continue
+	    fi
+	    if test "$pass" = scan; then
+	      deplibs="$deplib $deplibs"
+	    else
+	      compile_deplibs="$deplib $compile_deplibs"
+	      finalize_deplibs="$deplib $finalize_deplibs"
+	    fi
+	    func_stripname '-L' '' "$deplib"
+	    func_resolve_sysroot "$func_stripname_result"
+	    func_append newlib_search_path " $func_resolve_sysroot_result"
+	    ;;
+	  *)
+	    func_warning "\`-L' is ignored for archives/objects"
+	    ;;
+	  esac # linkmode
+	  continue
+	  ;; # -L
+	-R*)
+	  if test "$pass" = link; then
+	    func_stripname '-R' '' "$deplib"
+	    func_resolve_sysroot "$func_stripname_result"
+	    dir=$func_resolve_sysroot_result
+	    # Make sure the xrpath contains only unique directories.
+	    case "$xrpath " in
+	    *" $dir "*) ;;
+	    *) func_append xrpath " $dir" ;;
+	    esac
+	  fi
+	  deplibs="$deplib $deplibs"
+	  continue
+	  ;;
+	*.la)
+	  func_resolve_sysroot "$deplib"
+	  lib=$func_resolve_sysroot_result
+	  ;;
+	*.$libext)
+	  if test "$pass" = conv; then
+	    deplibs="$deplib $deplibs"
+	    continue
+	  fi
+	  case $linkmode in
+	  lib)
+	    # Linking convenience modules into shared libraries is allowed,
+	    # but linking other static libraries is non-portable.
+	    case " $dlpreconveniencelibs " in
+	    *" $deplib "*) ;;
+	    *)
+	      valid_a_lib=no
+	      case $deplibs_check_method in
+		match_pattern*)
+		  set dummy $deplibs_check_method; shift
+		  match_pattern_regex=`expr "$deplibs_check_method" : "$1 \(.*\)"`
+		  if eval "\$ECHO \"$deplib\"" 2>/dev/null | $SED 10q \
+		    | $EGREP "$match_pattern_regex" > /dev/null; then
+		    valid_a_lib=yes
+		  fi
+		;;
+		pass_all)
+		  valid_a_lib=yes
+		;;
+	      esac
+	      if test "$valid_a_lib" != yes; then
+		echo
+		$ECHO "*** Warning: Trying to link with static lib archive $deplib."
+		echo "*** I have the capability to make that library automatically link in when"
+		echo "*** you link to this library.  But I can only do this if you have a"
+		echo "*** shared version of the library, which you do not appear to have"
+		echo "*** because the file extensions .$libext of this argument makes me believe"
+		echo "*** that it is just a static archive that I should not use here."
+	      else
+		echo
+		$ECHO "*** Warning: Linking the shared library $output against the"
+		$ECHO "*** static library $deplib is not portable!"
+		deplibs="$deplib $deplibs"
+	      fi
+	      ;;
+	    esac
+	    continue
+	    ;;
+	  prog)
+	    if test "$pass" != link; then
+	      deplibs="$deplib $deplibs"
+	    else
+	      compile_deplibs="$deplib $compile_deplibs"
+	      finalize_deplibs="$deplib $finalize_deplibs"
+	    fi
+	    continue
+	    ;;
+	  esac # linkmode
+	  ;; # *.$libext
+	*.lo | *.$objext)
+	  if test "$pass" = conv; then
+	    deplibs="$deplib $deplibs"
+	  elif test "$linkmode" = prog; then
+	    if test "$pass" = dlpreopen || test "$dlopen_support" != yes || test "$build_libtool_libs" = no; then
+	      # If there is no dlopen support or we're linking statically,
+	      # we need to preload.
+	      func_append newdlprefiles " $deplib"
+	      compile_deplibs="$deplib $compile_deplibs"
+	      finalize_deplibs="$deplib $finalize_deplibs"
+	    else
+	      func_append newdlfiles " $deplib"
+	    fi
+	  fi
+	  continue
+	  ;;
+	%DEPLIBS%)
+	  alldeplibs=yes
+	  continue
+	  ;;
+	esac # case $deplib
+
+	if test "$found" = yes || test -f "$lib"; then :
+	else
+	  func_fatal_error "cannot find the library \`$lib' or unhandled argument \`$deplib'"
+	fi
+
+	# Check to see that this really is a libtool archive.
+	func_lalib_unsafe_p "$lib" \
+	  || func_fatal_error "\`$lib' is not a valid libtool archive"
+
+	func_dirname "$lib" "" "."
+	ladir="$func_dirname_result"
+
+	dlname=
+	dlopen=
+	dlpreopen=
+	libdir=
+	library_names=
+	old_library=
+	inherited_linker_flags=
+	# If the library was installed with an old release of libtool,
+	# it will not redefine variables installed, or shouldnotlink
+	installed=yes
+	shouldnotlink=no
+	avoidtemprpath=
+
+
+	# Read the .la file
+	func_source "$lib"
+
+	# Convert "-framework foo" to "foo.ltframework"
+	if test -n "$inherited_linker_flags"; then
+	  tmp_inherited_linker_flags=`$ECHO "$inherited_linker_flags" | $SED 's/-framework \([^ $]*\)/\1.ltframework/g'`
+	  for tmp_inherited_linker_flag in $tmp_inherited_linker_flags; do
+	    case " $new_inherited_linker_flags " in
+	      *" $tmp_inherited_linker_flag "*) ;;
+	      *) func_append new_inherited_linker_flags " $tmp_inherited_linker_flag";;
+	    esac
+	  done
+	fi
+	dependency_libs=`$ECHO " $dependency_libs" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	if test "$linkmode,$pass" = "lib,link" ||
+	   test "$linkmode,$pass" = "prog,scan" ||
+	   { test "$linkmode" != prog && test "$linkmode" != lib; }; then
+	  test -n "$dlopen" && func_append dlfiles " $dlopen"
+	  test -n "$dlpreopen" && func_append dlprefiles " $dlpreopen"
+	fi
+
+	if test "$pass" = conv; then
+	  # Only check for convenience libraries
+	  deplibs="$lib $deplibs"
+	  if test -z "$libdir"; then
+	    if test -z "$old_library"; then
+	      func_fatal_error "cannot find name of link library for \`$lib'"
+	    fi
+	    # It is a libtool convenience library, so add in its objects.
+	    func_append convenience " $ladir/$objdir/$old_library"
+	    func_append old_convenience " $ladir/$objdir/$old_library"
+	    tmp_libs=
+	    for deplib in $dependency_libs; do
+	      deplibs="$deplib $deplibs"
+	      if $opt_preserve_dup_deps ; then
+		case "$tmp_libs " in
+		*" $deplib "*) func_append specialdeplibs " $deplib" ;;
+		esac
+	      fi
+	      func_append tmp_libs " $deplib"
+	    done
+	  elif test "$linkmode" != prog && test "$linkmode" != lib; then
+	    func_fatal_error "\`$lib' is not a convenience library"
+	  fi
+	  continue
+	fi # $pass = conv
+
+
+	# Get the name of the library we link against.
+	linklib=
+	if test -n "$old_library" &&
+	   { test "$prefer_static_libs" = yes ||
+	     test "$prefer_static_libs,$installed" = "built,no"; }; then
+	  linklib=$old_library
+	else
+	  for l in $old_library $library_names; do
+	    linklib="$l"
+	  done
+	fi
+	if test -z "$linklib"; then
+	  func_fatal_error "cannot find name of link library for \`$lib'"
+	fi
+
+	# This library was specified with -dlopen.
+	if test "$pass" = dlopen; then
+	  if test -z "$libdir"; then
+	    func_fatal_error "cannot -dlopen a convenience library: \`$lib'"
+	  fi
+	  if test -z "$dlname" ||
+	     test "$dlopen_support" != yes ||
+	     test "$build_libtool_libs" = no; then
+	    # If there is no dlname, no dlopen support or we're linking
+	    # statically, we need to preload.  We also need to preload any
+	    # dependent libraries so libltdl's deplib preloader doesn't
+	    # bomb out in the load deplibs phase.
+	    func_append dlprefiles " $lib $dependency_libs"
+	  else
+	    func_append newdlfiles " $lib"
+	  fi
+	  continue
+	fi # $pass = dlopen
+
+	# We need an absolute path.
+	case $ladir in
+	[\\/]* | [A-Za-z]:[\\/]*) abs_ladir="$ladir" ;;
+	*)
+	  abs_ladir=`cd "$ladir" && pwd`
+	  if test -z "$abs_ladir"; then
+	    func_warning "cannot determine absolute directory name of \`$ladir'"
+	    func_warning "passing it literally to the linker, although it might fail"
+	    abs_ladir="$ladir"
+	  fi
+	  ;;
+	esac
+	func_basename "$lib"
+	laname="$func_basename_result"
+
+	# Find the relevant object directory and library name.
+	if test "X$installed" = Xyes; then
+	  if test ! -f "$lt_sysroot$libdir/$linklib" && test -f "$abs_ladir/$linklib"; then
+	    func_warning "library \`$lib' was moved."
+	    dir="$ladir"
+	    absdir="$abs_ladir"
+	    libdir="$abs_ladir"
+	  else
+	    dir="$lt_sysroot$libdir"
+	    absdir="$lt_sysroot$libdir"
+	  fi
+	  test "X$hardcode_automatic" = Xyes && avoidtemprpath=yes
+	else
+	  if test ! -f "$ladir/$objdir/$linklib" && test -f "$abs_ladir/$linklib"; then
+	    dir="$ladir"
+	    absdir="$abs_ladir"
+	    # Remove this search path later
+	    func_append notinst_path " $abs_ladir"
+	  else
+	    dir="$ladir/$objdir"
+	    absdir="$abs_ladir/$objdir"
+	    # Remove this search path later
+	    func_append notinst_path " $abs_ladir"
+	  fi
+	fi # $installed = yes
+	func_stripname 'lib' '.la' "$laname"
+	name=$func_stripname_result
+
+	# This library was specified with -dlpreopen.
+	if test "$pass" = dlpreopen; then
+	  if test -z "$libdir" && test "$linkmode" = prog; then
+	    func_fatal_error "only libraries may -dlpreopen a convenience library: \`$lib'"
+	  fi
+	  case "$host" in
+	    # special handling for platforms with PE-DLLs.
+	    *cygwin* | *mingw* | *cegcc* )
+	      # Linker will automatically link against shared library if both
+	      # static and shared are present.  Therefore, ensure we extract
+	      # symbols from the import library if a shared library is present
+	      # (otherwise, the dlopen module name will be incorrect).  We do
+	      # this by putting the import library name into $newdlprefiles.
+	      # We recover the dlopen module name by 'saving' the la file
+	      # name in a special purpose variable, and (later) extracting the
+	      # dlname from the la file.
+	      if test -n "$dlname"; then
+	        func_tr_sh "$dir/$linklib"
+	        eval "libfile_$func_tr_sh_result=\$abs_ladir/\$laname"
+	        func_append newdlprefiles " $dir/$linklib"
+	      else
+	        func_append newdlprefiles " $dir/$old_library"
+	        # Keep a list of preopened convenience libraries to check
+	        # that they are being used correctly in the link pass.
+	        test -z "$libdir" && \
+	          func_append dlpreconveniencelibs " $dir/$old_library"
+	      fi
+	    ;;
+	    * )
+	      # Prefer using a static library (so that no silly _DYNAMIC symbols
+	      # are required to link).
+	      if test -n "$old_library"; then
+	        func_append newdlprefiles " $dir/$old_library"
+	        # Keep a list of preopened convenience libraries to check
+	        # that they are being used correctly in the link pass.
+	        test -z "$libdir" && \
+	          func_append dlpreconveniencelibs " $dir/$old_library"
+	      # Otherwise, use the dlname, so that lt_dlopen finds it.
+	      elif test -n "$dlname"; then
+	        func_append newdlprefiles " $dir/$dlname"
+	      else
+	        func_append newdlprefiles " $dir/$linklib"
+	      fi
+	    ;;
+	  esac
+	fi # $pass = dlpreopen
+
+	if test -z "$libdir"; then
+	  # Link the convenience library
+	  if test "$linkmode" = lib; then
+	    deplibs="$dir/$old_library $deplibs"
+	  elif test "$linkmode,$pass" = "prog,link"; then
+	    compile_deplibs="$dir/$old_library $compile_deplibs"
+	    finalize_deplibs="$dir/$old_library $finalize_deplibs"
+	  else
+	    deplibs="$lib $deplibs" # used for prog,scan pass
+	  fi
+	  continue
+	fi
+
+
+	if test "$linkmode" = prog && test "$pass" != link; then
+	  func_append newlib_search_path " $ladir"
+	  deplibs="$lib $deplibs"
+
+	  linkalldeplibs=no
+	  if test "$link_all_deplibs" != no || test -z "$library_names" ||
+	     test "$build_libtool_libs" = no; then
+	    linkalldeplibs=yes
+	  fi
+
+	  tmp_libs=
+	  for deplib in $dependency_libs; do
+	    case $deplib in
+	    -L*) func_stripname '-L' '' "$deplib"
+	         func_resolve_sysroot "$func_stripname_result"
+	         func_append newlib_search_path " $func_resolve_sysroot_result"
+		 ;;
+	    esac
+	    # Need to link against all dependency_libs?
+	    if test "$linkalldeplibs" = yes; then
+	      deplibs="$deplib $deplibs"
+	    else
+	      # Need to hardcode shared library paths
+	      # or/and link against static libraries
+	      newdependency_libs="$deplib $newdependency_libs"
+	    fi
+	    if $opt_preserve_dup_deps ; then
+	      case "$tmp_libs " in
+	      *" $deplib "*) func_append specialdeplibs " $deplib" ;;
+	      esac
+	    fi
+	    func_append tmp_libs " $deplib"
+	  done # for deplib
+	  continue
+	fi # $linkmode = prog...
+
+	if test "$linkmode,$pass" = "prog,link"; then
+	  if test -n "$library_names" &&
+	     { { test "$prefer_static_libs" = no ||
+	         test "$prefer_static_libs,$installed" = "built,yes"; } ||
+	       test -z "$old_library"; }; then
+	    # We need to hardcode the library path
+	    if test -n "$shlibpath_var" && test -z "$avoidtemprpath" ; then
+	      # Make sure the rpath contains only unique directories.
+	      case "$temp_rpath:" in
+	      *"$absdir:"*) ;;
+	      *) func_append temp_rpath "$absdir:" ;;
+	      esac
+	    fi
+
+	    # Hardcode the library path.
+	    # Skip directories that are in the system default run-time
+	    # search path.
+	    case " $sys_lib_dlsearch_path " in
+	    *" $absdir "*) ;;
+	    *)
+	      case "$compile_rpath " in
+	      *" $absdir "*) ;;
+	      *) func_append compile_rpath " $absdir" ;;
+	      esac
+	      ;;
+	    esac
+	    case " $sys_lib_dlsearch_path " in
+	    *" $libdir "*) ;;
+	    *)
+	      case "$finalize_rpath " in
+	      *" $libdir "*) ;;
+	      *) func_append finalize_rpath " $libdir" ;;
+	      esac
+	      ;;
+	    esac
+	  fi # $linkmode,$pass = prog,link...
+
+	  if test "$alldeplibs" = yes &&
+	     { test "$deplibs_check_method" = pass_all ||
+	       { test "$build_libtool_libs" = yes &&
+		 test -n "$library_names"; }; }; then
+	    # We only need to search for static libraries
+	    continue
+	  fi
+	fi
+
+	link_static=no # Whether the deplib will be linked statically
+	use_static_libs=$prefer_static_libs
+	if test "$use_static_libs" = built && test "$installed" = yes; then
+	  use_static_libs=no
+	fi
+	if test -n "$library_names" &&
+	   { test "$use_static_libs" = no || test -z "$old_library"; }; then
+	  case $host in
+	  *cygwin* | *mingw* | *cegcc*)
+	      # No point in relinking DLLs because paths are not encoded
+	      func_append notinst_deplibs " $lib"
+	      need_relink=no
+	    ;;
+	  *)
+	    if test "$installed" = no; then
+	      func_append notinst_deplibs " $lib"
+	      need_relink=yes
+	    fi
+	    ;;
+	  esac
+	  # This is a shared library
+
+	  # Warn about portability, can't link against -module's on some
+	  # systems (darwin).  Don't bleat about dlopened modules though!
+	  dlopenmodule=""
+	  for dlpremoduletest in $dlprefiles; do
+	    if test "X$dlpremoduletest" = "X$lib"; then
+	      dlopenmodule="$dlpremoduletest"
+	      break
+	    fi
+	  done
+	  if test -z "$dlopenmodule" && test "$shouldnotlink" = yes && test "$pass" = link; then
+	    echo
+	    if test "$linkmode" = prog; then
+	      $ECHO "*** Warning: Linking the executable $output against the loadable module"
+	    else
+	      $ECHO "*** Warning: Linking the shared library $output against the loadable module"
+	    fi
+	    $ECHO "*** $linklib is not portable!"
+	  fi
+	  if test "$linkmode" = lib &&
+	     test "$hardcode_into_libs" = yes; then
+	    # Hardcode the library path.
+	    # Skip directories that are in the system default run-time
+	    # search path.
+	    case " $sys_lib_dlsearch_path " in
+	    *" $absdir "*) ;;
+	    *)
+	      case "$compile_rpath " in
+	      *" $absdir "*) ;;
+	      *) func_append compile_rpath " $absdir" ;;
+	      esac
+	      ;;
+	    esac
+	    case " $sys_lib_dlsearch_path " in
+	    *" $libdir "*) ;;
+	    *)
+	      case "$finalize_rpath " in
+	      *" $libdir "*) ;;
+	      *) func_append finalize_rpath " $libdir" ;;
+	      esac
+	      ;;
+	    esac
+	  fi
+
+	  if test -n "$old_archive_from_expsyms_cmds"; then
+	    # figure out the soname
+	    set dummy $library_names
+	    shift
+	    realname="$1"
+	    shift
+	    libname=`eval "\\$ECHO \"$libname_spec\""`
+	    # use dlname if we got it. it's perfectly good, no?
+	    if test -n "$dlname"; then
+	      soname="$dlname"
+	    elif test -n "$soname_spec"; then
+	      # bleh windows
+	      case $host in
+	      *cygwin* | mingw* | *cegcc*)
+	        func_arith $current - $age
+		major=$func_arith_result
+		versuffix="-$major"
+		;;
+	      esac
+	      eval soname=\"$soname_spec\"
+	    else
+	      soname="$realname"
+	    fi
+
+	    # Make a new name for the extract_expsyms_cmds to use
+	    soroot="$soname"
+	    func_basename "$soroot"
+	    soname="$func_basename_result"
+	    func_stripname 'lib' '.dll' "$soname"
+	    newlib=libimp-$func_stripname_result.a
+
+	    # If the library has no export list, then create one now
+	    if test -f "$output_objdir/$soname-def"; then :
+	    else
+	      func_verbose "extracting exported symbol list from \`$soname'"
+	      func_execute_cmds "$extract_expsyms_cmds" 'exit $?'
+	    fi
+
+	    # Create $newlib
+	    if test -f "$output_objdir/$newlib"; then :; else
+	      func_verbose "generating import library for \`$soname'"
+	      func_execute_cmds "$old_archive_from_expsyms_cmds" 'exit $?'
+	    fi
+	    # make sure the library variables are pointing to the new library
+	    dir=$output_objdir
+	    linklib=$newlib
+	  fi # test -n "$old_archive_from_expsyms_cmds"
+
+	  if test "$linkmode" = prog || test "$opt_mode" != relink; then
+	    add_shlibpath=
+	    add_dir=
+	    add=
+	    lib_linked=yes
+	    case $hardcode_action in
+	    immediate | unsupported)
+	      if test "$hardcode_direct" = no; then
+		add="$dir/$linklib"
+		case $host in
+		  *-*-sco3.2v5.0.[024]*) add_dir="-L$dir" ;;
+		  *-*-sysv4*uw2*) add_dir="-L$dir" ;;
+		  *-*-sysv5OpenUNIX* | *-*-sysv5UnixWare7.[01].[10]* | \
+		    *-*-unixware7*) add_dir="-L$dir" ;;
+		  *-*-darwin* )
+		    # if the lib is a (non-dlopened) module then we can not
+		    # link against it, someone is ignoring the earlier warnings
+		    if /usr/bin/file -L $add 2> /dev/null |
+			 $GREP ": [^:]* bundle" >/dev/null ; then
+		      if test "X$dlopenmodule" != "X$lib"; then
+			$ECHO "*** Warning: lib $linklib is a module, not a shared library"
+			if test -z "$old_library" ; then
+			  echo
+			  echo "*** And there doesn't seem to be a static archive available"
+			  echo "*** The link will probably fail, sorry"
+			else
+			  add="$dir/$old_library"
+			fi
+		      elif test -n "$old_library"; then
+			add="$dir/$old_library"
+		      fi
+		    fi
+		esac
+	      elif test "$hardcode_minus_L" = no; then
+		case $host in
+		*-*-sunos*) add_shlibpath="$dir" ;;
+		esac
+		add_dir="-L$dir"
+		add="-l$name"
+	      elif test "$hardcode_shlibpath_var" = no; then
+		add_shlibpath="$dir"
+		add="-l$name"
+	      else
+		lib_linked=no
+	      fi
+	      ;;
+	    relink)
+	      if test "$hardcode_direct" = yes &&
+	         test "$hardcode_direct_absolute" = no; then
+		add="$dir/$linklib"
+	      elif test "$hardcode_minus_L" = yes; then
+		add_dir="-L$absdir"
+		# Try looking first in the location we're being installed to.
+		if test -n "$inst_prefix_dir"; then
+		  case $libdir in
+		    [\\/]*)
+		      func_append add_dir " -L$inst_prefix_dir$libdir"
+		      ;;
+		  esac
+		fi
+		add="-l$name"
+	      elif test "$hardcode_shlibpath_var" = yes; then
+		add_shlibpath="$dir"
+		add="-l$name"
+	      else
+		lib_linked=no
+	      fi
+	      ;;
+	    *) lib_linked=no ;;
+	    esac
+
+	    if test "$lib_linked" != yes; then
+	      func_fatal_configuration "unsupported hardcode properties"
+	    fi
+
+	    if test -n "$add_shlibpath"; then
+	      case :$compile_shlibpath: in
+	      *":$add_shlibpath:"*) ;;
+	      *) func_append compile_shlibpath "$add_shlibpath:" ;;
+	      esac
+	    fi
+	    if test "$linkmode" = prog; then
+	      test -n "$add_dir" && compile_deplibs="$add_dir $compile_deplibs"
+	      test -n "$add" && compile_deplibs="$add $compile_deplibs"
+	    else
+	      test -n "$add_dir" && deplibs="$add_dir $deplibs"
+	      test -n "$add" && deplibs="$add $deplibs"
+	      if test "$hardcode_direct" != yes &&
+		 test "$hardcode_minus_L" != yes &&
+		 test "$hardcode_shlibpath_var" = yes; then
+		case :$finalize_shlibpath: in
+		*":$libdir:"*) ;;
+		*) func_append finalize_shlibpath "$libdir:" ;;
+		esac
+	      fi
+	    fi
+	  fi
+
+	  if test "$linkmode" = prog || test "$opt_mode" = relink; then
+	    add_shlibpath=
+	    add_dir=
+	    add=
+	    # Finalize command for both is simple: just hardcode it.
+	    if test "$hardcode_direct" = yes &&
+	       test "$hardcode_direct_absolute" = no; then
+	      add="$libdir/$linklib"
+	    elif test "$hardcode_minus_L" = yes; then
+	      add_dir="-L$libdir"
+	      add="-l$name"
+	    elif test "$hardcode_shlibpath_var" = yes; then
+	      case :$finalize_shlibpath: in
+	      *":$libdir:"*) ;;
+	      *) func_append finalize_shlibpath "$libdir:" ;;
+	      esac
+	      add="-l$name"
+	    elif test "$hardcode_automatic" = yes; then
+	      if test -n "$inst_prefix_dir" &&
+		 test -f "$inst_prefix_dir$libdir/$linklib" ; then
+		add="$inst_prefix_dir$libdir/$linklib"
+	      else
+		add="$libdir/$linklib"
+	      fi
+	    else
+	      # We cannot seem to hardcode it, guess we'll fake it.
+	      add_dir="-L$libdir"
+	      # Try looking first in the location we're being installed to.
+	      if test -n "$inst_prefix_dir"; then
+		case $libdir in
+		  [\\/]*)
+		    func_append add_dir " -L$inst_prefix_dir$libdir"
+		    ;;
+		esac
+	      fi
+	      add="-l$name"
+	    fi
+
+	    if test "$linkmode" = prog; then
+	      test -n "$add_dir" && finalize_deplibs="$add_dir $finalize_deplibs"
+	      test -n "$add" && finalize_deplibs="$add $finalize_deplibs"
+	    else
+	      test -n "$add_dir" && deplibs="$add_dir $deplibs"
+	      test -n "$add" && deplibs="$add $deplibs"
+	    fi
+	  fi
+	elif test "$linkmode" = prog; then
+	  # Here we assume that one of hardcode_direct or hardcode_minus_L
+	  # is not unsupported.  This is valid on all known static and
+	  # shared platforms.
+	  if test "$hardcode_direct" != unsupported; then
+	    test -n "$old_library" && linklib="$old_library"
+	    compile_deplibs="$dir/$linklib $compile_deplibs"
+	    finalize_deplibs="$dir/$linklib $finalize_deplibs"
+	  else
+	    compile_deplibs="-l$name -L$dir $compile_deplibs"
+	    finalize_deplibs="-l$name -L$dir $finalize_deplibs"
+	  fi
+	elif test "$build_libtool_libs" = yes; then
+	  # Not a shared library
+	  if test "$deplibs_check_method" != pass_all; then
+	    # We're trying link a shared library against a static one
+	    # but the system doesn't support it.
+
+	    # Just print a warning and add the library to dependency_libs so
+	    # that the program can be linked against the static library.
+	    echo
+	    $ECHO "*** Warning: This system can not link to static lib archive $lib."
+	    echo "*** I have the capability to make that library automatically link in when"
+	    echo "*** you link to this library.  But I can only do this if you have a"
+	    echo "*** shared version of the library, which you do not appear to have."
+	    if test "$module" = yes; then
+	      echo "*** But as you try to build a module library, libtool will still create "
+	      echo "*** a static module, that should work as long as the dlopening application"
+	      echo "*** is linked with the -dlopen flag to resolve symbols at runtime."
+	      if test -z "$global_symbol_pipe"; then
+		echo
+		echo "*** However, this would only work if libtool was able to extract symbol"
+		echo "*** lists from a program, using \`nm' or equivalent, but libtool could"
+		echo "*** not find such a program.  So, this module is probably useless."
+		echo "*** \`nm' from GNU binutils and a full rebuild may help."
+	      fi
+	      if test "$build_old_libs" = no; then
+		build_libtool_libs=module
+		build_old_libs=yes
+	      else
+		build_libtool_libs=no
+	      fi
+	    fi
+	  else
+	    deplibs="$dir/$old_library $deplibs"
+	    link_static=yes
+	  fi
+	fi # link shared/static library?
+
+	if test "$linkmode" = lib; then
+	  if test -n "$dependency_libs" &&
+	     { test "$hardcode_into_libs" != yes ||
+	       test "$build_old_libs" = yes ||
+	       test "$link_static" = yes; }; then
+	    # Extract -R from dependency_libs
+	    temp_deplibs=
+	    for libdir in $dependency_libs; do
+	      case $libdir in
+	      -R*) func_stripname '-R' '' "$libdir"
+	           temp_xrpath=$func_stripname_result
+		   case " $xrpath " in
+		   *" $temp_xrpath "*) ;;
+		   *) func_append xrpath " $temp_xrpath";;
+		   esac;;
+	      *) func_append temp_deplibs " $libdir";;
+	      esac
+	    done
+	    dependency_libs="$temp_deplibs"
+	  fi
+
+	  func_append newlib_search_path " $absdir"
+	  # Link against this library
+	  test "$link_static" = no && newdependency_libs="$abs_ladir/$laname $newdependency_libs"
+	  # ... and its dependency_libs
+	  tmp_libs=
+	  for deplib in $dependency_libs; do
+	    newdependency_libs="$deplib $newdependency_libs"
+	    case $deplib in
+              -L*) func_stripname '-L' '' "$deplib"
+                   func_resolve_sysroot "$func_stripname_result";;
+              *) func_resolve_sysroot "$deplib" ;;
+            esac
+	    if $opt_preserve_dup_deps ; then
+	      case "$tmp_libs " in
+	      *" $func_resolve_sysroot_result "*)
+                func_append specialdeplibs " $func_resolve_sysroot_result" ;;
+	      esac
+	    fi
+	    func_append tmp_libs " $func_resolve_sysroot_result"
+	  done
+
+	  if test "$link_all_deplibs" != no; then
+	    # Add the search paths of all dependency libraries
+	    for deplib in $dependency_libs; do
+	      path=
+	      case $deplib in
+	      -L*) path="$deplib" ;;
+	      *.la)
+	        func_resolve_sysroot "$deplib"
+	        deplib=$func_resolve_sysroot_result
+	        func_dirname "$deplib" "" "."
+		dir=$func_dirname_result
+		# We need an absolute path.
+		case $dir in
+		[\\/]* | [A-Za-z]:[\\/]*) absdir="$dir" ;;
+		*)
+		  absdir=`cd "$dir" && pwd`
+		  if test -z "$absdir"; then
+		    func_warning "cannot determine absolute directory name of \`$dir'"
+		    absdir="$dir"
+		  fi
+		  ;;
+		esac
+		if $GREP "^installed=no" $deplib > /dev/null; then
+		case $host in
+		*-*-darwin*)
+		  depdepl=
+		  eval deplibrary_names=`${SED} -n -e 's/^library_names=\(.*\)$/\1/p' $deplib`
+		  if test -n "$deplibrary_names" ; then
+		    for tmp in $deplibrary_names ; do
+		      depdepl=$tmp
+		    done
+		    if test -f "$absdir/$objdir/$depdepl" ; then
+		      depdepl="$absdir/$objdir/$depdepl"
+		      darwin_install_name=`${OTOOL} -L $depdepl | awk '{if (NR == 2) {print $1;exit}}'`
+                      if test -z "$darwin_install_name"; then
+                          darwin_install_name=`${OTOOL64} -L $depdepl  | awk '{if (NR == 2) {print $1;exit}}'`
+                      fi
+		      func_append compiler_flags " ${wl}-dylib_file ${wl}${darwin_install_name}:${depdepl}"
+		      func_append linker_flags " -dylib_file ${darwin_install_name}:${depdepl}"
+		      path=
+		    fi
+		  fi
+		  ;;
+		*)
+		  path="-L$absdir/$objdir"
+		  ;;
+		esac
+		else
+		  eval libdir=`${SED} -n -e 's/^libdir=\(.*\)$/\1/p' $deplib`
+		  test -z "$libdir" && \
+		    func_fatal_error "\`$deplib' is not a valid libtool archive"
+		  test "$absdir" != "$libdir" && \
+		    func_warning "\`$deplib' seems to be moved"
+
+		  path="-L$absdir"
+		fi
+		;;
+	      esac
+	      case " $deplibs " in
+	      *" $path "*) ;;
+	      *) deplibs="$path $deplibs" ;;
+	      esac
+	    done
+	  fi # link_all_deplibs != no
+	fi # linkmode = lib
+      done # for deplib in $libs
+      if test "$pass" = link; then
+	if test "$linkmode" = "prog"; then
+	  compile_deplibs="$new_inherited_linker_flags $compile_deplibs"
+	  finalize_deplibs="$new_inherited_linker_flags $finalize_deplibs"
+	else
+	  compiler_flags="$compiler_flags "`$ECHO " $new_inherited_linker_flags" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	fi
+      fi
+      dependency_libs="$newdependency_libs"
+      if test "$pass" = dlpreopen; then
+	# Link the dlpreopened libraries before other libraries
+	for deplib in $save_deplibs; do
+	  deplibs="$deplib $deplibs"
+	done
+      fi
+      if test "$pass" != dlopen; then
+	if test "$pass" != conv; then
+	  # Make sure lib_search_path contains only unique directories.
+	  lib_search_path=
+	  for dir in $newlib_search_path; do
+	    case "$lib_search_path " in
+	    *" $dir "*) ;;
+	    *) func_append lib_search_path " $dir" ;;
+	    esac
+	  done
+	  newlib_search_path=
+	fi
+
+	if test "$linkmode,$pass" != "prog,link"; then
+	  vars="deplibs"
+	else
+	  vars="compile_deplibs finalize_deplibs"
+	fi
+	for var in $vars dependency_libs; do
+	  # Add libraries to $var in reverse order
+	  eval tmp_libs=\"\$$var\"
+	  new_libs=
+	  for deplib in $tmp_libs; do
+	    # FIXME: Pedantically, this is the right thing to do, so
+	    #        that some nasty dependency loop isn't accidentally
+	    #        broken:
+	    #new_libs="$deplib $new_libs"
+	    # Pragmatically, this seems to cause very few problems in
+	    # practice:
+	    case $deplib in
+	    -L*) new_libs="$deplib $new_libs" ;;
+	    -R*) ;;
+	    *)
+	      # And here is the reason: when a library appears more
+	      # than once as an explicit dependence of a library, or
+	      # is implicitly linked in more than once by the
+	      # compiler, it is considered special, and multiple
+	      # occurrences thereof are not removed.  Compare this
+	      # with having the same library being listed as a
+	      # dependency of multiple other libraries: in this case,
+	      # we know (pedantically, we assume) the library does not
+	      # need to be listed more than once, so we keep only the
+	      # last copy.  This is not always right, but it is rare
+	      # enough that we require users that really mean to play
+	      # such unportable linking tricks to link the library
+	      # using -Wl,-lname, so that libtool does not consider it
+	      # for duplicate removal.
+	      case " $specialdeplibs " in
+	      *" $deplib "*) new_libs="$deplib $new_libs" ;;
+	      *)
+		case " $new_libs " in
+		*" $deplib "*) ;;
+		*) new_libs="$deplib $new_libs" ;;
+		esac
+		;;
+	      esac
+	      ;;
+	    esac
+	  done
+	  tmp_libs=
+	  for deplib in $new_libs; do
+	    case $deplib in
+	    -L*)
+	      case " $tmp_libs " in
+	      *" $deplib "*) ;;
+	      *) func_append tmp_libs " $deplib" ;;
+	      esac
+	      ;;
+	    *) func_append tmp_libs " $deplib" ;;
+	    esac
+	  done
+	  eval $var=\"$tmp_libs\"
+	done # for var
+      fi
+      # Last step: remove runtime libs from dependency_libs
+      # (they stay in deplibs)
+      tmp_libs=
+      for i in $dependency_libs ; do
+	case " $predeps $postdeps $compiler_lib_search_path " in
+	*" $i "*)
+	  i=""
+	  ;;
+	esac
+	if test -n "$i" ; then
+	  func_append tmp_libs " $i"
+	fi
+      done
+      dependency_libs=$tmp_libs
+    done # for pass
+    if test "$linkmode" = prog; then
+      dlfiles="$newdlfiles"
+    fi
+    if test "$linkmode" = prog || test "$linkmode" = lib; then
+      dlprefiles="$newdlprefiles"
+    fi
+
+    case $linkmode in
+    oldlib)
+      if test -n "$dlfiles$dlprefiles" || test "$dlself" != no; then
+	func_warning "\`-dlopen' is ignored for archives"
+      fi
+
+      case " $deplibs" in
+      *\ -l* | *\ -L*)
+	func_warning "\`-l' and \`-L' are ignored for archives" ;;
+      esac
+
+      test -n "$rpath" && \
+	func_warning "\`-rpath' is ignored for archives"
+
+      test -n "$xrpath" && \
+	func_warning "\`-R' is ignored for archives"
+
+      test -n "$vinfo" && \
+	func_warning "\`-version-info/-version-number' is ignored for archives"
+
+      test -n "$release" && \
+	func_warning "\`-release' is ignored for archives"
+
+      test -n "$export_symbols$export_symbols_regex" && \
+	func_warning "\`-export-symbols' is ignored for archives"
+
+      # Now set the variables for building old libraries.
+      build_libtool_libs=no
+      oldlibs="$output"
+      func_append objs "$old_deplibs"
+      ;;
+
+    lib)
+      # Make sure we only generate libraries of the form `libNAME.la'.
+      case $outputname in
+      lib*)
+	func_stripname 'lib' '.la' "$outputname"
+	name=$func_stripname_result
+	eval shared_ext=\"$shrext_cmds\"
+	eval libname=\"$libname_spec\"
+	;;
+      *)
+	test "$module" = no && \
+	  func_fatal_help "libtool library \`$output' must begin with \`lib'"
+
+	if test "$need_lib_prefix" != no; then
+	  # Add the "lib" prefix for modules if required
+	  func_stripname '' '.la' "$outputname"
+	  name=$func_stripname_result
+	  eval shared_ext=\"$shrext_cmds\"
+	  eval libname=\"$libname_spec\"
+	else
+	  func_stripname '' '.la' "$outputname"
+	  libname=$func_stripname_result
+	fi
+	;;
+      esac
+
+      if test -n "$objs"; then
+	if test "$deplibs_check_method" != pass_all; then
+	  func_fatal_error "cannot build libtool library \`$output' from non-libtool objects on this host:$objs"
+	else
+	  echo
+	  $ECHO "*** Warning: Linking the shared library $output against the non-libtool"
+	  $ECHO "*** objects $objs is not portable!"
+	  func_append libobjs " $objs"
+	fi
+      fi
+
+      test "$dlself" != no && \
+	func_warning "\`-dlopen self' is ignored for libtool libraries"
+
+      set dummy $rpath
+      shift
+      test "$#" -gt 1 && \
+	func_warning "ignoring multiple \`-rpath's for a libtool library"
+
+      install_libdir="$1"
+
+      oldlibs=
+      if test -z "$rpath"; then
+	if test "$build_libtool_libs" = yes; then
+	  # Building a libtool convenience library.
+	  # Some compilers have problems with a `.al' extension so
+	  # convenience libraries should have the same extension an
+	  # archive normally would.
+	  oldlibs="$output_objdir/$libname.$libext $oldlibs"
+	  build_libtool_libs=convenience
+	  build_old_libs=yes
+	fi
+
+	test -n "$vinfo" && \
+	  func_warning "\`-version-info/-version-number' is ignored for convenience libraries"
+
+	test -n "$release" && \
+	  func_warning "\`-release' is ignored for convenience libraries"
+      else
+
+	# Parse the version information argument.
+	save_ifs="$IFS"; IFS=':'
+	set dummy $vinfo 0 0 0
+	shift
+	IFS="$save_ifs"
+
+	test -n "$7" && \
+	  func_fatal_help "too many parameters to \`-version-info'"
+
+	# convert absolute version numbers to libtool ages
+	# this retains compatibility with .la files and attempts
+	# to make the code below a bit more comprehensible
+
+	case $vinfo_number in
+	yes)
+	  number_major="$1"
+	  number_minor="$2"
+	  number_revision="$3"
+	  #
+	  # There are really only two kinds -- those that
+	  # use the current revision as the major version
+	  # and those that subtract age and use age as
+	  # a minor version.  But, then there is irix
+	  # which has an extra 1 added just for fun
+	  #
+	  case $version_type in
+	  # correct linux to gnu/linux during the next big refactor
+	  darwin|linux|osf|windows|none)
+	    func_arith $number_major + $number_minor
+	    current=$func_arith_result
+	    age="$number_minor"
+	    revision="$number_revision"
+	    ;;
+	  freebsd-aout|freebsd-elf|qnx|sunos)
+	    current="$number_major"
+	    revision="$number_minor"
+	    age="0"
+	    ;;
+	  irix|nonstopux)
+	    func_arith $number_major + $number_minor
+	    current=$func_arith_result
+	    age="$number_minor"
+	    revision="$number_minor"
+	    lt_irix_increment=no
+	    ;;
+	  *)
+	    func_fatal_configuration "$modename: unknown library version type \`$version_type'"
+	    ;;
+	  esac
+	  ;;
+	no)
+	  current="$1"
+	  revision="$2"
+	  age="$3"
+	  ;;
+	esac
+
+	# Check that each of the things are valid numbers.
+	case $current in
+	0|[1-9]|[1-9][0-9]|[1-9][0-9][0-9]|[1-9][0-9][0-9][0-9]|[1-9][0-9][0-9][0-9][0-9]) ;;
+	*)
+	  func_error "CURRENT \`$current' must be a nonnegative integer"
+	  func_fatal_error "\`$vinfo' is not valid version information"
+	  ;;
+	esac
+
+	case $revision in
+	0|[1-9]|[1-9][0-9]|[1-9][0-9][0-9]|[1-9][0-9][0-9][0-9]|[1-9][0-9][0-9][0-9][0-9]) ;;
+	*)
+	  func_error "REVISION \`$revision' must be a nonnegative integer"
+	  func_fatal_error "\`$vinfo' is not valid version information"
+	  ;;
+	esac
+
+	case $age in
+	0|[1-9]|[1-9][0-9]|[1-9][0-9][0-9]|[1-9][0-9][0-9][0-9]|[1-9][0-9][0-9][0-9][0-9]) ;;
+	*)
+	  func_error "AGE \`$age' must be a nonnegative integer"
+	  func_fatal_error "\`$vinfo' is not valid version information"
+	  ;;
+	esac
+
+	if test "$age" -gt "$current"; then
+	  func_error "AGE \`$age' is greater than the current interface number \`$current'"
+	  func_fatal_error "\`$vinfo' is not valid version information"
+	fi
+
+	# Calculate the version variables.
+	major=
+	versuffix=
+	verstring=
+	case $version_type in
+	none) ;;
+
+	darwin)
+	  # Like Linux, but with the current version available in
+	  # verstring for coding it into the library header
+	  func_arith $current - $age
+	  major=.$func_arith_result
+	  versuffix="$major.$age.$revision"
+	  # Darwin ld doesn't like 0 for these options...
+	  func_arith $current + 1
+	  minor_current=$func_arith_result
+	  xlcverstring="${wl}-compatibility_version ${wl}$minor_current ${wl}-current_version ${wl}$minor_current.$revision"
+	  verstring="-compatibility_version $minor_current -current_version $minor_current.$revision"
+	  ;;
+
+	freebsd-aout)
+	  major=".$current"
+	  versuffix=".$current.$revision";
+	  ;;
+
+	freebsd-elf)
+	  major=".$current"
+	  versuffix=".$current"
+	  ;;
+
+	irix | nonstopux)
+	  if test "X$lt_irix_increment" = "Xno"; then
+	    func_arith $current - $age
+	  else
+	    func_arith $current - $age + 1
+	  fi
+	  major=$func_arith_result
+
+	  case $version_type in
+	    nonstopux) verstring_prefix=nonstopux ;;
+	    *)         verstring_prefix=sgi ;;
+	  esac
+	  verstring="$verstring_prefix$major.$revision"
+
+	  # Add in all the interfaces that we are compatible with.
+	  loop=$revision
+	  while test "$loop" -ne 0; do
+	    func_arith $revision - $loop
+	    iface=$func_arith_result
+	    func_arith $loop - 1
+	    loop=$func_arith_result
+	    verstring="$verstring_prefix$major.$iface:$verstring"
+	  done
+
+	  # Before this point, $major must not contain `.'.
+	  major=.$major
+	  versuffix="$major.$revision"
+	  ;;
+
+	linux) # correct to gnu/linux during the next big refactor
+	  func_arith $current - $age
+	  major=.$func_arith_result
+	  versuffix="$major.$age.$revision"
+	  ;;
+
+	osf)
+	  func_arith $current - $age
+	  major=.$func_arith_result
+	  versuffix=".$current.$age.$revision"
+	  verstring="$current.$age.$revision"
+
+	  # Add in all the interfaces that we are compatible with.
+	  loop=$age
+	  while test "$loop" -ne 0; do
+	    func_arith $current - $loop
+	    iface=$func_arith_result
+	    func_arith $loop - 1
+	    loop=$func_arith_result
+	    verstring="$verstring:${iface}.0"
+	  done
+
+	  # Make executables depend on our current version.
+	  func_append verstring ":${current}.0"
+	  ;;
+
+	qnx)
+	  major=".$current"
+	  versuffix=".$current"
+	  ;;
+
+	sunos)
+	  major=".$current"
+	  versuffix=".$current.$revision"
+	  ;;
+
+	windows)
+	  # Use '-' rather than '.', since we only want one
+	  # extension on DOS 8.3 filesystems.
+	  func_arith $current - $age
+	  major=$func_arith_result
+	  versuffix="-$major"
+	  ;;
+
+	*)
+	  func_fatal_configuration "unknown library version type \`$version_type'"
+	  ;;
+	esac
+
+	# Clear the version info if we defaulted, and they specified a release.
+	if test -z "$vinfo" && test -n "$release"; then
+	  major=
+	  case $version_type in
+	  darwin)
+	    # we can't check for "0.0" in archive_cmds due to quoting
+	    # problems, so we reset it completely
+	    verstring=
+	    ;;
+	  *)
+	    verstring="0.0"
+	    ;;
+	  esac
+	  if test "$need_version" = no; then
+	    versuffix=
+	  else
+	    versuffix=".0.0"
+	  fi
+	fi
+
+	# Remove version info from name if versioning should be avoided
+	if test "$avoid_version" = yes && test "$need_version" = no; then
+	  major=
+	  versuffix=
+	  verstring=""
+	fi
+
+	# Check to see if the archive will have undefined symbols.
+	if test "$allow_undefined" = yes; then
+	  if test "$allow_undefined_flag" = unsupported; then
+	    func_warning "undefined symbols not allowed in $host shared libraries"
+	    build_libtool_libs=no
+	    build_old_libs=yes
+	  fi
+	else
+	  # Don't allow undefined symbols.
+	  allow_undefined_flag="$no_undefined_flag"
+	fi
+
+      fi
+
+      func_generate_dlsyms "$libname" "$libname" "yes"
+      func_append libobjs " $symfileobj"
+      test "X$libobjs" = "X " && libobjs=
+
+      if test "$opt_mode" != relink; then
+	# Remove our outputs, but don't remove object files since they
+	# may have been created when compiling PIC objects.
+	removelist=
+	tempremovelist=`$ECHO "$output_objdir/*"`
+	for p in $tempremovelist; do
+	  case $p in
+	    *.$objext | *.gcno)
+	       ;;
+	    $output_objdir/$outputname | $output_objdir/$libname.* | $output_objdir/${libname}${release}.*)
+	       if test "X$precious_files_regex" != "X"; then
+		 if $ECHO "$p" | $EGREP -e "$precious_files_regex" >/dev/null 2>&1
+		 then
+		   continue
+		 fi
+	       fi
+	       func_append removelist " $p"
+	       ;;
+	    *) ;;
+	  esac
+	done
+	test -n "$removelist" && \
+	  func_show_eval "${RM}r \$removelist"
+      fi
+
+      # Now set the variables for building old libraries.
+      if test "$build_old_libs" = yes && test "$build_libtool_libs" != convenience ; then
+	func_append oldlibs " $output_objdir/$libname.$libext"
+
+	# Transform .lo files to .o files.
+	oldobjs="$objs "`$ECHO "$libobjs" | $SP2NL | $SED "/\.${libext}$/d; $lo2o" | $NL2SP`
+      fi
+
+      # Eliminate all temporary directories.
+      #for path in $notinst_path; do
+      #	lib_search_path=`$ECHO "$lib_search_path " | $SED "s% $path % %g"`
+      #	deplibs=`$ECHO "$deplibs " | $SED "s% -L$path % %g"`
+      #	dependency_libs=`$ECHO "$dependency_libs " | $SED "s% -L$path % %g"`
+      #done
+
+      if test -n "$xrpath"; then
+	# If the user specified any rpath flags, then add them.
+	temp_xrpath=
+	for libdir in $xrpath; do
+	  func_replace_sysroot "$libdir"
+	  func_append temp_xrpath " -R$func_replace_sysroot_result"
+	  case "$finalize_rpath " in
+	  *" $libdir "*) ;;
+	  *) func_append finalize_rpath " $libdir" ;;
+	  esac
+	done
+	if test "$hardcode_into_libs" != yes || test "$build_old_libs" = yes; then
+	  dependency_libs="$temp_xrpath $dependency_libs"
+	fi
+      fi
+
+      # Make sure dlfiles contains only unique files that won't be dlpreopened
+      old_dlfiles="$dlfiles"
+      dlfiles=
+      for lib in $old_dlfiles; do
+	case " $dlprefiles $dlfiles " in
+	*" $lib "*) ;;
+	*) func_append dlfiles " $lib" ;;
+	esac
+      done
+
+      # Make sure dlprefiles contains only unique files
+      old_dlprefiles="$dlprefiles"
+      dlprefiles=
+      for lib in $old_dlprefiles; do
+	case "$dlprefiles " in
+	*" $lib "*) ;;
+	*) func_append dlprefiles " $lib" ;;
+	esac
+      done
+
+      if test "$build_libtool_libs" = yes; then
+	if test -n "$rpath"; then
+	  case $host in
+	  *-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-os2* | *-*-beos* | *-cegcc* | *-*-haiku*)
+	    # these systems don't actually have a c library (as such)!
+	    ;;
+	  *-*-rhapsody* | *-*-darwin1.[012])
+	    # Rhapsody C library is in the System framework
+	    func_append deplibs " System.ltframework"
+	    ;;
+	  *-*-netbsd*)
+	    # Don't link with libc until the a.out ld.so is fixed.
+	    ;;
+	  *-*-openbsd* | *-*-freebsd* | *-*-dragonfly*)
+	    # Do not include libc due to us having libc/libc_r.
+	    ;;
+	  *-*-sco3.2v5* | *-*-sco5v6*)
+	    # Causes problems with __ctype
+	    ;;
+	  *-*-sysv4.2uw2* | *-*-sysv5* | *-*-unixware* | *-*-OpenUNIX*)
+	    # Compiler inserts libc in the correct place for threads to work
+	    ;;
+	  *)
+	    # Add libc to deplibs on all other systems if necessary.
+	    if test "$build_libtool_need_lc" = "yes"; then
+	      func_append deplibs " -lc"
+	    fi
+	    ;;
+	  esac
+	fi
+
+	# Transform deplibs into only deplibs that can be linked in shared.
+	name_save=$name
+	libname_save=$libname
+	release_save=$release
+	versuffix_save=$versuffix
+	major_save=$major
+	# I'm not sure if I'm treating the release correctly.  I think
+	# release should show up in the -l (ie -lgmp5) so we don't want to
+	# add it in twice.  Is that correct?
+	release=""
+	versuffix=""
+	major=""
+	newdeplibs=
+	droppeddeps=no
+	case $deplibs_check_method in
+	pass_all)
+	  # Don't check for shared/static.  Everything works.
+	  # This might be a little naive.  We might want to check
+	  # whether the library exists or not.  But this is on
+	  # osf3 & osf4 and I'm not really sure... Just
+	  # implementing what was already the behavior.
+	  newdeplibs=$deplibs
+	  ;;
+	test_compile)
+	  # This code stresses the "libraries are programs" paradigm to its
+	  # limits. Maybe even breaks it.  We compile a program, linking it
+	  # against the deplibs as a proxy for the library.  Then we can check
+	  # whether they linked in statically or dynamically with ldd.
+	  $opt_dry_run || $RM conftest.c
+	  cat > conftest.c <<EOF
+	  int main() { return 0; }
+EOF
+	  $opt_dry_run || $RM conftest
+	  if $LTCC $LTCFLAGS -o conftest conftest.c $deplibs; then
+	    ldd_output=`ldd conftest`
+	    for i in $deplibs; do
+	      case $i in
+	      -l*)
+		func_stripname -l '' "$i"
+		name=$func_stripname_result
+		if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+		  case " $predeps $postdeps " in
+		  *" $i "*)
+		    func_append newdeplibs " $i"
+		    i=""
+		    ;;
+		  esac
+		fi
+		if test -n "$i" ; then
+		  libname=`eval "\\$ECHO \"$libname_spec\""`
+		  deplib_matches=`eval "\\$ECHO \"$library_names_spec\""`
+		  set dummy $deplib_matches; shift
+		  deplib_match=$1
+		  if test `expr "$ldd_output" : ".*$deplib_match"` -ne 0 ; then
+		    func_append newdeplibs " $i"
+		  else
+		    droppeddeps=yes
+		    echo
+		    $ECHO "*** Warning: dynamic linker does not accept needed library $i."
+		    echo "*** I have the capability to make that library automatically link in when"
+		    echo "*** you link to this library.  But I can only do this if you have a"
+		    echo "*** shared version of the library, which I believe you do not have"
+		    echo "*** because a test_compile did reveal that the linker did not use it for"
+		    echo "*** its dynamic dependency list that programs get resolved with at runtime."
+		  fi
+		fi
+		;;
+	      *)
+		func_append newdeplibs " $i"
+		;;
+	      esac
+	    done
+	  else
+	    # Error occurred in the first compile.  Let's try to salvage
+	    # the situation: Compile a separate program for each library.
+	    for i in $deplibs; do
+	      case $i in
+	      -l*)
+		func_stripname -l '' "$i"
+		name=$func_stripname_result
+		$opt_dry_run || $RM conftest
+		if $LTCC $LTCFLAGS -o conftest conftest.c $i; then
+		  ldd_output=`ldd conftest`
+		  if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+		    case " $predeps $postdeps " in
+		    *" $i "*)
+		      func_append newdeplibs " $i"
+		      i=""
+		      ;;
+		    esac
+		  fi
+		  if test -n "$i" ; then
+		    libname=`eval "\\$ECHO \"$libname_spec\""`
+		    deplib_matches=`eval "\\$ECHO \"$library_names_spec\""`
+		    set dummy $deplib_matches; shift
+		    deplib_match=$1
+		    if test `expr "$ldd_output" : ".*$deplib_match"` -ne 0 ; then
+		      func_append newdeplibs " $i"
+		    else
+		      droppeddeps=yes
+		      echo
+		      $ECHO "*** Warning: dynamic linker does not accept needed library $i."
+		      echo "*** I have the capability to make that library automatically link in when"
+		      echo "*** you link to this library.  But I can only do this if you have a"
+		      echo "*** shared version of the library, which you do not appear to have"
+		      echo "*** because a test_compile did reveal that the linker did not use this one"
+		      echo "*** as a dynamic dependency that programs can get resolved with at runtime."
+		    fi
+		  fi
+		else
+		  droppeddeps=yes
+		  echo
+		  $ECHO "*** Warning!  Library $i is needed by this library but I was not able to"
+		  echo "*** make it link in!  You will probably need to install it or some"
+		  echo "*** library that it depends on before this library will be fully"
+		  echo "*** functional.  Installing it before continuing would be even better."
+		fi
+		;;
+	      *)
+		func_append newdeplibs " $i"
+		;;
+	      esac
+	    done
+	  fi
+	  ;;
+	file_magic*)
+	  set dummy $deplibs_check_method; shift
+	  file_magic_regex=`expr "$deplibs_check_method" : "$1 \(.*\)"`
+	  for a_deplib in $deplibs; do
+	    case $a_deplib in
+	    -l*)
+	      func_stripname -l '' "$a_deplib"
+	      name=$func_stripname_result
+	      if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+		case " $predeps $postdeps " in
+		*" $a_deplib "*)
+		  func_append newdeplibs " $a_deplib"
+		  a_deplib=""
+		  ;;
+		esac
+	      fi
+	      if test -n "$a_deplib" ; then
+		libname=`eval "\\$ECHO \"$libname_spec\""`
+		if test -n "$file_magic_glob"; then
+		  libnameglob=`func_echo_all "$libname" | $SED -e $file_magic_glob`
+		else
+		  libnameglob=$libname
+		fi
+		test "$want_nocaseglob" = yes && nocaseglob=`shopt -p nocaseglob`
+		for i in $lib_search_path $sys_lib_search_path $shlib_search_path; do
+		  if test "$want_nocaseglob" = yes; then
+		    shopt -s nocaseglob
+		    potential_libs=`ls $i/$libnameglob[.-]* 2>/dev/null`
+		    $nocaseglob
+		  else
+		    potential_libs=`ls $i/$libnameglob[.-]* 2>/dev/null`
+		  fi
+		  for potent_lib in $potential_libs; do
+		      # Follow soft links.
+		      if ls -lLd "$potent_lib" 2>/dev/null |
+			 $GREP " -> " >/dev/null; then
+			continue
+		      fi
+		      # The statement above tries to avoid entering an
+		      # endless loop below, in case of cyclic links.
+		      # We might still enter an endless loop, since a link
+		      # loop can be closed while we follow links,
+		      # but so what?
+		      potlib="$potent_lib"
+		      while test -h "$potlib" 2>/dev/null; do
+			potliblink=`ls -ld $potlib | ${SED} 's/.* -> //'`
+			case $potliblink in
+			[\\/]* | [A-Za-z]:[\\/]*) potlib="$potliblink";;
+			*) potlib=`$ECHO "$potlib" | $SED 's,[^/]*$,,'`"$potliblink";;
+			esac
+		      done
+		      if eval $file_magic_cmd \"\$potlib\" 2>/dev/null |
+			 $SED -e 10q |
+			 $EGREP "$file_magic_regex" > /dev/null; then
+			func_append newdeplibs " $a_deplib"
+			a_deplib=""
+			break 2
+		      fi
+		  done
+		done
+	      fi
+	      if test -n "$a_deplib" ; then
+		droppeddeps=yes
+		echo
+		$ECHO "*** Warning: linker path does not have real file for library $a_deplib."
+		echo "*** I have the capability to make that library automatically link in when"
+		echo "*** you link to this library.  But I can only do this if you have a"
+		echo "*** shared version of the library, which you do not appear to have"
+		echo "*** because I did check the linker path looking for a file starting"
+		if test -z "$potlib" ; then
+		  $ECHO "*** with $libname but no candidates were found. (...for file magic test)"
+		else
+		  $ECHO "*** with $libname and none of the candidates passed a file format test"
+		  $ECHO "*** using a file magic. Last file checked: $potlib"
+		fi
+	      fi
+	      ;;
+	    *)
+	      # Add a -L argument.
+	      func_append newdeplibs " $a_deplib"
+	      ;;
+	    esac
+	  done # Gone through all deplibs.
+	  ;;
+	match_pattern*)
+	  set dummy $deplibs_check_method; shift
+	  match_pattern_regex=`expr "$deplibs_check_method" : "$1 \(.*\)"`
+	  for a_deplib in $deplibs; do
+	    case $a_deplib in
+	    -l*)
+	      func_stripname -l '' "$a_deplib"
+	      name=$func_stripname_result
+	      if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+		case " $predeps $postdeps " in
+		*" $a_deplib "*)
+		  func_append newdeplibs " $a_deplib"
+		  a_deplib=""
+		  ;;
+		esac
+	      fi
+	      if test -n "$a_deplib" ; then
+		libname=`eval "\\$ECHO \"$libname_spec\""`
+		for i in $lib_search_path $sys_lib_search_path $shlib_search_path; do
+		  potential_libs=`ls $i/$libname[.-]* 2>/dev/null`
+		  for potent_lib in $potential_libs; do
+		    potlib="$potent_lib" # see symlink-check above in file_magic test
+		    if eval "\$ECHO \"$potent_lib\"" 2>/dev/null | $SED 10q | \
+		       $EGREP "$match_pattern_regex" > /dev/null; then
+		      func_append newdeplibs " $a_deplib"
+		      a_deplib=""
+		      break 2
+		    fi
+		  done
+		done
+	      fi
+	      if test -n "$a_deplib" ; then
+		droppeddeps=yes
+		echo
+		$ECHO "*** Warning: linker path does not have real file for library $a_deplib."
+		echo "*** I have the capability to make that library automatically link in when"
+		echo "*** you link to this library.  But I can only do this if you have a"
+		echo "*** shared version of the library, which you do not appear to have"
+		echo "*** because I did check the linker path looking for a file starting"
+		if test -z "$potlib" ; then
+		  $ECHO "*** with $libname but no candidates were found. (...for regex pattern test)"
+		else
+		  $ECHO "*** with $libname and none of the candidates passed a file format test"
+		  $ECHO "*** using a regex pattern. Last file checked: $potlib"
+		fi
+	      fi
+	      ;;
+	    *)
+	      # Add a -L argument.
+	      func_append newdeplibs " $a_deplib"
+	      ;;
+	    esac
+	  done # Gone through all deplibs.
+	  ;;
+	none | unknown | *)
+	  newdeplibs=""
+	  tmp_deplibs=`$ECHO " $deplibs" | $SED 's/ -lc$//; s/ -[LR][^ ]*//g'`
+	  if test "X$allow_libtool_libs_with_static_runtimes" = "Xyes" ; then
+	    for i in $predeps $postdeps ; do
+	      # can't use Xsed below, because $i might contain '/'
+	      tmp_deplibs=`$ECHO " $tmp_deplibs" | $SED "s,$i,,"`
+	    done
+	  fi
+	  case $tmp_deplibs in
+	  *[!\	\ ]*)
+	    echo
+	    if test "X$deplibs_check_method" = "Xnone"; then
+	      echo "*** Warning: inter-library dependencies are not supported in this platform."
+	    else
+	      echo "*** Warning: inter-library dependencies are not known to be supported."
+	    fi
+	    echo "*** All declared inter-library dependencies are being dropped."
+	    droppeddeps=yes
+	    ;;
+	  esac
+	  ;;
+	esac
+	versuffix=$versuffix_save
+	major=$major_save
+	release=$release_save
+	libname=$libname_save
+	name=$name_save
+
+	case $host in
+	*-*-rhapsody* | *-*-darwin1.[012])
+	  # On Rhapsody replace the C library with the System framework
+	  newdeplibs=`$ECHO " $newdeplibs" | $SED 's/ -lc / System.ltframework /'`
+	  ;;
+	esac
+
+	if test "$droppeddeps" = yes; then
+	  if test "$module" = yes; then
+	    echo
+	    echo "*** Warning: libtool could not satisfy all declared inter-library"
+	    $ECHO "*** dependencies of module $libname.  Therefore, libtool will create"
+	    echo "*** a static module, that should work as long as the dlopening"
+	    echo "*** application is linked with the -dlopen flag."
+	    if test -z "$global_symbol_pipe"; then
+	      echo
+	      echo "*** However, this would only work if libtool was able to extract symbol"
+	      echo "*** lists from a program, using \`nm' or equivalent, but libtool could"
+	      echo "*** not find such a program.  So, this module is probably useless."
+	      echo "*** \`nm' from GNU binutils and a full rebuild may help."
+	    fi
+	    if test "$build_old_libs" = no; then
+	      oldlibs="$output_objdir/$libname.$libext"
+	      build_libtool_libs=module
+	      build_old_libs=yes
+	    else
+	      build_libtool_libs=no
+	    fi
+	  else
+	    echo "*** The inter-library dependencies that have been dropped here will be"
+	    echo "*** automatically added whenever a program is linked with this library"
+	    echo "*** or is declared to -dlopen it."
+
+	    if test "$allow_undefined" = no; then
+	      echo
+	      echo "*** Since this library must not contain undefined symbols,"
+	      echo "*** because either the platform does not support them or"
+	      echo "*** it was explicitly requested with -no-undefined,"
+	      echo "*** libtool will only create a static version of it."
+	      if test "$build_old_libs" = no; then
+		oldlibs="$output_objdir/$libname.$libext"
+		build_libtool_libs=module
+		build_old_libs=yes
+	      else
+		build_libtool_libs=no
+	      fi
+	    fi
+	  fi
+	fi
+	# Done checking deplibs!
+	deplibs=$newdeplibs
+      fi
+      # Time to change all our "foo.ltframework" stuff back to "-framework foo"
+      case $host in
+	*-*-darwin*)
+	  newdeplibs=`$ECHO " $newdeplibs" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	  new_inherited_linker_flags=`$ECHO " $new_inherited_linker_flags" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	  deplibs=`$ECHO " $deplibs" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	  ;;
+      esac
+
+      # move library search paths that coincide with paths to not yet
+      # installed libraries to the beginning of the library search list
+      new_libs=
+      for path in $notinst_path; do
+	case " $new_libs " in
+	*" -L$path/$objdir "*) ;;
+	*)
+	  case " $deplibs " in
+	  *" -L$path/$objdir "*)
+	    func_append new_libs " -L$path/$objdir" ;;
+	  esac
+	  ;;
+	esac
+      done
+      for deplib in $deplibs; do
+	case $deplib in
+	-L*)
+	  case " $new_libs " in
+	  *" $deplib "*) ;;
+	  *) func_append new_libs " $deplib" ;;
+	  esac
+	  ;;
+	*) func_append new_libs " $deplib" ;;
+	esac
+      done
+      deplibs="$new_libs"
+
+      # All the library-specific variables (install_libdir is set above).
+      library_names=
+      old_library=
+      dlname=
+
+      # Test again, we may have decided not to build it any more
+      if test "$build_libtool_libs" = yes; then
+	# Remove ${wl} instances when linking with ld.
+	# FIXME: should test the right _cmds variable.
+	case $archive_cmds in
+	  *\$LD\ *) wl= ;;
+        esac
+	if test "$hardcode_into_libs" = yes; then
+	  # Hardcode the library paths
+	  hardcode_libdirs=
+	  dep_rpath=
+	  rpath="$finalize_rpath"
+	  test "$opt_mode" != relink && rpath="$compile_rpath$rpath"
+	  for libdir in $rpath; do
+	    if test -n "$hardcode_libdir_flag_spec"; then
+	      if test -n "$hardcode_libdir_separator"; then
+		func_replace_sysroot "$libdir"
+		libdir=$func_replace_sysroot_result
+		if test -z "$hardcode_libdirs"; then
+		  hardcode_libdirs="$libdir"
+		else
+		  # Just accumulate the unique libdirs.
+		  case $hardcode_libdir_separator$hardcode_libdirs$hardcode_libdir_separator in
+		  *"$hardcode_libdir_separator$libdir$hardcode_libdir_separator"*)
+		    ;;
+		  *)
+		    func_append hardcode_libdirs "$hardcode_libdir_separator$libdir"
+		    ;;
+		  esac
+		fi
+	      else
+		eval flag=\"$hardcode_libdir_flag_spec\"
+		func_append dep_rpath " $flag"
+	      fi
+	    elif test -n "$runpath_var"; then
+	      case "$perm_rpath " in
+	      *" $libdir "*) ;;
+	      *) func_append perm_rpath " $libdir" ;;
+	      esac
+	    fi
+	  done
+	  # Substitute the hardcoded libdirs into the rpath.
+	  if test -n "$hardcode_libdir_separator" &&
+	     test -n "$hardcode_libdirs"; then
+	    libdir="$hardcode_libdirs"
+	    eval "dep_rpath=\"$hardcode_libdir_flag_spec\""
+	  fi
+	  if test -n "$runpath_var" && test -n "$perm_rpath"; then
+	    # We should set the runpath_var.
+	    rpath=
+	    for dir in $perm_rpath; do
+	      func_append rpath "$dir:"
+	    done
+	    eval "$runpath_var='$rpath\$$runpath_var'; export $runpath_var"
+	  fi
+	  test -n "$dep_rpath" && deplibs="$dep_rpath $deplibs"
+	fi
+
+	shlibpath="$finalize_shlibpath"
+	test "$opt_mode" != relink && shlibpath="$compile_shlibpath$shlibpath"
+	if test -n "$shlibpath"; then
+	  eval "$shlibpath_var='$shlibpath\$$shlibpath_var'; export $shlibpath_var"
+	fi
+
+	# Get the real and link names of the library.
+	eval shared_ext=\"$shrext_cmds\"
+	eval library_names=\"$library_names_spec\"
+	set dummy $library_names
+	shift
+	realname="$1"
+	shift
+
+	if test -n "$soname_spec"; then
+	  eval soname=\"$soname_spec\"
+	else
+	  soname="$realname"
+	fi
+	if test -z "$dlname"; then
+	  dlname=$soname
+	fi
+
+	lib="$output_objdir/$realname"
+	linknames=
+	for link
+	do
+	  func_append linknames " $link"
+	done
+
+	# Use standard objects if they are pic
+	test -z "$pic_flag" && libobjs=`$ECHO "$libobjs" | $SP2NL | $SED "$lo2o" | $NL2SP`
+	test "X$libobjs" = "X " && libobjs=
+
+	delfiles=
+	if test -n "$export_symbols" && test -n "$include_expsyms"; then
+	  $opt_dry_run || cp "$export_symbols" "$output_objdir/$libname.uexp"
+	  export_symbols="$output_objdir/$libname.uexp"
+	  func_append delfiles " $export_symbols"
+	fi
+
+	orig_export_symbols=
+	case $host_os in
+	cygwin* | mingw* | cegcc*)
+	  if test -n "$export_symbols" && test -z "$export_symbols_regex"; then
+	    # exporting using user supplied symfile
+	    if test "x`$SED 1q $export_symbols`" != xEXPORTS; then
+	      # and it's NOT already a .def file. Must figure out
+	      # which of the given symbols are data symbols and tag
+	      # them as such. So, trigger use of export_symbols_cmds.
+	      # export_symbols gets reassigned inside the "prepare
+	      # the list of exported symbols" if statement, so the
+	      # include_expsyms logic still works.
+	      orig_export_symbols="$export_symbols"
+	      export_symbols=
+	      always_export_symbols=yes
+	    fi
+	  fi
+	  ;;
+	esac
+
+	# Prepare the list of exported symbols
+	if test -z "$export_symbols"; then
+	  if test "$always_export_symbols" = yes || test -n "$export_symbols_regex"; then
+	    func_verbose "generating symbol list for \`$libname.la'"
+	    export_symbols="$output_objdir/$libname.exp"
+	    $opt_dry_run || $RM $export_symbols
+	    cmds=$export_symbols_cmds
+	    save_ifs="$IFS"; IFS='~'
+	    for cmd1 in $cmds; do
+	      IFS="$save_ifs"
+	      # Take the normal branch if the nm_file_list_spec branch
+	      # doesn't work or if tool conversion is not needed.
+	      case $nm_file_list_spec~$to_tool_file_cmd in
+		*~func_convert_file_noop | *~func_convert_file_msys_to_w32 | ~*)
+		  try_normal_branch=yes
+		  eval cmd=\"$cmd1\"
+		  func_len " $cmd"
+		  len=$func_len_result
+		  ;;
+		*)
+		  try_normal_branch=no
+		  ;;
+	      esac
+	      if test "$try_normal_branch" = yes \
+		 && { test "$len" -lt "$max_cmd_len" \
+		      || test "$max_cmd_len" -le -1; }
+	      then
+		func_show_eval "$cmd" 'exit $?'
+		skipped_export=false
+	      elif test -n "$nm_file_list_spec"; then
+		func_basename "$output"
+		output_la=$func_basename_result
+		save_libobjs=$libobjs
+		save_output=$output
+		output=${output_objdir}/${output_la}.nm
+		func_to_tool_file "$output"
+		libobjs=$nm_file_list_spec$func_to_tool_file_result
+		func_append delfiles " $output"
+		func_verbose "creating $NM input file list: $output"
+		for obj in $save_libobjs; do
+		  func_to_tool_file "$obj"
+		  $ECHO "$func_to_tool_file_result"
+		done > "$output"
+		eval cmd=\"$cmd1\"
+		func_show_eval "$cmd" 'exit $?'
+		output=$save_output
+		libobjs=$save_libobjs
+		skipped_export=false
+	      else
+		# The command line is too long to execute in one step.
+		func_verbose "using reloadable object file for export list..."
+		skipped_export=:
+		# Break out early, otherwise skipped_export may be
+		# set to false by a later but shorter cmd.
+		break
+	      fi
+	    done
+	    IFS="$save_ifs"
+	    if test -n "$export_symbols_regex" && test "X$skipped_export" != "X:"; then
+	      func_show_eval '$EGREP -e "$export_symbols_regex" "$export_symbols" > "${export_symbols}T"'
+	      func_show_eval '$MV "${export_symbols}T" "$export_symbols"'
+	    fi
+	  fi
+	fi
+
+	if test -n "$export_symbols" && test -n "$include_expsyms"; then
+	  tmp_export_symbols="$export_symbols"
+	  test -n "$orig_export_symbols" && tmp_export_symbols="$orig_export_symbols"
+	  $opt_dry_run || eval '$ECHO "$include_expsyms" | $SP2NL >> "$tmp_export_symbols"'
+	fi
+
+	if test "X$skipped_export" != "X:" && test -n "$orig_export_symbols"; then
+	  # The given exports_symbols file has to be filtered, so filter it.
+	  func_verbose "filter symbol list for \`$libname.la' to tag DATA exports"
+	  # FIXME: $output_objdir/$libname.filter potentially contains lots of
+	  # 's' commands which not all seds can handle. GNU sed should be fine
+	  # though. Also, the filter scales superlinearly with the number of
+	  # global variables. join(1) would be nice here, but unfortunately
+	  # isn't a blessed tool.
+	  $opt_dry_run || $SED -e '/[ ,]DATA/!d;s,\(.*\)\([ \,].*\),s|^\1$|\1\2|,' < $export_symbols > $output_objdir/$libname.filter
+	  func_append delfiles " $export_symbols $output_objdir/$libname.filter"
+	  export_symbols=$output_objdir/$libname.def
+	  $opt_dry_run || $SED -f $output_objdir/$libname.filter < $orig_export_symbols > $export_symbols
+	fi
+
+	tmp_deplibs=
+	for test_deplib in $deplibs; do
+	  case " $convenience " in
+	  *" $test_deplib "*) ;;
+	  *)
+	    func_append tmp_deplibs " $test_deplib"
+	    ;;
+	  esac
+	done
+	deplibs="$tmp_deplibs"
+
+	if test -n "$convenience"; then
+	  if test -n "$whole_archive_flag_spec" &&
+	    test "$compiler_needs_object" = yes &&
+	    test -z "$libobjs"; then
+	    # extract the archives, so we have objects to list.
+	    # TODO: could optimize this to just extract one archive.
+	    whole_archive_flag_spec=
+	  fi
+	  if test -n "$whole_archive_flag_spec"; then
+	    save_libobjs=$libobjs
+	    eval libobjs=\"\$libobjs $whole_archive_flag_spec\"
+	    test "X$libobjs" = "X " && libobjs=
+	  else
+	    gentop="$output_objdir/${outputname}x"
+	    func_append generated " $gentop"
+
+	    func_extract_archives $gentop $convenience
+	    func_append libobjs " $func_extract_archives_result"
+	    test "X$libobjs" = "X " && libobjs=
+	  fi
+	fi
+
+	if test "$thread_safe" = yes && test -n "$thread_safe_flag_spec"; then
+	  eval flag=\"$thread_safe_flag_spec\"
+	  func_append linker_flags " $flag"
+	fi
+
+	# Make a backup of the uninstalled library when relinking
+	if test "$opt_mode" = relink; then
+	  $opt_dry_run || eval '(cd $output_objdir && $RM ${realname}U && $MV $realname ${realname}U)' || exit $?
+	fi
+
+	# Do each of the archive commands.
+	if test "$module" = yes && test -n "$module_cmds" ; then
+	  if test -n "$export_symbols" && test -n "$module_expsym_cmds"; then
+	    eval test_cmds=\"$module_expsym_cmds\"
+	    cmds=$module_expsym_cmds
+	  else
+	    eval test_cmds=\"$module_cmds\"
+	    cmds=$module_cmds
+	  fi
+	else
+	  if test -n "$export_symbols" && test -n "$archive_expsym_cmds"; then
+	    eval test_cmds=\"$archive_expsym_cmds\"
+	    cmds=$archive_expsym_cmds
+	  else
+	    eval test_cmds=\"$archive_cmds\"
+	    cmds=$archive_cmds
+	  fi
+	fi
+
+	if test "X$skipped_export" != "X:" &&
+	   func_len " $test_cmds" &&
+	   len=$func_len_result &&
+	   test "$len" -lt "$max_cmd_len" || test "$max_cmd_len" -le -1; then
+	  :
+	else
+	  # The command line is too long to link in one step, link piecewise
+	  # or, if using GNU ld and skipped_export is not :, use a linker
+	  # script.
+
+	  # Save the value of $output and $libobjs because we want to
+	  # use them later.  If we have whole_archive_flag_spec, we
+	  # want to use save_libobjs as it was before
+	  # whole_archive_flag_spec was expanded, because we can't
+	  # assume the linker understands whole_archive_flag_spec.
+	  # This may have to be revisited, in case too many
+	  # convenience libraries get linked in and end up exceeding
+	  # the spec.
+	  if test -z "$convenience" || test -z "$whole_archive_flag_spec"; then
+	    save_libobjs=$libobjs
+	  fi
+	  save_output=$output
+	  func_basename "$output"
+	  output_la=$func_basename_result
+
+	  # Clear the reloadable object creation command queue and
+	  # initialize k to one.
+	  test_cmds=
+	  concat_cmds=
+	  objlist=
+	  last_robj=
+	  k=1
+
+	  if test -n "$save_libobjs" && test "X$skipped_export" != "X:" && test "$with_gnu_ld" = yes; then
+	    output=${output_objdir}/${output_la}.lnkscript
+	    func_verbose "creating GNU ld script: $output"
+	    echo 'INPUT (' > $output
+	    for obj in $save_libobjs
+	    do
+	      func_to_tool_file "$obj"
+	      $ECHO "$func_to_tool_file_result" >> $output
+	    done
+	    echo ')' >> $output
+	    func_append delfiles " $output"
+	    func_to_tool_file "$output"
+	    output=$func_to_tool_file_result
+	  elif test -n "$save_libobjs" && test "X$skipped_export" != "X:" && test "X$file_list_spec" != X; then
+	    output=${output_objdir}/${output_la}.lnk
+	    func_verbose "creating linker input file list: $output"
+	    : > $output
+	    set x $save_libobjs
+	    shift
+	    firstobj=
+	    if test "$compiler_needs_object" = yes; then
+	      firstobj="$1 "
+	      shift
+	    fi
+	    for obj
+	    do
+	      func_to_tool_file "$obj"
+	      $ECHO "$func_to_tool_file_result" >> $output
+	    done
+	    func_append delfiles " $output"
+	    func_to_tool_file "$output"
+	    output=$firstobj\"$file_list_spec$func_to_tool_file_result\"
+	  else
+	    if test -n "$save_libobjs"; then
+	      func_verbose "creating reloadable object files..."
+	      output=$output_objdir/$output_la-${k}.$objext
+	      eval test_cmds=\"$reload_cmds\"
+	      func_len " $test_cmds"
+	      len0=$func_len_result
+	      len=$len0
+
+	      # Loop over the list of objects to be linked.
+	      for obj in $save_libobjs
+	      do
+		func_len " $obj"
+		func_arith $len + $func_len_result
+		len=$func_arith_result
+		if test "X$objlist" = X ||
+		   test "$len" -lt "$max_cmd_len"; then
+		  func_append objlist " $obj"
+		else
+		  # The command $test_cmds is almost too long, add a
+		  # command to the queue.
+		  if test "$k" -eq 1 ; then
+		    # The first file doesn't have a previous command to add.
+		    reload_objs=$objlist
+		    eval concat_cmds=\"$reload_cmds\"
+		  else
+		    # All subsequent reloadable object files will link in
+		    # the last one created.
+		    reload_objs="$objlist $last_robj"
+		    eval concat_cmds=\"\$concat_cmds~$reload_cmds~\$RM $last_robj\"
+		  fi
+		  last_robj=$output_objdir/$output_la-${k}.$objext
+		  func_arith $k + 1
+		  k=$func_arith_result
+		  output=$output_objdir/$output_la-${k}.$objext
+		  objlist=" $obj"
+		  func_len " $last_robj"
+		  func_arith $len0 + $func_len_result
+		  len=$func_arith_result
+		fi
+	      done
+	      # Handle the remaining objects by creating one last
+	      # reloadable object file.  All subsequent reloadable object
+	      # files will link in the last one created.
+	      test -z "$concat_cmds" || concat_cmds=$concat_cmds~
+	      reload_objs="$objlist $last_robj"
+	      eval concat_cmds=\"\${concat_cmds}$reload_cmds\"
+	      if test -n "$last_robj"; then
+	        eval concat_cmds=\"\${concat_cmds}~\$RM $last_robj\"
+	      fi
+	      func_append delfiles " $output"
+
+	    else
+	      output=
+	    fi
+
+	    if ${skipped_export-false}; then
+	      func_verbose "generating symbol list for \`$libname.la'"
+	      export_symbols="$output_objdir/$libname.exp"
+	      $opt_dry_run || $RM $export_symbols
+	      libobjs=$output
+	      # Append the command to create the export file.
+	      test -z "$concat_cmds" || concat_cmds=$concat_cmds~
+	      eval concat_cmds=\"\$concat_cmds$export_symbols_cmds\"
+	      if test -n "$last_robj"; then
+		eval concat_cmds=\"\$concat_cmds~\$RM $last_robj\"
+	      fi
+	    fi
+
+	    test -n "$save_libobjs" &&
+	      func_verbose "creating a temporary reloadable object file: $output"
+
+	    # Loop through the commands generated above and execute them.
+	    save_ifs="$IFS"; IFS='~'
+	    for cmd in $concat_cmds; do
+	      IFS="$save_ifs"
+	      $opt_silent || {
+		  func_quote_for_expand "$cmd"
+		  eval "func_echo $func_quote_for_expand_result"
+	      }
+	      $opt_dry_run || eval "$cmd" || {
+		lt_exit=$?
+
+		# Restore the uninstalled library and exit
+		if test "$opt_mode" = relink; then
+		  ( cd "$output_objdir" && \
+		    $RM "${realname}T" && \
+		    $MV "${realname}U" "$realname" )
+		fi
+
+		exit $lt_exit
+	      }
+	    done
+	    IFS="$save_ifs"
+
+	    if test -n "$export_symbols_regex" && ${skipped_export-false}; then
+	      func_show_eval '$EGREP -e "$export_symbols_regex" "$export_symbols" > "${export_symbols}T"'
+	      func_show_eval '$MV "${export_symbols}T" "$export_symbols"'
+	    fi
+	  fi
+
+          if ${skipped_export-false}; then
+	    if test -n "$export_symbols" && test -n "$include_expsyms"; then
+	      tmp_export_symbols="$export_symbols"
+	      test -n "$orig_export_symbols" && tmp_export_symbols="$orig_export_symbols"
+	      $opt_dry_run || eval '$ECHO "$include_expsyms" | $SP2NL >> "$tmp_export_symbols"'
+	    fi
+
+	    if test -n "$orig_export_symbols"; then
+	      # The given exports_symbols file has to be filtered, so filter it.
+	      func_verbose "filter symbol list for \`$libname.la' to tag DATA exports"
+	      # FIXME: $output_objdir/$libname.filter potentially contains lots of
+	      # 's' commands which not all seds can handle. GNU sed should be fine
+	      # though. Also, the filter scales superlinearly with the number of
+	      # global variables. join(1) would be nice here, but unfortunately
+	      # isn't a blessed tool.
+	      $opt_dry_run || $SED -e '/[ ,]DATA/!d;s,\(.*\)\([ \,].*\),s|^\1$|\1\2|,' < $export_symbols > $output_objdir/$libname.filter
+	      func_append delfiles " $export_symbols $output_objdir/$libname.filter"
+	      export_symbols=$output_objdir/$libname.def
+	      $opt_dry_run || $SED -f $output_objdir/$libname.filter < $orig_export_symbols > $export_symbols
+	    fi
+	  fi
+
+	  libobjs=$output
+	  # Restore the value of output.
+	  output=$save_output
+
+	  if test -n "$convenience" && test -n "$whole_archive_flag_spec"; then
+	    eval libobjs=\"\$libobjs $whole_archive_flag_spec\"
+	    test "X$libobjs" = "X " && libobjs=
+	  fi
+	  # Expand the library linking commands again to reset the
+	  # value of $libobjs for piecewise linking.
+
+	  # Do each of the archive commands.
+	  if test "$module" = yes && test -n "$module_cmds" ; then
+	    if test -n "$export_symbols" && test -n "$module_expsym_cmds"; then
+	      cmds=$module_expsym_cmds
+	    else
+	      cmds=$module_cmds
+	    fi
+	  else
+	    if test -n "$export_symbols" && test -n "$archive_expsym_cmds"; then
+	      cmds=$archive_expsym_cmds
+	    else
+	      cmds=$archive_cmds
+	    fi
+	  fi
+	fi
+
+	if test -n "$delfiles"; then
+	  # Append the command to remove temporary files to $cmds.
+	  eval cmds=\"\$cmds~\$RM $delfiles\"
+	fi
+
+	# Add any objects from preloaded convenience libraries
+	if test -n "$dlprefiles"; then
+	  gentop="$output_objdir/${outputname}x"
+	  func_append generated " $gentop"
+
+	  func_extract_archives $gentop $dlprefiles
+	  func_append libobjs " $func_extract_archives_result"
+	  test "X$libobjs" = "X " && libobjs=
+	fi
+
+	save_ifs="$IFS"; IFS='~'
+	for cmd in $cmds; do
+	  IFS="$save_ifs"
+	  eval cmd=\"$cmd\"
+	  $opt_silent || {
+	    func_quote_for_expand "$cmd"
+	    eval "func_echo $func_quote_for_expand_result"
+	  }
+	  $opt_dry_run || eval "$cmd" || {
+	    lt_exit=$?
+
+	    # Restore the uninstalled library and exit
+	    if test "$opt_mode" = relink; then
+	      ( cd "$output_objdir" && \
+	        $RM "${realname}T" && \
+		$MV "${realname}U" "$realname" )
+	    fi
+
+	    exit $lt_exit
+	  }
+	done
+	IFS="$save_ifs"
+
+	# Restore the uninstalled library and exit
+	if test "$opt_mode" = relink; then
+	  $opt_dry_run || eval '(cd $output_objdir && $RM ${realname}T && $MV $realname ${realname}T && $MV ${realname}U $realname)' || exit $?
+
+	  if test -n "$convenience"; then
+	    if test -z "$whole_archive_flag_spec"; then
+	      func_show_eval '${RM}r "$gentop"'
+	    fi
+	  fi
+
+	  exit $EXIT_SUCCESS
+	fi
+
+	# Create links to the real library.
+	for linkname in $linknames; do
+	  if test "$realname" != "$linkname"; then
+	    func_show_eval '(cd "$output_objdir" && $RM "$linkname" && $LN_S "$realname" "$linkname")' 'exit $?'
+	  fi
+	done
+
+	# If -module or -export-dynamic was specified, set the dlname.
+	if test "$module" = yes || test "$export_dynamic" = yes; then
+	  # On all known operating systems, these are identical.
+	  dlname="$soname"
+	fi
+      fi
+      ;;
+
+    obj)
+      if test -n "$dlfiles$dlprefiles" || test "$dlself" != no; then
+	func_warning "\`-dlopen' is ignored for objects"
+      fi
+
+      case " $deplibs" in
+      *\ -l* | *\ -L*)
+	func_warning "\`-l' and \`-L' are ignored for objects" ;;
+      esac
+
+      test -n "$rpath" && \
+	func_warning "\`-rpath' is ignored for objects"
+
+      test -n "$xrpath" && \
+	func_warning "\`-R' is ignored for objects"
+
+      test -n "$vinfo" && \
+	func_warning "\`-version-info' is ignored for objects"
+
+      test -n "$release" && \
+	func_warning "\`-release' is ignored for objects"
+
+      case $output in
+      *.lo)
+	test -n "$objs$old_deplibs" && \
+	  func_fatal_error "cannot build library object \`$output' from non-libtool objects"
+
+	libobj=$output
+	func_lo2o "$libobj"
+	obj=$func_lo2o_result
+	;;
+      *)
+	libobj=
+	obj="$output"
+	;;
+      esac
+
+      # Delete the old objects.
+      $opt_dry_run || $RM $obj $libobj
+
+      # Objects from convenience libraries.  This assumes
+      # single-version convenience libraries.  Whenever we create
+      # different ones for PIC/non-PIC, this we'll have to duplicate
+      # the extraction.
+      reload_conv_objs=
+      gentop=
+      # reload_cmds runs $LD directly, so let us get rid of
+      # -Wl from whole_archive_flag_spec and hope we can get by with
+      # turning comma into space..
+      wl=
+
+      if test -n "$convenience"; then
+	if test -n "$whole_archive_flag_spec"; then
+	  eval tmp_whole_archive_flags=\"$whole_archive_flag_spec\"
+	  reload_conv_objs=$reload_objs\ `$ECHO "$tmp_whole_archive_flags" | $SED 's|,| |g'`
+	else
+	  gentop="$output_objdir/${obj}x"
+	  func_append generated " $gentop"
+
+	  func_extract_archives $gentop $convenience
+	  reload_conv_objs="$reload_objs $func_extract_archives_result"
+	fi
+      fi
+
+      # If we're not building shared, we need to use non_pic_objs
+      test "$build_libtool_libs" != yes && libobjs="$non_pic_objects"
+
+      # Create the old-style object.
+      reload_objs="$objs$old_deplibs "`$ECHO "$libobjs" | $SP2NL | $SED "/\.${libext}$/d; /\.lib$/d; $lo2o" | $NL2SP`" $reload_conv_objs" ### testsuite: skip nested quoting test
+
+      output="$obj"
+      func_execute_cmds "$reload_cmds" 'exit $?'
+
+      # Exit if we aren't doing a library object file.
+      if test -z "$libobj"; then
+	if test -n "$gentop"; then
+	  func_show_eval '${RM}r "$gentop"'
+	fi
+
+	exit $EXIT_SUCCESS
+      fi
+
+      if test "$build_libtool_libs" != yes; then
+	if test -n "$gentop"; then
+	  func_show_eval '${RM}r "$gentop"'
+	fi
+
+	# Create an invalid libtool object if no PIC, so that we don't
+	# accidentally link it into a program.
+	# $show "echo timestamp > $libobj"
+	# $opt_dry_run || eval "echo timestamp > $libobj" || exit $?
+	exit $EXIT_SUCCESS
+      fi
+
+      if test -n "$pic_flag" || test "$pic_mode" != default; then
+	# Only do commands if we really have different PIC objects.
+	reload_objs="$libobjs $reload_conv_objs"
+	output="$libobj"
+	func_execute_cmds "$reload_cmds" 'exit $?'
+      fi
+
+      if test -n "$gentop"; then
+	func_show_eval '${RM}r "$gentop"'
+      fi
+
+      exit $EXIT_SUCCESS
+      ;;
+
+    prog)
+      case $host in
+	*cygwin*) func_stripname '' '.exe' "$output"
+	          output=$func_stripname_result.exe;;
+      esac
+      test -n "$vinfo" && \
+	func_warning "\`-version-info' is ignored for programs"
+
+      test -n "$release" && \
+	func_warning "\`-release' is ignored for programs"
+
+      test "$preload" = yes \
+        && test "$dlopen_support" = unknown \
+	&& test "$dlopen_self" = unknown \
+	&& test "$dlopen_self_static" = unknown && \
+	  func_warning "\`LT_INIT([dlopen])' not used. Assuming no dlopen support."
+
+      case $host in
+      *-*-rhapsody* | *-*-darwin1.[012])
+	# On Rhapsody replace the C library is the System framework
+	compile_deplibs=`$ECHO " $compile_deplibs" | $SED 's/ -lc / System.ltframework /'`
+	finalize_deplibs=`$ECHO " $finalize_deplibs" | $SED 's/ -lc / System.ltframework /'`
+	;;
+      esac
+
+      case $host in
+      *-*-darwin*)
+	# Don't allow lazy linking, it breaks C++ global constructors
+	# But is supposedly fixed on 10.4 or later (yay!).
+	if test "$tagname" = CXX ; then
+	  case ${MACOSX_DEPLOYMENT_TARGET-10.0} in
+	    10.[0123])
+	      func_append compile_command " ${wl}-bind_at_load"
+	      func_append finalize_command " ${wl}-bind_at_load"
+	    ;;
+	  esac
+	fi
+	# Time to change all our "foo.ltframework" stuff back to "-framework foo"
+	compile_deplibs=`$ECHO " $compile_deplibs" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	finalize_deplibs=`$ECHO " $finalize_deplibs" | $SED 's% \([^ $]*\).ltframework% -framework \1%g'`
+	;;
+      esac
+
+
+      # move library search paths that coincide with paths to not yet
+      # installed libraries to the beginning of the library search list
+      new_libs=
+      for path in $notinst_path; do
+	case " $new_libs " in
+	*" -L$path/$objdir "*) ;;
+	*)
+	  case " $compile_deplibs " in
+	  *" -L$path/$objdir "*)
+	    func_append new_libs " -L$path/$objdir" ;;
+	  esac
+	  ;;
+	esac
+      done
+      for deplib in $compile_deplibs; do
+	case $deplib in
+	-L*)
+	  case " $new_libs " in
+	  *" $deplib "*) ;;
+	  *) func_append new_libs " $deplib" ;;
+	  esac
+	  ;;
+	*) func_append new_libs " $deplib" ;;
+	esac
+      done
+      compile_deplibs="$new_libs"
+
+
+      func_append compile_command " $compile_deplibs"
+      func_append finalize_command " $finalize_deplibs"
+
+      if test -n "$rpath$xrpath"; then
+	# If the user specified any rpath flags, then add them.
+	for libdir in $rpath $xrpath; do
+	  # This is the magic to use -rpath.
+	  case "$finalize_rpath " in
+	  *" $libdir "*) ;;
+	  *) func_append finalize_rpath " $libdir" ;;
+	  esac
+	done
+      fi
+
+      # Now hardcode the library paths
+      rpath=
+      hardcode_libdirs=
+      for libdir in $compile_rpath $finalize_rpath; do
+	if test -n "$hardcode_libdir_flag_spec"; then
+	  if test -n "$hardcode_libdir_separator"; then
+	    if test -z "$hardcode_libdirs"; then
+	      hardcode_libdirs="$libdir"
+	    else
+	      # Just accumulate the unique libdirs.
+	      case $hardcode_libdir_separator$hardcode_libdirs$hardcode_libdir_separator in
+	      *"$hardcode_libdir_separator$libdir$hardcode_libdir_separator"*)
+		;;
+	      *)
+		func_append hardcode_libdirs "$hardcode_libdir_separator$libdir"
+		;;
+	      esac
+	    fi
+	  else
+	    eval flag=\"$hardcode_libdir_flag_spec\"
+	    func_append rpath " $flag"
+	  fi
+	elif test -n "$runpath_var"; then
+	  case "$perm_rpath " in
+	  *" $libdir "*) ;;
+	  *) func_append perm_rpath " $libdir" ;;
+	  esac
+	fi
+	case $host in
+	*-*-cygwin* | *-*-mingw* | *-*-pw32* | *-*-os2* | *-cegcc*)
+	  testbindir=`${ECHO} "$libdir" | ${SED} -e 's*/lib$*/bin*'`
+	  case :$dllsearchpath: in
+	  *":$libdir:"*) ;;
+	  ::) dllsearchpath=$libdir;;
+	  *) func_append dllsearchpath ":$libdir";;
+	  esac
+	  case :$dllsearchpath: in
+	  *":$testbindir:"*) ;;
+	  ::) dllsearchpath=$testbindir;;
+	  *) func_append dllsearchpath ":$testbindir";;
+	  esac
+	  ;;
+	esac
+      done
+      # Substitute the hardcoded libdirs into the rpath.
+      if test -n "$hardcode_libdir_separator" &&
+	 test -n "$hardcode_libdirs"; then
+	libdir="$hardcode_libdirs"
+	eval rpath=\" $hardcode_libdir_flag_spec\"
+      fi
+      compile_rpath="$rpath"
+
+      rpath=
+      hardcode_libdirs=
+      for libdir in $finalize_rpath; do
+	if test -n "$hardcode_libdir_flag_spec"; then
+	  if test -n "$hardcode_libdir_separator"; then
+	    if test -z "$hardcode_libdirs"; then
+	      hardcode_libdirs="$libdir"
+	    else
+	      # Just accumulate the unique libdirs.
+	      case $hardcode_libdir_separator$hardcode_libdirs$hardcode_libdir_separator in
+	      *"$hardcode_libdir_separator$libdir$hardcode_libdir_separator"*)
+		;;
+	      *)
+		func_append hardcode_libdirs "$hardcode_libdir_separator$libdir"
+		;;
+	      esac
+	    fi
+	  else
+	    eval flag=\"$hardcode_libdir_flag_spec\"
+	    func_append rpath " $flag"
+	  fi
+	elif test -n "$runpath_var"; then
+	  case "$finalize_perm_rpath " in
+	  *" $libdir "*) ;;
+	  *) func_append finalize_perm_rpath " $libdir" ;;
+	  esac
+	fi
+      done
+      # Substitute the hardcoded libdirs into the rpath.
+      if test -n "$hardcode_libdir_separator" &&
+	 test -n "$hardcode_libdirs"; then
+	libdir="$hardcode_libdirs"
+	eval rpath=\" $hardcode_libdir_flag_spec\"
+      fi
+      finalize_rpath="$rpath"
+
+      if test -n "$libobjs" && test "$build_old_libs" = yes; then
+	# Transform all the library objects into standard objects.
+	compile_command=`$ECHO "$compile_command" | $SP2NL | $SED "$lo2o" | $NL2SP`
+	finalize_command=`$ECHO "$finalize_command" | $SP2NL | $SED "$lo2o" | $NL2SP`
+      fi
+
+      func_generate_dlsyms "$outputname" "@PROGRAM@" "no"
+
+      # template prelinking step
+      if test -n "$prelink_cmds"; then
+	func_execute_cmds "$prelink_cmds" 'exit $?'
+      fi
+
+      wrappers_required=yes
+      case $host in
+      *cegcc* | *mingw32ce*)
+        # Disable wrappers for cegcc and mingw32ce hosts, we are cross compiling anyway.
+        wrappers_required=no
+        ;;
+      *cygwin* | *mingw* )
+        if test "$build_libtool_libs" != yes; then
+          wrappers_required=no
+        fi
+        ;;
+      *)
+        if test "$need_relink" = no || test "$build_libtool_libs" != yes; then
+          wrappers_required=no
+        fi
+        ;;
+      esac
+      if test "$wrappers_required" = no; then
+	# Replace the output file specification.
+	compile_command=`$ECHO "$compile_command" | $SED 's%@OUTPUT@%'"$output"'%g'`
+	link_command="$compile_command$compile_rpath"
+
+	# We have no uninstalled library dependencies, so finalize right now.
+	exit_status=0
+	func_show_eval "$link_command" 'exit_status=$?'
+
+	if test -n "$postlink_cmds"; then
+	  func_to_tool_file "$output"
+	  postlink_cmds=`func_echo_all "$postlink_cmds" | $SED -e 's%@OUTPUT@%'"$output"'%g' -e 's%@TOOL_OUTPUT@%'"$func_to_tool_file_result"'%g'`
+	  func_execute_cmds "$postlink_cmds" 'exit $?'
+	fi
+
+	# Delete the generated files.
+	if test -f "$output_objdir/${outputname}S.${objext}"; then
+	  func_show_eval '$RM "$output_objdir/${outputname}S.${objext}"'
+	fi
+
+	exit $exit_status
+      fi
+
+      if test -n "$compile_shlibpath$finalize_shlibpath"; then
+	compile_command="$shlibpath_var=\"$compile_shlibpath$finalize_shlibpath\$$shlibpath_var\" $compile_command"
+      fi
+      if test -n "$finalize_shlibpath"; then
+	finalize_command="$shlibpath_var=\"$finalize_shlibpath\$$shlibpath_var\" $finalize_command"
+      fi
+
+      compile_var=
+      finalize_var=
+      if test -n "$runpath_var"; then
+	if test -n "$perm_rpath"; then
+	  # We should set the runpath_var.
+	  rpath=
+	  for dir in $perm_rpath; do
+	    func_append rpath "$dir:"
+	  done
+	  compile_var="$runpath_var=\"$rpath\$$runpath_var\" "
+	fi
+	if test -n "$finalize_perm_rpath"; then
+	  # We should set the runpath_var.
+	  rpath=
+	  for dir in $finalize_perm_rpath; do
+	    func_append rpath "$dir:"
+	  done
+	  finalize_var="$runpath_var=\"$rpath\$$runpath_var\" "
+	fi
+      fi
+
+      if test "$no_install" = yes; then
+	# We don't need to create a wrapper script.
+	link_command="$compile_var$compile_command$compile_rpath"
+	# Replace the output file specification.
+	link_command=`$ECHO "$link_command" | $SED 's%@OUTPUT@%'"$output"'%g'`
+	# Delete the old output file.
+	$opt_dry_run || $RM $output
+	# Link the executable and exit
+	func_show_eval "$link_command" 'exit $?'
+
+	if test -n "$postlink_cmds"; then
+	  func_to_tool_file "$output"
+	  postlink_cmds=`func_echo_all "$postlink_cmds" | $SED -e 's%@OUTPUT@%'"$output"'%g' -e 's%@TOOL_OUTPUT@%'"$func_to_tool_file_result"'%g'`
+	  func_execute_cmds "$postlink_cmds" 'exit $?'
+	fi
+
+	exit $EXIT_SUCCESS
+      fi
+
+      if test "$hardcode_action" = relink; then
+	# Fast installation is not supported
+	link_command="$compile_var$compile_command$compile_rpath"
+	relink_command="$finalize_var$finalize_command$finalize_rpath"
+
+	func_warning "this platform does not like uninstalled shared libraries"
+	func_warning "\`$output' will be relinked during installation"
+      else
+	if test "$fast_install" != no; then
+	  link_command="$finalize_var$compile_command$finalize_rpath"
+	  if test "$fast_install" = yes; then
+	    relink_command=`$ECHO "$compile_var$compile_command$compile_rpath" | $SED 's%@OUTPUT@%\$progdir/\$file%g'`
+	  else
+	    # fast_install is set to needless
+	    relink_command=
+	  fi
+	else
+	  link_command="$compile_var$compile_command$compile_rpath"
+	  relink_command="$finalize_var$finalize_command$finalize_rpath"
+	fi
+      fi
+
+      # Replace the output file specification.
+      link_command=`$ECHO "$link_command" | $SED 's%@OUTPUT@%'"$output_objdir/$outputname"'%g'`
+
+      # Delete the old output files.
+      $opt_dry_run || $RM $output $output_objdir/$outputname $output_objdir/lt-$outputname
+
+      func_show_eval "$link_command" 'exit $?'
+
+      if test -n "$postlink_cmds"; then
+	func_to_tool_file "$output_objdir/$outputname"
+	postlink_cmds=`func_echo_all "$postlink_cmds" | $SED -e 's%@OUTPUT@%'"$output_objdir/$outputname"'%g' -e 's%@TOOL_OUTPUT@%'"$func_to_tool_file_result"'%g'`
+	func_execute_cmds "$postlink_cmds" 'exit $?'
+      fi
+
+      # Now create the wrapper script.
+      func_verbose "creating $output"
+
+      # Quote the relink command for shipping.
+      if test -n "$relink_command"; then
+	# Preserve any variables that may affect compiler behavior
+	for var in $variables_saved_for_relink; do
+	  if eval test -z \"\${$var+set}\"; then
+	    relink_command="{ test -z \"\${$var+set}\" || $lt_unset $var || { $var=; export $var; }; }; $relink_command"
+	  elif eval var_value=\$$var; test -z "$var_value"; then
+	    relink_command="$var=; export $var; $relink_command"
+	  else
+	    func_quote_for_eval "$var_value"
+	    relink_command="$var=$func_quote_for_eval_result; export $var; $relink_command"
+	  fi
+	done
+	relink_command="(cd `pwd`; $relink_command)"
+	relink_command=`$ECHO "$relink_command" | $SED "$sed_quote_subst"`
+      fi
+
+      # Only actually do things if not in dry run mode.
+      $opt_dry_run || {
+	# win32 will think the script is a binary if it has
+	# a .exe suffix, so we strip it off here.
+	case $output in
+	  *.exe) func_stripname '' '.exe' "$output"
+	         output=$func_stripname_result ;;
+	esac
+	# test for cygwin because mv fails w/o .exe extensions
+	case $host in
+	  *cygwin*)
+	    exeext=.exe
+	    func_stripname '' '.exe' "$outputname"
+	    outputname=$func_stripname_result ;;
+	  *) exeext= ;;
+	esac
+	case $host in
+	  *cygwin* | *mingw* )
+	    func_dirname_and_basename "$output" "" "."
+	    output_name=$func_basename_result
+	    output_path=$func_dirname_result
+	    cwrappersource="$output_path/$objdir/lt-$output_name.c"
+	    cwrapper="$output_path/$output_name.exe"
+	    $RM $cwrappersource $cwrapper
+	    trap "$RM $cwrappersource $cwrapper; exit $EXIT_FAILURE" 1 2 15
+
+	    func_emit_cwrapperexe_src > $cwrappersource
+
+	    # The wrapper executable is built using the $host compiler,
+	    # because it contains $host paths and files. If cross-
+	    # compiling, it, like the target executable, must be
+	    # executed on the $host or under an emulation environment.
+	    $opt_dry_run || {
+	      $LTCC $LTCFLAGS -o $cwrapper $cwrappersource
+	      $STRIP $cwrapper
+	    }
+
+	    # Now, create the wrapper script for func_source use:
+	    func_ltwrapper_scriptname $cwrapper
+	    $RM $func_ltwrapper_scriptname_result
+	    trap "$RM $func_ltwrapper_scriptname_result; exit $EXIT_FAILURE" 1 2 15
+	    $opt_dry_run || {
+	      # note: this script will not be executed, so do not chmod.
+	      if test "x$build" = "x$host" ; then
+		$cwrapper --lt-dump-script > $func_ltwrapper_scriptname_result
+	      else
+		func_emit_wrapper no > $func_ltwrapper_scriptname_result
+	      fi
+	    }
+	  ;;
+	  * )
+	    $RM $output
+	    trap "$RM $output; exit $EXIT_FAILURE" 1 2 15
+
+	    func_emit_wrapper no > $output
+	    chmod +x $output
+	  ;;
+	esac
+      }
+      exit $EXIT_SUCCESS
+      ;;
+    esac
+
+    # See if we need to build an old-fashioned archive.
+    for oldlib in $oldlibs; do
+
+      if test "$build_libtool_libs" = convenience; then
+	oldobjs="$libobjs_save $symfileobj"
+	addlibs="$convenience"
+	build_libtool_libs=no
+      else
+	if test "$build_libtool_libs" = module; then
+	  oldobjs="$libobjs_save"
+	  build_libtool_libs=no
+	else
+	  oldobjs="$old_deplibs $non_pic_objects"
+	  if test "$preload" = yes && test -f "$symfileobj"; then
+	    func_append oldobjs " $symfileobj"
+	  fi
+	fi
+	addlibs="$old_convenience"
+      fi
+
+      if test -n "$addlibs"; then
+	gentop="$output_objdir/${outputname}x"
+	func_append generated " $gentop"
+
+	func_extract_archives $gentop $addlibs
+	func_append oldobjs " $func_extract_archives_result"
+      fi
+
+      # Do each command in the archive commands.
+      if test -n "$old_archive_from_new_cmds" && test "$build_libtool_libs" = yes; then
+	cmds=$old_archive_from_new_cmds
+      else
+
+	# Add any objects from preloaded convenience libraries
+	if test -n "$dlprefiles"; then
+	  gentop="$output_objdir/${outputname}x"
+	  func_append generated " $gentop"
+
+	  func_extract_archives $gentop $dlprefiles
+	  func_append oldobjs " $func_extract_archives_result"
+	fi
+
+	# POSIX demands no paths to be encoded in archives.  We have
+	# to avoid creating archives with duplicate basenames if we
+	# might have to extract them afterwards, e.g., when creating a
+	# static archive out of a convenience library, or when linking
+	# the entirety of a libtool archive into another (currently
+	# not supported by libtool).
+	if (for obj in $oldobjs
+	    do
+	      func_basename "$obj"
+	      $ECHO "$func_basename_result"
+	    done | sort | sort -uc >/dev/null 2>&1); then
+	  :
+	else
+	  echo "copying selected object files to avoid basename conflicts..."
+	  gentop="$output_objdir/${outputname}x"
+	  func_append generated " $gentop"
+	  func_mkdir_p "$gentop"
+	  save_oldobjs=$oldobjs
+	  oldobjs=
+	  counter=1
+	  for obj in $save_oldobjs
+	  do
+	    func_basename "$obj"
+	    objbase="$func_basename_result"
+	    case " $oldobjs " in
+	    " ") oldobjs=$obj ;;
+	    *[\ /]"$objbase "*)
+	      while :; do
+		# Make sure we don't pick an alternate name that also
+		# overlaps.
+		newobj=lt$counter-$objbase
+		func_arith $counter + 1
+		counter=$func_arith_result
+		case " $oldobjs " in
+		*[\ /]"$newobj "*) ;;
+		*) if test ! -f "$gentop/$newobj"; then break; fi ;;
+		esac
+	      done
+	      func_show_eval "ln $obj $gentop/$newobj || cp $obj $gentop/$newobj"
+	      func_append oldobjs " $gentop/$newobj"
+	      ;;
+	    *) func_append oldobjs " $obj" ;;
+	    esac
+	  done
+	fi
+	func_to_tool_file "$oldlib" func_convert_file_msys_to_w32
+	tool_oldlib=$func_to_tool_file_result
+	eval cmds=\"$old_archive_cmds\"
+
+	func_len " $cmds"
+	len=$func_len_result
+	if test "$len" -lt "$max_cmd_len" || test "$max_cmd_len" -le -1; then
+	  cmds=$old_archive_cmds
+	elif test -n "$archiver_list_spec"; then
+	  func_verbose "using command file archive linking..."
+	  for obj in $oldobjs
+	  do
+	    func_to_tool_file "$obj"
+	    $ECHO "$func_to_tool_file_result"
+	  done > $output_objdir/$libname.libcmd
+	  func_to_tool_file "$output_objdir/$libname.libcmd"
+	  oldobjs=" $archiver_list_spec$func_to_tool_file_result"
+	  cmds=$old_archive_cmds
+	else
+	  # the command line is too long to link in one step, link in parts
+	  func_verbose "using piecewise archive linking..."
+	  save_RANLIB=$RANLIB
+	  RANLIB=:
+	  objlist=
+	  concat_cmds=
+	  save_oldobjs=$oldobjs
+	  oldobjs=
+	  # Is there a better way of finding the last object in the list?
+	  for obj in $save_oldobjs
+	  do
+	    last_oldobj=$obj
+	  done
+	  eval test_cmds=\"$old_archive_cmds\"
+	  func_len " $test_cmds"
+	  len0=$func_len_result
+	  len=$len0
+	  for obj in $save_oldobjs
+	  do
+	    func_len " $obj"
+	    func_arith $len + $func_len_result
+	    len=$func_arith_result
+	    func_append objlist " $obj"
+	    if test "$len" -lt "$max_cmd_len"; then
+	      :
+	    else
+	      # the above command should be used before it gets too long
+	      oldobjs=$objlist
+	      if test "$obj" = "$last_oldobj" ; then
+		RANLIB=$save_RANLIB
+	      fi
+	      test -z "$concat_cmds" || concat_cmds=$concat_cmds~
+	      eval concat_cmds=\"\${concat_cmds}$old_archive_cmds\"
+	      objlist=
+	      len=$len0
+	    fi
+	  done
+	  RANLIB=$save_RANLIB
+	  oldobjs=$objlist
+	  if test "X$oldobjs" = "X" ; then
+	    eval cmds=\"\$concat_cmds\"
+	  else
+	    eval cmds=\"\$concat_cmds~\$old_archive_cmds\"
+	  fi
+	fi
+      fi
+      func_execute_cmds "$cmds" 'exit $?'
+    done
+
+    test -n "$generated" && \
+      func_show_eval "${RM}r$generated"
+
+    # Now create the libtool archive.
+    case $output in
+    *.la)
+      old_library=
+      test "$build_old_libs" = yes && old_library="$libname.$libext"
+      func_verbose "creating $output"
+
+      # Preserve any variables that may affect compiler behavior
+      for var in $variables_saved_for_relink; do
+	if eval test -z \"\${$var+set}\"; then
+	  relink_command="{ test -z \"\${$var+set}\" || $lt_unset $var || { $var=; export $var; }; }; $relink_command"
+	elif eval var_value=\$$var; test -z "$var_value"; then
+	  relink_command="$var=; export $var; $relink_command"
+	else
+	  func_quote_for_eval "$var_value"
+	  relink_command="$var=$func_quote_for_eval_result; export $var; $relink_command"
+	fi
+      done
+      # Quote the link command for shipping.
+      relink_command="(cd `pwd`; $SHELL $progpath $preserve_args --mode=relink $libtool_args @inst_prefix_dir@)"
+      relink_command=`$ECHO "$relink_command" | $SED "$sed_quote_subst"`
+      if test "$hardcode_automatic" = yes ; then
+	relink_command=
+      fi
+
+      # Only create the output if not a dry run.
+      $opt_dry_run || {
+	for installed in no yes; do
+	  if test "$installed" = yes; then
+	    if test -z "$install_libdir"; then
+	      break
+	    fi
+	    output="$output_objdir/$outputname"i
+	    # Replace all uninstalled libtool libraries with the installed ones
+	    newdependency_libs=
+	    for deplib in $dependency_libs; do
+	      case $deplib in
+	      *.la)
+		func_basename "$deplib"
+		name="$func_basename_result"
+		func_resolve_sysroot "$deplib"
+		eval libdir=`${SED} -n -e 's/^libdir=\(.*\)$/\1/p' $func_resolve_sysroot_result`
+		test -z "$libdir" && \
+		  func_fatal_error "\`$deplib' is not a valid libtool archive"
+		func_append newdependency_libs " ${lt_sysroot:+=}$libdir/$name"
+		;;
+	      -L*)
+		func_stripname -L '' "$deplib"
+		func_replace_sysroot "$func_stripname_result"
+		func_append newdependency_libs " -L$func_replace_sysroot_result"
+		;;
+	      -R*)
+		func_stripname -R '' "$deplib"
+		func_replace_sysroot "$func_stripname_result"
+		func_append newdependency_libs " -R$func_replace_sysroot_result"
+		;;
+	      *) func_append newdependency_libs " $deplib" ;;
+	      esac
+	    done
+	    dependency_libs="$newdependency_libs"
+	    newdlfiles=
+
+	    for lib in $dlfiles; do
+	      case $lib in
+	      *.la)
+	        func_basename "$lib"
+		name="$func_basename_result"
+		eval libdir=`${SED} -n -e 's/^libdir=\(.*\)$/\1/p' $lib`
+		test -z "$libdir" && \
+		  func_fatal_error "\`$lib' is not a valid libtool archive"
+		func_append newdlfiles " ${lt_sysroot:+=}$libdir/$name"
+		;;
+	      *) func_append newdlfiles " $lib" ;;
+	      esac
+	    done
+	    dlfiles="$newdlfiles"
+	    newdlprefiles=
+	    for lib in $dlprefiles; do
+	      case $lib in
+	      *.la)
+		# Only pass preopened files to the pseudo-archive (for
+		# eventual linking with the app. that links it) if we
+		# didn't already link the preopened objects directly into
+		# the library:
+		func_basename "$lib"
+		name="$func_basename_result"
+		eval libdir=`${SED} -n -e 's/^libdir=\(.*\)$/\1/p' $lib`
+		test -z "$libdir" && \
+		  func_fatal_error "\`$lib' is not a valid libtool archive"
+		func_append newdlprefiles " ${lt_sysroot:+=}$libdir/$name"
+		;;
+	      esac
+	    done
+	    dlprefiles="$newdlprefiles"
+	  else
+	    newdlfiles=
+	    for lib in $dlfiles; do
+	      case $lib in
+		[\\/]* | [A-Za-z]:[\\/]*) abs="$lib" ;;
+		*) abs=`pwd`"/$lib" ;;
+	      esac
+	      func_append newdlfiles " $abs"
+	    done
+	    dlfiles="$newdlfiles"
+	    newdlprefiles=
+	    for lib in $dlprefiles; do
+	      case $lib in
+		[\\/]* | [A-Za-z]:[\\/]*) abs="$lib" ;;
+		*) abs=`pwd`"/$lib" ;;
+	      esac
+	      func_append newdlprefiles " $abs"
+	    done
+	    dlprefiles="$newdlprefiles"
+	  fi
+	  $RM $output
+	  # place dlname in correct position for cygwin
+	  # In fact, it would be nice if we could use this code for all target
+	  # systems that can't hard-code library paths into their executables
+	  # and that have no shared library path variable independent of PATH,
+	  # but it turns out we can't easily determine that from inspecting
+	  # libtool variables, so we have to hard-code the OSs to which it
+	  # applies here; at the moment, that means platforms that use the PE
+	  # object format with DLL files.  See the long comment at the top of
+	  # tests/bindir.at for full details.
+	  tdlname=$dlname
+	  case $host,$output,$installed,$module,$dlname in
+	    *cygwin*,*lai,yes,no,*.dll | *mingw*,*lai,yes,no,*.dll | *cegcc*,*lai,yes,no,*.dll)
+	      # If a -bindir argument was supplied, place the dll there.
+	      if test "x$bindir" != x ;
+	      then
+		func_relative_path "$install_libdir" "$bindir"
+		tdlname=$func_relative_path_result$dlname
+	      else
+		# Otherwise fall back on heuristic.
+		tdlname=../bin/$dlname
+	      fi
+	      ;;
+	  esac
+	  $ECHO > $output "\
+# $outputname - a libtool library file
+# Generated by $PROGRAM (GNU $PACKAGE$TIMESTAMP) $VERSION
+#
+# Please DO NOT delete this file!
+# It is necessary for linking the library.
+
+# The name that we can dlopen(3).
+dlname='$tdlname'
+
+# Names of this library.
+library_names='$library_names'
+
+# The name of the static archive.
+old_library='$old_library'
+
+# Linker flags that can not go in dependency_libs.
+inherited_linker_flags='$new_inherited_linker_flags'
+
+# Libraries that this one depends upon.
+dependency_libs='$dependency_libs'
+
+# Names of additional weak libraries provided by this library
+weak_library_names='$weak_libs'
+
+# Version information for $libname.
+current=$current
+age=$age
+revision=$revision
+
+# Is this an already installed library?
+installed=$installed
+
+# Should we warn about portability when linking against -modules?
+shouldnotlink=$module
+
+# Files to dlopen/dlpreopen
+dlopen='$dlfiles'
+dlpreopen='$dlprefiles'
+
+# Directory that this library needs to be installed in:
+libdir='$install_libdir'"
+	  if test "$installed" = no && test "$need_relink" = yes; then
+	    $ECHO >> $output "\
+relink_command=\"$relink_command\""
+	  fi
+	done
+      }
+
+      # Do a symbolic link so that the libtool archive can be found in
+      # LD_LIBRARY_PATH before the program is installed.
+      func_show_eval '( cd "$output_objdir" && $RM "$outputname" && $LN_S "../$outputname" "$outputname" )' 'exit $?'
+      ;;
+    esac
+    exit $EXIT_SUCCESS
+}
+
+{ test "$opt_mode" = link || test "$opt_mode" = relink; } &&
+    func_mode_link ${1+"$@"}
+
+
+# func_mode_uninstall arg...
+func_mode_uninstall ()
+{
+    $opt_debug
+    RM="$nonopt"
+    files=
+    rmforce=
+    exit_status=0
+
+    # This variable tells wrapper scripts just to set variables rather
+    # than running their programs.
+    libtool_install_magic="$magic"
+
+    for arg
+    do
+      case $arg in
+      -f) func_append RM " $arg"; rmforce=yes ;;
+      -*) func_append RM " $arg" ;;
+      *) func_append files " $arg" ;;
+      esac
+    done
+
+    test -z "$RM" && \
+      func_fatal_help "you must specify an RM program"
+
+    rmdirs=
+
+    for file in $files; do
+      func_dirname "$file" "" "."
+      dir="$func_dirname_result"
+      if test "X$dir" = X.; then
+	odir="$objdir"
+      else
+	odir="$dir/$objdir"
+      fi
+      func_basename "$file"
+      name="$func_basename_result"
+      test "$opt_mode" = uninstall && odir="$dir"
+
+      # Remember odir for removal later, being careful to avoid duplicates
+      if test "$opt_mode" = clean; then
+	case " $rmdirs " in
+	  *" $odir "*) ;;
+	  *) func_append rmdirs " $odir" ;;
+	esac
+      fi
+
+      # Don't error if the file doesn't exist and rm -f was used.
+      if { test -L "$file"; } >/dev/null 2>&1 ||
+	 { test -h "$file"; } >/dev/null 2>&1 ||
+	 test -f "$file"; then
+	:
+      elif test -d "$file"; then
+	exit_status=1
+	continue
+      elif test "$rmforce" = yes; then
+	continue
+      fi
+
+      rmfiles="$file"
+
+      case $name in
+      *.la)
+	# Possibly a libtool archive, so verify it.
+	if func_lalib_p "$file"; then
+	  func_source $dir/$name
+
+	  # Delete the libtool libraries and symlinks.
+	  for n in $library_names; do
+	    func_append rmfiles " $odir/$n"
+	  done
+	  test -n "$old_library" && func_append rmfiles " $odir/$old_library"
+
+	  case "$opt_mode" in
+	  clean)
+	    case " $library_names " in
+	    *" $dlname "*) ;;
+	    *) test -n "$dlname" && func_append rmfiles " $odir/$dlname" ;;
+	    esac
+	    test -n "$libdir" && func_append rmfiles " $odir/$name $odir/${name}i"
+	    ;;
+	  uninstall)
+	    if test -n "$library_names"; then
+	      # Do each command in the postuninstall commands.
+	      func_execute_cmds "$postuninstall_cmds" 'test "$rmforce" = yes || exit_status=1'
+	    fi
+
+	    if test -n "$old_library"; then
+	      # Do each command in the old_postuninstall commands.
+	      func_execute_cmds "$old_postuninstall_cmds" 'test "$rmforce" = yes || exit_status=1'
+	    fi
+	    # FIXME: should reinstall the best remaining shared library.
+	    ;;
+	  esac
+	fi
+	;;
+
+      *.lo)
+	# Possibly a libtool object, so verify it.
+	if func_lalib_p "$file"; then
+
+	  # Read the .lo file
+	  func_source $dir/$name
+
+	  # Add PIC object to the list of files to remove.
+	  if test -n "$pic_object" &&
+	     test "$pic_object" != none; then
+	    func_append rmfiles " $dir/$pic_object"
+	  fi
+
+	  # Add non-PIC object to the list of files to remove.
+	  if test -n "$non_pic_object" &&
+	     test "$non_pic_object" != none; then
+	    func_append rmfiles " $dir/$non_pic_object"
+	  fi
+	fi
+	;;
+
+      *)
+	if test "$opt_mode" = clean ; then
+	  noexename=$name
+	  case $file in
+	  *.exe)
+	    func_stripname '' '.exe' "$file"
+	    file=$func_stripname_result
+	    func_stripname '' '.exe' "$name"
+	    noexename=$func_stripname_result
+	    # $file with .exe has already been added to rmfiles,
+	    # add $file without .exe
+	    func_append rmfiles " $file"
+	    ;;
+	  esac
+	  # Do a test to see if this is a libtool program.
+	  if func_ltwrapper_p "$file"; then
+	    if func_ltwrapper_executable_p "$file"; then
+	      func_ltwrapper_scriptname "$file"
+	      relink_command=
+	      func_source $func_ltwrapper_scriptname_result
+	      func_append rmfiles " $func_ltwrapper_scriptname_result"
+	    else
+	      relink_command=
+	      func_source $dir/$noexename
+	    fi
+
+	    # note $name still contains .exe if it was in $file originally
+	    # as does the version of $file that was added into $rmfiles
+	    func_append rmfiles " $odir/$name $odir/${name}S.${objext}"
+	    if test "$fast_install" = yes && test -n "$relink_command"; then
+	      func_append rmfiles " $odir/lt-$name"
+	    fi
+	    if test "X$noexename" != "X$name" ; then
+	      func_append rmfiles " $odir/lt-${noexename}.c"
+	    fi
+	  fi
+	fi
+	;;
+      esac
+      func_show_eval "$RM $rmfiles" 'exit_status=1'
+    done
+
+    # Try to remove the ${objdir}s in the directories where we deleted files
+    for dir in $rmdirs; do
+      if test -d "$dir"; then
+	func_show_eval "rmdir $dir >/dev/null 2>&1"
+      fi
+    done
+
+    exit $exit_status
+}
+
+{ test "$opt_mode" = uninstall || test "$opt_mode" = clean; } &&
+    func_mode_uninstall ${1+"$@"}
+
+test -z "$opt_mode" && {
+  help="$generic_help"
+  func_fatal_help "you must specify a MODE"
+}
+
+test -z "$exec_cmd" && \
+  func_fatal_help "invalid operation mode \`$opt_mode'"
+
+if test -n "$exec_cmd"; then
+  eval exec "$exec_cmd"
+  exit $EXIT_FAILURE
+fi
+
+exit $exit_status
+
+
+# The TAGs below are defined such that we never get into a situation
+# in which we disable both kinds of libraries.  Given conflicting
+# choices, we go for a static library, that is the most portable,
+# since we can't tell whether shared libraries were disabled because
+# the user asked for that or because the platform doesn't support
+# them.  This is particularly important on AIX, because we don't
+# support having both static and shared libraries enabled at the same
+# time on that platform, so we default to a shared-only configuration.
+# If a disable-shared tag is given, we'll fallback to a static-only
+# configuration.  But we'll never go from static-only to shared-only.
+
+# ### BEGIN LIBTOOL TAG CONFIG: disable-shared
+build_libtool_libs=no
+build_old_libs=yes
+# ### END LIBTOOL TAG CONFIG: disable-shared
+
+# ### BEGIN LIBTOOL TAG CONFIG: disable-static
+build_old_libs=`case $build_libtool_libs in yes) echo no;; *) echo yes;; esac`
+# ### END LIBTOOL TAG CONFIG: disable-static
+
+# Local Variables:
+# mode:shell-script
+# sh-indentation:2
+# End:
+# vi:sw=2
+
diff --git a/src/gc/bdwgc/mach_dep.c b/src/gc/bdwgc/mach_dep.c
new file mode 100644
index 0000000..1ad1a3a
--- /dev/null
+++ b/src/gc/bdwgc/mach_dep.c
@@ -0,0 +1,308 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifdef NAUT
+# include <nautilus/setjmp.h>
+#else
+# include <stdio.h>
+# include <setjmp.h>
+#endif
+
+#if defined(OS2) || defined(CX_UX) || defined(__CC_ARM)
+# define _setjmp(b) setjmp(b)
+# define _longjmp(b,v) longjmp(b,v)
+#endif
+
+#ifdef AMIGA
+# ifndef __GNUC__
+#   include <dos.h>
+# else
+#   include <machine/reg.h>
+# endif
+#endif
+
+#if defined(__MWERKS__) && !defined(POWERPC)
+
+asm static void PushMacRegisters()
+{
+    sub.w   #4,sp                   // reserve space for one parameter.
+    move.l  a2,(sp)
+    jsr         GC_push_one
+    move.l  a3,(sp)
+    jsr         GC_push_one
+    move.l  a4,(sp)
+    jsr         GC_push_one
+#   if !__option(a6frames)
+        // <pcb> perhaps a6 should be pushed if stack frames are not being used.
+        move.l  a6,(sp)
+        jsr             GC_push_one
+#   endif
+        // skip a5 (globals), a6 (frame pointer), and a7 (stack pointer)
+    move.l  d2,(sp)
+    jsr         GC_push_one
+    move.l  d3,(sp)
+    jsr         GC_push_one
+    move.l  d4,(sp)
+    jsr         GC_push_one
+    move.l  d5,(sp)
+    jsr         GC_push_one
+    move.l  d6,(sp)
+    jsr         GC_push_one
+    move.l  d7,(sp)
+    jsr         GC_push_one
+    add.w   #4,sp                   // fix stack.
+    rts
+}
+
+#endif /* __MWERKS__ */
+
+# if defined(SPARC) || defined(IA64)
+    /* Value returned from register flushing routine; either sp (SPARC) */
+    /* or ar.bsp (IA64).                                                */
+    GC_INNER ptr_t GC_save_regs_ret_val = NULL;
+# endif
+
+/* Routine to mark from registers that are preserved by the C compiler. */
+/* This must be ported to every new architecture.  It is not optional,  */
+/* and should not be used on platforms that are either UNIX-like, or    */
+/* require thread support.                                              */
+
+#undef HAVE_PUSH_REGS
+
+#if defined(USE_ASM_PUSH_REGS)
+# define HAVE_PUSH_REGS
+#else  /* No asm implementation */
+
+# if defined(M68K) && defined(AMIGA)
+    /* This function is not static because it could also be             */
+    /* erroneously defined in .S file, so this error would be caught    */
+    /* by the linker.                                                   */
+    void GC_push_regs(void)
+    {
+         /*  AMIGA - could be replaced by generic code                  */
+         /* a0, a1, d0 and d1 are caller save */
+
+#       ifdef __GNUC__
+          asm("subq.w &0x4,%sp");       /* allocate word on top of stack */
+
+          asm("mov.l %a2,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %a3,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %a4,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %a5,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %a6,(%sp)"); asm("jsr _GC_push_one");
+          /* Skip frame pointer and stack pointer */
+          asm("mov.l %d2,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %d3,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %d4,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %d5,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %d6,(%sp)"); asm("jsr _GC_push_one");
+          asm("mov.l %d7,(%sp)"); asm("jsr _GC_push_one");
+
+          asm("addq.w &0x4,%sp");       /* put stack back where it was  */
+#       else /* !__GNUC__ */
+          GC_push_one(getreg(REG_A2));
+          GC_push_one(getreg(REG_A3));
+#         ifndef __SASC
+            /* Can probably be changed to #if 0 -Kjetil M. (a4=globals) */
+            GC_push_one(getreg(REG_A4));
+#         endif
+          GC_push_one(getreg(REG_A5));
+          GC_push_one(getreg(REG_A6));
+          /* Skip stack pointer */
+          GC_push_one(getreg(REG_D2));
+          GC_push_one(getreg(REG_D3));
+          GC_push_one(getreg(REG_D4));
+          GC_push_one(getreg(REG_D5));
+          GC_push_one(getreg(REG_D6));
+          GC_push_one(getreg(REG_D7));
+#       endif /* !__GNUC__ */
+    }
+#   define HAVE_PUSH_REGS
+
+# elif defined(M68K) && defined(MACOS)
+
+#   if defined(THINK_C)
+#     define PushMacReg(reg) \
+              move.l  reg,(sp) \
+              jsr             GC_push_one
+      void GC_push_regs(void)
+      {
+          asm {
+              sub.w   #4,sp          ; reserve space for one parameter.
+              PushMacReg(a2);
+              PushMacReg(a3);
+              PushMacReg(a4);
+              ; skip a5 (globals), a6 (frame pointer), and a7 (stack pointer)
+              PushMacReg(d2);
+              PushMacReg(d3);
+              PushMacReg(d4);
+              PushMacReg(d5);
+              PushMacReg(d6);
+              PushMacReg(d7);
+              add.w   #4,sp          ; fix stack.
+          }
+      }
+#     define HAVE_PUSH_REGS
+#     undef PushMacReg
+#   elif defined(__MWERKS__)
+      void GC_push_regs(void)
+      {
+          PushMacRegisters();
+      }
+#     define HAVE_PUSH_REGS
+#   endif /* __MWERKS__ */
+# endif /* MACOS */
+
+#endif /* !USE_ASM_PUSH_REGS */
+
+#if defined(HAVE_PUSH_REGS) && defined(THREADS)
+# error GC_push_regs cannot be used with threads
+ /* Would fail for GC_do_blocking.  There are probably other safety     */
+ /* issues.                                                             */
+# undef HAVE_PUSH_REGS
+#endif
+
+#if !defined(HAVE_PUSH_REGS) && defined(UNIX_LIKE)
+# include <signal.h>
+# ifndef NO_GETCONTEXT
+#   include <ucontext.h>
+#   ifdef GETCONTEXT_FPU_EXCMASK_BUG
+#     include <fenv.h>
+#   endif
+# endif
+#endif /* !HAVE_PUSH_REGS */
+
+/* Ensure that either registers are pushed, or callee-save registers    */
+/* are somewhere on the stack, and then call fn(arg, ctxt).             */
+/* ctxt is either a pointer to a ucontext_t we generated, or NULL.      */
+GC_INNER void GC_with_callee_saves_pushed(void (*fn)(ptr_t, void *),
+                                          ptr_t arg)
+{
+    volatile int dummy;
+    void * context = 0;
+
+#   if defined(HAVE_PUSH_REGS)
+      GC_push_regs();
+#   elif defined(UNIX_LIKE) && !defined(NO_GETCONTEXT)
+      /* Older versions of Darwin seem to lack getcontext(). */
+      /* ARM and MIPS Linux often doesn't support a real     */
+      /* getcontext().                                       */
+      ucontext_t ctxt;
+#     ifdef GETCONTEXT_FPU_EXCMASK_BUG
+        /* Workaround a bug (clearing the FPU exception mask) in        */
+        /* getcontext on Linux/x86_64.                                  */
+#       ifdef X86_64
+          /* We manipulate FPU control word here just not to force the  */
+          /* client application to use -lm linker option.               */
+          unsigned short old_fcw;
+          __asm__ __volatile__ ("fstcw %0" : "=m" (*&old_fcw));
+#       else
+          int except_mask = fegetexcept();
+#       endif
+#     endif
+      if (getcontext(&ctxt) < 0)
+        ABORT ("getcontext failed: Use another register retrieval method?");
+#     ifdef GETCONTEXT_FPU_EXCMASK_BUG
+#       ifdef X86_64
+          __asm__ __volatile__ ("fldcw %0" : : "m" (*&old_fcw));
+          {
+            unsigned mxcsr;
+            /* And now correct the exception mask in SSE MXCSR. */
+            __asm__ __volatile__ ("stmxcsr %0" : "=m" (*&mxcsr));
+            mxcsr = (mxcsr & ~(FE_ALL_EXCEPT << 7)) |
+                        ((old_fcw & FE_ALL_EXCEPT) << 7);
+            __asm__ __volatile__ ("ldmxcsr %0" : : "m" (*&mxcsr));
+          }
+#       else /* !X86_64 */
+          if (feenableexcept(except_mask) < 0)
+            ABORT("feenableexcept failed");
+#       endif
+#     endif
+      context = &ctxt;
+#     if defined(SPARC) || defined(IA64)
+        /* On a register window machine, we need to save register       */
+        /* contents on the stack for this to work.  This may already be */
+        /* subsumed by the getcontext() call.                           */
+        GC_save_regs_ret_val = GC_save_regs_in_stack();
+#     endif /* register windows. */
+#   elif defined(HAVE_BUILTIN_UNWIND_INIT) \
+         && !(defined(POWERPC) && defined(DARWIN)) \
+         && !(defined(I386) && defined(RTEMS))
+      /* This was suggested by Richard Henderson as the way to  */
+      /* force callee-save registers and register windows onto  */
+      /* the stack.                                             */
+      /* Mark Sibly points out that this doesn't seem to work   */
+      /* on MacOS 10.3.9/PowerPC.                               */
+      __builtin_unwind_init();
+#   else /* !HAVE_BUILTIN_UNWIND_INIT && !UNIX_LIKE  */
+         /* && !HAVE_PUSH_REGS                       */
+        /* Generic code                          */
+        /* The idea is due to Parag Patel at HP. */
+        /* We're not sure whether he would like  */
+        /* to be acknowledged for it or not.     */
+        jmp_buf regs;
+          
+#       ifdef NAUT 
+
+        /* Nautilus version is modified to avoid compiler warnings */
+        for (unsigned i = 0; i < (sizeof(__jmp_buf)/sizeof(uint64_t)); ++i ) {
+          regs->__jmpbuf[i] = 0; 
+        }
+
+        (void) setjmp(regs);
+        
+#       else 
+
+        register word * i = (word *) regs;
+        register ptr_t lim = (ptr_t)(regs) + (sizeof regs);
+
+        // Setjmp doesn't always clear all of the buffer.               
+        // That tends to preserve garbage.  Clear it.            
+        for (; (char *)i < lim; i++) {
+            *i = 0;
+        }
+
+#         if defined(MSWIN32) || defined(MSWINCE) || defined(UTS4) \
+             || defined(LINUX) || defined(EWS4800) || defined(RTEMS) 
+          
+            (void) setjmp(regs);
+#         else
+            (void) _setjmp(regs);
+            /* We don't want to mess with signals. According to   */
+            /* SUSV3, setjmp() may or may not save signal mask.   */
+            /* _setjmp won't, but is less portable.               */
+#         endif
+#       endif
+#   endif /* !HAVE_PUSH_REGS ... */
+    /* FIXME: context here is sometimes just zero.  At the moment the   */
+    /* callees don't really need it.                                    */
+    fn(arg, context);
+    /* Strongly discourage the compiler from treating the above */
+    /* as a tail-call, since that would pop the register        */
+    /* contents before we get a chance to look at them.         */
+    GC_noop1((word)(&dummy));
+}
+
+#if defined(ASM_CLEAR_CODE)
+# ifdef LINT
+    /*ARGSUSED*/
+    ptr_t GC_clear_stack_inner(ptr_t arg, word limit)
+    {
+      return(arg);
+    }
+    /* The real version is in a .S file */
+# endif
+#endif /* ASM_CLEAR_CODE */
diff --git a/src/gc/bdwgc/malloc.c b/src/gc/bdwgc/malloc.c
new file mode 100644
index 0000000..bf74c1a
--- /dev/null
+++ b/src/gc/bdwgc/malloc.c
@@ -0,0 +1,579 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifndef NAUT
+# include <stdio.h>
+# include <string.h>
+#endif
+
+/* Allocate reclaim list for kind:      */
+/* Return TRUE on success               */
+STATIC GC_bool GC_alloc_reclaim_list(struct obj_kind *kind)
+{
+    struct hblk ** result = (struct hblk **)
+                GC_scratch_alloc((MAXOBJGRANULES+1) * sizeof(struct hblk *));
+    if (result == 0) return(FALSE);
+    BZERO(result, (MAXOBJGRANULES+1)*sizeof(struct hblk *));
+    kind -> ok_reclaim_list = result;
+    return(TRUE);
+}
+
+GC_INNER GC_bool GC_collect_or_expand(word needed_blocks,
+                                      GC_bool ignore_off_page,
+                                      GC_bool retry); /* from alloc.c */
+
+/* Allocate a large block of size lb bytes.     */
+/* The block is not cleared.                    */
+/* Flags is 0 or IGNORE_OFF_PAGE.               */
+/* We hold the allocation lock.                 */
+/* EXTRA_BYTES were already added to lb.        */
+GC_INNER ptr_t GC_alloc_large(size_t lb, int k, unsigned flags)
+{
+    struct hblk * h;
+    word n_blocks;
+    ptr_t result;
+    GC_bool retry = FALSE;
+
+    /* Round up to a multiple of a granule. */
+      lb = (lb + GRANULE_BYTES - 1) & ~(GRANULE_BYTES - 1);
+    n_blocks = OBJ_SZ_TO_BLOCKS(lb);
+    if (!GC_is_initialized) GC_init();
+    /* Do our share of marking work */
+        if (GC_incremental && !GC_dont_gc)
+            GC_collect_a_little_inner((int)n_blocks);
+    h = GC_allochblk(lb, k, flags);
+#   ifdef USE_MUNMAP
+        if (0 == h) {
+            GC_merge_unmapped();
+            h = GC_allochblk(lb, k, flags);
+        }
+#   endif
+    while (0 == h && GC_collect_or_expand(n_blocks, flags != 0, retry)) {
+        h = GC_allochblk(lb, k, flags);
+        retry = TRUE;
+    }
+    if (h == 0) {
+        result = 0;
+    } else {
+        size_t total_bytes = n_blocks * HBLKSIZE;
+        if (n_blocks > 1) {
+            GC_large_allocd_bytes += total_bytes;
+            if (GC_large_allocd_bytes > GC_max_large_allocd_bytes)
+                GC_max_large_allocd_bytes = GC_large_allocd_bytes;
+        }
+        result = h -> hb_body;
+    }
+    return result;
+}
+
+/* Allocate a large block of size lb bytes.  Clear if appropriate.      */
+/* We hold the allocation lock.                                         */
+/* EXTRA_BYTES were already added to lb.                                */
+STATIC ptr_t GC_alloc_large_and_clear(size_t lb, int k, unsigned flags)
+{
+    ptr_t result = GC_alloc_large(lb, k, flags);
+    word n_blocks = OBJ_SZ_TO_BLOCKS(lb);
+
+    if (0 == result) return 0;
+    if (GC_debugging_started || GC_obj_kinds[k].ok_init) {
+        /* Clear the whole block, in case of GC_realloc call. */
+        BZERO(result, n_blocks * HBLKSIZE);
+    }
+    return result;
+}
+
+/* allocate lb bytes for an object of kind k.   */
+/* Should not be used to directly to allocate   */
+/* objects such as STUBBORN objects that        */
+/* require special handling on allocation.      */
+/* First a version that assumes we already      */
+/* hold lock:                                   */
+GC_INNER void * GC_generic_malloc_inner(size_t lb, int k)
+{
+    void *op;
+
+    if(SMALL_OBJ(lb)) {
+        struct obj_kind * kind = GC_obj_kinds + k;
+        size_t lg = GC_size_map[lb];
+        void ** opp = &(kind -> ok_freelist[lg]);
+
+        if( (op = *opp) == 0 ) {
+            if (GC_size_map[lb] == 0) {
+              if (!GC_is_initialized) GC_init();
+              if (GC_size_map[lb] == 0) GC_extend_size_map(lb);
+              return(GC_generic_malloc_inner(lb, k));
+            }
+            if (kind -> ok_reclaim_list == 0) {
+                if (!GC_alloc_reclaim_list(kind)) goto out;
+            }
+            op = GC_allocobj(lg, k);
+            if (op == 0) goto out;
+        }
+        *opp = obj_link(op);
+        obj_link(op) = 0;
+        GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+    } else {
+        op = (ptr_t)GC_alloc_large_and_clear(ADD_SLOP(lb), k, 0);
+        GC_bytes_allocd += lb;
+    }
+
+out:
+    return op;
+}
+
+/* Allocate a composite object of size n bytes.  The caller guarantees  */
+/* that pointers past the first page are not relevant.  Caller holds    */
+/* allocation lock.                                                     */
+GC_INNER void * GC_generic_malloc_inner_ignore_off_page(size_t lb, int k)
+{
+    word lb_adjusted;
+    void * op;
+
+    if (lb <= HBLKSIZE)
+        return(GC_generic_malloc_inner(lb, k));
+    lb_adjusted = ADD_SLOP(lb);
+    op = GC_alloc_large_and_clear(lb_adjusted, k, IGNORE_OFF_PAGE);
+    GC_bytes_allocd += lb_adjusted;
+    return op;
+}
+
+GC_API void * GC_CALL GC_generic_malloc(size_t lb, int k)
+{
+    void * result;
+    DCL_LOCK_STATE;
+
+    if (GC_have_errors) GC_print_all_errors();
+    GC_INVOKE_FINALIZERS();
+    if (SMALL_OBJ(lb)) {
+        LOCK();
+        result = GC_generic_malloc_inner((word)lb, k);
+        UNLOCK();
+    } else {
+        size_t lg;
+        size_t lb_rounded;
+        word n_blocks;
+        GC_bool init;
+        lg = ROUNDED_UP_GRANULES(lb);
+        lb_rounded = GRANULES_TO_BYTES(lg);
+        if (lb_rounded < lb)
+            return((*GC_get_oom_fn())(lb));
+        n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);
+        init = GC_obj_kinds[k].ok_init;
+        LOCK();
+        result = (ptr_t)GC_alloc_large(lb_rounded, k, 0);
+        if (0 != result) {
+          if (GC_debugging_started) {
+            BZERO(result, n_blocks * HBLKSIZE);
+          } else {
+#           ifdef THREADS
+              /* Clear any memory that might be used for GC descriptors */
+              /* before we release the lock.                            */
+                ((word *)result)[0] = 0;
+                ((word *)result)[1] = 0;
+                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;
+                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;
+#           endif
+          }
+        }
+        GC_bytes_allocd += lb_rounded;
+        UNLOCK();
+        if (init && !GC_debugging_started && 0 != result) {
+            BZERO(result, n_blocks * HBLKSIZE);
+        }
+    }
+    if (0 == result) {
+        return((*GC_get_oom_fn())(lb));
+    } else {
+        return(result);
+    }
+}
+
+/* Allocate lb bytes of atomic (pointer-free) data. */
+#ifdef THREAD_LOCAL_ALLOC
+  GC_INNER void * GC_core_malloc_atomic(size_t lb)
+#else
+  GC_API void * GC_CALL GC_malloc_atomic(size_t lb)
+#endif
+{
+    void *op;
+    void ** opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    if(SMALL_OBJ(lb)) {
+        lg = GC_size_map[lb];
+        opp = &(GC_aobjfreelist[lg]);
+        LOCK();
+        if (EXPECT((op = *opp) == 0, FALSE)) {
+            UNLOCK();
+            return(GENERAL_MALLOC((word)lb, PTRFREE));
+        }
+        *opp = obj_link(op);
+        GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+        UNLOCK();
+        return((void *) op);
+   } else {
+       return(GENERAL_MALLOC((word)lb, PTRFREE));
+   }
+}
+
+/* Allocate lb bytes of composite (pointerful) data */
+#ifdef THREAD_LOCAL_ALLOC
+  GC_INNER void * GC_core_malloc(size_t lb)
+#else
+  GC_API void * GC_CALL GC_malloc(size_t lb)
+#endif
+{
+    void *op;
+    void **opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    if(SMALL_OBJ(lb)) {
+        lg = GC_size_map[lb];
+        opp = (void **)&(GC_objfreelist[lg]);
+        LOCK();
+        if (EXPECT((op = *opp) == 0, FALSE)) {
+            UNLOCK();
+            return (GENERAL_MALLOC((word)lb, NORMAL));
+        }
+        GC_ASSERT(0 == obj_link(op)
+                  || ((word)obj_link(op)
+                        <= (word)GC_greatest_plausible_heap_addr
+                     && (word)obj_link(op)
+                        >= (word)GC_least_plausible_heap_addr));
+        *opp = obj_link(op);
+        obj_link(op) = 0;
+        GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+        UNLOCK();
+        return op;
+   } else {
+       return(GENERAL_MALLOC(lb, NORMAL));
+   }
+}
+
+/* Allocate lb bytes of pointerful, traced, but not collectible data.   */
+GC_API void * GC_CALL GC_malloc_uncollectable(size_t lb)
+{
+    void *op;
+    void **opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    if( SMALL_OBJ(lb) ) {
+        if (EXTRA_BYTES != 0 && lb != 0) lb--;
+                  /* We don't need the extra byte, since this won't be  */
+                  /* collected anyway.                                  */
+        lg = GC_size_map[lb];
+        opp = &(GC_uobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) != 0 ) {
+            *opp = obj_link(op);
+            obj_link(op) = 0;
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+            /* Mark bit was already set on free list.  It will be       */
+            /* cleared only temporarily during a collection, as a       */
+            /* result of the normal free list mark bit clearing.        */
+            GC_non_gc_bytes += GRANULES_TO_BYTES(lg);
+            UNLOCK();
+        } else {
+            UNLOCK();
+            op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);
+            /* For small objects, the free lists are completely marked. */
+        }
+        GC_ASSERT(0 == op || GC_is_marked(op));
+        return((void *) op);
+    } else {
+        hdr * hhdr;
+
+        op = (ptr_t)GC_generic_malloc((word)lb, UNCOLLECTABLE);
+        if (0 == op) return(0);
+
+        GC_ASSERT(((word)op & (HBLKSIZE - 1)) == 0); /* large block */
+        hhdr = HDR(op);
+        /* We don't need the lock here, since we have an undisguised    */
+        /* pointer.  We do need to hold the lock while we adjust        */
+        /* mark bits.                                                   */
+        LOCK();
+        set_mark_bit_from_hdr(hhdr, 0); /* Only object. */
+#       ifndef THREADS
+          GC_ASSERT(hhdr -> hb_n_marks == 0);
+                /* This is not guaranteed in the multi-threaded case    */
+                /* because the counter could be updated before locking. */
+#       endif
+        hhdr -> hb_n_marks = 1;
+        UNLOCK();
+        return((void *) op);
+    }
+}
+
+#ifdef REDIRECT_MALLOC
+
+# ifndef MSWINCE
+#  include <errno.h>
+# endif
+
+/* Avoid unnecessary nested procedure calls here, by #defining some     */
+/* malloc replacements.  Otherwise we end up saving a                   */
+/* meaningless return address in the object.  It also speeds things up, */
+/* but it is admittedly quite ugly.                                     */
+# define GC_debug_malloc_replacement(lb) GC_debug_malloc(lb, GC_DBG_EXTRAS)
+
+void * malloc(size_t lb)
+{
+    /* It might help to manually inline the GC_malloc call here.        */
+    /* But any decent compiler should reduce the extra procedure call   */
+    /* to at most a jump instruction in this case.                      */
+#   if defined(I386) && defined(GC_SOLARIS_THREADS)
+      /* Thread initialization can call malloc before we're ready for.  */
+      /* It's not clear that this is enough to help matters.            */
+      /* The thread implementation may well call malloc at other        */
+      /* inopportune times.                                             */
+      if (!GC_is_initialized) return sbrk(lb);
+#   endif /* I386 && GC_SOLARIS_THREADS */
+    return((void *)REDIRECT_MALLOC(lb));
+}
+
+#if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */
+  STATIC ptr_t GC_libpthread_start = 0;
+  STATIC ptr_t GC_libpthread_end = 0;
+  STATIC ptr_t GC_libld_start = 0;
+  STATIC ptr_t GC_libld_end = 0;
+
+  STATIC void GC_init_lib_bounds(void)
+  {
+    if (GC_libpthread_start != 0) return;
+    GC_init(); /* if not called yet */
+    if (!GC_text_mapping("libpthread-",
+                         &GC_libpthread_start, &GC_libpthread_end)) {
+        WARN("Failed to find libpthread.so text mapping: Expect crash\n", 0);
+        /* This might still work with some versions of libpthread,      */
+        /* so we don't abort.  Perhaps we should.                       */
+        /* Generate message only once:                                  */
+          GC_libpthread_start = (ptr_t)1;
+    }
+    if (!GC_text_mapping("ld-", &GC_libld_start, &GC_libld_end)) {
+        WARN("Failed to find ld.so text mapping: Expect crash\n", 0);
+    }
+  }
+#endif /* GC_LINUX_THREADS */
+
+#include <limits.h>
+#ifdef SIZE_MAX
+# define GC_SIZE_MAX SIZE_MAX
+#else
+# define GC_SIZE_MAX (~(size_t)0)
+#endif
+
+#define GC_SQRT_SIZE_MAX ((1U << (WORDSZ / 2)) - 1)
+
+void * calloc(size_t n, size_t lb)
+{
+    if ((lb | n) > GC_SQRT_SIZE_MAX /* fast initial test */
+        && lb && n > GC_SIZE_MAX / lb)
+      return NULL;
+#   if defined(GC_LINUX_THREADS) /* && !defined(USE_PROC_FOR_LIBRARIES) */
+        /* libpthread allocated some memory that is only pointed to by  */
+        /* mmapped thread stacks.  Make sure it is not collectible.     */
+        {
+          static GC_bool lib_bounds_set = FALSE;
+          ptr_t caller = (ptr_t)__builtin_return_address(0);
+          /* This test does not need to ensure memory visibility, since */
+          /* the bounds will be set when/if we create another thread.   */
+          if (!lib_bounds_set) {
+            GC_init_lib_bounds();
+            lib_bounds_set = TRUE;
+          }
+          if ((caller >= GC_libpthread_start && caller < GC_libpthread_end)
+              || (caller >= GC_libld_start && caller < GC_libld_end))
+            return GC_malloc_uncollectable(n*lb);
+          /* The two ranges are actually usually adjacent, so there may */
+          /* be a way to speed this up.                                 */
+        }
+#   endif
+    return((void *)REDIRECT_MALLOC(n*lb));
+}
+
+#ifndef strdup
+  char *strdup(const char *s)
+  {
+    size_t lb = strlen(s) + 1;
+    char *result = (char *)REDIRECT_MALLOC(lb);
+    if (result == 0) {
+      errno = ENOMEM;
+      return 0;
+    }
+    BCOPY(s, result, lb);
+    return result;
+  }
+#endif /* !defined(strdup) */
+ /* If strdup is macro defined, we assume that it actually calls malloc, */
+ /* and thus the right thing will happen even without overriding it.     */
+ /* This seems to be true on most Linux systems.                         */
+
+#ifndef strndup
+  /* This is similar to strdup().       */
+  char *strndup(const char *str, size_t size)
+  {
+    char *copy;
+    size_t len = strlen(str);
+    if (len > size)
+      len = size;
+    copy = (char *)REDIRECT_MALLOC(len + 1);
+    if (copy == NULL) {
+      errno = ENOMEM;
+      return NULL;
+    }
+    BCOPY(str, copy, len);
+    copy[len] = '\0';
+    return copy;
+  }
+#endif /* !strndup */
+
+#undef GC_debug_malloc_replacement
+
+#endif /* REDIRECT_MALLOC */
+
+/* Explicitly deallocate an object p.                           */
+GC_API void GC_CALL GC_free(void * p)
+{
+    struct hblk *h;
+    hdr *hhdr;
+    size_t sz; /* In bytes */
+    size_t ngranules;   /* sz in granules */
+    void **flh;
+    int knd;
+    struct obj_kind * ok;
+    DCL_LOCK_STATE;
+
+    if (p == 0) return;
+        /* Required by ANSI.  It's not my fault ...     */
+#   ifdef LOG_ALLOCS
+      GC_err_printf("GC_free(%p): %lu\n", p, (unsigned long)GC_gc_no);
+#   endif
+    h = HBLKPTR(p);
+    hhdr = HDR(h);
+#   if defined(REDIRECT_MALLOC) && \
+        (defined(GC_SOLARIS_THREADS) || defined(GC_LINUX_THREADS) \
+         || defined(MSWIN32))
+        /* For Solaris, we have to redirect malloc calls during         */
+        /* initialization.  For the others, this seems to happen        */
+        /* implicitly.                                                  */
+        /* Don't try to deallocate that memory.                         */
+        if (0 == hhdr) return;
+#   endif
+    GC_ASSERT(GC_base(p) == p);
+    sz = hhdr -> hb_sz;
+    ngranules = BYTES_TO_GRANULES(sz);
+    knd = hhdr -> hb_obj_kind;
+    ok = &GC_obj_kinds[knd];
+    if (EXPECT(ngranules <= MAXOBJGRANULES, TRUE)) {
+        LOCK();
+        GC_bytes_freed += sz;
+        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;
+                /* Its unnecessary to clear the mark bit.  If the       */
+                /* object is reallocated, it doesn't matter.  O.w. the  */
+                /* collector will do it, since it's on a free list.     */
+        if (ok -> ok_init) {
+            BZERO((word *)p + 1, sz-sizeof(word));
+        }
+        flh = &(ok -> ok_freelist[ngranules]);
+        obj_link(p) = *flh;
+        *flh = (ptr_t)p;
+        UNLOCK();
+    } else {
+        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);
+        LOCK();
+        GC_bytes_freed += sz;
+        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;
+        if (nblocks > 1) {
+          GC_large_allocd_bytes -= nblocks * HBLKSIZE;
+        }
+        GC_freehblk(h);
+        UNLOCK();
+    }
+}
+
+/* Explicitly deallocate an object p when we already hold lock.         */
+/* Only used for internally allocated objects, so we can take some      */
+/* shortcuts.                                                           */
+#ifdef THREADS
+  GC_INNER void GC_free_inner(void * p)
+  {
+    struct hblk *h;
+    hdr *hhdr;
+    size_t sz; /* bytes */
+    size_t ngranules;  /* sz in granules */
+    void ** flh;
+    int knd;
+    struct obj_kind * ok;
+    DCL_LOCK_STATE;
+
+    h = HBLKPTR(p);
+    hhdr = HDR(h);
+    knd = hhdr -> hb_obj_kind;
+    sz = hhdr -> hb_sz;
+    ngranules = BYTES_TO_GRANULES(sz);
+    ok = &GC_obj_kinds[knd];
+    if (ngranules <= MAXOBJGRANULES) {
+        GC_bytes_freed += sz;
+        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;
+        if (ok -> ok_init) {
+            BZERO((word *)p + 1, sz-sizeof(word));
+        }
+        flh = &(ok -> ok_freelist[ngranules]);
+        obj_link(p) = *flh;
+        *flh = (ptr_t)p;
+    } else {
+        size_t nblocks = OBJ_SZ_TO_BLOCKS(sz);
+        GC_bytes_freed += sz;
+        if (IS_UNCOLLECTABLE(knd)) GC_non_gc_bytes -= sz;
+        if (nblocks > 1) {
+          GC_large_allocd_bytes -= nblocks * HBLKSIZE;
+        }
+        GC_freehblk(h);
+    }
+  }
+#endif /* THREADS */
+
+#if defined(REDIRECT_MALLOC) && !defined(REDIRECT_FREE)
+# define REDIRECT_FREE GC_free
+#endif
+
+#ifdef REDIRECT_FREE
+  void free(void * p)
+  {
+#   if defined(GC_LINUX_THREADS) && !defined(USE_PROC_FOR_LIBRARIES)
+        {
+          /* Don't bother with initialization checks.  If nothing       */
+          /* has been initialized, the check fails, and that's safe,    */
+          /* since we have not allocated uncollectible objects neither. */
+          ptr_t caller = (ptr_t)__builtin_return_address(0);
+          /* This test does not need to ensure memory visibility, since */
+          /* the bounds will be set when/if we create another thread.   */
+          if (caller >= GC_libpthread_start && caller < GC_libpthread_end
+              || (caller >= GC_libld_start && caller < GC_libld_end)) {
+            GC_free(p);
+            return;
+          }
+        }
+#   endif
+#   ifndef IGNORE_FREE
+      REDIRECT_FREE(p);
+#   endif
+  }
+#endif /* REDIRECT_FREE */
diff --git a/src/gc/bdwgc/mallocx.c b/src/gc/bdwgc/mallocx.c
new file mode 100644
index 0000000..3a0c40a
--- /dev/null
+++ b/src/gc/bdwgc/mallocx.c
@@ -0,0 +1,612 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+/*
+ * These are extra allocation routines which are likely to be less
+ * frequently used than those in malloc.c.  They are separate in the
+ * hope that the .o file will be excluded from statically linked
+ * executables.  We should probably break this up further.
+ */
+
+
+#ifdef NAUT
+# include <nautilus/errno.h>
+# include <nautilus/printk.h>
+#else 
+# include <stdio.h>
+# include <string.h>
+# ifdef MSWINCE
+#  ifndef WIN32_LEAN_AND_MEAN
+#    define WIN32_LEAN_AND_MEAN 1
+#  endif
+#  define NOSERVICE
+#  include <windows.h>
+# else
+#  include <errno.h>
+# endif
+#endif
+
+/* Some externally visible but unadvertised variables to allow access to */
+/* free lists from inlined allocators without including gc_priv.h        */
+/* or introducing dependencies on internal data structure layouts.       */
+void ** const GC_objfreelist_ptr = GC_objfreelist;
+void ** const GC_aobjfreelist_ptr = GC_aobjfreelist;
+void ** const GC_uobjfreelist_ptr = GC_uobjfreelist;
+# ifdef ATOMIC_UNCOLLECTABLE
+    void ** const GC_auobjfreelist_ptr = GC_auobjfreelist;
+# endif
+
+
+STATIC void * GC_generic_or_special_malloc(size_t lb, int knd)
+{
+    switch(knd) {
+#     ifdef STUBBORN_ALLOC
+        case STUBBORN:
+            return(GC_malloc_stubborn((size_t)lb));
+#     endif
+        case PTRFREE:
+            return(GC_malloc_atomic((size_t)lb));
+        case NORMAL:
+            return(GC_malloc((size_t)lb));
+        case UNCOLLECTABLE:
+            return(GC_malloc_uncollectable((size_t)lb));
+#       ifdef ATOMIC_UNCOLLECTABLE
+          case AUNCOLLECTABLE:
+            return(GC_malloc_atomic_uncollectable((size_t)lb));
+#       endif /* ATOMIC_UNCOLLECTABLE */
+        default:
+            return(GC_generic_malloc(lb,knd));
+    }
+}
+
+/* Change the size of the block pointed to by p to contain at least   */
+/* lb bytes.  The object may be (and quite likely will be) moved.     */
+/* The kind (e.g. atomic) is the same as that of the old.             */
+/* Shrinking of large blocks is not implemented well.                 */
+GC_API void * GC_CALL GC_realloc(void * p, size_t lb)
+{
+    struct hblk * h;
+    hdr * hhdr;
+    size_t sz;   /* Current size in bytes       */
+    size_t orig_sz;      /* Original sz in bytes        */
+    int obj_kind;
+
+    if (p == 0) return(GC_malloc(lb));  /* Required by ANSI */
+    h = HBLKPTR(p);
+    hhdr = HDR(h);
+    sz = hhdr -> hb_sz;
+    obj_kind = hhdr -> hb_obj_kind;
+    orig_sz = sz;
+
+    if (sz > MAXOBJBYTES) {
+        /* Round it up to the next whole heap block */
+          word descr;
+
+          sz = (sz+HBLKSIZE-1) & (~HBLKMASK);
+          hhdr -> hb_sz = sz;
+          descr = GC_obj_kinds[obj_kind].ok_descriptor;
+          if (GC_obj_kinds[obj_kind].ok_relocate_descr) descr += sz;
+          hhdr -> hb_descr = descr;
+#         ifdef MARK_BIT_PER_OBJ
+            GC_ASSERT(hhdr -> hb_inv_sz == LARGE_INV_SZ);
+#         else
+            GC_ASSERT(hhdr -> hb_large_block &&
+                      hhdr -> hb_map[ANY_INDEX] == 1);
+#         endif
+          if (IS_UNCOLLECTABLE(obj_kind)) GC_non_gc_bytes += (sz - orig_sz);
+          /* Extra area is already cleared by GC_alloc_large_and_clear. */
+    }
+    if (ADD_SLOP(lb) <= sz) {
+        if (lb >= (sz >> 1)) {
+#           ifdef STUBBORN_ALLOC
+                if (obj_kind == STUBBORN) GC_change_stubborn(p);
+#           endif
+            if (orig_sz > lb) {
+              /* Clear unneeded part of object to avoid bogus pointer */
+              /* tracing.                                             */
+              /* Safe for stubborn objects.                           */
+                BZERO(((ptr_t)p) + lb, orig_sz - lb);
+            }
+            return(p);
+        } else {
+            /* shrink */
+              void * result =
+                        GC_generic_or_special_malloc((word)lb, obj_kind);
+
+              if (result == 0) return(0);
+                  /* Could also return original object.  But this       */
+                  /* gives the client warning of imminent disaster.     */
+              BCOPY(p, result, lb);
+#             ifndef IGNORE_FREE
+                GC_free(p);
+#             endif
+              return(result);
+        }
+    } else {
+        /* grow */
+          void * result =
+                GC_generic_or_special_malloc((word)lb, obj_kind);
+
+          if (result == 0) return(0);
+          BCOPY(p, result, sz);
+#         ifndef IGNORE_FREE
+            GC_free(p);
+#         endif
+          return(result);
+    }
+}
+
+# if defined(REDIRECT_MALLOC) && !defined(REDIRECT_REALLOC)
+#   define REDIRECT_REALLOC GC_realloc
+# endif
+
+# ifdef REDIRECT_REALLOC
+
+/* As with malloc, avoid two levels of extra calls here.        */
+# define GC_debug_realloc_replacement(p, lb) \
+        GC_debug_realloc(p, lb, GC_DBG_EXTRAS)
+
+void * realloc(void * p, size_t lb)
+  {
+    return(REDIRECT_REALLOC(p, lb));
+  }
+
+# undef GC_debug_realloc_replacement
+# endif /* REDIRECT_REALLOC */
+
+
+/* Allocate memory such that only pointers to near the          */
+/* beginning of the object are considered.                      */
+/* We avoid holding allocation lock while we clear memory.      */
+GC_INNER void * GC_generic_malloc_ignore_off_page(size_t lb, int k)
+{
+    void *result;
+    size_t lg;
+    size_t lb_rounded;
+    word n_blocks;
+    GC_bool init;
+    DCL_LOCK_STATE;
+
+    if (SMALL_OBJ(lb))
+        return(GC_generic_malloc((word)lb, k));
+    lg = ROUNDED_UP_GRANULES(lb);
+    lb_rounded = GRANULES_TO_BYTES(lg);
+    if (lb_rounded < lb)
+        return((*GC_get_oom_fn())(lb));
+    n_blocks = OBJ_SZ_TO_BLOCKS(lb_rounded);
+    init = GC_obj_kinds[k].ok_init;
+    if (GC_have_errors) GC_print_all_errors();
+    GC_INVOKE_FINALIZERS();
+    LOCK();
+    result = (ptr_t)GC_alloc_large(ADD_SLOP(lb), k, IGNORE_OFF_PAGE);
+    if (0 != result) {
+        if (GC_debugging_started) {
+            BZERO(result, n_blocks * HBLKSIZE);
+        } else {
+#           ifdef THREADS
+              /* Clear any memory that might be used for GC descriptors */
+              /* before we release the lock.                          */
+                ((word *)result)[0] = 0;
+                ((word *)result)[1] = 0;
+                ((word *)result)[GRANULES_TO_WORDS(lg)-1] = 0;
+                ((word *)result)[GRANULES_TO_WORDS(lg)-2] = 0;
+#           endif
+        }
+    }
+    GC_bytes_allocd += lb_rounded;
+    if (0 == result) {
+        GC_oom_func oom_fn = GC_oom_fn;
+        UNLOCK();
+        return((*oom_fn)(lb));
+    } else {
+        UNLOCK();
+        if (init && !GC_debugging_started) {
+            BZERO(result, n_blocks * HBLKSIZE);
+        }
+        return(result);
+    }
+}
+
+GC_API void * GC_CALL GC_malloc_ignore_off_page(size_t lb)
+{
+    return((void *)GC_generic_malloc_ignore_off_page(lb, NORMAL));
+}
+
+GC_API void * GC_CALL GC_malloc_atomic_ignore_off_page(size_t lb)
+{
+    return((void *)GC_generic_malloc_ignore_off_page(lb, PTRFREE));
+}
+
+/* Increment GC_bytes_allocd from code that doesn't have direct access  */
+/* to GC_arrays.                                                        */
+GC_API void GC_CALL GC_incr_bytes_allocd(size_t n)
+{
+    GC_bytes_allocd += n;
+}
+
+/* The same for GC_bytes_freed.                         */
+GC_API void GC_CALL GC_incr_bytes_freed(size_t n)
+{
+    GC_bytes_freed += n;
+}
+
+# ifdef PARALLEL_MARK
+    STATIC volatile signed_word GC_bytes_allocd_tmp = 0;
+                        /* Number of bytes of memory allocated since    */
+                        /* we released the GC lock.  Instead of         */
+                        /* reacquiring the GC lock just to add this in, */
+                        /* we add it in the next time we reacquire      */
+                        /* the lock.  (Atomically adding it doesn't     */
+                        /* work, since we would have to atomically      */
+                        /* update it in GC_malloc, which is too         */
+                        /* expensive.)                                  */
+# endif /* PARALLEL_MARK */
+
+/* Return a list of 1 or more objects of the indicated size, linked     */
+/* through the first word in the object.  This has the advantage that   */
+/* it acquires the allocation lock only once, and may greatly reduce    */
+/* time wasted contending for the allocation lock.  Typical usage would */
+/* be in a thread that requires many items of the same size.  It would  */
+/* keep its own free list in thread-local storage, and call             */
+/* GC_malloc_many or friends to replenish it.  (We do not round up      */
+/* object sizes, since a call indicates the intention to consume many   */
+/* objects of exactly this size.)                                       */
+/* We assume that the size is a multiple of GRANULE_BYTES.              */
+/* We return the free-list by assigning it to *result, since it is      */
+/* not safe to return, e.g. a linked list of pointer-free objects,      */
+/* since the collector would not retain the entire list if it were      */
+/* invoked just as we were returning.                                   */
+/* Note that the client should usually clear the link field.            */
+GC_API void GC_CALL GC_generic_malloc_many(size_t lb, int k, void **result)
+{
+    void *op;
+    void *p;
+    void **opp;
+    size_t lw;      /* Length in words.     */
+    size_t lg;      /* Length in granules.  */
+    signed_word my_bytes_allocd = 0;
+    struct obj_kind * ok = &(GC_obj_kinds[k]);
+    struct hblk ** rlh;
+    DCL_LOCK_STATE;
+    GC_ASSERT(lb != 0 && (lb & (GRANULE_BYTES-1)) == 0);
+    if (!SMALL_OBJ(lb)) {
+        op = GC_generic_malloc(lb, k);
+        if(0 != op) obj_link(op) = 0;
+        *result = op;
+        return;
+    }
+    lw = BYTES_TO_WORDS(lb);
+    lg = BYTES_TO_GRANULES(lb);
+    if (GC_have_errors) GC_print_all_errors();
+    GC_INVOKE_FINALIZERS();
+    LOCK();
+    if (!GC_is_initialized) GC_init();
+    /* Do our share of marking work */
+      if (GC_incremental && !GC_dont_gc) {
+        ENTER_GC();
+        GC_collect_a_little_inner(1);
+        EXIT_GC();
+      }
+    /* First see if we can reclaim a page of objects waiting to be */
+    /* reclaimed.                                                  */
+    rlh = ok -> ok_reclaim_list;
+    if (rlh != NULL) {
+        struct hblk * hbp;
+        hdr * hhdr;
+
+        rlh += lg;
+        while ((hbp = *rlh) != 0) {
+            hhdr = HDR(hbp);
+            *rlh = hhdr -> hb_next;
+            GC_ASSERT(hhdr -> hb_sz == lb);
+            hhdr -> hb_last_reclaimed = (unsigned short) GC_gc_no;
+#           ifdef PARALLEL_MARK
+              if (GC_parallel) {
+                  signed_word my_bytes_allocd_tmp = GC_bytes_allocd_tmp;
+
+                  GC_ASSERT(my_bytes_allocd_tmp >= 0);
+                  /* We only decrement it while holding the GC lock.    */
+                  /* Thus we can't accidentally adjust it down in more  */
+                  /* than one thread simultaneously.                    */
+                  if (my_bytes_allocd_tmp != 0) {
+                    (void)AO_fetch_and_add(
+                                (volatile void *)(&GC_bytes_allocd_tmp),
+                                (AO_t)(-my_bytes_allocd_tmp));
+                    GC_bytes_allocd += my_bytes_allocd_tmp;
+                  }
+                  GC_acquire_mark_lock();
+                  ++ GC_fl_builder_count;
+                  UNLOCK();
+                  GC_release_mark_lock();
+              }
+#           endif
+            op = GC_reclaim_generic(hbp, hhdr, lb,
+                                    ok -> ok_init, 0, &my_bytes_allocd);
+            if (op != 0) {
+              /* We also reclaimed memory, so we need to adjust         */
+              /* that count.                                            */
+              /* This should be atomic, so the results may be           */
+              /* inaccurate.                                            */
+              GC_bytes_found += my_bytes_allocd;
+#             ifdef PARALLEL_MARK
+                if (GC_parallel) {
+                  *result = op;
+                  (void)AO_fetch_and_add(
+                                (volatile AO_t *)(&GC_bytes_allocd_tmp),
+                                (AO_t)(my_bytes_allocd));
+                  GC_acquire_mark_lock();
+                  -- GC_fl_builder_count;
+                  if (GC_fl_builder_count == 0) GC_notify_all_builder();
+                  GC_release_mark_lock();
+                  (void) GC_clear_stack(0);
+                  return;
+                }
+#             endif
+              GC_bytes_allocd += my_bytes_allocd;
+              goto out;
+            }
+#           ifdef PARALLEL_MARK
+              if (GC_parallel) {
+                GC_acquire_mark_lock();
+                -- GC_fl_builder_count;
+                if (GC_fl_builder_count == 0) GC_notify_all_builder();
+                GC_release_mark_lock();
+                LOCK();
+                /* GC lock is needed for reclaim list access.   We      */
+                /* must decrement fl_builder_count before reaquiring GC */
+                /* lock.  Hopefully this path is rare.                  */
+              }
+#           endif
+        }
+    }
+    /* Next try to use prefix of global free list if there is one.      */
+    /* We don't refill it, but we need to use it up before allocating   */
+    /* a new block ourselves.                                           */
+      opp = &(GC_obj_kinds[k].ok_freelist[lg]);
+      if ( (op = *opp) != 0 ) {
+        *opp = 0;
+        my_bytes_allocd = 0;
+        for (p = op; p != 0; p = obj_link(p)) {
+          my_bytes_allocd += lb;
+          if ((word)my_bytes_allocd >= HBLKSIZE) {
+            *opp = obj_link(p);
+            obj_link(p) = 0;
+            break;
+          }
+        }
+        GC_bytes_allocd += my_bytes_allocd;
+        goto out;
+      }
+    /* Next try to allocate a new block worth of objects of this size.  */
+    {
+        struct hblk *h = GC_allochblk(lb, k, 0);
+        if (h != 0) {
+          if (IS_UNCOLLECTABLE(k)) GC_set_hdr_marks(HDR(h));
+          GC_bytes_allocd += HBLKSIZE - HBLKSIZE % lb;
+#         ifdef PARALLEL_MARK
+            if (GC_parallel) {
+              GC_acquire_mark_lock();
+              ++ GC_fl_builder_count;
+              UNLOCK();
+              GC_release_mark_lock();
+
+              op = GC_build_fl(h, lw,
+                        (ok -> ok_init || GC_debugging_started), 0);
+
+              *result = op;
+              GC_acquire_mark_lock();
+              -- GC_fl_builder_count;
+              if (GC_fl_builder_count == 0) GC_notify_all_builder();
+              GC_release_mark_lock();
+              (void) GC_clear_stack(0);
+              return;
+            }
+#         endif
+          op = GC_build_fl(h, lw, (ok -> ok_init || GC_debugging_started), 0);
+          goto out;
+        }
+    }
+    /* As a last attempt, try allocating a single object.  Note that    */
+    /* this may trigger a collection or expand the heap.                */
+      op = GC_generic_malloc_inner(lb, k);
+      if (0 != op) obj_link(op) = 0;
+
+  out:
+    *result = op;
+    UNLOCK();
+    (void) GC_clear_stack(0);
+}
+
+/* Note that the "atomic" version of this would be unsafe, since the    */
+/* links would not be seen by the collector.                            */
+GC_API void * GC_CALL GC_malloc_many(size_t lb)
+{
+    void *result;
+    GC_generic_malloc_many((lb + EXTRA_BYTES + GRANULE_BYTES-1)
+                           & ~(GRANULE_BYTES-1),
+                           NORMAL, &result);
+    return result;
+}
+
+/* Not well tested nor integrated.      */
+/* Debug version is tricky and currently missing.       */
+#include <limits.h>
+
+GC_API void * GC_CALL GC_memalign(size_t align, size_t lb)
+{
+    size_t new_lb;
+    size_t offset;
+    ptr_t result;
+
+    if (align <= GRANULE_BYTES) return GC_malloc(lb);
+    if (align >= HBLKSIZE/2 || lb >= HBLKSIZE/2) {
+        if (align > HBLKSIZE) {
+          return (*GC_get_oom_fn())(LONG_MAX-1024); /* Fail */
+        }
+        return GC_malloc(lb <= HBLKSIZE? HBLKSIZE : lb);
+            /* Will be HBLKSIZE aligned.        */
+    }
+    /* We could also try to make sure that the real rounded-up object size */
+    /* is a multiple of align.  That would be correct up to HBLKSIZE.      */
+    new_lb = lb + align - 1;
+    result = GC_malloc(new_lb);
+    offset = (word)result % align;
+    if (offset != 0) {
+        offset = align - offset;
+        if (!GC_all_interior_pointers) {
+            if (offset >= VALID_OFFSET_SZ) return GC_malloc(HBLKSIZE);
+            GC_register_displacement(offset);
+        }
+    }
+    result = (void *) ((ptr_t)result + offset);
+    GC_ASSERT((word)result % align == 0);
+    return result;
+}
+
+/* This one exists largely to redirect posix_memalign for leaks finding. */
+GC_API int GC_CALL GC_posix_memalign(void **memptr, size_t align, size_t lb)
+{
+  /* Check alignment properly.  */
+  if (((align - 1) & align) != 0 || align < sizeof(void *)) {
+#   ifdef MSWINCE
+      return ERROR_INVALID_PARAMETER;
+#   else
+      return EINVAL;
+#   endif
+  }
+
+  if ((*memptr = GC_memalign(align, lb)) == NULL) {
+#   ifdef MSWINCE
+      return ERROR_NOT_ENOUGH_MEMORY;
+#   else
+      return ENOMEM;
+#   endif
+  }
+  return 0;
+}
+
+#ifdef ATOMIC_UNCOLLECTABLE
+  /* Allocate lb bytes of pointer-free, untraced, uncollectible data    */
+  /* This is normally roughly equivalent to the system malloc.          */
+  /* But it may be useful if malloc is redefined.                       */
+  GC_API void * GC_CALL GC_malloc_atomic_uncollectable(size_t lb)
+  {
+    void *op;
+    void **opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    if( SMALL_OBJ(lb) ) {
+        if (EXTRA_BYTES != 0 && lb != 0) lb--;
+                  /* We don't need the extra byte, since this won't be  */
+                  /* collected anyway.                                  */
+        lg = GC_size_map[lb];
+        opp = &(GC_auobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) != 0 ) {
+            *opp = obj_link(op);
+            obj_link(op) = 0;
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+            /* Mark bit was already set while object was on free list. */
+            GC_non_gc_bytes += GRANULES_TO_BYTES(lg);
+            UNLOCK();
+        } else {
+            UNLOCK();
+            op = (ptr_t)GC_generic_malloc(lb, AUNCOLLECTABLE);
+        }
+        GC_ASSERT(0 == op || GC_is_marked(op));
+        return((void *) op);
+    } else {
+        hdr * hhdr;
+
+        op = (ptr_t)GC_generic_malloc(lb, AUNCOLLECTABLE);
+        if (0 == op) return(0);
+
+        GC_ASSERT(((word)op & (HBLKSIZE - 1)) == 0);
+        hhdr = HDR(op);
+
+        LOCK();
+        set_mark_bit_from_hdr(hhdr, 0); /* Only object. */
+#       ifndef THREADS
+          GC_ASSERT(hhdr -> hb_n_marks == 0);
+#       endif
+        hhdr -> hb_n_marks = 1;
+        UNLOCK();
+        return((void *) op);
+    }
+  }
+#endif /* ATOMIC_UNCOLLECTABLE */
+
+/* provide a version of strdup() that uses the collector to allocate the
+   copy of the string */
+GC_API char * GC_CALL GC_strdup(const char *s)
+{
+  char *copy;
+  size_t lb;
+  if (s == NULL) return NULL;
+  lb = strlen(s) + 1;
+  if ((copy = GC_malloc_atomic(lb)) == NULL) {
+#   if !defined(MSWINCE) && !defined(NAUT)
+      errno = ENOMEM;
+#   endif
+    return NULL;
+  }
+# ifndef MSWINCE
+    strcpy(copy, s);
+# else
+    /* strcpy() is deprecated in WinCE */
+    memcpy(copy, s, lb);
+# endif
+  return copy;
+}
+
+GC_API char * GC_CALL GC_strndup(const char *str, size_t size)
+{
+  char *copy;
+  size_t len = strlen(str); /* str is expected to be non-NULL  */
+  if (len > size)
+    len = size;
+  copy = GC_malloc_atomic(len + 1);
+  if (copy == NULL) {
+#   if !defined(MSWINCE) && !defined(NAUT)
+      errno = ENOMEM;
+#   endif
+    return NULL;
+  }
+  BCOPY(str, copy, len);
+  copy[len] = '\0';
+  return copy;
+}
+
+#ifdef GC_REQUIRE_WCSDUP
+# include <wchar.h> /* for wcslen() */
+
+  GC_API wchar_t * GC_CALL GC_wcsdup(const wchar_t *str)
+  {
+    size_t lb = (wcslen(str) + 1) * sizeof(wchar_t);
+    wchar_t *copy = GC_malloc_atomic(lb);
+    if (copy == NULL) {
+#     ifndef MSWINCE
+        errno = ENOMEM;
+#     endif
+      return NULL;
+    }
+    BCOPY(str, copy, lb);
+    return copy;
+  }
+#endif /* GC_REQUIRE_WCSDUP */
diff --git a/src/gc/bdwgc/mark.c b/src/gc/bdwgc/mark.c
new file mode 100644
index 0000000..143cc49
--- /dev/null
+++ b/src/gc/bdwgc/mark.c
@@ -0,0 +1,1914 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "private/gc_pmark.h"
+
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+#if defined(MSWIN32) && defined(__GNUC__)
+# include <excpt.h>
+#endif
+
+/* We put this here to minimize the risk of inlining. */
+/*VARARGS*/
+#if defined(__BORLANDC__) || defined(__WATCOMC__) || defined(__CC_ARM)
+  /*ARGSUSED*/
+  void GC_noop(void *p, ...) {}
+#else
+# ifdef __DMC__
+    void GC_noop(...) {}
+# else
+    void GC_noop() {}
+# endif
+#endif
+
+/* Single argument version, robust against whole program analysis. */
+volatile word GC_noop_sink;
+GC_API void GC_CALL GC_noop1(word x)
+{
+    GC_noop_sink = x;
+}
+
+/* mark_proc GC_mark_procs[MAX_MARK_PROCS] = {0} -- declared in gc_priv.h */
+
+GC_INNER unsigned GC_n_mark_procs = GC_RESERVED_MARK_PROCS;
+
+/* Initialize GC_obj_kinds properly and standard free lists properly.   */
+/* This must be done statically since they may be accessed before       */
+/* GC_init is called.                                                   */
+/* It's done here, since we need to deal with mark descriptors.         */
+GC_INNER struct obj_kind GC_obj_kinds[MAXOBJKINDS] = {
+/* PTRFREE */ { &GC_aobjfreelist[0], 0 /* filled in dynamically */,
+                0 | GC_DS_LENGTH, FALSE, FALSE },
+/* NORMAL  */ { &GC_objfreelist[0], 0,
+                0 | GC_DS_LENGTH,  /* Adjusted in GC_init for EXTRA_BYTES */
+                TRUE /* add length to descr */, TRUE },
+/* UNCOLLECTABLE */
+              { &GC_uobjfreelist[0], 0,
+                0 | GC_DS_LENGTH, TRUE /* add length to descr */, TRUE },
+# ifdef ATOMIC_UNCOLLECTABLE
+   /* AUNCOLLECTABLE */
+              { &GC_auobjfreelist[0], 0,
+                0 | GC_DS_LENGTH, FALSE /* add length to descr */, FALSE },
+# endif
+# ifdef STUBBORN_ALLOC
+/*STUBBORN*/ { (void **)&GC_sobjfreelist[0], 0,
+                0 | GC_DS_LENGTH, TRUE /* add length to descr */, TRUE },
+# endif
+};
+
+# ifdef ATOMIC_UNCOLLECTABLE
+#   ifdef STUBBORN_ALLOC
+#     define GC_N_KINDS_INITIAL_VALUE 5
+#   else
+#     define GC_N_KINDS_INITIAL_VALUE 4
+#   endif
+# else
+#   ifdef STUBBORN_ALLOC
+#     define GC_N_KINDS_INITIAL_VALUE 4
+#   else
+#     define GC_N_KINDS_INITIAL_VALUE 3
+#   endif
+# endif
+
+GC_INNER unsigned GC_n_kinds = GC_N_KINDS_INITIAL_VALUE;
+
+# ifndef INITIAL_MARK_STACK_SIZE
+#   define INITIAL_MARK_STACK_SIZE (1*HBLKSIZE)
+                /* INITIAL_MARK_STACK_SIZE * sizeof(mse) should be a    */
+                /* multiple of HBLKSIZE.                                */
+                /* The incremental collector actually likes a larger    */
+                /* size, since it want to push all marked dirty objs    */
+                /* before marking anything new.  Currently we let it    */
+                /* grow dynamically.                                    */
+# endif
+
+STATIC word GC_n_rescuing_pages = 0;
+                                /* Number of dirty pages we marked from */
+                                /* excludes ptrfree pages, etc.         */
+
+GC_INNER size_t GC_mark_stack_size = 0;
+
+#ifdef PARALLEL_MARK
+  STATIC volatile AO_t GC_first_nonempty = 0;
+        /* Lowest entry on mark stack   */
+        /* that may be nonempty.        */
+        /* Updated only by initiating   */
+        /* thread.                      */
+#endif
+
+GC_INNER mark_state_t GC_mark_state = MS_NONE;
+
+GC_INNER GC_bool GC_mark_stack_too_small = FALSE;
+
+static struct hblk * scan_ptr;
+
+STATIC GC_bool GC_objects_are_marked = FALSE;
+                /* Are there collectible marked objects in the heap?    */
+
+/* Is a collection in progress?  Note that this can return true in the  */
+/* nonincremental case, if a collection has been abandoned and the      */
+/* mark state is now MS_INVALID.                                        */
+GC_INNER GC_bool GC_collection_in_progress(void)
+{
+    return(GC_mark_state != MS_NONE);
+}
+
+/* clear all mark bits in the header */
+GC_INNER void GC_clear_hdr_marks(hdr *hhdr)
+{
+    size_t last_bit = FINAL_MARK_BIT(hhdr -> hb_sz);
+    BZERO(hhdr -> hb_marks, sizeof(hhdr->hb_marks));
+    set_mark_bit_from_hdr(hhdr, last_bit);
+    hhdr -> hb_n_marks = 0;
+}
+
+/* Set all mark bits in the header.  Used for uncollectible blocks. */
+GC_INNER void GC_set_hdr_marks(hdr *hhdr)
+{
+    unsigned i;
+    size_t sz = hhdr -> hb_sz;
+    unsigned n_marks = (unsigned)FINAL_MARK_BIT(sz);
+
+#   ifdef USE_MARK_BYTES
+      for (i = 0; i <= n_marks; i += (unsigned)MARK_BIT_OFFSET(sz)) {
+        hhdr -> hb_marks[i] = 1;
+      }
+#   else
+      for (i = 0; i < divWORDSZ(n_marks + WORDSZ); ++i) {
+        hhdr -> hb_marks[i] = ONES;
+      }
+#   endif
+#   ifdef MARK_BIT_PER_OBJ
+      hhdr -> hb_n_marks = n_marks - 1;
+#   else
+      hhdr -> hb_n_marks = HBLK_OBJS(sz);
+#   endif
+}
+
+/*
+ * Clear all mark bits associated with block h.
+ */
+/*ARGSUSED*/
+static void clear_marks_for_block(struct hblk *h, word dummy)
+{
+    register hdr * hhdr = HDR(h);
+
+    if (IS_UNCOLLECTABLE(hhdr -> hb_obj_kind)) return;
+        /* Mark bit for these is cleared only once the object is        */
+        /* explicitly deallocated.  This either frees the block, or     */
+        /* the bit is cleared once the object is on the free list.      */
+    GC_clear_hdr_marks(hhdr);
+}
+
+/* Slow but general routines for setting/clearing/asking about mark bits */
+void GC_set_mark_bit(ptr_t p)
+{
+    struct hblk *h = HBLKPTR(p);
+    hdr * hhdr = HDR(h);
+    word bit_no = MARK_BIT_NO(p - (ptr_t)h, hhdr -> hb_sz);
+
+    if (!mark_bit_from_hdr(hhdr, bit_no)) {
+      set_mark_bit_from_hdr(hhdr, bit_no);
+      ++hhdr -> hb_n_marks;
+    }
+}
+
+void GC_clear_mark_bit(ptr_t p)
+{
+    struct hblk *h = HBLKPTR(p);
+    hdr * hhdr = HDR(h);
+    word bit_no = MARK_BIT_NO(p - (ptr_t)h, hhdr -> hb_sz);
+
+    if (mark_bit_from_hdr(hhdr, bit_no)) {
+      size_t n_marks;
+      clear_mark_bit_from_hdr(hhdr, bit_no);
+      n_marks = hhdr -> hb_n_marks - 1;
+#     ifdef PARALLEL_MARK
+        if (n_marks != 0 || !GC_parallel)
+          hhdr -> hb_n_marks = n_marks;
+        /* Don't decrement to zero.  The counts are approximate due to  */
+        /* concurrency issues, but we need to ensure that a count of    */
+        /* zero implies an empty block.                                 */
+#     else
+          hhdr -> hb_n_marks = n_marks;
+#     endif
+    }
+}
+
+GC_bool GC_is_marked(ptr_t p)
+{
+    struct hblk *h = HBLKPTR(p);
+    hdr * hhdr = HDR(h);
+    word bit_no = MARK_BIT_NO(p - (ptr_t)h, hhdr -> hb_sz);
+
+    return((GC_bool)mark_bit_from_hdr(hhdr, bit_no));
+}
+
+
+/*
+ * Clear mark bits in all allocated heap blocks.  This invalidates
+ * the marker invariant, and sets GC_mark_state to reflect this.
+ * (This implicitly starts marking to reestablish the invariant.)
+ */
+GC_INNER void GC_clear_marks(void)
+{
+    GC_apply_to_all_blocks(clear_marks_for_block, (word)0);
+    GC_objects_are_marked = FALSE;
+    GC_mark_state = MS_INVALID;
+    scan_ptr = 0;
+}
+
+#ifdef CHECKSUMS
+  void GC_check_dirty(void);
+#endif
+
+/* Initiate a garbage collection.  Initiates a full collection if the   */
+/* mark state is invalid.                                               */
+GC_INNER void GC_initiate_gc(void)
+{
+#   ifndef GC_DISABLE_INCREMENTAL
+        if (GC_dirty_maintained) GC_read_dirty();
+#   endif
+#   ifdef STUBBORN_ALLOC
+        GC_read_changed();
+#   endif
+#   ifdef CHECKSUMS
+        if (GC_dirty_maintained) GC_check_dirty();
+#   endif
+    GC_n_rescuing_pages = 0;
+    if (GC_mark_state == MS_NONE) {
+        GC_mark_state = MS_PUSH_RESCUERS;
+    } else if (GC_mark_state != MS_INVALID) {
+        ABORT("Unexpected state");
+    } /* else this is really a full collection, and mark        */
+      /* bits are invalid.                                      */
+    scan_ptr = 0;
+}
+
+#ifdef PARALLEL_MARK
+    STATIC void GC_do_parallel_mark(void); /* initiate parallel marking. */
+#endif /* PARALLEL_MARK */
+
+#ifdef GC_DISABLE_INCREMENTAL
+# define GC_push_next_marked_dirty(h) GC_push_next_marked(h)
+#else
+  STATIC struct hblk * GC_push_next_marked_dirty(struct hblk *h);
+                /* Invoke GC_push_marked on next dirty block above h.   */
+                /* Return a pointer just past the end of this block.    */
+#endif /* !GC_DISABLE_INCREMENTAL */
+STATIC struct hblk * GC_push_next_marked(struct hblk *h);
+                /* Ditto, but also mark from clean pages.       */
+STATIC struct hblk * GC_push_next_marked_uncollectable(struct hblk *h);
+                /* Ditto, but mark only from uncollectible pages.       */
+
+static void alloc_mark_stack(size_t);
+
+# if (defined(MSWIN32) || defined(MSWINCE)) && !defined(__GNUC__) \
+        || defined(MSWIN32) && defined(I386) /* for Win98 */ \
+        || defined(USE_PROC_FOR_LIBRARIES) && defined(THREADS)
+    /* Under rare conditions, we may end up marking from nonexistent memory. */
+    /* Hence we need to be prepared to recover by running GC_mark_some       */
+    /* with a suitable handler in place.                                     */
+    /* FIXME: Should we really need it for WinCE?  If yes then          */
+    /* WRAP_MARK_SOME should be also defined for CeGCC which requires   */
+    /* CPU/OS-specific code in mark_ex_handler() and GC_mark_some()     */
+    /* (for manual stack unwinding and exception handler installation). */
+#   define WRAP_MARK_SOME
+# endif
+
+/* Perform a small amount of marking.                   */
+/* We try to touch roughly a page of memory.            */
+/* Return TRUE if we just finished a mark phase.        */
+/* Cold_gc_frame is an address inside a GC frame that   */
+/* remains valid until all marking is complete.         */
+/* A zero value indicates that it's OK to miss some     */
+/* register values.                                     */
+/* We hold the allocation lock.  In the case of         */
+/* incremental collection, the world may not be stopped.*/
+#ifdef WRAP_MARK_SOME
+  /* For win32, this is called after we establish a structured  */
+  /* exception handler, in case Windows unmaps one of our root  */
+  /* segments.  See below.  In either case, we acquire the      */
+  /* allocator lock long before we get here.                    */
+  STATIC GC_bool GC_mark_some_inner(ptr_t cold_gc_frame)
+#else
+  GC_INNER GC_bool GC_mark_some(ptr_t cold_gc_frame)
+#endif
+{
+    switch(GC_mark_state) {
+        case MS_NONE:
+            return(FALSE);
+
+        case MS_PUSH_RESCUERS:
+            if (GC_mark_stack_top
+                >= GC_mark_stack_limit - INITIAL_MARK_STACK_SIZE/2) {
+                /* Go ahead and mark, even though that might cause us to */
+                /* see more marked dirty objects later on.  Avoid this   */
+                /* in the future.                                        */
+                GC_mark_stack_too_small = TRUE;
+                MARK_FROM_MARK_STACK();
+                return(FALSE);
+            } else {
+                scan_ptr = GC_push_next_marked_dirty(scan_ptr);
+                if (scan_ptr == 0) {
+                    if (GC_print_stats) {
+                        GC_log_printf("Marked from %lu dirty pages\n",
+                                      (unsigned long)GC_n_rescuing_pages);
+                    }
+                    GC_push_roots(FALSE, cold_gc_frame);
+                    GC_objects_are_marked = TRUE;
+                    if (GC_mark_state != MS_INVALID) {
+                        GC_mark_state = MS_ROOTS_PUSHED;
+                    }
+                }
+            }
+            return(FALSE);
+
+        case MS_PUSH_UNCOLLECTABLE:
+            if (GC_mark_stack_top
+                >= GC_mark_stack + GC_mark_stack_size/4) {
+#               ifdef PARALLEL_MARK
+                  /* Avoid this, since we don't parallelize the marker  */
+                  /* here.                                              */
+                  if (GC_parallel) GC_mark_stack_too_small = TRUE;
+#               endif
+                MARK_FROM_MARK_STACK();
+                return(FALSE);
+            } else {
+                scan_ptr = GC_push_next_marked_uncollectable(scan_ptr);
+                if (scan_ptr == 0) {
+                    GC_push_roots(TRUE, cold_gc_frame);
+                    GC_objects_are_marked = TRUE;
+                    if (GC_mark_state != MS_INVALID) {
+                        GC_mark_state = MS_ROOTS_PUSHED;
+                    }
+                }
+            }
+            return(FALSE);
+
+        case MS_ROOTS_PUSHED:
+#           ifdef PARALLEL_MARK
+              /* In the incremental GC case, this currently doesn't     */
+              /* quite do the right thing, since it runs to             */
+              /* completion.  On the other hand, starting a             */
+              /* parallel marker is expensive, so perhaps it is         */
+              /* the right thing?                                       */
+              /* Eventually, incremental marking should run             */
+              /* asynchronously in multiple threads, without grabbing   */
+              /* the allocation lock.                                   */
+                if (GC_parallel) {
+                  GC_do_parallel_mark();
+                  GC_ASSERT(GC_mark_stack_top < (mse *)GC_first_nonempty);
+                  GC_mark_stack_top = GC_mark_stack - 1;
+                  if (GC_mark_stack_too_small) {
+                    alloc_mark_stack(2*GC_mark_stack_size);
+                  }
+                  if (GC_mark_state == MS_ROOTS_PUSHED) {
+                    GC_mark_state = MS_NONE;
+                    return(TRUE);
+                  } else {
+                    return(FALSE);
+                  }
+                }
+#           endif
+            if (GC_mark_stack_top >= GC_mark_stack) {
+                MARK_FROM_MARK_STACK();
+                return(FALSE);
+            } else {
+                GC_mark_state = MS_NONE;
+                if (GC_mark_stack_too_small) {
+                    alloc_mark_stack(2*GC_mark_stack_size);
+                }
+                return(TRUE);
+            }
+
+        case MS_INVALID:
+        case MS_PARTIALLY_INVALID:
+            if (!GC_objects_are_marked) {
+                GC_mark_state = MS_PUSH_UNCOLLECTABLE;
+                return(FALSE);
+            }
+            if (GC_mark_stack_top >= GC_mark_stack) {
+                MARK_FROM_MARK_STACK();
+                return(FALSE);
+            }
+            if (scan_ptr == 0 && GC_mark_state == MS_INVALID) {
+                /* About to start a heap scan for marked objects. */
+                /* Mark stack is empty.  OK to reallocate.        */
+                if (GC_mark_stack_too_small) {
+                    alloc_mark_stack(2*GC_mark_stack_size);
+                }
+                GC_mark_state = MS_PARTIALLY_INVALID;
+            }
+            scan_ptr = GC_push_next_marked(scan_ptr);
+            if (scan_ptr == 0 && GC_mark_state == MS_PARTIALLY_INVALID) {
+                GC_push_roots(TRUE, cold_gc_frame);
+                GC_objects_are_marked = TRUE;
+                if (GC_mark_state != MS_INVALID) {
+                    GC_mark_state = MS_ROOTS_PUSHED;
+                }
+            }
+            return(FALSE);
+        default:
+            ABORT("GC_mark_some: bad state");
+            return(FALSE);
+    }
+}
+
+#ifdef WRAP_MARK_SOME
+
+# if (defined(MSWIN32) || defined(MSWINCE)) && defined(__GNUC__)
+
+    typedef struct {
+      EXCEPTION_REGISTRATION ex_reg;
+      void *alt_path;
+    } ext_ex_regn;
+
+    static EXCEPTION_DISPOSITION mark_ex_handler(
+        struct _EXCEPTION_RECORD *ex_rec,
+        void *est_frame,
+        struct _CONTEXT *context,
+        void *disp_ctxt)
+    {
+        if (ex_rec->ExceptionCode == STATUS_ACCESS_VIOLATION) {
+          ext_ex_regn *xer = (ext_ex_regn *)est_frame;
+
+          /* Unwind from the inner function assuming the standard */
+          /* function prologue.                                   */
+          /* Assumes code has not been compiled with              */
+          /* -fomit-frame-pointer.                                */
+          context->Esp = context->Ebp;
+          context->Ebp = *((DWORD *)context->Esp);
+          context->Esp = context->Esp - 8;
+
+          /* Resume execution at the "real" handler within the    */
+          /* wrapper function.                                    */
+          context->Eip = (DWORD )(xer->alt_path);
+
+          return ExceptionContinueExecution;
+
+        } else {
+            return ExceptionContinueSearch;
+        }
+    }
+# endif /* __GNUC__  && MSWIN32 */
+
+#if defined(GC_WIN32_THREADS) && !defined(__GNUC__)
+  GC_bool GC_started_thread_while_stopped(void);
+  /* In win32_threads.c.  Did we invalidate mark phase with an  */
+  /* unexpected thread start?                                   */
+#endif
+
+  GC_INNER GC_bool GC_mark_some(ptr_t cold_gc_frame)
+  {
+      GC_bool ret_val;
+
+#   if defined(MSWIN32) || defined(MSWINCE)
+#    ifndef __GNUC__
+      /* Windows 98 appears to asynchronously create and remove  */
+      /* writable memory mappings, for reasons we haven't yet    */
+      /* understood.  Since we look for writable regions to      */
+      /* determine the root set, we may try to mark from an      */
+      /* address range that disappeared since we started the     */
+      /* collection.  Thus we have to recover from faults here.  */
+      /* This code does not appear to be necessary for Windows   */
+      /* 95/NT/2000+. Note that this code should never generate  */
+      /* an incremental GC write fault.                          */
+      /* This code seems to be necessary for WinCE (at least in  */
+      /* the case we'd decide to add MEM_PRIVATE sections to     */
+      /* data roots in GC_register_dynamic_libraries()).         */
+      /* It's conceivable that this is the same issue with       */
+      /* terminating threads that we see with Linux and          */
+      /* USE_PROC_FOR_LIBRARIES.                                 */
+
+      __try {
+          ret_val = GC_mark_some_inner(cold_gc_frame);
+      } __except (GetExceptionCode() == EXCEPTION_ACCESS_VIOLATION ?
+                EXCEPTION_EXECUTE_HANDLER : EXCEPTION_CONTINUE_SEARCH) {
+          goto handle_ex;
+      }
+#     ifdef GC_WIN32_THREADS
+        /* With DllMain-based thread tracking, a thread may have        */
+        /* started while we were marking.  This is logically equivalent */
+        /* to the exception case; our results are invalid and we have   */
+        /* to start over.  This cannot be prevented since we can't      */
+        /* block in DllMain.                                            */
+        if (GC_started_thread_while_stopped()) goto handle_ex;
+#     endif
+     rm_handler:
+      return ret_val;
+
+#    else /* __GNUC__ */
+
+      /* Manually install an exception handler since GCC does    */
+      /* not yet support Structured Exception Handling (SEH) on  */
+      /* Win32.                                                  */
+
+      ext_ex_regn er;
+
+      er.alt_path = &&handle_ex;
+      er.ex_reg.handler = mark_ex_handler;
+      __asm__ __volatile__ ("movl %%fs:0, %0" : "=r" (er.ex_reg.prev));
+      __asm__ __volatile__ ("movl %0, %%fs:0" : : "r" (&er));
+      ret_val = GC_mark_some_inner(cold_gc_frame);
+      /* Prevent GCC from considering the following code unreachable */
+      /* and thus eliminating it.                                    */
+        if (er.alt_path == 0)
+          goto handle_ex;
+    rm_handler:
+      /* Uninstall the exception handler */
+      __asm__ __volatile__ ("mov %0, %%fs:0" : : "r" (er.ex_reg.prev));
+      return ret_val;
+
+#    endif /* __GNUC__ */
+#   else /* !MSWIN32 */
+      /* Here we are handling the case in which /proc is used for root  */
+      /* finding, and we have threads.  We may find a stack for a       */
+      /* thread that is in the process of exiting, and disappears       */
+      /* while we are marking it.  This seems extremely difficult to    */
+      /* avoid otherwise.                                               */
+      if (GC_incremental) {
+        WARN("Incremental GC incompatible with /proc roots\n", 0);
+        /* I'm not sure if this could still work ...    */
+      }
+      GC_setup_temporary_fault_handler();
+      if(SETJMP(GC_jmp_buf) != 0) goto handle_ex;
+      ret_val = GC_mark_some_inner(cold_gc_frame);
+    rm_handler:
+      GC_reset_fault_handler();
+      return ret_val;
+
+#   endif /* !MSWIN32 */
+
+handle_ex:
+    /* Exception handler starts here for all cases. */
+      if (GC_print_stats) {
+        GC_log_printf(
+          "Caught ACCESS_VIOLATION in marker; memory mapping disappeared\n");
+      }
+
+      /* We have bad roots on the stack.  Discard mark stack.   */
+      /* Rescan from marked objects.  Redetermine roots.        */
+      GC_invalidate_mark_state();
+      scan_ptr = 0;
+
+      ret_val = FALSE;
+      goto rm_handler;  /* Back to platform-specific code. */
+  }
+#endif /* WRAP_MARK_SOME */
+
+GC_INNER GC_bool GC_mark_stack_empty(void)
+{
+    return(GC_mark_stack_top < GC_mark_stack);
+}
+
+GC_INNER void GC_invalidate_mark_state(void)
+{
+    GC_mark_state = MS_INVALID;
+    GC_mark_stack_top = GC_mark_stack-1;
+}
+
+GC_INNER mse * GC_signal_mark_stack_overflow(mse *msp)
+{
+    GC_mark_state = MS_INVALID;
+#   ifdef PARALLEL_MARK
+      /* We are using a local_mark_stack in parallel mode, so   */
+      /* do not signal the global mark stack to be resized.     */
+      /* That will be done if required in GC_return_mark_stack. */
+      if (!GC_parallel)
+        GC_mark_stack_too_small = TRUE;
+#   else
+      GC_mark_stack_too_small = TRUE;
+#   endif
+    if (GC_print_stats) {
+        GC_log_printf("Mark stack overflow; current size = %lu entries\n",
+                      (unsigned long)GC_mark_stack_size);
+    }
+    return(msp - GC_MARK_STACK_DISCARDS);
+}
+
+/*
+ * Mark objects pointed to by the regions described by
+ * mark stack entries between mark_stack and mark_stack_top,
+ * inclusive.  Assumes the upper limit of a mark stack entry
+ * is never 0.  A mark stack entry never has size 0.
+ * We try to traverse on the order of a hblk of memory before we return.
+ * Caller is responsible for calling this until the mark stack is empty.
+ * Note that this is the most performance critical routine in the
+ * collector.  Hence it contains all sorts of ugly hacks to speed
+ * things up.  In particular, we avoid procedure calls on the common
+ * path, we take advantage of peculiarities of the mark descriptor
+ * encoding, we optionally maintain a cache for the block address to
+ * header mapping, we prefetch when an object is "grayed", etc.
+ */
+GC_INNER mse * GC_mark_from(mse *mark_stack_top, mse *mark_stack,
+                            mse *mark_stack_limit)
+{
+  signed_word credit = HBLKSIZE;  /* Remaining credit for marking work  */
+  ptr_t current_p;      /* Pointer to current candidate ptr.    */
+  word current; /* Candidate pointer.                   */
+  ptr_t limit;  /* (Incl) limit of current candidate    */
+                                /* range                                */
+  word descr;
+  ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+  ptr_t least_ha = GC_least_plausible_heap_addr;
+  DECLARE_HDR_CACHE;
+
+# define SPLIT_RANGE_WORDS 128  /* Must be power of 2.          */
+
+  GC_objects_are_marked = TRUE;
+  INIT_HDR_CACHE;
+# ifdef OS2 /* Use untweaked version to circumvent compiler problem */
+    while (mark_stack_top >= mark_stack && credit >= 0)
+# else
+    while ((((ptr_t)mark_stack_top - (ptr_t)mark_stack) | credit) >= 0)
+# endif
+  {
+    current_p = mark_stack_top -> mse_start;
+    descr = mark_stack_top -> mse_descr;
+  retry:
+    /* current_p and descr describe the current object.         */
+    /* *mark_stack_top is vacant.                               */
+    /* The following is 0 only for small objects described by a simple  */
+    /* length descriptor.  For many applications this is the common     */
+    /* case, so we try to detect it quickly.                            */
+    if (descr & ((~(WORDS_TO_BYTES(SPLIT_RANGE_WORDS) - 1)) | GC_DS_TAGS)) {
+      word tag = descr & GC_DS_TAGS;
+
+      switch(tag) {
+        case GC_DS_LENGTH:
+          /* Large length.                                              */
+          /* Process part of the range to avoid pushing too much on the */
+          /* stack.                                                     */
+          GC_ASSERT(descr < (word)GC_greatest_plausible_heap_addr
+                            - (word)GC_least_plausible_heap_addr);
+#         ifdef ENABLE_TRACE
+            if (GC_trace_addr >= current_p
+                && GC_trace_addr < current_p + descr) {
+              GC_log_printf("GC:%u Large section; start %p len %lu\n",
+                        (unsigned)GC_gc_no, current_p, (unsigned long)descr);
+            }
+#         endif /* ENABLE_TRACE */
+#         ifdef PARALLEL_MARK
+#           define SHARE_BYTES 2048
+            if (descr > SHARE_BYTES && GC_parallel
+                && mark_stack_top < mark_stack_limit - 1) {
+              int new_size = (descr/2) & ~(sizeof(word)-1);
+              mark_stack_top -> mse_start = current_p;
+              mark_stack_top -> mse_descr = new_size + sizeof(word);
+                                        /* makes sure we handle         */
+                                        /* misaligned pointers.         */
+              mark_stack_top++;
+#             ifdef ENABLE_TRACE
+                if (GC_trace_addr >= current_p
+                    && GC_trace_addr < current_p + descr) {
+                  GC_log_printf("GC:%u Splitting (parallel) %p at %p\n",
+                        (unsigned)GC_gc_no, current_p, current_p + new_size);
+                }
+#             endif /* ENABLE_TRACE */
+              current_p += new_size;
+              descr -= new_size;
+              goto retry;
+            }
+#         endif /* PARALLEL_MARK */
+          mark_stack_top -> mse_start =
+                limit = current_p + WORDS_TO_BYTES(SPLIT_RANGE_WORDS-1);
+          mark_stack_top -> mse_descr =
+                        descr - WORDS_TO_BYTES(SPLIT_RANGE_WORDS-1);
+#         ifdef ENABLE_TRACE
+            if (GC_trace_addr >= current_p
+                && GC_trace_addr < current_p + descr) {
+              GC_log_printf("GC:%u Splitting %p at %p\n",
+                            (unsigned)GC_gc_no, current_p, limit);
+            }
+#         endif /* ENABLE_TRACE */
+          /* Make sure that pointers overlapping the two ranges are     */
+          /* considered.                                                */
+          limit += sizeof(word) - ALIGNMENT;
+          break;
+        case GC_DS_BITMAP:
+          mark_stack_top--;
+#         ifdef ENABLE_TRACE
+            if (GC_trace_addr >= current_p
+                && GC_trace_addr < current_p + WORDS_TO_BYTES(WORDSZ-2)) {
+              GC_log_printf("GC:%u Tracing from %p bitmap descr %lu\n",
+                        (unsigned)GC_gc_no, current_p, (unsigned long)descr);
+            }
+#         endif /* ENABLE_TRACE */
+          descr &= ~GC_DS_TAGS;
+          credit -= WORDS_TO_BYTES(WORDSZ/2); /* guess */
+          while (descr != 0) {
+            if ((signed_word)descr < 0) {
+              current = *(word *)current_p;
+              FIXUP_POINTER(current);
+              if ((ptr_t)current >= least_ha && (ptr_t)current < greatest_ha) {
+                PREFETCH((ptr_t)current);
+#               ifdef ENABLE_TRACE
+                  if (GC_trace_addr == current_p) {
+                    GC_log_printf("GC:%u Considering(3) %p -> %p\n",
+                               (unsigned)GC_gc_no, current_p, (ptr_t)current);
+                  }
+#               endif /* ENABLE_TRACE */
+                PUSH_CONTENTS((ptr_t)current, mark_stack_top,
+                              mark_stack_limit, current_p, exit1);
+              }
+            }
+            descr <<= 1;
+            current_p += sizeof(word);
+          }
+          continue;
+        case GC_DS_PROC:
+          mark_stack_top--;
+#         ifdef ENABLE_TRACE
+            if (GC_trace_addr >= current_p
+                && GC_base(current_p) != 0
+                && GC_base(current_p) == GC_base(GC_trace_addr)) {
+              GC_log_printf("GC:%u Tracing from %p proc descr %lu\n",
+                        (unsigned)GC_gc_no, current_p, (unsigned long)descr);
+            }
+#         endif /* ENABLE_TRACE */
+          credit -= GC_PROC_BYTES;
+          mark_stack_top =
+              (*PROC(descr))
+                    ((word *)current_p, mark_stack_top,
+                    mark_stack_limit, ENV(descr));
+          continue;
+        case GC_DS_PER_OBJECT:
+          if ((signed_word)descr >= 0) {
+            /* Descriptor is in the object.     */
+            descr = *(word *)(current_p + descr - GC_DS_PER_OBJECT);
+          } else {
+            /* Descriptor is in type descriptor pointed to by first     */
+            /* word in object.                                          */
+            ptr_t type_descr = *(ptr_t *)current_p;
+            /* type_descr is either a valid pointer to the descriptor   */
+            /* structure, or this object was on a free list.  If it     */
+            /* it was anything but the last object on the free list,    */
+            /* we will misinterpret the next object on the free list as */
+            /* the type descriptor, and get a 0 GC descriptor, which    */
+            /* is ideal.  Unfortunately, we need to check for the last  */
+            /* object case explicitly.                                  */
+            if (0 == type_descr) {
+                /* Rarely executed.     */
+                mark_stack_top--;
+                continue;
+            }
+            if ((GC_word)(type_descr) >= (GC_word)GC_least_plausible_heap_addr
+                    && (GC_word)(type_descr)
+                        <= (GC_word)GC_greatest_plausible_heap_addr) {
+                /* type_descr looks like a pointer into the heap.       */
+                /* It could still be the link pointer in a free list    */
+                /* though.  That's not a problem as long as the offset  */
+                /* of the actual descriptor in the pointed to object is */
+                /* within the same object.  In that case it will either */
+                /* point at the next free object in the list (if offset */
+                /* is 0) or be zeroed (which we check for below,        */
+                /* descr == 0).  If the offset is larger than the       */
+                /* objects in the block type_descr points to it cannot  */
+                /* be a proper pointer.                                 */
+                word offset = ~(descr + (GC_INDIR_PER_OBJ_BIAS
+                                         - GC_DS_PER_OBJECT - 1));
+                hdr *hhdr;
+                GET_HDR(type_descr, hhdr);
+                if (NULL == hhdr || hhdr->hb_sz - sizeof(word) < offset) {
+                    mark_stack_top--;
+                    continue;
+                }
+            }
+            descr = *(word *)(type_descr
+                              - (descr + (GC_INDIR_PER_OBJ_BIAS
+                                          - GC_DS_PER_OBJECT)));
+          }
+          if (0 == descr) {
+              /* Can happen either because we generated a 0 descriptor  */
+              /* or we saw a pointer to a free object.          */
+              mark_stack_top--;
+              continue;
+          }
+          goto retry;
+        default:
+          /* Can't happen. */
+          limit = 0; /* initialized to prevent warning. */
+      }
+    } else /* Small object with length descriptor */ {
+      mark_stack_top--;
+#     ifndef SMALL_CONFIG
+        if (descr < sizeof(word))
+          continue;
+#     endif
+      limit = current_p + (word)descr;
+    }
+#   ifdef ENABLE_TRACE
+        if (GC_trace_addr >= current_p
+            && GC_trace_addr < limit) {
+          GC_log_printf("GC:%u Tracing from %p len %lu\n",
+                        (int)GC_gc_no, current_p, (unsigned long)descr);
+        }
+#   endif /* ENABLE_TRACE */
+    /* The simple case in which we're scanning a range. */
+    GC_ASSERT(!((word)current_p & (ALIGNMENT-1)));
+    credit -= limit - current_p;
+    limit -= sizeof(word);
+    {
+#     define PREF_DIST 4
+
+#     ifndef SMALL_CONFIG
+        word deferred;
+
+        /* Try to prefetch the next pointer to be examined ASAP.        */
+        /* Empirically, this also seems to help slightly without        */
+        /* prefetches, at least on linux/X86.  Presumably this loop     */
+        /* ends up with less register pressure, and gcc thus ends up    */
+        /* generating slightly better code.  Overall gcc code quality   */
+        /* for this loop is still not great.                            */
+        for(;;) {
+          PREFETCH(limit - PREF_DIST*CACHE_LINE_SIZE);
+          GC_ASSERT(limit >= current_p);
+          deferred = *(word *)limit;
+          FIXUP_POINTER(deferred);
+          limit -= ALIGNMENT;
+          if ((ptr_t)deferred >= least_ha && (ptr_t)deferred <  greatest_ha) {
+            PREFETCH((ptr_t)deferred);
+            break;
+          }
+          if (current_p > limit) goto next_object;
+          /* Unroll once, so we don't do too many of the prefetches     */
+          /* based on limit.                                            */
+          deferred = *(word *)limit;
+          FIXUP_POINTER(deferred);
+          limit -= ALIGNMENT;
+          if ((ptr_t)deferred >= least_ha && (ptr_t)deferred <  greatest_ha) {
+            PREFETCH((ptr_t)deferred);
+            break;
+          }
+          if (current_p > limit) goto next_object;
+        }
+#     endif
+
+      while (current_p <= limit) {
+        /* Empirically, unrolling this loop doesn't help a lot. */
+        /* Since PUSH_CONTENTS expands to a lot of code,        */
+        /* we don't.                                            */
+        current = *(word *)current_p;
+        FIXUP_POINTER(current);
+        PREFETCH(current_p + PREF_DIST*CACHE_LINE_SIZE);
+        if ((ptr_t)current >= least_ha && (ptr_t)current <  greatest_ha) {
+          /* Prefetch the contents of the object we just pushed.  It's  */
+          /* likely we will need them soon.                             */
+          PREFETCH((ptr_t)current);
+#         ifdef ENABLE_TRACE
+            if (GC_trace_addr == current_p) {
+              GC_log_printf("GC:%u Considering(1) %p -> %p\n",
+                            (unsigned)GC_gc_no, current_p, (ptr_t)current);
+            }
+#         endif /* ENABLE_TRACE */
+          PUSH_CONTENTS((ptr_t)current, mark_stack_top,
+                           mark_stack_limit, current_p, exit2);
+        }
+        current_p += ALIGNMENT;
+      }
+
+#     ifndef SMALL_CONFIG
+        /* We still need to mark the entry we previously prefetched.    */
+        /* We already know that it passes the preliminary pointer       */
+        /* validity test.                                               */
+#       ifdef ENABLE_TRACE
+            if (GC_trace_addr == current_p) {
+              GC_log_printf("GC:%u Considering(2) %p -> %p\n",
+                            (unsigned)GC_gc_no, current_p, (ptr_t)deferred);
+            }
+#       endif /* ENABLE_TRACE */
+        PUSH_CONTENTS((ptr_t)deferred, mark_stack_top,
+                         mark_stack_limit, current_p, exit4);
+        next_object:;
+#     endif
+    }
+  }
+  return mark_stack_top;
+}
+
+#ifdef PARALLEL_MARK
+
+STATIC GC_bool GC_help_wanted = FALSE;  /* Protected by mark lock       */
+STATIC unsigned GC_helper_count = 0;    /* Number of running helpers.   */
+                                        /* Protected by mark lock       */
+STATIC unsigned GC_active_count = 0;    /* Number of active helpers.    */
+                                        /* Protected by mark lock       */
+                                        /* May increase and decrease    */
+                                        /* within each mark cycle.  But */
+                                        /* once it returns to 0, it     */
+                                        /* stays zero for the cycle.    */
+
+GC_INNER word GC_mark_no = 0;
+
+#define LOCAL_MARK_STACK_SIZE HBLKSIZE
+        /* Under normal circumstances, this is big enough to guarantee  */
+        /* We don't overflow half of it in a single call to             */
+        /* GC_mark_from.                                                */
+
+/* Wait all markers to finish initialization (i.e. store        */
+/* marker_[b]sp, marker_mach_threads, GC_marker_Id).            */
+GC_INNER void GC_wait_for_markers_init(void)
+{
+  word count;
+
+  if (GC_markers == 1)
+    return;
+
+  /* Reuse marker lock and builders count to synchronize        */
+  /* marker threads startup.                                    */
+  GC_acquire_mark_lock();
+  GC_fl_builder_count += (word)(GC_markers - 1);
+  count = GC_fl_builder_count;
+  GC_release_mark_lock();
+  if (count != 0)
+    GC_wait_for_reclaim();
+}
+
+/* Steal mark stack entries starting at mse low into mark stack local   */
+/* until we either steal mse high, or we have max entries.              */
+/* Return a pointer to the top of the local mark stack.                 */
+/* *next is replaced by a pointer to the next unscanned mark stack      */
+/* entry.                                                               */
+STATIC mse * GC_steal_mark_stack(mse * low, mse * high, mse * local,
+                                 unsigned max, mse **next)
+{
+    mse *p;
+    mse *top = local - 1;
+    unsigned i = 0;
+
+    GC_ASSERT(high >= low-1 && (word)(high - low + 1) <= GC_mark_stack_size);
+    for (p = low; p <= high && i <= max; ++p) {
+        word descr = AO_load((volatile AO_t *) &(p -> mse_descr));
+        if (descr != 0) {
+            /* Must be ordered after read of descr: */
+            AO_store_release_write((volatile AO_t *) &(p -> mse_descr), 0);
+            /* More than one thread may get this entry, but that's only */
+            /* a minor performance problem.                             */
+            ++top;
+            top -> mse_descr = descr;
+            top -> mse_start = p -> mse_start;
+            GC_ASSERT((top -> mse_descr & GC_DS_TAGS) != GC_DS_LENGTH ||
+                      top -> mse_descr < (word)GC_greatest_plausible_heap_addr
+                                         - (word)GC_least_plausible_heap_addr);
+            /* If this is a big object, count it as                     */
+            /* size/256 + 1 objects.                                    */
+            ++i;
+            if ((descr & GC_DS_TAGS) == GC_DS_LENGTH) i += (int)(descr >> 8);
+        }
+    }
+    *next = p;
+    return top;
+}
+
+/* Copy back a local mark stack.        */
+/* low and high are inclusive bounds.   */
+STATIC void GC_return_mark_stack(mse * low, mse * high)
+{
+    mse * my_top;
+    mse * my_start;
+    size_t stack_size;
+
+    if (high < low) return;
+    stack_size = high - low + 1;
+    GC_acquire_mark_lock();
+    my_top = GC_mark_stack_top; /* Concurrent modification impossible. */
+    my_start = my_top + 1;
+    if (my_start - GC_mark_stack + stack_size > GC_mark_stack_size) {
+      if (GC_print_stats) {
+          GC_log_printf("No room to copy back mark stack\n");
+      }
+      GC_mark_state = MS_INVALID;
+      GC_mark_stack_too_small = TRUE;
+      /* We drop the local mark stack.  We'll fix things later. */
+    } else {
+      BCOPY(low, my_start, stack_size * sizeof(mse));
+      GC_ASSERT((mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top))
+                == my_top);
+      AO_store_release_write((volatile AO_t *)(&GC_mark_stack_top),
+                             (AO_t)(my_top + stack_size));
+                /* Ensures visibility of previously written stack contents. */
+    }
+    GC_release_mark_lock();
+    GC_notify_all_marker();
+}
+
+/* Mark from the local mark stack.              */
+/* On return, the local mark stack is empty.    */
+/* But this may be achieved by copying the      */
+/* local mark stack back into the global one.   */
+STATIC void GC_do_local_mark(mse *local_mark_stack, mse *local_top)
+{
+    unsigned n;
+#   define N_LOCAL_ITERS 1
+
+#   ifdef GC_ASSERTIONS
+      /* Make sure we don't hold mark lock. */
+        GC_acquire_mark_lock();
+        GC_release_mark_lock();
+#   endif
+    for (;;) {
+        for (n = 0; n < N_LOCAL_ITERS; ++n) {
+            local_top = GC_mark_from(local_top, local_mark_stack,
+                                     local_mark_stack + LOCAL_MARK_STACK_SIZE);
+            if (local_top < local_mark_stack) return;
+            if ((word)(local_top - local_mark_stack)
+                        >= LOCAL_MARK_STACK_SIZE / 2) {
+                GC_return_mark_stack(local_mark_stack, local_top);
+                return;
+            }
+        }
+        if ((mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top))
+            < (mse *)AO_load(&GC_first_nonempty)
+            && GC_active_count < GC_helper_count
+            && local_top > local_mark_stack + 1) {
+            /* Try to share the load, since the main stack is empty,    */
+            /* and helper threads are waiting for a refill.             */
+            /* The entries near the bottom of the stack are likely      */
+            /* to require more work.  Thus we return those, even though */
+            /* it's harder.                                             */
+            mse * new_bottom = local_mark_stack
+                                + (local_top - local_mark_stack)/2;
+            GC_ASSERT(new_bottom > local_mark_stack
+                      && new_bottom < local_top);
+            GC_return_mark_stack(local_mark_stack, new_bottom - 1);
+            memmove(local_mark_stack, new_bottom,
+                    (local_top - new_bottom + 1) * sizeof(mse));
+            local_top -= (new_bottom - local_mark_stack);
+        }
+    }
+}
+
+#define ENTRIES_TO_GET 5
+
+GC_INNER long GC_markers = 2;   /* Normally changed by thread-library-  */
+                                /* -specific code.                      */
+
+/* Mark using the local mark stack until the global mark stack is empty */
+/* and there are no active workers. Update GC_first_nonempty to reflect */
+/* progress.                                                            */
+/* Caller does not hold mark lock.                                      */
+/* Caller has already incremented GC_helper_count.  We decrement it,    */
+/* and maintain GC_active_count.                                        */
+STATIC void GC_mark_local(mse *local_mark_stack, int id)
+{
+    mse * my_first_nonempty;
+
+    GC_acquire_mark_lock();
+    GC_active_count++;
+    my_first_nonempty = (mse *)AO_load(&GC_first_nonempty);
+    GC_ASSERT((mse *)AO_load(&GC_first_nonempty) >= GC_mark_stack &&
+              (mse *)AO_load(&GC_first_nonempty) <=
+              (mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top)) + 1);
+    if (GC_print_stats == VERBOSE)
+        GC_log_printf("Starting mark helper %lu\n", (unsigned long)id);
+    GC_release_mark_lock();
+    for (;;) {
+        size_t n_on_stack;
+        unsigned n_to_get;
+        mse * my_top;
+        mse * local_top;
+        mse * global_first_nonempty = (mse *)AO_load(&GC_first_nonempty);
+
+        GC_ASSERT(my_first_nonempty >= GC_mark_stack &&
+                  my_first_nonempty <=
+                  (mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top))  + 1);
+        GC_ASSERT(global_first_nonempty >= GC_mark_stack &&
+                  global_first_nonempty <=
+                  (mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top))  + 1);
+        if (my_first_nonempty < global_first_nonempty) {
+            my_first_nonempty = global_first_nonempty;
+        } else if (global_first_nonempty < my_first_nonempty) {
+            AO_compare_and_swap(&GC_first_nonempty,
+                                (AO_t) global_first_nonempty,
+                                (AO_t) my_first_nonempty);
+            /* If this fails, we just go ahead, without updating        */
+            /* GC_first_nonempty.                                       */
+        }
+        /* Perhaps we should also update GC_first_nonempty, if it */
+        /* is less.  But that would require using atomic updates. */
+        my_top = (mse *)AO_load_acquire((volatile AO_t *)(&GC_mark_stack_top));
+        n_on_stack = my_top - my_first_nonempty + 1;
+        if (0 == n_on_stack) {
+            GC_acquire_mark_lock();
+            my_top = GC_mark_stack_top;
+                /* Asynchronous modification impossible here,   */
+                /* since we hold mark lock.                     */
+            n_on_stack = my_top - my_first_nonempty + 1;
+            if (0 == n_on_stack) {
+                GC_active_count--;
+                GC_ASSERT(GC_active_count <= GC_helper_count);
+                /* Other markers may redeposit objects  */
+                /* on the stack.                                */
+                if (0 == GC_active_count) GC_notify_all_marker();
+                while (GC_active_count > 0
+                       && (mse *)AO_load(&GC_first_nonempty)
+                          > GC_mark_stack_top) {
+                    /* We will be notified if either GC_active_count    */
+                    /* reaches zero, or if more objects are pushed on   */
+                    /* the global mark stack.                           */
+                    GC_wait_marker();
+                }
+                if (GC_active_count == 0 &&
+                    (mse *)AO_load(&GC_first_nonempty) > GC_mark_stack_top) {
+                    GC_bool need_to_notify = FALSE;
+                    /* The above conditions can't be falsified while we */
+                    /* hold the mark lock, since neither                */
+                    /* GC_active_count nor GC_mark_stack_top can        */
+                    /* change.  GC_first_nonempty can only be           */
+                    /* incremented asynchronously.  Thus we know that   */
+                    /* both conditions actually held simultaneously.    */
+                    GC_helper_count--;
+                    if (0 == GC_helper_count) need_to_notify = TRUE;
+                    if (GC_print_stats == VERBOSE)
+                      GC_log_printf("Finished mark helper %lu\n",
+                                    (unsigned long)id);
+                    GC_release_mark_lock();
+                    if (need_to_notify) GC_notify_all_marker();
+                    return;
+                }
+                /* else there's something on the stack again, or        */
+                /* another helper may push something.                   */
+                GC_active_count++;
+                GC_ASSERT(GC_active_count > 0);
+                GC_release_mark_lock();
+                continue;
+            } else {
+                GC_release_mark_lock();
+            }
+        }
+        n_to_get = ENTRIES_TO_GET;
+        if (n_on_stack < 2 * ENTRIES_TO_GET) n_to_get = 1;
+        local_top = GC_steal_mark_stack(my_first_nonempty, my_top,
+                                        local_mark_stack, n_to_get,
+                                        &my_first_nonempty);
+        GC_ASSERT(my_first_nonempty >= GC_mark_stack &&
+                  my_first_nonempty <=
+                    (mse *)AO_load((volatile AO_t *)(&GC_mark_stack_top)) + 1);
+        GC_do_local_mark(local_mark_stack, local_top);
+    }
+}
+
+/* Perform Parallel mark.                       */
+/* We hold the GC lock, not the mark lock.      */
+/* Currently runs until the mark stack is       */
+/* empty.                                       */
+STATIC void GC_do_parallel_mark(void)
+{
+    mse local_mark_stack[LOCAL_MARK_STACK_SIZE];
+
+    GC_acquire_mark_lock();
+    GC_ASSERT(I_HOLD_LOCK());
+    /* This could be a GC_ASSERT, but it seems safer to keep it on      */
+    /* all the time, especially since it's cheap.                       */
+    if (GC_help_wanted || GC_active_count != 0 || GC_helper_count != 0)
+        ABORT("Tried to start parallel mark in bad state");
+    if (GC_print_stats == VERBOSE)
+        GC_log_printf("Starting marking for mark phase number %lu\n",
+                      (unsigned long)GC_mark_no);
+    GC_first_nonempty = (AO_t)GC_mark_stack;
+    GC_active_count = 0;
+    GC_helper_count = 1;
+    GC_help_wanted = TRUE;
+    GC_release_mark_lock();
+    GC_notify_all_marker();
+        /* Wake up potential helpers.   */
+    GC_mark_local(local_mark_stack, 0);
+    GC_acquire_mark_lock();
+    GC_help_wanted = FALSE;
+    /* Done; clean up.  */
+    while (GC_helper_count > 0) GC_wait_marker();
+    /* GC_helper_count cannot be incremented while GC_help_wanted == FALSE */
+    if (GC_print_stats == VERBOSE)
+        GC_log_printf("Finished marking for mark phase number %lu\n",
+                      (unsigned long)GC_mark_no);
+    GC_mark_no++;
+    GC_release_mark_lock();
+    GC_notify_all_marker();
+}
+
+
+/* Try to help out the marker, if it's running.         */
+/* We do not hold the GC lock, but the requestor does.  */
+GC_INNER void GC_help_marker(word my_mark_no)
+{
+    mse local_mark_stack[LOCAL_MARK_STACK_SIZE];
+    unsigned my_id;
+
+    if (!GC_parallel) return;
+    GC_acquire_mark_lock();
+    while (GC_mark_no < my_mark_no
+           || (!GC_help_wanted && GC_mark_no == my_mark_no)) {
+      GC_wait_marker();
+    }
+    my_id = GC_helper_count;
+    if (GC_mark_no != my_mark_no || my_id >= (unsigned)GC_markers) {
+      /* Second test is useful only if original threads can also        */
+      /* act as helpers.  Under Linux they can't.                       */
+      GC_release_mark_lock();
+      return;
+    }
+    GC_helper_count = my_id + 1;
+    GC_release_mark_lock();
+    GC_mark_local(local_mark_stack, my_id);
+    /* GC_mark_local decrements GC_helper_count. */
+}
+
+#endif /* PARALLEL_MARK */
+
+/* Allocate or reallocate space for mark stack of size n entries.  */
+/* May silently fail.                                              */
+static void alloc_mark_stack(size_t n)
+{
+    mse * new_stack = (mse *)GC_scratch_alloc(n * sizeof(struct GC_ms_entry));
+#   ifdef GWW_VDB
+      /* Don't recycle a stack segment obtained with the wrong flags.   */
+      /* Win32 GetWriteWatch requires the right kind of memory.         */
+      static GC_bool GC_incremental_at_stack_alloc = FALSE;
+      GC_bool recycle_old = (!GC_incremental || GC_incremental_at_stack_alloc);
+
+      GC_incremental_at_stack_alloc = GC_incremental;
+#   else
+#     define recycle_old TRUE
+#   endif
+
+    GC_mark_stack_too_small = FALSE;
+    if (GC_mark_stack_size != 0) {
+        if (new_stack != 0) {
+          if (recycle_old) {
+            /* Recycle old space */
+              size_t page_offset = (word)GC_mark_stack & (GC_page_size - 1);
+              size_t size = GC_mark_stack_size * sizeof(struct GC_ms_entry);
+              size_t displ = 0;
+
+              if (0 != page_offset) displ = GC_page_size - page_offset;
+              size = (size - displ) & ~(GC_page_size - 1);
+              if (size > 0) {
+                GC_add_to_heap((struct hblk *)
+                                ((word)GC_mark_stack + displ), (word)size);
+              }
+          }
+          GC_mark_stack = new_stack;
+          GC_mark_stack_size = n;
+          GC_mark_stack_limit = new_stack + n;
+          if (GC_print_stats) {
+              GC_log_printf("Grew mark stack to %lu frames\n",
+                            (unsigned long) GC_mark_stack_size);
+          }
+        } else {
+          if (GC_print_stats) {
+              GC_log_printf("Failed to grow mark stack to %lu frames\n",
+                            (unsigned long) n);
+          }
+        }
+    } else {
+        if (new_stack == 0) {
+            GC_err_printf("No space for mark stack\n");
+            EXIT();
+        }
+        GC_mark_stack = new_stack;
+        GC_mark_stack_size = n;
+        GC_mark_stack_limit = new_stack + n;
+    }
+    GC_mark_stack_top = GC_mark_stack-1;
+}
+
+GC_INNER void GC_mark_init(void)
+{
+    alloc_mark_stack(INITIAL_MARK_STACK_SIZE);
+}
+
+/*
+ * Push all locations between b and t onto the mark stack.
+ * b is the first location to be checked. t is one past the last
+ * location to be checked.
+ * Should only be used if there is no possibility of mark stack
+ * overflow.
+ */
+void GC_push_all(ptr_t bottom, ptr_t top)
+{
+
+    register word length;
+
+    bottom = (ptr_t)(((word) bottom + ALIGNMENT-1) & ~(ALIGNMENT-1));
+    top = (ptr_t)(((word) top) & ~(ALIGNMENT-1));
+    if (bottom >= top) return;
+
+    GC_mark_stack_top++;
+
+    BDWGC_DEBUG("GC_push_all: %p-%p mark_stack_top = %p\n", bottom,top,GC_mark_stack_top);
+    BACKTRACE(BDWGC_DEBUG,4);
+    if (GC_mark_stack_top >= GC_mark_stack_limit) {
+        ABORT("Unexpected mark stack overflow");
+    }
+    length = top - bottom;
+#   if GC_DS_TAGS > ALIGNMENT - 1
+        length += GC_DS_TAGS;
+        length &= ~GC_DS_TAGS;
+#   endif
+    GC_mark_stack_top -> mse_start = bottom;
+    GC_mark_stack_top -> mse_descr = length;
+}
+
+#ifndef GC_DISABLE_INCREMENTAL
+
+  /* Analogous to the above, but push only those pages h with           */
+  /* dirty_fn(h) != 0.  We use GC_push_all to actually push the block.  */
+  /* Used both to selectively push dirty pages, or to push a block in   */
+  /* piecemeal fashion, to allow for more marking concurrency.          */
+  /* Will not overflow mark stack if GC_push_all pushes a small fixed   */
+  /* number of entries.  (This is invoked only if GC_push_all pushes    */
+  /* a single entry, or if it marks each object before pushing it, thus */
+  /* ensuring progress in the event of a stack overflow.)               */
+  STATIC void GC_push_selected(ptr_t bottom, ptr_t top,
+                               GC_bool (*dirty_fn)(struct hblk *))
+  {
+    struct hblk * h;
+
+    bottom = (ptr_t)(((word) bottom + ALIGNMENT-1) & ~(ALIGNMENT-1));
+    top = (ptr_t)(((word) top) & ~(ALIGNMENT-1));
+    if (bottom >= top) return;
+
+    h = HBLKPTR(bottom + HBLKSIZE);
+    if (top <= (ptr_t) h) {
+        if ((*dirty_fn)(h-1)) {
+            GC_push_all(bottom, top);
+        }
+        return;
+    }
+    if ((*dirty_fn)(h-1)) {
+        GC_push_all(bottom, (ptr_t)h);
+    }
+
+    while ((ptr_t)(h+1) <= top) {
+        if ((*dirty_fn)(h)) {
+            if ((word)(GC_mark_stack_top - GC_mark_stack)
+                > 3 * GC_mark_stack_size / 4) {
+                /* Danger of mark stack overflow */
+                GC_push_all((ptr_t)h, top);
+                return;
+            } else {
+                GC_push_all((ptr_t)h, (ptr_t)(h+1));
+            }
+        }
+        h++;
+    }
+
+    if ((ptr_t)h != top && (*dirty_fn)(h)) {
+       GC_push_all((ptr_t)h, top);
+    }
+    if (GC_mark_stack_top >= GC_mark_stack_limit) {
+        ABORT("Unexpected mark stack overflow");
+    }
+  }
+
+  void GC_push_conditional(ptr_t bottom, ptr_t top, GC_bool all)
+  {
+    if (!all) {
+      GC_push_selected(bottom, top, GC_page_was_dirty);
+    } else {
+#     ifdef PROC_VDB
+        if (GC_dirty_maintained) {
+          /* Pages that were never dirtied cannot contain pointers.     */
+          GC_push_selected(bottom, top, GC_page_was_ever_dirty);
+        } else
+#     endif
+      /* else */ {
+        GC_push_all(bottom, top);
+      }
+    }
+  }
+#endif /* !GC_DISABLE_INCREMENTAL */
+
+#if defined(MSWIN32) || defined(MSWINCE)
+  void __cdecl GC_push_one(word p)
+#else
+  void GC_push_one(word p)
+#endif
+{
+    GC_PUSH_ONE_STACK(p, MARKED_FROM_REGISTER);
+}
+
+/*ARGSUSED*/
+GC_API struct GC_ms_entry * GC_CALL GC_mark_and_push(void *obj,
+                                                mse *mark_stack_ptr,
+                                                mse *mark_stack_limit,
+                                                void **src)
+{
+    hdr * hhdr;
+
+    PREFETCH(obj);
+    GET_HDR(obj, hhdr);
+    if (EXPECT(IS_FORWARDING_ADDR_OR_NIL(hhdr), FALSE)) {
+      if (GC_all_interior_pointers) {
+        hhdr = GC_find_header(GC_base(obj));
+        if (hhdr == 0) {
+          GC_ADD_TO_BLACK_LIST_NORMAL(obj, (ptr_t)src);
+          return mark_stack_ptr;
+        }
+      } else {
+        GC_ADD_TO_BLACK_LIST_NORMAL(obj, (ptr_t)src);
+        return mark_stack_ptr;
+      }
+    }
+    if (EXPECT(HBLK_IS_FREE(hhdr), FALSE)) {
+        GC_ADD_TO_BLACK_LIST_NORMAL(obj, (ptr_t)src);
+        return mark_stack_ptr;
+    }
+
+    PUSH_CONTENTS_HDR(obj, mark_stack_ptr /* modified */, mark_stack_limit,
+                      (ptr_t)src, was_marked, hhdr, TRUE);
+ was_marked:
+    return mark_stack_ptr;
+}
+
+#if defined(MANUAL_VDB) && defined(THREADS)
+  void GC_dirty(ptr_t p);
+#endif
+
+/* Mark and push (i.e. gray) a single object p onto the main    */
+/* mark stack.  Consider p to be valid if it is an interior     */
+/* pointer.                                                     */
+/* The object p has passed a preliminary pointer validity       */
+/* test, but we do not definitely know whether it is valid.     */
+/* Mark bits are NOT atomically updated.  Thus this must be the */
+/* only thread setting them.                                    */
+# if defined(PRINT_BLACK_LIST) || defined(KEEP_BACK_PTRS)
+    GC_INNER void GC_mark_and_push_stack(ptr_t p, ptr_t source)
+# else
+    GC_INNER void GC_mark_and_push_stack(ptr_t p)
+#   define source ((ptr_t)0)
+# endif
+{
+    hdr * hhdr;
+    ptr_t r = p;
+
+    PREFETCH(p);
+    GET_HDR(p, hhdr);
+    if (EXPECT(IS_FORWARDING_ADDR_OR_NIL(hhdr), FALSE)) {
+        if (hhdr != 0) {
+          r = GC_base(p);
+          hhdr = HDR(r);
+        }
+        if (hhdr == 0) {
+            GC_ADD_TO_BLACK_LIST_STACK(p, source);
+            return;
+        }
+    }
+    if (EXPECT(HBLK_IS_FREE(hhdr), FALSE)) {
+        GC_ADD_TO_BLACK_LIST_NORMAL(p, source);
+        return;
+    }
+#   if defined(MANUAL_VDB) && defined(THREADS)
+      /* Pointer is on the stack.  We may have dirtied the object       */
+      /* it points to, but not yet have called GC_dirty();              */
+      GC_dirty(p);      /* Implicitly affects entire object.            */
+#   endif
+    PUSH_CONTENTS_HDR(r, GC_mark_stack_top, GC_mark_stack_limit,
+                      source, mark_and_push_exit, hhdr, FALSE);
+  mark_and_push_exit: ;
+    /* We silently ignore pointers to near the end of a block,  */
+    /* which is very mildly suboptimal.                         */
+    /* FIXME: We should probably add a header word to address   */
+    /* this.                                                    */
+}
+# undef source
+
+# ifdef TRACE_BUF
+
+# define TRACE_ENTRIES 1000
+
+struct trace_entry {
+    char * kind;
+    word gc_no;
+    word bytes_allocd;
+    word arg1;
+    word arg2;
+} GC_trace_buf[TRACE_ENTRIES];
+
+int GC_trace_buf_ptr = 0;
+
+void GC_add_trace_entry(char *kind, word arg1, word arg2)
+{
+    GC_trace_buf[GC_trace_buf_ptr].kind = kind;
+    GC_trace_buf[GC_trace_buf_ptr].gc_no = GC_gc_no;
+    GC_trace_buf[GC_trace_buf_ptr].bytes_allocd = GC_bytes_allocd;
+    GC_trace_buf[GC_trace_buf_ptr].arg1 = arg1 ^ 0x80000000;
+    GC_trace_buf[GC_trace_buf_ptr].arg2 = arg2 ^ 0x80000000;
+    GC_trace_buf_ptr++;
+    if (GC_trace_buf_ptr >= TRACE_ENTRIES) GC_trace_buf_ptr = 0;
+}
+
+void GC_print_trace(word gc_no, GC_bool lock)
+{
+    int i;
+    struct trace_entry *p;
+    DCL_LOCK_STATE;
+
+    if (lock) LOCK();
+    for (i = GC_trace_buf_ptr-1; i != GC_trace_buf_ptr; i--) {
+        if (i < 0) i = TRACE_ENTRIES-1;
+        p = GC_trace_buf + i;
+        if (p -> gc_no < gc_no || p -> kind == 0) {
+            if (lock) UNLOCK();
+            return;
+        }
+        printf("Trace:%s (gc:%u,bytes:%lu) 0x%X, 0x%X\n",
+                p -> kind, (unsigned)p -> gc_no,
+                (unsigned long)p -> bytes_allocd,
+                (p -> arg1) ^ 0x80000000, (p -> arg2) ^ 0x80000000);
+    }
+    printf("Trace incomplete\n");
+    if (lock) UNLOCK();
+}
+
+# endif /* TRACE_BUF */
+
+/*
+ * A version of GC_push_all that treats all interior pointers as valid
+ * and scans the entire region immediately, in case the contents
+ * change.
+ */
+GC_INNER void GC_push_all_eager(ptr_t bottom, ptr_t top)
+{
+    word * b = (word *)(((word) bottom + ALIGNMENT-1) & ~(ALIGNMENT-1));
+    word * t = (word *)(((word) top) & ~(ALIGNMENT-1));
+    register word *p;
+    register word q;
+    register word *lim;
+    register ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+    register ptr_t least_ha = GC_least_plausible_heap_addr;
+#   define GC_greatest_plausible_heap_addr greatest_ha
+#   define GC_least_plausible_heap_addr least_ha
+
+    if (top == 0) return;
+    /* check all pointers in range and push if they appear      */
+    /* to be valid.                                             */
+      lim = t - 1 /* longword */;
+      for (p = b; p <= lim; p = (word *)(((ptr_t)p) + ALIGNMENT)) {
+        q = *p;
+        GC_PUSH_ONE_STACK(q, p);
+      }
+#   undef GC_greatest_plausible_heap_addr
+#   undef GC_least_plausible_heap_addr
+}
+
+GC_INNER void GC_push_all_stack(ptr_t bottom, ptr_t top)
+{
+# if defined(THREADS) && defined(MPROTECT_VDB)
+    GC_push_all_eager(bottom, top);
+# else
+    if (!NEED_FIXUP_POINTER && GC_all_interior_pointers) {
+      GC_push_all(bottom, top);
+    } else {
+      GC_push_all_eager(bottom, top);
+    }
+# endif
+}
+
+#if !defined(SMALL_CONFIG) && !defined(USE_MARK_BYTES) && \
+    defined(MARK_BIT_PER_GRANULE)
+# if GC_GRANULE_WORDS == 1
+#   define USE_PUSH_MARKED_ACCELERATORS
+#   define PUSH_GRANULE(q) \
+                { word qcontents = (q)[0]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)); }
+# elif GC_GRANULE_WORDS == 2
+#   define USE_PUSH_MARKED_ACCELERATORS
+#   define PUSH_GRANULE(q) \
+                { word qcontents = (q)[0]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)); \
+                  qcontents = (q)[1]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)+1); }
+# elif GC_GRANULE_WORDS == 4
+#   define USE_PUSH_MARKED_ACCELERATORS
+#   define PUSH_GRANULE(q) \
+                { word qcontents = (q)[0]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)); \
+                  qcontents = (q)[1]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)+1); \
+                  qcontents = (q)[2]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)+2); \
+                  qcontents = (q)[3]; \
+                  GC_PUSH_ONE_HEAP(qcontents, (q)+3); }
+# endif
+#endif /* !USE_MARK_BYTES && MARK_BIT_PER_GRANULE */
+
+#ifdef USE_PUSH_MARKED_ACCELERATORS
+/* Push all objects reachable from marked objects in the given block */
+/* containing objects of size 1 granule.                             */
+STATIC void GC_push_marked1(struct hblk *h, hdr *hhdr)
+{
+    word * mark_word_addr = &(hhdr->hb_marks[0]);
+    word *p;
+    word *plim;
+    word *q;
+    word mark_word;
+
+    /* Allow registers to be used for some frequently accessed  */
+    /* global variables.  Otherwise aliasing issues are likely  */
+    /* to prevent that.                                         */
+    ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+    ptr_t least_ha = GC_least_plausible_heap_addr;
+    mse * mark_stack_top = GC_mark_stack_top;
+    mse * mark_stack_limit = GC_mark_stack_limit;
+
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_top mark_stack_top
+#   define GC_mark_stack_limit mark_stack_limit
+#   define GC_greatest_plausible_heap_addr greatest_ha
+#   define GC_least_plausible_heap_addr least_ha
+
+    p = (word *)(h->hb_body);
+    plim = (word *)(((word)h) + HBLKSIZE);
+
+    /* go through all words in block */
+        while (p < plim) {
+            mark_word = *mark_word_addr++;
+            q = p;
+            while(mark_word != 0) {
+              if (mark_word & 1) {
+                  PUSH_GRANULE(q);
+              }
+              q += GC_GRANULE_WORDS;
+              mark_word >>= 1;
+            }
+            p += WORDSZ*GC_GRANULE_WORDS;
+        }
+
+#   undef GC_greatest_plausible_heap_addr
+#   undef GC_least_plausible_heap_addr
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_limit GC_arrays._mark_stack_limit
+#   define GC_mark_stack_top GC_arrays._mark_stack_top
+    GC_mark_stack_top = mark_stack_top;
+}
+
+
+#ifndef UNALIGNED_PTRS
+
+/* Push all objects reachable from marked objects in the given block */
+/* of size 2 (granules) objects.                                     */
+STATIC void GC_push_marked2(struct hblk *h, hdr *hhdr)
+{
+    word * mark_word_addr = &(hhdr->hb_marks[0]);
+    word *p;
+    word *plim;
+    word *q;
+    word mark_word;
+
+    ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+    ptr_t least_ha = GC_least_plausible_heap_addr;
+    mse * mark_stack_top = GC_mark_stack_top;
+    mse * mark_stack_limit = GC_mark_stack_limit;
+
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_top mark_stack_top
+#   define GC_mark_stack_limit mark_stack_limit
+#   define GC_greatest_plausible_heap_addr greatest_ha
+#   define GC_least_plausible_heap_addr least_ha
+
+    p = (word *)(h->hb_body);
+    plim = (word *)(((word)h) + HBLKSIZE);
+
+    /* go through all words in block */
+        while (p < plim) {
+            mark_word = *mark_word_addr++;
+            q = p;
+            while(mark_word != 0) {
+              if (mark_word & 1) {
+                  PUSH_GRANULE(q);
+                  PUSH_GRANULE(q + GC_GRANULE_WORDS);
+              }
+              q += 2 * GC_GRANULE_WORDS;
+              mark_word >>= 2;
+            }
+            p += WORDSZ*GC_GRANULE_WORDS;
+        }
+
+#   undef GC_greatest_plausible_heap_addr
+#   undef GC_least_plausible_heap_addr
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_limit GC_arrays._mark_stack_limit
+#   define GC_mark_stack_top GC_arrays._mark_stack_top
+    GC_mark_stack_top = mark_stack_top;
+}
+
+# if GC_GRANULE_WORDS < 4
+/* Push all objects reachable from marked objects in the given block */
+/* of size 4 (granules) objects.                                     */
+/* There is a risk of mark stack overflow here.  But we handle that. */
+/* And only unmarked objects get pushed, so it's not very likely.    */
+STATIC void GC_push_marked4(struct hblk *h, hdr *hhdr)
+{
+    word * mark_word_addr = &(hhdr->hb_marks[0]);
+    word *p;
+    word *plim;
+    word *q;
+    word mark_word;
+
+    ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+    ptr_t least_ha = GC_least_plausible_heap_addr;
+    mse * mark_stack_top = GC_mark_stack_top;
+    mse * mark_stack_limit = GC_mark_stack_limit;
+
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_top mark_stack_top
+#   define GC_mark_stack_limit mark_stack_limit
+#   define GC_greatest_plausible_heap_addr greatest_ha
+#   define GC_least_plausible_heap_addr least_ha
+
+    p = (word *)(h->hb_body);
+    plim = (word *)(((word)h) + HBLKSIZE);
+
+    /* go through all words in block */
+        while (p < plim) {
+            mark_word = *mark_word_addr++;
+            q = p;
+            while(mark_word != 0) {
+              if (mark_word & 1) {
+                  PUSH_GRANULE(q);
+                  PUSH_GRANULE(q + GC_GRANULE_WORDS);
+                  PUSH_GRANULE(q + 2*GC_GRANULE_WORDS);
+                  PUSH_GRANULE(q + 3*GC_GRANULE_WORDS);
+              }
+              q += 4 * GC_GRANULE_WORDS;
+              mark_word >>= 4;
+            }
+            p += WORDSZ*GC_GRANULE_WORDS;
+        }
+#   undef GC_greatest_plausible_heap_addr
+#   undef GC_least_plausible_heap_addr
+#   undef GC_mark_stack_top
+#   undef GC_mark_stack_limit
+#   define GC_mark_stack_limit GC_arrays._mark_stack_limit
+#   define GC_mark_stack_top GC_arrays._mark_stack_top
+    GC_mark_stack_top = mark_stack_top;
+}
+
+#endif /* GC_GRANULE_WORDS < 4 */
+
+#endif /* UNALIGNED_PTRS */
+
+#endif /* USE_PUSH_MARKED_ACCELERATORS */
+
+/* Push all objects reachable from marked objects in the given block */
+STATIC void GC_push_marked(struct hblk *h, hdr *hhdr)
+{
+    size_t sz = hhdr -> hb_sz;
+    word descr = hhdr -> hb_descr;
+    ptr_t p;
+    word bit_no;
+    ptr_t lim;
+    mse * GC_mark_stack_top_reg;
+    mse * mark_stack_limit = GC_mark_stack_limit;
+
+    /* Some quick shortcuts: */
+        if ((0 | GC_DS_LENGTH) == descr) return;
+        if (GC_block_empty(hhdr)/* nothing marked */) return;
+    GC_n_rescuing_pages++;
+    GC_objects_are_marked = TRUE;
+    if (sz > MAXOBJBYTES) {
+        lim = h -> hb_body;
+    } else {
+        lim = (h + 1)->hb_body - sz;
+    }
+
+    switch(BYTES_TO_GRANULES(sz)) {
+#   if defined(USE_PUSH_MARKED_ACCELERATORS)
+     case 1:
+       GC_push_marked1(h, hhdr);
+       break;
+#    if !defined(UNALIGNED_PTRS)
+       case 2:
+         GC_push_marked2(h, hhdr);
+         break;
+#     if GC_GRANULE_WORDS < 4
+       case 4:
+         GC_push_marked4(h, hhdr);
+         break;
+#     endif
+#    endif
+#   endif
+     default:
+      GC_mark_stack_top_reg = GC_mark_stack_top;
+      for (p = h -> hb_body, bit_no = 0; p <= lim;
+           p += sz, bit_no += MARK_BIT_OFFSET(sz)) {
+         if (mark_bit_from_hdr(hhdr, bit_no)) {
+           /* Mark from fields inside the object */
+             PUSH_OBJ(p, hhdr, GC_mark_stack_top_reg, mark_stack_limit);
+         }
+      }
+      GC_mark_stack_top = GC_mark_stack_top_reg;
+    }
+}
+
+#ifndef GC_DISABLE_INCREMENTAL
+  /* Test whether any page in the given block is dirty.   */
+  STATIC GC_bool GC_block_was_dirty(struct hblk *h, hdr *hhdr)
+  {
+    size_t sz = hhdr -> hb_sz;
+
+    if (sz <= MAXOBJBYTES) {
+         return(GC_page_was_dirty(h));
+    } else {
+         ptr_t p = (ptr_t)h;
+         while (p < (ptr_t)h + sz) {
+             if (GC_page_was_dirty((struct hblk *)p)) return(TRUE);
+             p += HBLKSIZE;
+         }
+         return(FALSE);
+    }
+  }
+#endif /* GC_DISABLE_INCREMENTAL */
+
+/* Similar to GC_push_marked, but skip over unallocated blocks  */
+/* and return address of next plausible block.                  */
+STATIC struct hblk * GC_push_next_marked(struct hblk *h)
+{
+    hdr * hhdr = HDR(h);
+
+    if (EXPECT(IS_FORWARDING_ADDR_OR_NIL(hhdr) || HBLK_IS_FREE(hhdr), FALSE)) {
+      h = GC_next_used_block(h);
+      if (h == 0) return(0);
+      hhdr = GC_find_header((ptr_t)h);
+    }
+    GC_push_marked(h, hhdr);
+    return(h + OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz));
+}
+
+#ifndef GC_DISABLE_INCREMENTAL
+  /* Identical to above, but mark only from dirty pages   */
+  STATIC struct hblk * GC_push_next_marked_dirty(struct hblk *h)
+  {
+    hdr * hhdr = HDR(h);
+
+    if (!GC_dirty_maintained) ABORT("Dirty bits not set up");
+    for (;;) {
+        if (EXPECT(IS_FORWARDING_ADDR_OR_NIL(hhdr)
+                   || HBLK_IS_FREE(hhdr), FALSE)) {
+          h = GC_next_used_block(h);
+          if (h == 0) return(0);
+          hhdr = GC_find_header((ptr_t)h);
+        }
+#       ifdef STUBBORN_ALLOC
+          if (hhdr -> hb_obj_kind == STUBBORN) {
+            if (GC_page_was_changed(h) && GC_block_was_dirty(h, hhdr)) {
+                break;
+            }
+          } else {
+            if (GC_block_was_dirty(h, hhdr)) break;
+          }
+#       else
+          if (GC_block_was_dirty(h, hhdr)) break;
+#       endif
+        h += OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz);
+        hhdr = HDR(h);
+    }
+    GC_push_marked(h, hhdr);
+    return(h + OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz));
+  }
+#endif /* !GC_DISABLE_INCREMENTAL */
+
+/* Similar to above, but for uncollectible pages.  Needed since we      */
+/* do not clear marks for such pages, even for full collections.        */
+STATIC struct hblk * GC_push_next_marked_uncollectable(struct hblk *h)
+{
+    hdr * hhdr = HDR(h);
+
+    for (;;) {
+        if (EXPECT(IS_FORWARDING_ADDR_OR_NIL(hhdr)
+                   || HBLK_IS_FREE(hhdr), FALSE)) {
+          h = GC_next_used_block(h);
+          if (h == 0) return(0);
+          hhdr = GC_find_header((ptr_t)h);
+        }
+        if (hhdr -> hb_obj_kind == UNCOLLECTABLE) break;
+        h += OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz);
+        hhdr = HDR(h);
+    }
+    GC_push_marked(h, hhdr);
+    return(h + OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz));
+}
diff --git a/src/gc/bdwgc/mark_rts.c b/src/gc/bdwgc/mark_rts.c
new file mode 100644
index 0000000..bddbf8e
--- /dev/null
+++ b/src/gc/bdwgc/mark_rts.c
@@ -0,0 +1,786 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+/* Data structure for list of root sets.                                */
+/* We keep a hash table, so that we can filter out duplicate additions. */
+/* Under Win32, we need to do a better job of filtering overlaps, so    */
+/* we resort to sequential search, and pay the price.                   */
+/* This is really declared in gc_priv.h:
+struct roots {
+        ptr_t r_start;
+        ptr_t r_end;
+#       if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+          struct roots * r_next;
+#       endif
+        GC_bool r_tmp;
+                -- Delete before registering new dynamic libraries
+};
+
+struct roots GC_static_roots[MAX_ROOT_SETS];
+*/
+
+int GC_no_dls = 0;      /* Register dynamic library data segments.      */
+
+static int n_root_sets = 0;
+        /* GC_static_roots[0..n_root_sets) contains the valid root sets. */
+
+#if !defined(NO_DEBUGGING)
+  /* For debugging:     */
+  void GC_print_static_roots(void)
+  {
+    int i;
+    size_t total = 0;
+
+    for (i = 0; i < n_root_sets; i++) {
+        GC_printf("From %p to %p%s\n",
+                  GC_static_roots[i].r_start,
+                  GC_static_roots[i].r_end,
+                  GC_static_roots[i].r_tmp ? " (temporary)" : "");
+        total += GC_static_roots[i].r_end - GC_static_roots[i].r_start;
+    }
+    GC_printf("Total size: %ld\n", (unsigned long) total);
+    if (GC_root_size != total) {
+        GC_err_printf("GC_root_size incorrect: %ld!!\n",
+                      (long) GC_root_size);
+    }
+  }
+#endif /* !NO_DEBUGGING */
+
+#ifndef THREADS
+  /* Primarily for debugging support:     */
+  /* Is the address p in one of the registered static root sections?      */
+  GC_INNER GC_bool GC_is_static_root(ptr_t p)
+  {
+    static int last_root_set = MAX_ROOT_SETS;
+    int i;
+
+    if (last_root_set < n_root_sets
+        && p >= GC_static_roots[last_root_set].r_start
+        && p < GC_static_roots[last_root_set].r_end) return(TRUE);
+    for (i = 0; i < n_root_sets; i++) {
+        if (p >= GC_static_roots[i].r_start
+            && p < GC_static_roots[i].r_end) {
+            last_root_set = i;
+            return(TRUE);
+        }
+    }
+    return(FALSE);
+  }
+#endif /* !THREADS */
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+/*
+#   define LOG_RT_SIZE 6
+#   define RT_SIZE (1 << LOG_RT_SIZE)  -- Power of 2, may be != MAX_ROOT_SETS
+
+    struct roots * GC_root_index[RT_SIZE];
+        -- Hash table header.  Used only to check whether a range is
+        -- already present.
+        -- really defined in gc_priv.h
+*/
+
+  GC_INLINE int rt_hash(ptr_t addr)
+  {
+    word result = (word) addr;
+#   if CPP_WORDSZ > 8*LOG_RT_SIZE
+        result ^= result >> 8*LOG_RT_SIZE;
+#   endif
+#   if CPP_WORDSZ > 4*LOG_RT_SIZE
+        result ^= result >> 4*LOG_RT_SIZE;
+#   endif
+    result ^= result >> 2*LOG_RT_SIZE;
+    result ^= result >> LOG_RT_SIZE;
+    result &= (RT_SIZE-1);
+    return(result);
+  }
+
+  /* Is a range starting at b already in the table? If so return a      */
+  /* pointer to it, else NULL.                                          */
+  GC_INNER void * GC_roots_present(ptr_t b)
+  {
+    int h = rt_hash(b);
+    struct roots *p = GC_root_index[h];
+
+    while (p != 0) {
+        if (p -> r_start == (ptr_t)b) return(p);
+        p = p -> r_next;
+    }
+    return NULL;
+  }
+
+  /* Add the given root structure to the index. */
+  GC_INLINE void add_roots_to_index(struct roots *p)
+  {
+    int h = rt_hash(p -> r_start);
+
+    p -> r_next = GC_root_index[h];
+    GC_root_index[h] = p;
+  }
+#endif /* !MSWIN32 && !MSWINCE && !CYGWIN32 */
+
+GC_INNER word GC_root_size = 0;
+
+GC_API void GC_CALL GC_add_roots(void *b, void *e)
+{
+    DCL_LOCK_STATE;
+
+    if (!GC_is_initialized) GC_init();
+    LOCK();
+    GC_add_roots_inner((ptr_t)b, (ptr_t)e, FALSE);
+    UNLOCK();
+}
+
+
+/* Add [b,e) to the root set.  Adding the same interval a second time   */
+/* is a moderately fast no-op, and hence benign.  We do not handle      */
+/* different but overlapping intervals efficiently.  (We do handle      */
+/* them correctly.)                                                     */
+/* Tmp specifies that the interval may be deleted before                */
+/* re-registering dynamic libraries.                                    */
+void GC_add_roots_inner(ptr_t b, ptr_t e, GC_bool tmp)
+{
+    struct roots * old;
+    GC_ASSERT(b <= e);
+    b = (ptr_t)(((word)b + (sizeof(word) - 1)) & ~(sizeof(word) - 1));
+                                        /* round b up to word boundary */
+    e = (ptr_t)((word)e & ~(sizeof(word) - 1));
+                                        /* round e down to word boundary */
+    if (b >= e) return; /* nothing to do */
+
+#   if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+      /* Spend the time to ensure that there are no overlapping */
+      /* or adjacent intervals.                                 */
+      /* This could be done faster with e.g. a                  */
+      /* balanced tree.  But the execution time here is         */
+      /* virtually guaranteed to be dominated by the time it    */
+      /* takes to scan the roots.                               */
+      {
+        register int i;
+        old = 0; /* initialized to prevent warning. */
+        for (i = 0; i < n_root_sets; i++) {
+            old = GC_static_roots + i;
+            if (b <= old -> r_end && e >= old -> r_start) {
+                if (b < old -> r_start) {
+                    GC_root_size += old->r_start - b;
+                    old -> r_start = b;
+                }
+                if (e > old -> r_end) {
+                    GC_root_size += e - old->r_end;
+                    old -> r_end = e;
+                }
+                old -> r_tmp &= tmp;
+                break;
+            }
+        }
+        if (i < n_root_sets) {
+          /* merge other overlapping intervals */
+            struct roots *other;
+
+            for (i++; i < n_root_sets; i++) {
+              other = GC_static_roots + i;
+              b = other -> r_start;
+              e = other -> r_end;
+              if (b <= old -> r_end && e >= old -> r_start) {
+                if (b < old -> r_start) {
+                    GC_root_size += old->r_start - b;
+                    old -> r_start = b;
+                }
+                if (e > old -> r_end) {
+                    GC_root_size += e - old->r_end;
+                    old -> r_end = e;
+                }
+                old -> r_tmp &= other -> r_tmp;
+                /* Delete this entry. */
+                  GC_root_size -= (other -> r_end - other -> r_start);
+                  other -> r_start = GC_static_roots[n_root_sets-1].r_start;
+                  other -> r_end = GC_static_roots[n_root_sets-1].r_end;
+                  n_root_sets--;
+              }
+            }
+          return;
+        }
+      }
+#   else
+      old = (struct roots *)GC_roots_present(b);
+      if (old != 0) {
+        if (e <= old -> r_end) /* already there */ return;
+        /* else extend */
+        GC_root_size += e - old -> r_end;
+        old -> r_end = e;
+        return;
+      }
+#   endif
+    if (n_root_sets == MAX_ROOT_SETS) {
+        ABORT("Too many root sets");
+    }
+    GC_static_roots[n_root_sets].r_start = (ptr_t)b;
+    GC_static_roots[n_root_sets].r_end = (ptr_t)e;
+    GC_static_roots[n_root_sets].r_tmp = tmp;
+#   if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+      GC_static_roots[n_root_sets].r_next = 0;
+      add_roots_to_index(GC_static_roots + n_root_sets);
+#   endif
+    GC_root_size += e - b;
+    n_root_sets++;
+}
+
+static GC_bool roots_were_cleared = FALSE;
+
+GC_API void GC_CALL GC_clear_roots(void)
+{
+    DCL_LOCK_STATE;
+
+    if (!GC_is_initialized) GC_init();
+    LOCK();
+    roots_were_cleared = TRUE;
+    n_root_sets = 0;
+    GC_root_size = 0;
+#   if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+      BZERO(GC_root_index, RT_SIZE * sizeof(void *));
+#   endif
+    UNLOCK();
+}
+
+/* Internal use only; lock held.        */
+STATIC void GC_remove_root_at_pos(int i)
+{
+    GC_root_size -= (GC_static_roots[i].r_end - GC_static_roots[i].r_start);
+    GC_static_roots[i].r_start = GC_static_roots[n_root_sets-1].r_start;
+    GC_static_roots[i].r_end = GC_static_roots[n_root_sets-1].r_end;
+    GC_static_roots[i].r_tmp = GC_static_roots[n_root_sets-1].r_tmp;
+    n_root_sets--;
+}
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+  STATIC void GC_rebuild_root_index(void)
+  {
+    int i;
+    BZERO(GC_root_index, RT_SIZE * sizeof(void *));
+    for (i = 0; i < n_root_sets; i++)
+        add_roots_to_index(GC_static_roots + i);
+  }
+#endif
+
+#if defined(DYNAMIC_LOADING) || defined(MSWIN32) || defined(MSWINCE) \
+     || defined(PCR) || defined(CYGWIN32)
+/* Internal use only; lock held.        */
+STATIC void GC_remove_tmp_roots(void)
+{
+    int i;
+
+    for (i = 0; i < n_root_sets; ) {
+        if (GC_static_roots[i].r_tmp) {
+            GC_remove_root_at_pos(i);
+        } else {
+            i++;
+        }
+    }
+#   if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+      GC_rebuild_root_index();
+#   endif
+}
+#endif
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32)
+  STATIC void GC_remove_roots_inner(ptr_t b, ptr_t e);
+
+  GC_API void GC_CALL GC_remove_roots(void *b, void *e)
+  {
+    DCL_LOCK_STATE;
+
+    /* Quick check whether has nothing to do */
+    if ((((word)b + (sizeof(word) - 1)) & ~(sizeof(word) - 1)) >=
+        ((word)e & ~(sizeof(word) - 1)))
+      return;
+
+    LOCK();
+    GC_remove_roots_inner((ptr_t)b, (ptr_t)e);
+    UNLOCK();
+  }
+
+  /* Should only be called when the lock is held */
+  STATIC void GC_remove_roots_inner(ptr_t b, ptr_t e)
+  {
+    int i;
+    for (i = 0; i < n_root_sets; ) {
+        if (GC_static_roots[i].r_start >= b
+            && GC_static_roots[i].r_end <= e) {
+            GC_remove_root_at_pos(i);
+        } else {
+            i++;
+        }
+    }
+    GC_rebuild_root_index();
+  }
+#endif /* !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32) */
+
+#if (defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)) \
+    && !defined(NO_DEBUGGING)
+  /* Not used at present (except for, may be, debugging purpose).       */
+  /* Workaround for the OS mapping and unmapping behind our back:       */
+  /* Is the address p in one of the temporary static root sections?     */
+  GC_bool GC_is_tmp_root(ptr_t p)
+  {
+    static int last_root_set = MAX_ROOT_SETS;
+    register int i;
+
+    if (last_root_set < n_root_sets
+        && p >= GC_static_roots[last_root_set].r_start
+        && p < GC_static_roots[last_root_set].r_end)
+        return GC_static_roots[last_root_set].r_tmp;
+    for (i = 0; i < n_root_sets; i++) {
+        if (p >= GC_static_roots[i].r_start
+            && p < GC_static_roots[i].r_end) {
+            last_root_set = i;
+            return GC_static_roots[i].r_tmp;
+        }
+    }
+    return(FALSE);
+  }
+#endif /* MSWIN32 || MSWINCE || CYGWIN32 */
+
+GC_INNER ptr_t GC_approx_sp(void)
+{
+    volatile word sp;
+    sp = (word)&sp;
+                /* Also force stack to grow if necessary. Otherwise the */
+                /* later accesses might cause the kernel to think we're */
+                /* doing something wrong.                               */
+    return((ptr_t)sp);
+                /* GNU C: alternatively, we may return the value of     */
+                /*__builtin_frame_address(0).                           */
+}
+
+/*
+ * Data structure for excluded static roots.
+ * Real declaration is in gc_priv.h.
+
+struct exclusion {
+    ptr_t e_start;
+    ptr_t e_end;
+};
+
+struct exclusion GC_excl_table[MAX_EXCLUSIONS];
+                                        -- Array of exclusions, ascending
+                                        -- address order.
+*/
+
+STATIC size_t GC_excl_table_entries = 0;/* Number of entries in use.      */
+
+/* Return the first exclusion range that includes an address >= start_addr */
+/* Assumes the exclusion table contains at least one entry (namely the     */
+/* GC data structures).                                                    */
+STATIC struct exclusion * GC_next_exclusion(ptr_t start_addr)
+{
+    size_t low = 0;
+    size_t high = GC_excl_table_entries - 1;
+    size_t mid;
+
+    while (high > low) {
+        mid = (low + high) >> 1;
+        /* low <= mid < high    */
+        if ((word) GC_excl_table[mid].e_end <= (word) start_addr) {
+            low = mid + 1;
+        } else {
+            high = mid;
+        }
+    }
+    if ((word) GC_excl_table[low].e_end <= (word) start_addr) return 0;
+    return GC_excl_table + low;
+}
+
+/* Should only be called when the lock is held.  The range boundaries   */
+/* should be properly aligned and valid.                                */
+GC_INNER void GC_exclude_static_roots_inner(void *start, void *finish)
+{
+    struct exclusion * next;
+    size_t next_index, i;
+
+    GC_ASSERT((word)start % sizeof(word) == 0);
+    GC_ASSERT(start < finish);
+
+    if (0 == GC_excl_table_entries) {
+        next = 0;
+    } else {
+        next = GC_next_exclusion(start);
+    }
+    if (0 != next) {
+      if ((word)(next -> e_start) < (word) finish) {
+        /* incomplete error check. */
+        ABORT("Exclusion ranges overlap");
+      }
+      if ((word)(next -> e_start) == (word) finish) {
+        /* extend old range backwards   */
+          next -> e_start = (ptr_t)start;
+          return;
+      }
+      next_index = next - GC_excl_table;
+      for (i = GC_excl_table_entries; i > next_index; --i) {
+        GC_excl_table[i] = GC_excl_table[i-1];
+      }
+    } else {
+      next_index = GC_excl_table_entries;
+    }
+    if (GC_excl_table_entries == MAX_EXCLUSIONS) ABORT("Too many exclusions");
+    GC_excl_table[next_index].e_start = (ptr_t)start;
+    GC_excl_table[next_index].e_end = (ptr_t)finish;
+    ++GC_excl_table_entries;
+}
+
+GC_API void GC_CALL GC_exclude_static_roots(void *b, void *e)
+{
+    DCL_LOCK_STATE;
+
+    /* Adjust the upper boundary for safety (round down) */
+    e = (void *)((word)e & ~(sizeof(word) - 1));
+
+    if (b == e) return;  /* nothing to exclude? */
+
+    LOCK();
+    GC_exclude_static_roots_inner(b, e);
+    UNLOCK();
+}
+
+/* Invoke push_conditional on ranges that are not excluded. */
+/*ARGSUSED*/
+STATIC void GC_push_conditional_with_exclusions(ptr_t bottom, ptr_t top,
+                                                GC_bool all)
+{
+    struct exclusion * next;
+    ptr_t excl_start;
+
+    while (bottom < top) {
+        next = GC_next_exclusion(bottom);
+        if (0 == next || (excl_start = next -> e_start) >= top) {
+            GC_push_conditional(bottom, top, all);
+            return;
+        }
+        if (excl_start > bottom) GC_push_conditional(bottom, excl_start, all);
+        bottom = next -> e_end;
+    }
+}
+
+#ifdef IA64
+  /* Similar to GC_push_all_stack_sections() but for IA-64 registers store. */
+  GC_INNER void GC_push_all_register_sections(ptr_t bs_lo, ptr_t bs_hi,
+                  int eager, struct GC_traced_stack_sect_s *traced_stack_sect)
+  {
+    while (traced_stack_sect != NULL) {
+        ptr_t frame_bs_lo = traced_stack_sect -> backing_store_end;
+        GC_ASSERT(frame_bs_lo <= bs_hi);
+        if (eager) {
+            GC_push_all_eager(frame_bs_lo, bs_hi);
+        } else {
+            GC_push_all_stack(frame_bs_lo, bs_hi);
+        }
+        bs_hi = traced_stack_sect -> saved_backing_store_ptr;
+        traced_stack_sect = traced_stack_sect -> prev;
+    }
+    GC_ASSERT(bs_lo <= bs_hi);
+    if (eager) {
+        GC_push_all_eager(bs_lo, bs_hi);
+    } else {
+        GC_push_all_stack(bs_lo, bs_hi);
+    }
+  }
+#endif /* IA64 */
+
+#ifdef THREADS
+
+GC_INNER void GC_push_all_stack_sections(ptr_t lo, ptr_t hi,
+                        struct GC_traced_stack_sect_s *traced_stack_sect)
+{
+    while (traced_stack_sect != NULL) {
+        GC_ASSERT(lo HOTTER_THAN (ptr_t)traced_stack_sect);
+#       ifdef STACK_GROWS_UP
+            GC_push_all_stack((ptr_t)traced_stack_sect, lo);
+#       else /* STACK_GROWS_DOWN */
+            GC_push_all_stack(lo, (ptr_t)traced_stack_sect);
+#       endif
+        lo = traced_stack_sect -> saved_stack_ptr;
+        GC_ASSERT(lo != NULL);
+        traced_stack_sect = traced_stack_sect -> prev;
+    }
+    GC_ASSERT(!(hi HOTTER_THAN lo));
+#   ifdef STACK_GROWS_UP
+        /* We got them backwards! */
+        GC_push_all_stack(hi, lo);
+#   else /* STACK_GROWS_DOWN */
+        GC_push_all_stack(lo, hi);
+#   endif
+}
+
+#else /* !THREADS */
+
+# ifdef TRACE_BUF
+    /* Defined in mark.c.       */
+    void GC_add_trace_entry(char *kind, word arg1, word arg2);
+# endif
+
+                        /* Similar to GC_push_all_eager, but only the   */
+                        /* part hotter than cold_gc_frame is scanned    */
+                        /* immediately.  Needed to ensure that callee-  */
+                        /* save registers are not missed.               */
+/*
+ * A version of GC_push_all that treats all interior pointers as valid
+ * and scans part of the area immediately, to make sure that saved
+ * register values are not lost.
+ * Cold_gc_frame delimits the stack section that must be scanned
+ * eagerly.  A zero value indicates that no eager scanning is needed.
+ * We don't need to worry about the MANUAL_VDB case here, since this
+ * is only called in the single-threaded case.  We assume that we
+ * cannot collect between an assignment and the corresponding
+ * GC_dirty() call.
+ */
+STATIC void GC_push_all_stack_partially_eager(ptr_t bottom, ptr_t top,
+                                              ptr_t cold_gc_frame)
+{
+  if (!NEED_FIXUP_POINTER && GC_all_interior_pointers) {
+    /* Push the hot end of the stack eagerly, so that register values   */
+    /* saved inside GC frames are marked before they disappear.         */
+    /* The rest of the marking can be deferred until later.             */
+    if (0 == cold_gc_frame) {
+        GC_push_all_stack(bottom, top);
+        return;
+    }
+    GC_ASSERT(bottom <= cold_gc_frame && cold_gc_frame <= top);
+#   ifdef STACK_GROWS_DOWN
+        GC_push_all(cold_gc_frame - sizeof(ptr_t), top);
+        GC_push_all_eager(bottom, cold_gc_frame);
+#   else /* STACK_GROWS_UP */
+        GC_push_all(bottom, cold_gc_frame + sizeof(ptr_t));
+        GC_push_all_eager(cold_gc_frame, top);
+#   endif /* STACK_GROWS_UP */
+  } else {
+    GC_push_all_eager(bottom, top);
+  }
+# ifdef TRACE_BUF
+      GC_add_trace_entry("GC_push_all_stack", bottom, top);
+# endif
+}
+
+/* Similar to GC_push_all_stack_sections() but also uses cold_gc_frame. */
+STATIC void GC_push_all_stack_part_eager_sections(ptr_t lo, ptr_t hi,
+        ptr_t cold_gc_frame, struct GC_traced_stack_sect_s *traced_stack_sect)
+{
+    GC_ASSERT(traced_stack_sect == NULL || cold_gc_frame == NULL ||
+                cold_gc_frame HOTTER_THAN (ptr_t)traced_stack_sect);
+
+    while (traced_stack_sect != NULL) {
+        GC_ASSERT(lo HOTTER_THAN (ptr_t)traced_stack_sect);
+#       ifdef STACK_GROWS_UP
+            GC_push_all_stack_partially_eager((ptr_t)traced_stack_sect, lo,
+                                              cold_gc_frame);
+#       else /* STACK_GROWS_DOWN */
+            GC_push_all_stack_partially_eager(lo, (ptr_t)traced_stack_sect,
+                                              cold_gc_frame);
+#       endif
+        lo = traced_stack_sect -> saved_stack_ptr;
+        GC_ASSERT(lo != NULL);
+        traced_stack_sect = traced_stack_sect -> prev;
+        cold_gc_frame = NULL; /* Use at most once.      */
+    }
+
+    GC_ASSERT(!(hi HOTTER_THAN lo));
+#   ifdef STACK_GROWS_UP
+        /* We got them backwards! */
+        GC_push_all_stack_partially_eager(hi, lo, cold_gc_frame);
+#   else /* STACK_GROWS_DOWN */
+        GC_push_all_stack_partially_eager(lo, hi, cold_gc_frame);
+#   endif
+}
+
+#endif /* !THREADS */
+
+                        /* Push enough of the current stack eagerly to  */
+                        /* ensure that callee-save registers saved in   */
+                        /* GC frames are scanned.                       */
+                        /* In the non-threads case, schedule entire     */
+                        /* stack for scanning.                          */
+                        /* The second argument is a pointer to the      */
+                        /* (possibly null) thread context, for          */
+                        /* (currently hypothetical) more precise        */
+                        /* stack scanning.                              */
+/*
+ * In the absence of threads, push the stack contents.
+ * In the presence of threads, push enough of the current stack
+ * to ensure that callee-save registers saved in collector frames have been
+ * seen.
+ * FIXME: Merge with per-thread stuff.
+ */
+/*ARGSUSED*/
+STATIC void GC_push_current_stack(ptr_t cold_gc_frame, void * context)
+{
+#   if defined(THREADS)
+        if (0 == cold_gc_frame) return;
+#       ifdef STACK_GROWS_DOWN
+          GC_push_all_eager(GC_approx_sp(), cold_gc_frame);
+          /* For IA64, the register stack backing store is handled      */
+          /* in the thread-specific code.                               */
+#       else
+          GC_push_all_eager(cold_gc_frame, GC_approx_sp());
+#       endif
+#   else
+        GC_push_all_stack_part_eager_sections(GC_approx_sp(), GC_stackbottom,
+                                        cold_gc_frame, GC_traced_stack_sect);
+#       ifdef IA64
+              /* We also need to push the register stack backing store. */
+              /* This should really be done in the same way as the      */
+              /* regular stack.  For now we fudge it a bit.             */
+              /* Note that the backing store grows up, so we can't use  */
+              /* GC_push_all_stack_partially_eager.                     */
+              {
+                ptr_t bsp = GC_save_regs_ret_val;
+                ptr_t cold_gc_bs_pointer = bsp - 2048;
+                if (GC_all_interior_pointers &&
+                    cold_gc_bs_pointer > BACKING_STORE_BASE) {
+                  /* Adjust cold_gc_bs_pointer if below our innermost   */
+                  /* "traced stack section" in backing store.           */
+                  if (GC_traced_stack_sect != NULL && cold_gc_bs_pointer <
+                                GC_traced_stack_sect->backing_store_end)
+                    cold_gc_bs_pointer =
+                                GC_traced_stack_sect->backing_store_end;
+                  GC_push_all_register_sections(BACKING_STORE_BASE,
+                        cold_gc_bs_pointer, FALSE, GC_traced_stack_sect);
+                  GC_push_all_eager(cold_gc_bs_pointer, bsp);
+                } else {
+                  GC_push_all_register_sections(BACKING_STORE_BASE, bsp,
+                                TRUE /* eager */, GC_traced_stack_sect);
+                }
+                /* All values should be sufficiently aligned that we    */
+                /* don't have to worry about the boundary.              */
+              }
+#       endif
+#   endif /* !THREADS */
+}
+
+GC_INNER void (*GC_push_typed_structures)(void) = 0;
+
+                        /* Push GC internal roots.  These are normally  */
+                        /* included in the static data segment, and     */
+                        /* Thus implicitly pushed.  But we must do this */
+                        /* explicitly if normal root processing is      */
+                        /* disabled.                                    */
+/*
+ * Push GC internal roots.  Only called if there is some reason to believe
+ * these would not otherwise get registered.
+ */
+STATIC void GC_push_gc_structures(void)
+{
+    GC_push_finalizer_structures();
+#   if defined(THREADS)
+      GC_push_thread_structures();
+#   endif
+    if( GC_push_typed_structures )
+      GC_push_typed_structures();
+}
+
+GC_INNER void GC_cond_register_dynamic_libraries(void)
+{
+# if defined(DYNAMIC_LOADING) || defined(MSWIN32) || defined(MSWINCE) \
+     || defined(CYGWIN32) || defined(PCR)
+    GC_remove_tmp_roots();
+    if (!GC_no_dls) GC_register_dynamic_libraries();
+# else
+    GC_no_dls = TRUE;
+# endif
+}
+
+STATIC void GC_push_regs_and_stack(ptr_t cold_gc_frame)
+{
+    GC_with_callee_saves_pushed(GC_push_current_stack, cold_gc_frame);
+}
+
+/*
+ * Call the mark routines (GC_tl_push for a single pointer, GC_push_conditional
+ * on groups of pointers) on every top level accessible pointer.
+ * If all is FALSE, arrange to push only possibly altered values.
+ * Cold_gc_frame is an address inside a GC frame that
+ * remains valid until all marking is complete.
+ * A zero value indicates that it's OK to miss some
+ * register values.
+ */
+GC_INNER void GC_push_roots(GC_bool all, ptr_t cold_gc_frame)
+{
+    int i;
+    unsigned kind;
+
+    /*
+     * Next push static data.  This must happen early on, since it's
+     * not robust against mark stack overflow.
+     */
+     /* Re-register dynamic libraries, in case one got added.           */
+     /* There is some argument for doing this as late as possible,      */
+     /* especially on win32, where it can change asynchronously.        */
+     /* In those cases, we do it here.  But on other platforms, it's    */
+     /* not safe with the world stopped, so we do it earlier.           */
+#      if !defined(REGISTER_LIBRARIES_EARLY)
+         GC_cond_register_dynamic_libraries();
+#      endif
+
+     /* Mark everything in static data areas                             */
+       for (i = 0; i < n_root_sets; i++) {
+         GC_push_conditional_with_exclusions(
+                             GC_static_roots[i].r_start,
+                             GC_static_roots[i].r_end, all);
+       }
+
+     /* Mark all free list header blocks, if those were allocated from  */
+     /* the garbage collected heap.  This makes sure they don't         */
+     /* disappear if we are not marking from static data.  It also      */
+     /* saves us the trouble of scanning them, and possibly that of     */
+     /* marking the freelists.                                          */
+       for (kind = 0; kind < GC_n_kinds; kind++) {
+         void *base = GC_base(GC_obj_kinds[kind].ok_freelist);
+         if (0 != base) {
+           GC_set_mark_bit(base);
+         }
+       }
+
+     /* Mark from GC internal roots if those might otherwise have       */
+     /* been excluded.                                                  */
+       if (GC_no_dls || roots_were_cleared) {
+           GC_push_gc_structures();
+       }
+
+     /* Mark thread local free lists, even if their mark        */
+     /* descriptor excludes the link field.                     */
+     /* If the world is not stopped, this is unsafe.  It is     */
+     /* also unnecessary, since we will do this again with the  */
+     /* world stopped.                                          */
+#      if defined(THREAD_LOCAL_ALLOC)
+         if (GC_world_stopped) GC_mark_thread_local_free_lists();
+#      endif
+
+    /*
+     * Now traverse stacks, and mark from register contents.
+     * These must be done last, since they can legitimately overflow
+     * the mark stack.
+     * This is usually done by saving the current context on the
+     * stack, and then just tracing from the stack.
+     */
+      GC_push_regs_and_stack(cold_gc_frame);
+
+    if (GC_push_other_roots != 0) (*GC_push_other_roots)();
+        /* In the threads case, this also pushes thread stacks. */
+        /* Note that without interior pointer recognition lots  */
+        /* of stuff may have been pushed already, and this      */
+        /* should be careful about mark stack overflows.        */
+}
diff --git a/src/gc/bdwgc/misc.c b/src/gc/bdwgc/misc.c
new file mode 100644
index 0000000..9e85cec
--- /dev/null
+++ b/src/gc/bdwgc/misc.c
@@ -0,0 +1,1943 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1999-2001 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "gc.h"
+#include "private/gc_pmark.h"
+
+#ifndef NAUT
+# include <limits.h>
+# include <stdarg.h>
+
+#ifndef MSWINCE
+# include <signal.h>
+#endif
+
+#ifdef GC_SOLARIS_THREADS
+# include <sys/syscall.h>
+#endif
+#if defined(MSWIN32) || defined(MSWINCE) \
+    || (defined(CYGWIN32) && defined(GC_READ_ENV_FILE))
+# ifndef WIN32_LEAN_AND_MEAN
+#   define WIN32_LEAN_AND_MEAN 1
+# endif
+# define NOSERVICE
+# include <windows.h>
+#endif
+
+#if (defined(UNIX_LIKE) || defined(CYGWIN32)) && !defined(NAUT)
+# include <fcntl.h>
+# include <sys/types.h>
+# include <sys/stat.h>
+#endif
+
+#ifdef NONSTOP
+# include <floss.h>
+#endif
+
+#endif
+
+#ifdef THREADS
+# ifdef PCR
+#   include "il/PCR_IL.h"
+    GC_INNER PCR_Th_ML GC_allocate_ml;
+# elif defined(SN_TARGET_PS3)
+#   include <pthread.h>
+    GC_INNER pthread_mutex_t GC_allocate_ml;
+# endif
+  /* For other platforms with threads, the lock and possibly            */
+  /* GC_lock_holder variables are defined in the thread support code.   */
+#endif /* THREADS */
+
+#ifdef DYNAMIC_LOADING
+  /* We need to register the main data segment.  Returns  TRUE unless   */
+  /* this is done implicitly as part of dynamic library registration.   */
+# define GC_REGISTER_MAIN_STATIC_DATA() GC_register_main_static_data()
+#else
+  /* Don't unnecessarily call GC_register_main_static_data() in case    */
+  /* dyn_load.c isn't linked in.                                        */
+# define GC_REGISTER_MAIN_STATIC_DATA() TRUE
+#endif
+
+#ifdef NEED_CANCEL_DISABLE_COUNT
+  __thread unsigned char GC_cancel_disable_count = 0;
+#endif
+
+GC_FAR struct _GC_arrays GC_arrays /* = { 0 } */;
+
+GC_INNER GC_bool GC_debugging_started = FALSE;
+        /* defined here so we don't have to load debug_malloc.o */
+
+ptr_t GC_stackbottom = 0;
+
+#ifdef IA64
+  ptr_t GC_register_stackbottom = 0;
+#endif
+
+GC_bool GC_dont_gc = 0;
+
+GC_bool GC_dont_precollect = 0;
+
+GC_bool GC_quiet = 0; /* used also in pcr_interface.c */
+
+#ifndef SMALL_CONFIG
+  GC_bool GC_print_stats = 0;
+#endif
+
+#ifdef GC_PRINT_BACK_HEIGHT
+  GC_INNER GC_bool GC_print_back_height = TRUE;
+#else
+  GC_INNER GC_bool GC_print_back_height = FALSE;
+#endif
+
+#ifndef NO_DEBUGGING
+  GC_INNER GC_bool GC_dump_regularly = FALSE;
+                                /* Generate regular debugging dumps. */
+#endif
+
+#ifdef KEEP_BACK_PTRS
+  GC_INNER long GC_backtraces = 0;
+                /* Number of random backtraces to generate for each GC. */
+#endif
+
+#ifdef FIND_LEAK
+  int GC_find_leak = 1;
+#else
+  int GC_find_leak = 0;
+#endif
+
+#ifndef SHORT_DBG_HDRS
+# ifdef GC_FINDLEAK_DELAY_FREE
+    GC_INNER GC_bool GC_findleak_delay_free = TRUE;
+# else
+    GC_INNER GC_bool GC_findleak_delay_free = FALSE;
+# endif
+#endif /* !SHORT_DBG_HDRS */
+
+#ifdef ALL_INTERIOR_POINTERS
+  int GC_all_interior_pointers = 1;
+#else
+  int GC_all_interior_pointers = 0;
+#endif
+
+#ifdef GC_FORCE_UNMAP_ON_GCOLLECT
+  /* Has no effect unless USE_MUNMAP.                           */
+  /* Has no effect on implicitly-initiated garbage collections. */
+  GC_INNER GC_bool GC_force_unmap_on_gcollect = TRUE;
+#else
+  GC_INNER GC_bool GC_force_unmap_on_gcollect = FALSE;
+#endif
+
+#ifndef GC_LARGE_ALLOC_WARN_INTERVAL
+# define GC_LARGE_ALLOC_WARN_INTERVAL 5
+#endif
+GC_INNER long GC_large_alloc_warn_interval = GC_LARGE_ALLOC_WARN_INTERVAL;
+                        /* Interval between unsuppressed warnings.      */
+
+/*ARGSUSED*/
+STATIC void * GC_CALLBACK GC_default_oom_fn(size_t bytes_requested)
+{
+    return(0);
+}
+
+/* All accesses to it should be synchronized to avoid data races.       */
+GC_oom_func GC_oom_fn = GC_default_oom_fn;
+
+#ifdef CAN_HANDLE_FORK
+# ifdef HANDLE_FORK
+    GC_INNER GC_bool GC_handle_fork = TRUE;
+                        /* The value is examined by GC_thr_init.        */
+# else
+    GC_INNER GC_bool GC_handle_fork = FALSE;
+# endif
+#endif /* CAN_HANDLE_FORK */
+
+/* Overrides the default handle-fork mode.  Non-zero value means GC     */
+/* should install proper pthread_atfork handlers (or abort if not       */
+/* supported).  Has effect only if called before GC_INIT.               */
+/*ARGSUSED*/
+GC_API void GC_CALL GC_set_handle_fork(int value)
+{
+# ifdef CAN_HANDLE_FORK
+    if (!GC_is_initialized)
+      GC_handle_fork = (GC_bool)value;
+# elif defined(THREADS) || (defined(DARWIN) && defined(MPROTECT_VDB))
+    if (!GC_is_initialized && value)
+      ABORT("fork() handling disabled");
+# else
+    /* No at-fork handler is needed in the single-threaded mode.        */
+# endif
+}
+
+/* Set things up so that GC_size_map[i] >= granules(i),                 */
+/* but not too much bigger                                              */
+/* and so that size_map contains relatively few distinct entries        */
+/* This was originally stolen from Russ Atkinson's Cedar                */
+/* quantization algorithm (but we precompute it).                       */
+STATIC void GC_init_size_map(void)
+{
+    int i;
+
+    /* Map size 0 to something bigger.                  */
+    /* This avoids problems at lower levels.            */
+      GC_size_map[0] = 1;
+    for (i = 1; i <= GRANULES_TO_BYTES(TINY_FREELISTS-1) - EXTRA_BYTES; i++) {
+        GC_size_map[i] = ROUNDED_UP_GRANULES(i);
+#       ifndef _MSC_VER
+          GC_ASSERT(GC_size_map[i] < TINY_FREELISTS);
+          /* Seems to tickle bug in VC++ 2008 for AMD64 */
+#       endif
+    }
+    /* We leave the rest of the array to be filled in on demand. */
+}
+
+/* Fill in additional entries in GC_size_map, including the ith one */
+/* We assume the ith entry is currently 0.                              */
+/* Note that a filled in section of the array ending at n always    */
+/* has length at least n/4.                                             */
+GC_INNER void GC_extend_size_map(size_t i)
+{
+    size_t orig_granule_sz = ROUNDED_UP_GRANULES(i);
+    size_t granule_sz = orig_granule_sz;
+    size_t byte_sz = GRANULES_TO_BYTES(granule_sz);
+                        /* The size we try to preserve.         */
+                        /* Close to i, unless this would        */
+                        /* introduce too many distinct sizes.   */
+    size_t smaller_than_i = byte_sz - (byte_sz >> 3);
+    size_t much_smaller_than_i = byte_sz - (byte_sz >> 2);
+    size_t low_limit;   /* The lowest indexed entry we  */
+                        /* initialize.                  */
+    size_t j;
+
+    if (GC_size_map[smaller_than_i] == 0) {
+        low_limit = much_smaller_than_i;
+        while (GC_size_map[low_limit] != 0) low_limit++;
+    } else {
+        low_limit = smaller_than_i + 1;
+        while (GC_size_map[low_limit] != 0) low_limit++;
+        granule_sz = ROUNDED_UP_GRANULES(low_limit);
+        granule_sz += granule_sz >> 3;
+        if (granule_sz < orig_granule_sz) granule_sz = orig_granule_sz;
+    }
+    /* For these larger sizes, we use an even number of granules.       */
+    /* This makes it easier to, for example, construct a 16byte-aligned */
+    /* allocator even if GRANULE_BYTES is 8.                            */
+        granule_sz += 1;
+        granule_sz &= ~1;
+    if (granule_sz > MAXOBJGRANULES) {
+        granule_sz = MAXOBJGRANULES;
+    }
+    /* If we can fit the same number of larger objects in a block,      */
+    /* do so.                                                   */
+    {
+        size_t number_of_objs = HBLK_GRANULES/granule_sz;
+        granule_sz = HBLK_GRANULES/number_of_objs;
+        granule_sz &= ~1;
+    }
+    byte_sz = GRANULES_TO_BYTES(granule_sz);
+    /* We may need one extra byte;                      */
+    /* don't always fill in GC_size_map[byte_sz]        */
+    byte_sz -= EXTRA_BYTES;
+
+    for (j = low_limit; j <= byte_sz; j++) GC_size_map[j] = granule_sz;
+}
+
+
+/*
+ * The following is a gross hack to deal with a problem that can occur
+ * on machines that are sloppy about stack frame sizes, notably SPARC.
+ * Bogus pointers may be written to the stack and not cleared for
+ * a LONG time, because they always fall into holes in stack frames
+ * that are not written.  We partially address this by clearing
+ * sections of the stack whenever we get control.
+ */
+
+void dump_tsi(void* arg, unsigned long size)
+{
+  nk_thread_t * t = get_cur_thread();
+  
+  printk("Clearing from %p to %p (rsp=%p, stack= %p to %p)\n",
+         arg, arg + size, &t, t->stack, t->stack + t->stack_size);
+}
+
+
+# ifdef THREADS
+#   define BIG_CLEAR_SIZE 2048  /* Clear this much now and then.        */
+#   define SMALL_CLEAR_SIZE 256 /* Clear this much every time.          */
+# else
+  STATIC word GC_stack_last_cleared = 0; /* GC_no when we last did this */
+  STATIC ptr_t GC_min_sp = NULL;
+                        /* Coolest stack pointer value from which       */
+                        /* we've already cleared the stack.             */
+  STATIC ptr_t GC_high_water = NULL;
+                        /* "hottest" stack pointer value we have seen   */
+                        /* recently.  Degrades over time.               */
+  STATIC word GC_bytes_allocd_at_reset = 0;
+#   define DEGRADE_RATE 50
+# endif
+
+# define CLEAR_SIZE 213  /* Granularity for GC_clear_stack_inner */
+
+#if defined(ASM_CLEAR_CODE)
+  void *GC_clear_stack_inner(void *, ptr_t);
+#else
+  /* Clear the stack up to about limit.  Return arg.  This function is  */
+  /* not static because it could also be erroneously defined in .S      */
+  /* file, so this error would be caught by the linker.                 */
+  /*ARGSUSED*/
+  void * GC_clear_stack_inner(void *arg, ptr_t limit)
+  {
+    volatile word dummy[CLEAR_SIZE];
+    BZERO((/* no volatile */ void *)dummy, sizeof(dummy));
+    if ((word)GC_approx_sp() COOLER_THAN (word)limit) {
+        (void) GC_clear_stack_inner(arg, limit);
+    }
+    /* Make sure the recursive call is not a tail call, and the bzero   */
+    /* call is not recognized as dead code.                             */
+    GC_noop1((word)dummy);
+    return(arg);
+  }
+#endif
+
+
+/* Clear some of the inaccessible part of the stack.  Returns its       */
+/* argument, so it can be used in a tail call position, hence clearing  */
+/* another frame.                                                       */
+GC_API void * GC_CALL GC_clear_stack(void *arg)
+{
+    ptr_t sp = GC_approx_sp();  /* Hotter than actual sp */
+#   ifdef THREADS
+        word volatile dummy[SMALL_CLEAR_SIZE];
+        static unsigned random_no = 0;
+                                 /* Should be more random than it is ... */
+                                 /* Used to occasionally clear a bigger  */
+                                 /* chunk.                               */
+#   endif
+    ptr_t limit;
+
+#   define SLOP 400
+        /* Extra bytes we clear every time.  This clears our own        */
+        /* activation record, and should cause more frequent            */
+        /* clearing near the cold end of the stack, a good thing.       */
+#   define GC_SLOP 4000
+        /* We make GC_high_water this much hotter than we really saw    */
+        /* saw it, to cover for GC noise etc. above our current frame.  */
+#   define CLEAR_THRESHOLD 100000
+        /* We restart the clearing process after this many bytes of     */
+        /* allocation.  Otherwise very heavily recursive programs       */
+        /* with sparse stacks may result in heaps that grow almost      */
+        /* without bounds.  As the heap gets larger, collection         */
+        /* frequency decreases, thus clearing frequency would decrease, */
+        /* thus more junk remains accessible, thus the heap gets        */
+        /* larger ...                                                   */
+# ifdef THREADS
+    if (++random_no % 13 == 0) {
+        limit = sp;
+        MAKE_HOTTER(limit, BIG_CLEAR_SIZE*sizeof(word));
+        limit = (ptr_t)((word)limit & ~0xf);
+                        /* Make it sufficiently aligned for assembly    */
+                        /* implementations of GC_clear_stack_inner.     */
+        return GC_clear_stack_inner(arg, limit);
+    } else {
+        BZERO((void *)dummy, SMALL_CLEAR_SIZE*sizeof(word));
+        return arg;
+    }
+# else
+    if (GC_gc_no > GC_stack_last_cleared) {
+        /* Start things over, so we clear the entire stack again */
+        if (GC_stack_last_cleared == 0) GC_high_water = (ptr_t)GC_stackbottom;
+        GC_min_sp = GC_high_water;
+        GC_stack_last_cleared = GC_gc_no;
+        GC_bytes_allocd_at_reset = GC_bytes_allocd;
+    }
+    /* Adjust GC_high_water */
+        MAKE_COOLER(GC_high_water, WORDS_TO_BYTES(DEGRADE_RATE) + GC_SLOP);
+        if (sp HOTTER_THAN GC_high_water) {
+            GC_high_water = sp;
+        }
+        MAKE_HOTTER(GC_high_water, GC_SLOP);
+    limit = GC_min_sp;
+    MAKE_HOTTER(limit, SLOP);
+    if (sp COOLER_THAN limit) {
+        limit = (ptr_t)((word)limit & ~0xf);
+                        /* Make it sufficiently aligned for assembly    */
+                        /* implementations of GC_clear_stack_inner.     */
+        GC_min_sp = sp;
+        return(GC_clear_stack_inner(arg, limit));
+    } else if (GC_bytes_allocd - GC_bytes_allocd_at_reset > CLEAR_THRESHOLD) {
+        /* Restart clearing process, but limit how much clearing we do. */
+        GC_min_sp = sp;
+        MAKE_HOTTER(GC_min_sp, CLEAR_THRESHOLD/4);
+        if (GC_min_sp HOTTER_THAN GC_high_water) GC_min_sp = GC_high_water;
+        GC_bytes_allocd_at_reset = GC_bytes_allocd;
+    }
+    return(arg);
+# endif
+}
+
+
+/* Return a pointer to the base address of p, given a pointer to a      */
+/* an address within an object.  Return 0 o.w.                          */
+GC_API void * GC_CALL GC_base(void * p)
+{
+    ptr_t r;
+    struct hblk *h;
+    bottom_index *bi;
+    hdr *candidate_hdr;
+    ptr_t limit;
+
+    r = p;
+    if (!GC_is_initialized) return 0;
+    h = HBLKPTR(r);
+    GET_BI(r, bi);
+    candidate_hdr = HDR_FROM_BI(bi, r);
+    if (candidate_hdr == 0) return(0);
+    /* If it's a pointer to the middle of a large object, move it       */
+    /* to the beginning.                                                */
+        while (IS_FORWARDING_ADDR_OR_NIL(candidate_hdr)) {
+           h = FORWARDED_ADDR(h,candidate_hdr);
+           r = (ptr_t)h;
+           candidate_hdr = HDR(h);
+        }
+    if (HBLK_IS_FREE(candidate_hdr)) return(0);
+    /* Make sure r points to the beginning of the object */
+        r = (ptr_t)((word)r & ~(WORDS_TO_BYTES(1) - 1));
+        {
+            size_t offset = HBLKDISPL(r);
+            word sz = candidate_hdr -> hb_sz;
+            size_t obj_displ = offset % sz;
+
+            r -= obj_displ;
+            limit = r + sz;
+            if (limit > (ptr_t)(h + 1) && sz <= HBLKSIZE) {
+                return(0);
+            }
+            if ((ptr_t)p >= limit) return(0);
+        }
+    return((void *)r);
+}
+
+
+/* Return the size of an object, given a pointer to its base.           */
+/* (For small objects this also happens to work from interior pointers, */
+/* but that shouldn't be relied upon.)                                  */
+GC_API size_t GC_CALL GC_size(const void * p)
+{
+    hdr * hhdr = HDR(p);
+
+    return hhdr -> hb_sz;
+}
+
+
+/* These getters remain unsynchronized for compatibility (since some    */
+/* clients could call some of them from a GC callback holding the       */
+/* allocator lock).                                                     */
+GC_API size_t GC_CALL GC_get_heap_size(void)
+{
+    /* ignore the memory space returned to OS (i.e. count only the      */
+    /* space owned by the garbage collector)                            */
+    return (size_t)(GC_heapsize - GC_unmapped_bytes);
+}
+
+GC_API size_t GC_CALL GC_get_free_bytes(void)
+{
+    /* ignore the memory space returned to OS */
+    return (size_t)(GC_large_free_bytes - GC_unmapped_bytes);
+}
+
+GC_API size_t GC_CALL GC_get_unmapped_bytes(void)
+{
+    return (size_t)GC_unmapped_bytes;
+}
+
+GC_API size_t GC_CALL GC_get_bytes_since_gc(void)
+{
+    return (size_t)GC_bytes_allocd;
+}
+
+GC_API size_t GC_CALL GC_get_total_bytes(void)
+{
+    return (size_t)(GC_bytes_allocd + GC_bytes_allocd_before_gc);
+}
+
+/* Return the heap usage information.  This is a thread-safe (atomic)   */
+/* alternative for the five above getters.  NULL pointer is allowed for */
+/* any argument.  Returned (filled in) values are of word type.         */
+GC_API void GC_CALL GC_get_heap_usage_safe(GC_word *pheap_size,
+                        GC_word *pfree_bytes, GC_word *punmapped_bytes,
+                        GC_word *pbytes_since_gc, GC_word *ptotal_bytes)
+{
+  DCL_LOCK_STATE;
+
+  LOCK();
+  if (pheap_size != NULL)
+    *pheap_size = GC_heapsize - GC_unmapped_bytes;
+  if (pfree_bytes != NULL)
+    *pfree_bytes = GC_large_free_bytes - GC_unmapped_bytes;
+  if (punmapped_bytes != NULL)
+    *punmapped_bytes = GC_unmapped_bytes;
+  if (pbytes_since_gc != NULL)
+    *pbytes_since_gc = GC_bytes_allocd;
+  if (ptotal_bytes != NULL)
+    *ptotal_bytes = GC_bytes_allocd + GC_bytes_allocd_before_gc;
+  UNLOCK();
+}
+
+
+#ifdef THREADS
+  GC_API int GC_CALL GC_get_suspend_signal(void)
+  {
+#   ifdef SIG_SUSPEND
+      return SIG_SUSPEND;
+#   else
+      return -1;
+#   endif
+  }
+#endif /* THREADS */
+
+#if !defined(_MAX_PATH) && (defined(MSWIN32) || defined(MSWINCE) \
+                            || defined(CYGWIN32))
+# define _MAX_PATH MAX_PATH
+#endif
+
+#ifdef GC_READ_ENV_FILE
+  /* This works for Win32/WinCE for now.  Really useful only for WinCE. */
+  STATIC char *GC_envfile_content = NULL;
+                        /* The content of the GC "env" file with CR and */
+                        /* LF replaced to '\0'.  NULL if the file is    */
+                        /* missing or empty.  Otherwise, always ends    */
+                        /* with '\0'.                                   */
+  STATIC unsigned GC_envfile_length = 0;
+                        /* Length of GC_envfile_content (if non-NULL).  */
+
+# ifndef GC_ENVFILE_MAXLEN
+#   define GC_ENVFILE_MAXLEN 0x4000
+# endif
+
+  /* The routine initializes GC_envfile_content from the GC "env" file. */
+  STATIC void GC_envfile_init(void)
+  {
+#   if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+      HANDLE hFile;
+      char *content;
+      unsigned ofs;
+      unsigned len;
+      DWORD nBytesRead;
+      TCHAR path[_MAX_PATH + 0x10]; /* buffer for path + ext */
+      len = (unsigned)GetModuleFileName(NULL /* hModule */, path,
+                                        _MAX_PATH + 1);
+      /* If GetModuleFileName() has failed then len is 0. */
+      if (len > 4 && path[len - 4] == (TCHAR)'.') {
+        len -= 4; /* strip executable file extension */
+      }
+      memcpy(&path[len], TEXT(".gc.env"), sizeof(TEXT(".gc.env")));
+      hFile = CreateFile(path, GENERIC_READ,
+                         FILE_SHARE_READ | FILE_SHARE_WRITE,
+                         NULL /* lpSecurityAttributes */, OPEN_EXISTING,
+                         FILE_ATTRIBUTE_NORMAL, NULL /* hTemplateFile */);
+      if (hFile == INVALID_HANDLE_VALUE)
+        return; /* the file is absent or the operation is failed */
+      len = (unsigned)GetFileSize(hFile, NULL);
+      if (len <= 1 || len >= GC_ENVFILE_MAXLEN) {
+        CloseHandle(hFile);
+        return; /* invalid file length - ignoring the file content */
+      }
+      /* At this execution point, GC_setpagesize() and GC_init_win32()  */
+      /* must already be called (for GET_MEM() to work correctly).      */
+      content = (char *)GET_MEM(ROUNDUP_PAGESIZE_IF_MMAP(len + 1));
+      if (content == NULL) {
+        CloseHandle(hFile);
+        return; /* allocation failure */
+      }
+      ofs = 0;
+      nBytesRead = (DWORD)-1L;
+          /* Last ReadFile() call should clear nBytesRead on success. */
+      while (ReadFile(hFile, content + ofs, len - ofs + 1, &nBytesRead,
+                      NULL /* lpOverlapped */) && nBytesRead != 0) {
+        if ((ofs += nBytesRead) > len)
+          break;
+      }
+      CloseHandle(hFile);
+      if (ofs != len || nBytesRead != 0)
+        return; /* read operation is failed - ignoring the file content */
+      content[ofs] = '\0';
+      while (ofs-- > 0) {
+       if (content[ofs] == '\r' || content[ofs] == '\n')
+         content[ofs] = '\0';
+      }
+      GC_envfile_length = len + 1;
+      GC_envfile_content = content;
+#   endif
+  }
+
+  /* This routine scans GC_envfile_content for the specified            */
+  /* environment variable (and returns its value if found).             */
+  GC_INNER char * GC_envfile_getenv(const char *name)
+  {
+    char *p;
+    char *end_of_content;
+    unsigned namelen;
+#   ifndef NO_GETENV
+      p = getenv(name); /* try the standard getenv() first */
+      if (p != NULL)
+        return *p != '\0' ? p : NULL;
+#   endif
+    p = GC_envfile_content;
+    if (p == NULL)
+      return NULL; /* "env" file is absent (or empty) */
+    namelen = strlen(name);
+    if (namelen == 0) /* a sanity check */
+      return NULL;
+    for (end_of_content = p + GC_envfile_length;
+         p != end_of_content; p += strlen(p) + 1) {
+      if (strncmp(p, name, namelen) == 0 && *(p += namelen) == '=') {
+        p++; /* the match is found; skip '=' */
+        return *p != '\0' ? p : NULL;
+      }
+      /* If not matching then skip to the next line. */
+    }
+    return NULL; /* no match found */
+  }
+#endif /* GC_READ_ENV_FILE */
+
+GC_INNER GC_bool GC_is_initialized = FALSE;
+
+#if (defined(MSWIN32) || defined(MSWINCE)) && defined(THREADS)
+    GC_INNER CRITICAL_SECTION GC_write_cs;
+#endif
+
+STATIC void GC_exit_check(void)
+{
+   GC_gcollect();
+}
+
+#ifdef UNIX_LIKE 
+  static void looping_handler(int sig)
+  {
+    GC_err_printf("Caught signal %d: looping in handler\n", sig);
+    for (;;) {}
+  }
+
+  static GC_bool installed_looping_handler = FALSE;
+
+  static void maybe_install_looping_handler(void)
+  {
+    /* Install looping handler before the write fault handler, so we    */
+    /* handle write faults correctly.                                   */
+    if (!installed_looping_handler && 0 != GETENV("GC_LOOP_ON_ABORT")) {
+      GC_set_and_save_fault_handler(looping_handler);
+      installed_looping_handler = TRUE;
+    }
+  }
+
+#else /* !UNIX_LIKE */
+# define maybe_install_looping_handler()
+#endif
+
+#if !defined(OS2) && !defined(MACOS) && !defined(MSWIN32) && !defined(MSWINCE)
+  STATIC int GC_stdout = 1;
+  STATIC int GC_stderr = 2;
+  STATIC int GC_log = 2; /* stderr */
+#endif
+
+
+STATIC word GC_parse_mem_size_arg(const char *str)
+{
+  char *endptr;
+  word result = 0; /* bad value */
+  char ch;
+
+  if (*str != '\0') {
+    result = (word)STRTOULL(str, &endptr, 10);
+    ch = *endptr;
+    if (ch != '\0') {
+      if (*(endptr + 1) != '\0')
+        return 0;
+      /* Allow k, M or G suffix. */
+      switch (ch) {
+      case 'K':
+      case 'k':
+        result <<= 10;
+        break;
+      case 'M':
+      case 'm':
+        result <<= 20;
+        break;
+      case 'G':
+      case 'g':
+        result <<= 30;
+        break;
+      default:
+        result = 0;
+      }
+    }
+  }
+  return result;
+}
+
+GC_API void GC_CALL GC_init(void)
+{
+    /* LOCK(); -- no longer does anything this early. */
+    word initial_heap_sz;
+    IF_CANCEL(int cancel_state;)
+
+    if (GC_is_initialized) return;
+#   ifdef REDIRECT_MALLOC
+      {
+        static GC_bool init_started = FALSE;
+        if (init_started)
+          ABORT("Redirected malloc() called during GC init");
+        init_started = TRUE;
+      }
+#   endif
+
+#   ifdef GC_INITIAL_HEAP_SIZE
+      initial_heap_sz = divHBLKSZ(GC_INITIAL_HEAP_SIZE);
+#   else
+      initial_heap_sz = (word)MINHINCR;
+#   endif
+    DISABLE_CANCEL(cancel_state);
+    /* Note that although we are nominally called with the */
+    /* allocation lock held, the allocation lock is now    */
+    /* only really acquired once a second thread is forked.*/
+    /* And the initialization code needs to run before     */
+    /* then.  Thus we really don't hold any locks, and can */
+    /* in fact safely initialize them here.                */
+#   ifdef THREADS
+      GC_ASSERT(!GC_need_to_lock);
+#     ifdef SN_TARGET_PS3
+        {
+          pthread_mutexattr_t mattr;
+          pthread_mutexattr_init(&mattr);
+          pthread_mutex_init(&GC_allocate_ml, &mattr);
+          pthread_mutexattr_destroy(&mattr);
+        }
+#     endif
+#   endif /* THREADS */
+#   if defined(GC_WIN32_THREADS) && !defined(GC_PTHREADS)
+     {
+#     ifndef MSWINCE
+        BOOL (WINAPI *pfn) (LPCRITICAL_SECTION, DWORD) = NULL;
+        HMODULE hK32 = GetModuleHandle(TEXT("kernel32.dll"));
+        if (hK32)
+          pfn = (BOOL (WINAPI *) (LPCRITICAL_SECTION, DWORD))
+                GetProcAddress (hK32,
+                                "InitializeCriticalSectionAndSpinCount");
+        if (pfn)
+            pfn(&GC_allocate_ml, 4000);
+        else
+#     endif /* !MSWINCE */
+        /* else */ InitializeCriticalSection (&GC_allocate_ml);
+     }
+#   endif /* GC_WIN32_THREADS */
+#   if (defined(MSWIN32) || defined(MSWINCE)) && defined(THREADS)
+      InitializeCriticalSection(&GC_write_cs);
+#   endif
+    GC_setpagesize();
+#   ifdef MSWIN32
+      GC_init_win32();
+#   endif
+#   ifdef GC_READ_ENV_FILE
+      GC_envfile_init();
+#   endif
+#   ifndef SMALL_CONFIG
+#     ifdef GC_PRINT_VERBOSE_STATS
+        /* This is useful for debugging and profiling on platforms with */
+        /* missing getenv() (like WinCE).                               */
+        GC_print_stats = VERBOSE;
+#     else
+        if (0 != GETENV("GC_PRINT_VERBOSE_STATS")) {
+          GC_print_stats = VERBOSE;
+        } else if (0 != GETENV("GC_PRINT_STATS")) {
+          GC_print_stats = 1;
+        }
+#     endif
+#     if defined(UNIX_LIKE) || defined(CYGWIN32)
+        {
+          char * file_name = GETENV("GC_LOG_FILE");
+          if (0 != file_name) {
+            int log_d = open(file_name, O_CREAT|O_WRONLY|O_APPEND, 0666);
+            if (log_d < 0) {
+              GC_err_printf("Failed to open %s as log file\n", file_name);
+            } else {
+              char *str;
+              GC_log = log_d;
+              str = GETENV("GC_ONLY_LOG_TO_FILE");
+#             ifdef GC_ONLY_LOG_TO_FILE
+                /* The similar environment variable set to "0"  */
+                /* overrides the effect of the macro defined.   */
+                if (str != NULL && *str == '0' && *(str + 1) == '\0')
+#             else
+                /* Otherwise setting the environment variable   */
+                /* to anything other than "0" will prevent from */
+                /* redirecting stdout/err to the log file.      */
+                if (str == NULL || (*str == '0' && *(str + 1) == '\0'))
+#             endif
+              {
+                GC_stdout = log_d;
+                GC_stderr = log_d;
+              }
+            }
+          }
+        }
+#     endif
+#   endif /* !SMALL_CONFIG */
+#   ifndef NO_DEBUGGING
+      if (0 != GETENV("GC_DUMP_REGULARLY")) {
+        GC_dump_regularly = TRUE;
+      }
+#   endif
+#   ifdef KEEP_BACK_PTRS
+      {
+        char * backtraces_string = GETENV("GC_BACKTRACES");
+        if (0 != backtraces_string) {
+          GC_backtraces = atol(backtraces_string);
+          if (backtraces_string[0] == '\0') GC_backtraces = 1;
+        }
+      }
+#   endif
+    if (0 != GETENV("GC_FIND_LEAK")) {
+      GC_find_leak = 1;
+    }
+#   ifndef SHORT_DBG_HDRS
+      if (0 != GETENV("GC_FINDLEAK_DELAY_FREE")) {
+        GC_findleak_delay_free = TRUE;
+      }
+#   endif
+    if (0 != GETENV("GC_ALL_INTERIOR_POINTERS")) {
+      GC_all_interior_pointers = 1;
+    }
+    if (0 != GETENV("GC_DONT_GC")) {
+      GC_dont_gc = 1;
+    }
+    if (0 != GETENV("GC_PRINT_BACK_HEIGHT")) {
+      GC_print_back_height = TRUE;
+    }
+    if (0 != GETENV("GC_NO_BLACKLIST_WARNING")) {
+      GC_large_alloc_warn_interval = LONG_MAX;
+    }
+    {
+      char * addr_string = GETENV("GC_TRACE");
+      if (0 != addr_string) {
+#       ifndef ENABLE_TRACE
+          WARN("Tracing not enabled: Ignoring GC_TRACE value\n", 0);
+#       else
+          word addr = (word)STRTOULL(addr_string, NULL, 16);
+          if (addr < 0x1000)
+              WARN("Unlikely trace address: %p\n", addr);
+          GC_trace_addr = (ptr_t)addr;
+#       endif
+      }
+    }
+#   ifndef GC_DISABLE_INCREMENTAL
+      {
+        char * time_limit_string = GETENV("GC_PAUSE_TIME_TARGET");
+        if (0 != time_limit_string) {
+          long time_limit = atol(time_limit_string);
+          if (time_limit < 5) {
+            WARN("GC_PAUSE_TIME_TARGET environment variable value too small "
+                 "or bad syntax: Ignoring\n", 0);
+          } else {
+            GC_time_limit = time_limit;
+          }
+        }
+      }
+#   endif
+#   ifndef SMALL_CONFIG
+      {
+        char * full_freq_string = GETENV("GC_FULL_FREQUENCY");
+        if (full_freq_string != NULL) {
+          int full_freq = atoi(full_freq_string);
+          if (full_freq > 0)
+            GC_full_freq = full_freq;
+        }
+      }
+#   endif
+    {
+      char * interval_string = GETENV("GC_LARGE_ALLOC_WARN_INTERVAL");
+      if (0 != interval_string) {
+        long interval = atol(interval_string);
+        if (interval <= 0) {
+          WARN("GC_LARGE_ALLOC_WARN_INTERVAL environment variable has "
+               "bad value: Ignoring\n", 0);
+        } else {
+          GC_large_alloc_warn_interval = interval;
+        }
+      }
+    }
+    {
+        char * space_divisor_string = GETENV("GC_FREE_SPACE_DIVISOR");
+        if (space_divisor_string != NULL) {
+          int space_divisor = atoi(space_divisor_string);
+          if (space_divisor > 0)
+            GC_free_space_divisor = (GC_word)space_divisor;
+        }
+    }
+#   ifdef USE_MUNMAP
+      {
+        char * string = GETENV("GC_UNMAP_THRESHOLD");
+        if (string != NULL) {
+          if (*string == '0' && *(string + 1) == '\0') {
+            /* "0" is used to disable unmapping. */
+            GC_unmap_threshold = 0;
+          } else {
+            int unmap_threshold = atoi(string);
+            if (unmap_threshold > 0)
+              GC_unmap_threshold = unmap_threshold;
+          }
+        }
+      }
+      {
+        char * string = GETENV("GC_FORCE_UNMAP_ON_GCOLLECT");
+        if (string != NULL) {
+          if (*string == '0' && *(string + 1) == '\0') {
+            /* "0" is used to turn off the mode. */
+            GC_force_unmap_on_gcollect = FALSE;
+          } else {
+            GC_force_unmap_on_gcollect = TRUE;
+          }
+        }
+      }
+      {
+        char * string = GETENV("GC_USE_ENTIRE_HEAP");
+        if (string != NULL) {
+          if (*string == '0' && *(string + 1) == '\0') {
+            /* "0" is used to turn off the mode. */
+            GC_use_entire_heap = FALSE;
+          } else {
+            GC_use_entire_heap = TRUE;
+          }
+        }
+      }
+#   endif
+    maybe_install_looping_handler();
+    /* Adjust normal object descriptor for extra allocation.    */
+    if (ALIGNMENT > GC_DS_TAGS && EXTRA_BYTES != 0) {
+      GC_obj_kinds[NORMAL].ok_descriptor = ((word)(-ALIGNMENT) | GC_DS_LENGTH);
+    }
+    GC_exclude_static_roots_inner(beginGC_arrays, endGC_arrays);
+    GC_exclude_static_roots_inner(beginGC_obj_kinds, endGC_obj_kinds);
+#   ifdef SEPARATE_GLOBALS
+      GC_exclude_static_roots_inner(beginGC_objfreelist, endGC_objfreelist);
+      GC_exclude_static_roots_inner(beginGC_aobjfreelist, endGC_aobjfreelist);
+#   endif
+#   if defined(USE_PROC_FOR_LIBRARIES) && defined(GC_LINUX_THREADS)
+        WARN("USE_PROC_FOR_LIBRARIES + GC_LINUX_THREADS performs poorly.\n", 0);
+        /* If thread stacks are cached, they tend to be scanned in      */
+        /* entirety as part of the root set.  This wil grow them to     */
+        /* maximum size, and is generally not desirable.                */
+#   endif
+#   if defined(SEARCH_FOR_DATA_START)
+        GC_init_linux_data_start();
+#   endif
+#   if defined(NETBSD) && defined(__ELF__)
+        GC_init_netbsd_elf();
+#   endif
+#   if !defined(THREADS) || defined(GC_PTHREADS) \
+        || defined(GC_WIN32_THREADS) || defined(GC_SOLARIS_THREADS)
+      if (GC_stackbottom == 0) {
+        GC_stackbottom = GC_get_main_stack_base();
+#       if (defined(LINUX) || defined(HPUX)) && defined(IA64)
+          GC_register_stackbottom = GC_get_register_stack_base();
+#       endif
+      } else {
+#       if (defined(LINUX) || defined(HPUX)) && defined(IA64)
+          if (GC_register_stackbottom == 0) {
+            WARN("GC_register_stackbottom should be set with GC_stackbottom\n", 0);
+            /* The following may fail, since we may rely on             */
+            /* alignment properties that may not hold with a user set   */
+            /* GC_stackbottom.                                          */
+            GC_register_stackbottom = GC_get_register_stack_base();
+          }
+#       endif
+      }
+#   endif
+    GC_STATIC_ASSERT(sizeof (ptr_t) == sizeof(word));
+    GC_STATIC_ASSERT(sizeof (signed_word) == sizeof(word));
+    GC_STATIC_ASSERT(sizeof (struct hblk) == HBLKSIZE);
+#   ifndef THREADS
+      GC_ASSERT(!((word)GC_stackbottom HOTTER_THAN (word)GC_approx_sp()));
+#   endif
+#   if !defined(_AUX_SOURCE) || defined(__GNUC__)
+      GC_STATIC_ASSERT((word)(-1) > (word)0);
+      /* word should be unsigned */
+#   endif
+#   if !defined(__BORLANDC__) && !defined(__CC_ARM) \
+       && !(defined(__clang__) && defined(X86_64)) /* Workaround */
+      GC_STATIC_ASSERT((ptr_t)(word)(-1) > (ptr_t)0);
+      /* Ptr_t comparisons should behave as unsigned comparisons.       */
+#   endif
+    GC_STATIC_ASSERT((signed_word)(-1) < (signed_word)0);
+#   ifndef GC_DISABLE_INCREMENTAL
+      if (GC_incremental || 0 != GETENV("GC_ENABLE_INCREMENTAL")) {
+        /* For GWW_VDB on Win32, this needs to happen before any        */
+        /* heap memory is allocated.                                    */
+        GC_dirty_init();
+        GC_ASSERT(GC_bytes_allocd == 0);
+        GC_incremental = TRUE;
+      }
+#   endif
+
+    /* Add initial guess of root sets.  Do this first, since sbrk(0)    */
+    /* might be used.                                                   */
+      if (GC_REGISTER_MAIN_STATIC_DATA()) GC_register_data_segments();
+    GC_init_headers();
+    GC_bl_init();
+    GC_mark_init();
+    {
+        char * sz_str = GETENV("GC_INITIAL_HEAP_SIZE");
+        if (sz_str != NULL) {
+          initial_heap_sz = GC_parse_mem_size_arg(sz_str);
+          if (initial_heap_sz <= MINHINCR * HBLKSIZE) {
+            WARN("Bad initial heap size %s - ignoring it.\n", sz_str);
+          }
+          initial_heap_sz = divHBLKSZ(initial_heap_sz);
+        }
+    }
+    {
+        char * sz_str = GETENV("GC_MAXIMUM_HEAP_SIZE");
+        if (sz_str != NULL) {
+          word max_heap_sz = GC_parse_mem_size_arg(sz_str);
+          if (max_heap_sz < initial_heap_sz * HBLKSIZE) {
+            WARN("Bad maximum heap size %s - ignoring it.\n", sz_str);
+          }
+          if (0 == GC_max_retries) GC_max_retries = 2;
+          GC_set_max_heap_size(max_heap_sz);
+        }
+    }
+    if (!GC_expand_hp_inner(initial_heap_sz)) {
+        GC_err_printf("Can't start up: not enough memory\n");
+        EXIT();
+    }
+    if (GC_all_interior_pointers)
+      GC_initialize_offsets();
+    GC_register_displacement_inner(0L);
+#   if defined(GC_LINUX_THREADS) && defined(REDIRECT_MALLOC)
+      if (!GC_all_interior_pointers) {
+        /* TLS ABI uses pointer-sized offsets for dtv. */
+        GC_register_displacement_inner(sizeof(void *));
+      }
+#   endif
+    GC_init_size_map();
+#   ifdef PCR
+      if (PCR_IL_Lock(PCR_Bool_false, PCR_allSigsBlocked, PCR_waitForever)
+          != PCR_ERes_okay) {
+          ABORT("Can't lock load state");
+      } else if (PCR_IL_Unlock() != PCR_ERes_okay) {
+          ABORT("Can't unlock load state");
+      }
+      PCR_IL_Unlock();
+      GC_pcr_install();
+#   endif
+    GC_is_initialized = TRUE;
+#   if defined(GC_PTHREADS) || defined(GC_WIN32_THREADS)
+        GC_thr_init();
+#   endif
+    COND_DUMP;
+    /* Get black list set up and/or incremental GC started */
+      if (!GC_dont_precollect || GC_incremental) GC_gcollect_inner();
+#   ifdef STUBBORN_ALLOC
+        GC_stubborn_init();
+#   endif
+    /* Convince lint that some things are used */
+#   ifdef LINT
+      {
+          extern char * const GC_copyright[];
+          GC_noop(GC_copyright, GC_find_header, GC_push_one,
+                  GC_call_with_alloc_lock, GC_dont_expand,
+#                 ifndef NO_DEBUGGING
+                    GC_dump,
+#                 endif
+                  GC_register_finalizer_no_order);
+      }
+#   endif
+
+    if (GC_find_leak) {
+      /* This is to give us at least one chance to detect leaks.        */
+      /* This may report some very benign leaks, but ...                */
+      #ifndef NAUT
+      atexit(GC_exit_check);
+      #endif
+    }
+
+    /* The rest of this again assumes we don't really hold      */
+    /* the allocation lock.                                     */
+#   if defined(PARALLEL_MARK) || defined(THREAD_LOCAL_ALLOC)
+        /* Make sure marker threads are started and thread local */
+        /* allocation is initialized, in case we didn't get      */
+        /* called from GC_init_parallel.                         */
+        GC_init_parallel();
+#   endif /* PARALLEL_MARK || THREAD_LOCAL_ALLOC */
+
+#   if defined(DYNAMIC_LOADING) && defined(DARWIN)
+        /* This must be called WITHOUT the allocation lock held */
+        /* and before any threads are created.                  */
+        GC_init_dyld();
+#   endif
+    RESTORE_CANCEL(cancel_state);
+}
+
+GC_API void GC_CALL GC_enable_incremental(void)
+{
+# if !defined(GC_DISABLE_INCREMENTAL) && !defined(KEEP_BACK_PTRS)
+    DCL_LOCK_STATE;
+    /* If we are keeping back pointers, the GC itself dirties all */
+    /* pages on which objects have been marked, making            */
+    /* incremental GC pointless.                                  */
+    if (!GC_find_leak && 0 == GETENV("GC_DISABLE_INCREMENTAL")) {
+      LOCK();
+      if (!GC_incremental) {
+        GC_setpagesize();
+        /* if (GC_no_win32_dlls) goto out; Should be win32S test? */
+        maybe_install_looping_handler(); /* Before write fault handler! */
+        GC_incremental = TRUE;
+        if (!GC_is_initialized) {
+          GC_init();
+        } else {
+          GC_dirty_init();
+        }
+        if (GC_dirty_maintained && !GC_dont_gc) {
+                                /* Can't easily do it if GC_dont_gc.    */
+          if (GC_bytes_allocd > 0) {
+            /* There may be unmarked reachable objects. */
+            GC_gcollect_inner();
+          }
+            /* else we're OK in assuming everything's   */
+            /* clean since nothing can point to an      */
+            /* unmarked object.                         */
+          GC_read_dirty();
+        }
+      }
+      UNLOCK();
+      return;
+    }
+# endif
+  GC_init();
+}
+
+#if defined(MSWIN32) || defined(MSWINCE)
+
+# if defined(_MSC_VER) && defined(_DEBUG) && !defined(MSWINCE)
+#   include <crtdbg.h>
+# endif
+
+  STATIC HANDLE GC_log = 0;
+
+  void GC_deinit(void)
+  {
+#   ifdef THREADS
+      if (GC_is_initialized) {
+        DeleteCriticalSection(&GC_write_cs);
+      }
+#   endif
+  }
+
+# ifdef THREADS
+#   ifdef PARALLEL_MARK
+#     define IF_NEED_TO_LOCK(x) if (GC_parallel || GC_need_to_lock) x
+#   else
+#     define IF_NEED_TO_LOCK(x) if (GC_need_to_lock) x
+#   endif
+# else
+#   define IF_NEED_TO_LOCK(x)
+# endif /* !THREADS */
+
+  STATIC HANDLE GC_CreateLogFile(void)
+  {
+#   if !defined(NO_GETENV_WIN32) || !defined(OLD_WIN32_LOG_FILE)
+      TCHAR logPath[_MAX_PATH + 0x10]; /* buffer for path + ext */
+#   endif
+    /* Use GetEnvironmentVariable instead of GETENV() for unicode support. */
+#   ifndef NO_GETENV_WIN32
+      if (GetEnvironmentVariable(TEXT("GC_LOG_FILE"), logPath,
+                                 _MAX_PATH + 1) - 1U >= (DWORD)_MAX_PATH)
+#   endif
+    {
+      /* Env var not found or its value too long.       */
+#     ifdef OLD_WIN32_LOG_FILE
+        return CreateFile(TEXT("gc.log"), GENERIC_WRITE, FILE_SHARE_READ,
+                          NULL /* lpSecurityAttributes */, CREATE_ALWAYS,
+                          FILE_FLAG_WRITE_THROUGH, NULL /* hTemplateFile */);
+#     else
+        int len = (int)GetModuleFileName(NULL /* hModule */, logPath,
+                                         _MAX_PATH + 1);
+        /* If GetModuleFileName() has failed then len is 0. */
+        if (len > 4 && logPath[len - 4] == (TCHAR)'.') {
+          len -= 4; /* strip executable file extension */
+        }
+        /* strcat/wcscat() are deprecated on WinCE, so use memcpy()     */
+        memcpy(&logPath[len], TEXT(".gc.log"), sizeof(TEXT(".gc.log")));
+#     endif
+    }
+#   if !defined(NO_GETENV_WIN32) || !defined(OLD_WIN32_LOG_FILE)
+      return CreateFile(logPath, GENERIC_WRITE, FILE_SHARE_READ,
+                        NULL /* lpSecurityAttributes */, CREATE_ALWAYS,
+                        GC_print_stats == VERBOSE ? FILE_ATTRIBUTE_NORMAL :
+                            /* immediately flush writes unless very verbose */
+                            FILE_ATTRIBUTE_NORMAL | FILE_FLAG_WRITE_THROUGH,
+                        NULL /* hTemplateFile */);
+#   endif
+  }
+
+  STATIC int GC_write(const char *buf, size_t len)
+  {
+      BOOL tmp;
+      DWORD written;
+#     if (defined(THREADS) && defined(GC_ASSERTIONS)) \
+         || !defined(GC_PRINT_VERBOSE_STATS)
+        static GC_bool inside_write = FALSE;
+                        /* to prevent infinite recursion at abort.      */
+        if (inside_write)
+          return -1;
+#     endif
+
+      if (len == 0)
+          return 0;
+      IF_NEED_TO_LOCK(EnterCriticalSection(&GC_write_cs));
+#     if defined(THREADS) && defined(GC_ASSERTIONS)
+        if (GC_write_disabled) {
+          inside_write = TRUE;
+          ABORT("Assertion failure: GC_write called with write_disabled");
+        }
+#     endif
+      if (GC_log == INVALID_HANDLE_VALUE) {
+          IF_NEED_TO_LOCK(LeaveCriticalSection(&GC_write_cs));
+          return -1;
+      } else if (GC_log == 0) {
+        GC_log = GC_CreateLogFile();
+        /* Ignore open log failure if the collector is built with       */
+        /* print_stats always set on.                                   */
+#       ifndef GC_PRINT_VERBOSE_STATS
+          if (GC_log == INVALID_HANDLE_VALUE) {
+            inside_write = TRUE;
+            ABORT("Open of log file failed");
+          }
+#       endif
+      }
+      tmp = WriteFile(GC_log, buf, (DWORD)len, &written, NULL);
+      if (!tmp)
+          DebugBreak();
+#     if defined(_MSC_VER) && defined(_DEBUG)
+#         ifdef MSWINCE
+              /* There is no CrtDbgReport() in WinCE */
+              {
+                  WCHAR wbuf[1024];
+                  /* Always use Unicode variant of OutputDebugString() */
+                  wbuf[MultiByteToWideChar(CP_ACP, 0 /* dwFlags */,
+                                buf, len, wbuf,
+                                sizeof(wbuf) / sizeof(wbuf[0]) - 1)] = 0;
+                  OutputDebugStringW(wbuf);
+              }
+#         else
+              _CrtDbgReport(_CRT_WARN, NULL, 0, NULL, "%.*s", len, buf);
+#         endif
+#     endif
+      IF_NEED_TO_LOCK(LeaveCriticalSection(&GC_write_cs));
+      return tmp ? (int)written : -1;
+  }
+
+  /* FIXME: This is pretty ugly ... */
+# define WRITE(f, buf, len) GC_write(buf, len)
+
+#elif defined(OS2) || defined(MACOS)
+  STATIC FILE * GC_stdout = NULL;
+  STATIC FILE * GC_stderr = NULL;
+  STATIC FILE * GC_log = NULL;
+
+  /* Initialize GC_log (and the friends) passed to GC_write().  */
+  STATIC void GC_set_files(void)
+  {
+    if (GC_stdout == NULL) {
+      GC_stdout = stdout;
+    }
+    if (GC_stderr == NULL) {
+      GC_stderr = stderr;
+    }
+    if (GC_log == NULL) {
+      GC_log = stderr;
+    }
+  }
+
+  GC_INLINE int GC_write(FILE *f, const char *buf, size_t len)
+  {
+    int res = fwrite(buf, 1, len, f);
+    fflush(f);
+    return res;
+  }
+
+# define WRITE(f, buf, len) (GC_set_files(), GC_write(f, buf, len))
+
+#else
+# if !defined(AMIGA) && !defined(__CC_ARM) && !defined(NAUT)
+#   include <unistd.h>
+# endif
+
+  STATIC int GC_write(int fd, const char *buf, size_t len)
+  {
+#   if defined(ECOS) || defined(NOSYS) || defined(NAUT)
+#     ifdef ECOS
+        /* FIXME: This seems to be defined nowhere at present.  */
+        /* _Jv_diag_write(buf, len); */
+#     else
+        /* No writing.  */
+#     endif
+      return len;
+#   else
+      int bytes_written = 0;
+      int result;
+      IF_CANCEL(int cancel_state;)
+
+      DISABLE_CANCEL(cancel_state);
+      while ((size_t)bytes_written < len) {
+#        ifdef GC_SOLARIS_THREADS
+             result = syscall(SYS_write, fd, buf + bytes_written,
+                                             len - bytes_written);
+#        else
+             result = write(fd, buf + bytes_written, len - bytes_written);
+#        endif
+         if (-1 == result) {
+             RESTORE_CANCEL(cancel_state);
+             return(result);
+         }
+         bytes_written += result;
+      }
+      RESTORE_CANCEL(cancel_state);
+      return(bytes_written);
+#   endif
+  }
+
+# define WRITE(f, buf, len) GC_write(f, buf, len)
+#endif /* !MSWIN32 && !OS2 && !MACOS */
+
+#define BUFSZ 1024
+
+#ifdef DJGPP
+  /* vsnprintf is missing in DJGPP (v2.0.3) */
+# define vsnprintf(buf, bufsz, format, args) vsprintf(buf, format, args)
+#elif defined(_MSC_VER)
+# ifdef MSWINCE
+    /* _vsnprintf is deprecated in WinCE */
+#   define vsnprintf StringCchVPrintfA
+# else
+#   define vsnprintf _vsnprintf
+# endif
+#endif
+
+#ifndef NAUT
+/* A version of printf that is unlikely to call malloc, and is thus safer */
+/* to call from the collector in case malloc has been bound to GC_malloc. */
+/* Floating point arguments and formats should be avoided, since fp       */
+/* conversion is more likely to allocate.                                 */
+/* Assumes that no more than BUFSZ-1 characters are written at once.      */
+void GC_printf(const char *format, ...)
+{
+    va_list args;
+    char buf[BUFSZ+1];
+
+    if (GC_quiet) return;
+    va_start(args, format);
+    buf[BUFSZ] = 0x15;
+    (void) vsnprintf(buf, BUFSZ, format, args);
+    va_end(args);
+    if (buf[BUFSZ] != 0x15) ABORT("GC_printf clobbered stack");
+    if (WRITE(GC_stdout, buf, strlen(buf)) < 0)
+      ABORT("write to stdout failed");
+}
+
+void GC_err_printf(const char *format, ...)
+{
+    va_list args;
+    char buf[BUFSZ+1];
+
+    va_start(args, format);
+    buf[BUFSZ] = 0x15;
+    (void) vsnprintf(buf, BUFSZ, format, args);
+    va_end(args);
+    if (buf[BUFSZ] != 0x15) ABORT("GC_printf clobbered stack");
+    if (WRITE(GC_stderr, buf, strlen(buf)) < 0)
+      ABORT("write to stderr failed");
+}
+
+void GC_log_printf(const char *format, ...)
+{
+    va_list args;
+    char buf[BUFSZ+1];
+
+    va_start(args, format);
+    buf[BUFSZ] = 0x15;
+    (void) vsnprintf(buf, BUFSZ, format, args);
+    va_end(args);
+    if (buf[BUFSZ] != 0x15) ABORT("GC_printf clobbered stack");
+    if (WRITE(GC_log, buf, strlen(buf)) < 0)
+      ABORT("write to log failed");
+}
+#endif
+
+
+/* This is equivalent to GC_err_printf("%s",s). */
+void GC_err_puts(const char *s)
+{
+    if (WRITE(GC_stderr, s, strlen(s)) < 0) ABORT("write to stderr failed");
+}
+
+STATIC void GC_CALLBACK GC_default_warn_proc(char *msg, GC_word arg)
+{
+    GC_err_printf(msg, arg);
+}
+
+GC_INNER GC_warn_proc GC_current_warn_proc = GC_default_warn_proc;
+
+/* This is recommended for production code (release). */
+GC_API void GC_CALLBACK GC_ignore_warn_proc(char *msg, GC_word arg)
+{
+    if (GC_print_stats) {
+      /* Don't ignore warnings if stats printing is on. */
+      GC_default_warn_proc(msg, arg);
+    }
+}
+
+GC_API void GC_CALL GC_set_warn_proc(GC_warn_proc p)
+{
+    DCL_LOCK_STATE;
+    GC_ASSERT(p != 0);
+#   ifdef GC_WIN32_THREADS
+#     ifdef CYGWIN32
+        /* Need explicit GC_INIT call */
+        GC_ASSERT(GC_is_initialized);
+#     else
+        if (!GC_is_initialized) GC_init();
+#     endif
+#   endif
+    LOCK();
+    GC_current_warn_proc = p;
+    UNLOCK();
+}
+
+GC_API GC_warn_proc GC_CALL GC_get_warn_proc(void)
+{
+    GC_warn_proc result;
+    DCL_LOCK_STATE;
+    LOCK();
+    result = GC_current_warn_proc;
+    UNLOCK();
+    return(result);
+}
+
+#if !defined(PCR) && !defined(SMALL_CONFIG)
+  /* Abort the program with a message. msg must not be NULL. */
+  void GC_abort(const char *msg)
+  {
+    BDWGC_DEBUG("WARNING: CALLING ABORT\n");
+#   if defined(MSWIN32)
+#     ifndef DONT_USE_USER32_DLL
+        /* Use static binding to "user32.dll".  */
+        (void)MessageBoxA(NULL, msg, "Fatal error in GC", MB_ICONERROR|MB_OK);
+#     else
+        /* This simplifies linking - resolve "MessageBoxA" at run-time. */
+        HINSTANCE hU32 = LoadLibrary(TEXT("user32.dll"));
+        if (hU32) {
+          FARPROC pfn = GetProcAddress(hU32, "MessageBoxA");
+          if (pfn)
+            (void)(*(int (WINAPI *)(HWND, LPCSTR, LPCSTR, UINT))pfn)(
+                                NULL /* hWnd */, msg, "Fatal error in GC",
+                                MB_ICONERROR | MB_OK);
+          (void)FreeLibrary(hU32);
+        }
+#     endif
+      /* Also duplicate msg to GC log file.     */
+#   endif
+      /* Avoid calling GC_err_printf() here, as GC_abort() could be     */
+      /* called from it.  Note 1: this is not an atomic output.         */
+      /* Note 2: possible write errors are ignored.                     */
+
+#     if defined(THREADS) && defined(GC_ASSERTIONS) \
+         && (defined(MSWIN32) || defined(MSWINCE))
+        if (!GC_write_disabled)
+#     endif
+      {
+        if (WRITE(GC_stderr, (void *)msg, strlen(msg)) >= 0)
+          (void)WRITE(GC_stderr, (void *)("\n"), 1);
+      }
+
+    if (GETENV("GC_LOOP_ON_ABORT") != NULL) {
+            /* In many cases it's easier to debug a running process.    */
+            /* It's arguably nicer to sleep, but that makes it harder   */
+            /* to look at the thread if the debugger doesn't know much  */
+            /* about threads.                                           */
+            for(;;) {}
+    }
+#   ifndef LINT2
+      if (!msg) return; /* to suppress compiler warnings in ABORT callers. */
+#   endif
+#   if defined(MSWIN32) && (defined(NO_DEBUGGING) || defined(LINT2))
+      /* A more user-friendly abort after showing fatal message.        */
+        _exit(-1); /* exit on error without running "at-exit" callbacks */
+#   elif defined(MSWINCE) && defined(NO_DEBUGGING)
+        ExitProcess(-1);
+#   elif defined(MSWIN32) || defined(MSWINCE)
+        DebugBreak();
+                /* Note that on a WinCE box, this could be silently     */
+                /* ignored (i.e., the program is not aborted).          */
+#   else
+        (void) abort();
+#   endif
+  }
+#endif /* !SMALL_CONFIG */
+
+GC_API void GC_CALL GC_enable(void)
+{
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_dont_gc--;
+    UNLOCK();
+}
+
+GC_API void GC_CALL GC_disable(void)
+{
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_dont_gc++;
+    UNLOCK();
+}
+
+GC_API int GC_CALL GC_is_disabled(void)
+{
+    return GC_dont_gc != 0;
+}
+
+/* Helper procedures for new kind creation.     */
+GC_API void ** GC_CALL GC_new_free_list_inner(void)
+{
+    void *result = GC_INTERNAL_MALLOC((MAXOBJGRANULES+1)*sizeof(ptr_t),
+                                      PTRFREE);
+    if (result == 0) ABORT("Failed to allocate freelist for new kind");
+    BZERO(result, (MAXOBJGRANULES+1)*sizeof(ptr_t));
+    return result;
+}
+
+GC_API void ** GC_CALL GC_new_free_list(void)
+{
+    void *result;
+    DCL_LOCK_STATE;
+    LOCK();
+    result = GC_new_free_list_inner();
+    UNLOCK();
+    return result;
+}
+
+GC_API unsigned GC_CALL GC_new_kind_inner(void **fl, GC_word descr,
+                                        int adjust, int clear)
+{
+    unsigned result = GC_n_kinds++;
+
+    if (GC_n_kinds > MAXOBJKINDS) ABORT("Too many kinds");
+    GC_obj_kinds[result].ok_freelist = fl;
+    GC_obj_kinds[result].ok_reclaim_list = 0;
+    GC_obj_kinds[result].ok_descriptor = descr;
+    GC_obj_kinds[result].ok_relocate_descr = adjust;
+    GC_obj_kinds[result].ok_init = clear;
+    return result;
+}
+
+GC_API unsigned GC_CALL GC_new_kind(void **fl, GC_word descr, int adjust,
+                                    int clear)
+{
+    unsigned result;
+    DCL_LOCK_STATE;
+    LOCK();
+    result = GC_new_kind_inner(fl, descr, adjust, clear);
+    UNLOCK();
+    return result;
+}
+
+GC_API unsigned GC_CALL GC_new_proc_inner(GC_mark_proc proc)
+{
+    unsigned result = GC_n_mark_procs++;
+
+    if (GC_n_mark_procs > MAX_MARK_PROCS) ABORT("Too many mark procedures");
+    GC_mark_procs[result] = proc;
+    return result;
+}
+
+GC_API unsigned GC_CALL GC_new_proc(GC_mark_proc proc)
+{
+    unsigned result;
+    DCL_LOCK_STATE;
+    LOCK();
+    result = GC_new_proc_inner(proc);
+    UNLOCK();
+    return result;
+}
+
+GC_API void * GC_CALL GC_call_with_stack_base(GC_stack_base_func fn, void *arg)
+{
+    struct GC_stack_base base;
+    void *result;
+
+    base.mem_base = (void *)&base;
+#   ifdef IA64
+      base.reg_base = (void *)GC_save_regs_in_stack();
+      /* Unnecessarily flushes register stack,          */
+      /* but that probably doesn't hurt.                */
+#   endif
+    result = fn(&base, arg);
+    /* Strongly discourage the compiler from treating the above */
+    /* as a tail call.                                          */
+    GC_noop1((word)(&base));
+    return result;
+}
+
+#ifndef THREADS
+
+GC_INNER ptr_t GC_blocked_sp = NULL;
+        /* NULL value means we are not inside GC_do_blocking() call. */
+# ifdef IA64
+    STATIC ptr_t GC_blocked_register_sp = NULL;
+# endif
+
+GC_INNER struct GC_traced_stack_sect_s *GC_traced_stack_sect = NULL;
+
+/* This is nearly the same as in win32_threads.c        */
+GC_API void * GC_CALL GC_call_with_gc_active(GC_fn_type fn,
+                                             void * client_data)
+{
+    struct GC_traced_stack_sect_s stacksect;
+    GC_ASSERT(GC_is_initialized);
+
+    /* Adjust our stack base value (this could happen if        */
+    /* GC_get_main_stack_base() is unimplemented or broken for  */
+    /* the platform).                                           */
+    if (GC_stackbottom HOTTER_THAN (ptr_t)(&stacksect))
+      GC_stackbottom = (ptr_t)(&stacksect);
+
+    if (GC_blocked_sp == NULL) {
+      /* We are not inside GC_do_blocking() - do nothing more.  */
+      client_data = fn(client_data);
+      /* Prevent treating the above as a tail call.     */
+      GC_noop1((word)(&stacksect));
+      return client_data; /* result */
+    }
+
+    /* Setup new "stack section".       */
+    stacksect.saved_stack_ptr = GC_blocked_sp;
+#   ifdef IA64
+      /* This is the same as in GC_call_with_stack_base().      */
+      stacksect.backing_store_end = GC_save_regs_in_stack();
+      /* Unnecessarily flushes register stack,          */
+      /* but that probably doesn't hurt.                */
+      stacksect.saved_backing_store_ptr = GC_blocked_register_sp;
+#   endif
+    stacksect.prev = GC_traced_stack_sect;
+    GC_blocked_sp = NULL;
+    GC_traced_stack_sect = &stacksect;
+
+    client_data = fn(client_data);
+    GC_ASSERT(GC_blocked_sp == NULL);
+    GC_ASSERT(GC_traced_stack_sect == &stacksect);
+
+    /* Restore original "stack section".        */
+    GC_traced_stack_sect = stacksect.prev;
+#   ifdef IA64
+      GC_blocked_register_sp = stacksect.saved_backing_store_ptr;
+#   endif
+    GC_blocked_sp = stacksect.saved_stack_ptr;
+
+    return client_data; /* result */
+}
+
+/* This is nearly the same as in win32_threads.c        */
+/*ARGSUSED*/
+STATIC void GC_do_blocking_inner(ptr_t data, void * context)
+{
+    struct blocking_data * d = (struct blocking_data *) data;
+    GC_ASSERT(GC_is_initialized);
+    GC_ASSERT(GC_blocked_sp == NULL);
+#   ifdef SPARC
+        GC_blocked_sp = GC_save_regs_in_stack();
+#   else
+        GC_blocked_sp = (ptr_t) &d; /* save approx. sp */
+#   endif
+#   ifdef IA64
+        GC_blocked_register_sp = GC_save_regs_in_stack();
+#   endif
+
+    d -> client_data = (d -> fn)(d -> client_data);
+
+#   ifdef SPARC
+        GC_ASSERT(GC_blocked_sp != NULL);
+#   else
+        GC_ASSERT(GC_blocked_sp == (ptr_t) &d);
+#   endif
+    GC_blocked_sp = NULL;
+}
+
+#endif /* !THREADS */
+
+/* Wrapper for functions that are likely to block (or, at least, do not */
+/* allocate garbage collected memory and/or manipulate pointers to the  */
+/* garbage collected heap) for an appreciable length of time.           */
+/* In the single threaded case, GC_do_blocking() (together              */
+/* with GC_call_with_gc_active()) might be used to make stack scanning  */
+/* more precise (i.e. scan only stack frames of functions that allocate */
+/* garbage collected memory and/or manipulate pointers to the garbage   */
+/* collected heap).                                                     */
+GC_API void * GC_CALL GC_do_blocking(GC_fn_type fn, void * client_data)
+{
+  BDWGC_DEBUG("GC_do_blocking\n");
+    struct blocking_data my_data;
+
+    my_data.fn = fn;
+    my_data.client_data = client_data;
+    BDWGC_DEBUG("GC_with_callee_saves_pushed\n");
+    GC_with_callee_saves_pushed(GC_do_blocking_inner, (ptr_t)(&my_data));
+    return my_data.client_data; /* result */
+}
+
+#if !defined(NO_DEBUGGING)
+  GC_API void GC_CALL GC_dump(void)
+  {
+    GC_printf("***Static roots:\n");
+    GC_print_static_roots();
+    GC_printf("\n***Heap sections:\n");
+    GC_print_heap_sects();
+    GC_printf("\n***Free blocks:\n");
+    GC_print_hblkfreelist();
+    GC_printf("\n***Blocks in use:\n");
+    GC_print_block_list();
+  }
+#endif /* !NO_DEBUGGING */
+
+/* Getter functions for the public Read-only variables.                 */
+
+/* GC_get_gc_no() is unsynchronized and should be typically called      */
+/* inside the context of GC_call_with_alloc_lock() to prevent data      */
+/* races (on multiprocessors).                                          */
+GC_API GC_word GC_CALL GC_get_gc_no(void)
+{
+    return GC_gc_no;
+}
+
+#ifdef THREADS
+  GC_API int GC_CALL GC_get_parallel(void)
+  {
+    /* GC_parallel is initialized at start-up.  */
+    return GC_parallel;
+  }
+#endif
+
+/* Setter and getter functions for the public R/W function variables.   */
+/* These functions are synchronized (like GC_set_warn_proc() and        */
+/* GC_get_warn_proc()).                                                 */
+
+GC_API void GC_CALL GC_set_oom_fn(GC_oom_func fn)
+{
+    GC_ASSERT(fn != 0);
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_oom_fn = fn;
+    UNLOCK();
+}
+
+GC_API GC_oom_func GC_CALL GC_get_oom_fn(void)
+{
+    GC_oom_func fn;
+    DCL_LOCK_STATE;
+    LOCK();
+    fn = GC_oom_fn;
+    UNLOCK();
+    return fn;
+}
+
+GC_API void GC_CALL GC_set_finalizer_notifier(GC_finalizer_notifier_proc fn)
+{
+    /* fn may be 0 (means no finalizer notifier). */
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_finalizer_notifier = fn;
+    UNLOCK();
+}
+
+GC_API GC_finalizer_notifier_proc GC_CALL GC_get_finalizer_notifier(void)
+{
+    GC_finalizer_notifier_proc fn;
+    DCL_LOCK_STATE;
+    LOCK();
+    fn = GC_finalizer_notifier;
+    UNLOCK();
+    return fn;
+}
+
+/* Setter and getter functions for the public numeric R/W variables.    */
+/* It is safe to call these functions even before GC_INIT().            */
+/* These functions are unsynchronized and should be typically called    */
+/* inside the context of GC_call_with_alloc_lock() (if called after     */
+/* GC_INIT()) to prevent data races (unless it is guaranteed the        */
+/* collector is not multi-threaded at that execution point).            */
+
+GC_API void GC_CALL GC_set_find_leak(int value)
+{
+    /* value is of boolean type. */
+    GC_find_leak = value;
+}
+
+GC_API int GC_CALL GC_get_find_leak(void)
+{
+    return GC_find_leak;
+}
+
+GC_API void GC_CALL GC_set_all_interior_pointers(int value)
+{
+    DCL_LOCK_STATE;
+
+    GC_all_interior_pointers = value ? 1 : 0;
+    if (GC_is_initialized) {
+      /* It is not recommended to change GC_all_interior_pointers value */
+      /* after GC is initialized but it seems GC could work correctly   */
+      /* even after switching the mode.                                 */
+      LOCK();
+      GC_initialize_offsets(); /* NOTE: this resets manual offsets as well */
+      if (!GC_all_interior_pointers)
+        GC_bl_init_no_interiors();
+      UNLOCK();
+    }
+}
+
+GC_API int GC_CALL GC_get_all_interior_pointers(void)
+{
+    return GC_all_interior_pointers;
+}
+
+GC_API void GC_CALL GC_set_finalize_on_demand(int value)
+{
+    GC_ASSERT(value != -1);
+    /* value is of boolean type. */
+    GC_finalize_on_demand = value;
+}
+
+GC_API int GC_CALL GC_get_finalize_on_demand(void)
+{
+    return GC_finalize_on_demand;
+}
+
+GC_API void GC_CALL GC_set_java_finalization(int value)
+{
+    GC_ASSERT(value != -1);
+    /* value is of boolean type. */
+    GC_java_finalization = value;
+}
+
+GC_API int GC_CALL GC_get_java_finalization(void)
+{
+    return GC_java_finalization;
+}
+
+GC_API void GC_CALL GC_set_dont_expand(int value)
+{
+    GC_ASSERT(value != -1);
+    /* value is of boolean type. */
+    GC_dont_expand = value;
+}
+
+GC_API int GC_CALL GC_get_dont_expand(void)
+{
+    return GC_dont_expand;
+}
+
+GC_API void GC_CALL GC_set_no_dls(int value)
+{
+    GC_ASSERT(value != -1);
+    /* value is of boolean type. */
+    GC_no_dls = value;
+}
+
+GC_API int GC_CALL GC_get_no_dls(void)
+{
+    return GC_no_dls;
+}
+
+GC_API void GC_CALL GC_set_non_gc_bytes(GC_word value)
+{
+    GC_non_gc_bytes = value;
+}
+
+GC_API GC_word GC_CALL GC_get_non_gc_bytes(void)
+{
+    return GC_non_gc_bytes;
+}
+
+GC_API void GC_CALL GC_set_free_space_divisor(GC_word value)
+{
+    GC_ASSERT(value > 0);
+    GC_free_space_divisor = value;
+}
+
+GC_API GC_word GC_CALL GC_get_free_space_divisor(void)
+{
+    return GC_free_space_divisor;
+}
+
+GC_API void GC_CALL GC_set_max_retries(GC_word value)
+{
+    GC_ASSERT(value != ~(GC_word)0);
+    GC_max_retries = value;
+}
+
+GC_API GC_word GC_CALL GC_get_max_retries(void)
+{
+    return GC_max_retries;
+}
+
+GC_API void GC_CALL GC_set_dont_precollect(int value)
+{
+    GC_ASSERT(value != -1);
+    /* value is of boolean type. */
+    GC_dont_precollect = value;
+}
+
+GC_API int GC_CALL GC_get_dont_precollect(void)
+{
+    return GC_dont_precollect;
+}
+
+GC_API void GC_CALL GC_set_full_freq(int value)
+{
+    GC_ASSERT(value >= 0);
+    GC_full_freq = value;
+}
+
+GC_API int GC_CALL GC_get_full_freq(void)
+{
+    return GC_full_freq;
+}
+
+GC_API void GC_CALL GC_set_time_limit(unsigned long value)
+{
+    GC_ASSERT(value != (unsigned long)-1L);
+    GC_time_limit = value;
+}
+
+GC_API unsigned long GC_CALL GC_get_time_limit(void)
+{
+    return GC_time_limit;
+}
+
+GC_API void GC_CALL GC_set_force_unmap_on_gcollect(int value)
+{
+    GC_force_unmap_on_gcollect = (GC_bool)value;
+}
+
+GC_API int GC_CALL GC_get_force_unmap_on_gcollect(void)
+{
+    return (int)GC_force_unmap_on_gcollect;
+}
diff --git a/src/gc/bdwgc/naut_stop_world.c b/src/gc/bdwgc/naut_stop_world.c
new file mode 100644
index 0000000..2077a84
--- /dev/null
+++ b/src/gc/bdwgc/naut_stop_world.c
@@ -0,0 +1,116 @@
+/* 
+ * This file is part of the Nautilus AeroKernel developed
+ * by the Hobbes and V3VEE Projects with funding from the 
+ * United States National  Science Foundation and the Department of Energy.  
+ *
+ * The V3VEE Project is a joint project between Northwestern University
+ * and the University of New Mexico.  The Hobbes Project is a collaboration
+ * led by Sandia National Laboratories that includes several national 
+ * laboratories and universities. You can find out more at:
+ * http://www.v3vee.org  and
+ * http://xstack.sandia.gov/hobbes
+ *
+ * Copyright (c) 2017, Matt George <11georgem@gmail.com>
+ * Copyright (c) 2017, The V3VEE Project  <http://www.v3vee.org> 
+ *                     The Hobbes Project <http://xstack.sandia.gov/hobbes>
+ * All rights reserved.
+ *
+ * Authors:  Matt George <11georgem@gmail.com>
+ *
+ * This is free software.  You are permitted to use,
+ * redistribute, and modify it as specified in the file "LICENSE.txt".
+ */
+
+// This is a port of the Boehm garbage collector to
+// the Nautilus kernel, based on code copyrighted as follows
+
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2009 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+
+#include <nautilus/scheduler.h>
+#include "private/pthread_support.h"
+
+
+#ifdef NAUT_THREADS
+
+
+GC_INNER void GC_stop_world(void)
+{
+  BDWGC_DEBUG("Stopping the world from thread %p tid %lu\n", get_cur_thread(),get_cur_thread()->tid);
+  // kick everyone else out into an interrupt handler here... 
+  cli();
+}
+
+
+/* Caller holds allocation lock, and has held it continuously since     */
+/* the world stopped.                                                   */
+GC_INNER void GC_start_world(void)
+{
+    BDWGC_DEBUG("Starting the world from %p (tid %lu)\n", get_cur_thread(),get_cur_thread()->tid);
+  sti();
+}
+
+
+static void push_thread_stack(nk_thread_t *t, void *state)
+{
+  ptr_t lo, hi;
+  struct GC_traced_stack_sect_s *traced_stack_sect;
+  word total_size = 0;
+
+  //if (!GC_thr_initialized) GC_thr_init();
+  
+  traced_stack_sect = BDWGC_SPECIFIC_THREAD_STATE(t) -> traced_stack_sect;
+
+  lo = t -> stack;
+  hi = BDWGC_SPECIFIC_STACK_BOTTOM(t);
+  
+  if (traced_stack_sect != NULL
+      && traced_stack_sect->saved_stack_ptr == lo) {
+    /* If the thread has never been stopped since the recent  */
+    /* GC_call_with_gc_active invocation then skip the top    */
+    /* "stack section" as stack_ptr already points to.        */
+    //    traced_stack_sect = traced_stack_sect->prev;
+  }
+  
+  BDWGC_DEBUG("Pushing stack for thread (%p, tid=%u), range = [%p,%p)\n", t, t->tid, lo, hi);
+      
+  if (0 == lo) panic("GC_push_all_stacks: sp not set!");
+  //GC_push_all_stack_sections(lo, hi, traced_stack_sect);
+  GC_push_all_stack_sections(lo, hi, NULL);
+  total_size += hi - lo; /* lo <= hi */
+      
+}
+
+/* We hold allocation lock.  Should do exactly the right thing if the   */
+/* world is stopped.  Should not fail if it isn't.                      */
+GC_INNER void GC_push_all_stacks(void)
+{
+  BDWGC_DEBUG("Pushing stacks from thread %p\n", get_cur_thread());
+  nk_sched_map_threads(-1,push_thread_stack,0);
+}
+
+
+GC_INNER void GC_stop_init(void)
+{
+  BDWGC_DEBUG("GC_stop_init ... does nothing..\n");
+}
+
+#endif // NAUT_THREADS
+
+
+
diff --git a/src/gc/bdwgc/naut_threads.c b/src/gc/bdwgc/naut_threads.c
new file mode 100644
index 0000000..28c985c
--- /dev/null
+++ b/src/gc/bdwgc/naut_threads.c
@@ -0,0 +1,361 @@
+/* 
+ * This file is part of the Nautilus AeroKernel developed
+ * by the Hobbes and V3VEE Projects with funding from the 
+ * United States National  Science Foundation and the Department of Energy.  
+ *
+ * The V3VEE Project is a joint project between Northwestern University
+ * and the University of New Mexico.  The Hobbes Project is a collaboration
+ * led by Sandia National Laboratories that includes several national 
+ * laboratories and universities. You can find out more at:
+ * http://www.v3vee.org  and
+ * http://xstack.sandia.gov/hobbes
+ *
+ * Copyright (c) 2017, Matt George <11georgem@gmail.com>
+ * Copyright (c) 2017, The V3VEE Project  <http://www.v3vee.org> 
+ *                     The Hobbes Project <http://xstack.sandia.gov/hobbes>
+ * All rights reserved.
+ *
+ * Authors:  Matt George <11georgem@gmail.com>
+ *
+ * This is free software.  You are permitted to use,
+ * redistribute, and modify it as specified in the file "LICENSE.txt".
+ */
+
+// This is a port of the Boehm garbage collector to
+// the Nautilus kernel base on code copyrighted as follows
+
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2005 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+struct nk_thread;
+struct nk_thread_constraints;
+#include <nautilus/thread.h>
+#include <nautilus/scheduler.h>
+#include <gc/bdwgc/bdwgc.h>
+#include "bdwgc_internal.h"
+#include "private/pthread_support.h"
+
+
+# include "gc_inline.h"
+
+STATIC long GC_nprocs = 1;
+GC_INNER GC_bool GC_need_to_lock = FALSE;
+GC_INNER GC_bool GC_in_thread_creation = FALSE;
+GC_INNER unsigned long GC_lock_holder = NO_THREAD;
+GC_INNER GC_bool GC_thr_initialized = FALSE;
+GC_INNER NK_LOCK_T GC_allocate_ml; //PTHREAD_MUTEX_INITIALIZER;
+
+
+/* Return the number of processors. */
+STATIC int GC_get_nprocs(void)
+{
+  BDWGC_DEBUG("(WARNING: UNIMPLEMENTED!) GC_get_npos: Returning number of proccesors as %d\n", 2);
+  return 2;
+}
+
+
+
+/* Called by GC_finalize() (in case of an allocation failure observed). */
+GC_INNER void GC_reset_finalizer_nested(void)
+{
+  bdwgc_thread_state* me = BDWGC_THREAD_STATE();
+  me->finalizer_nested = 0;
+}
+
+
+/* Checks and updates the thread-local level of finalizers recursion.   */
+/* Returns NULL if GC_invoke_finalizers() should not be called by the   */
+/* collector (to minimize the risk of a deep finalizers recursion),     */
+/* otherwise returns a pointer to the thread-local finalizer_nested.    */
+/* Called by GC_notify_or_invoke_finalizers() only (the lock is held).  */
+GC_INNER unsigned char *GC_check_finalizer_nested(void)
+{
+  bdwgc_thread_state* me = BDWGC_THREAD_STATE(); 
+  unsigned nesting_level = me->finalizer_nested;
+  if (nesting_level) {
+    /* We are inside another GC_invoke_finalizers().            */
+    /* Skip some implicitly-called GC_invoke_finalizers()       */
+    /* depending on the nesting (recursion) level.              */
+    if (++me->finalizer_skipped < (1U << nesting_level)) return NULL;
+    me->finalizer_skipped = 0;
+  }
+  me->finalizer_nested = (unsigned char)(nesting_level + 1);
+  return &me->finalizer_nested;
+}
+
+
+void GC_push_thread_structures(void)
+{
+  //  GC_push_all((ptr_t)(get_cur_thread()),
+  //(ptr_t)(get_cur_thread()) + sizeof(nk_thread_t));
+
+  //BDWGC_DEBUG("Pushing local thread structures between %p and %p\n",
+  //get_cur_thread(), get_cur_thread() + sizeof(nk_thread_t));
+}
+
+
+
+static void mark_thread_local_fls(nk_thread_t *t, void *state)
+{
+  if (t->gc_state == NULL) {
+    BDWGC_DEBUG("Initializing gc_state for %p (tid %d)\n", t, t->tid);
+    t->gc_state = nk_gc_bdwgc_thread_state_init(t);
+  }
+  
+  BDWGC_DEBUG("Marking tlfs for  %p (tid %d) with gc_state %p \n", t, t->tid, t->gc_state);
+  GC_mark_thread_local_fls_for(&(BDWGC_SPECIFIC_THREAD_STATE(t) -> tlfs));
+}
+
+
+
+/* We must explicitly mark ptrfree and gcj free lists, since the free */
+/* list links wouldn't otherwise be found.  We also set them in the   */
+/* normal free lists, since that involves touching less memory than   */
+/* if we scanned them normally.                                       */
+GC_INNER void GC_mark_thread_local_free_lists(void)
+{
+    BDWGC_DEBUG("Marking all thread local free lists from %p (tid %lu)\n", get_cur_thread(),get_cur_thread()->tid);
+    nk_sched_map_threads(-1, mark_thread_local_fls,0);
+}
+
+
+/* Perform all initializations, including those that    */
+/* may require allocation.                              */
+/* Called without allocation lock.                      */
+/* Must be called before a second thread is created.    */
+/* Did we say it's called without the allocation lock?  */
+static GC_bool parallel_initialized = FALSE;
+
+GC_INNER void GC_init_parallel(void)
+{
+  BDWGC_DEBUG("GC_init_parallel\n");
+  
+  DCL_LOCK_STATE;
+  
+  if (parallel_initialized) return;
+  parallel_initialized = TRUE;
+
+  /* GC_init() calls us back, so set flag first.      */
+  if (!GC_is_initialized) GC_init();
+  
+  /* Initialize thread local free lists if used.      */
+  LOCK();
+  GC_init_thread_local(&(BDWGC_THREAD_STATE()->tlfs));
+  UNLOCK();
+}
+
+
+/* Wrapper for functions that are likely to block for an appreciable    */
+/* length of time.                                                      */
+
+/*ARGSUSED*/
+GC_INNER void GC_do_blocking_inner(ptr_t data, void * context)
+{
+    struct blocking_data * d = (struct blocking_data *) data;
+    bdwgc_thread_state* me = BDWGC_THREAD_STATE();
+
+    DCL_LOCK_STATE;
+    LOCK();
+    GC_ASSERT(!(me -> thread_blocked));
+    //        me -> stop_info.stack_ptr = GC_approx_sp();
+    me -> thread_blocked = (unsigned char)TRUE;
+    /* Save context here if we want to support precise stack marking */
+    UNLOCK();
+    d -> client_data = (d -> fn)(d -> client_data);
+    LOCK();   /* This will block if the world is stopped.       */
+    me -> thread_blocked = FALSE;
+
+    UNLOCK();
+}
+
+
+/* GC_call_with_gc_active() has the opposite to GC_do_blocking()        */
+/* functionality.  It might be called from a user function invoked by   */
+/* GC_do_blocking() to temporarily back allow calling any GC function   */
+/* and/or manipulating pointers to the garbage collected heap.          */
+GC_API void * GC_CALL GC_call_with_gc_active(GC_fn_type fn,
+                                             void * client_data)
+{
+    struct GC_traced_stack_sect_s stacksect;
+    nk_thread_t* me = get_cur_thread();
+    DCL_LOCK_STATE;
+
+    LOCK();   /* This will block if the world is stopped.       */
+
+    /* Adjust our stack base value (this could happen unless    */
+    /* GC_get_stack_base() was used which returned GC_SUCCESS). */
+    //    if ((me -> flags & MAIN_THREAD) == 0) {
+    //GC_ASSERT(me -> stack_end != NULL);
+
+    //    if (me -> stack HOTTER_THAN (ptr_t)(&stacksect))  // SHOULD ENABLE
+            //me -> stack_end = (ptr_t)(&stacksect);  // SHOULD ENABLE
+    
+    //else {
+      /* The original stack. */
+      if (GC_stackbottom HOTTER_THAN (ptr_t)(&stacksect))
+        GC_stackbottom = (ptr_t)(&stacksect);
+      //}
+
+    /* Setup new "stack section".       */
+    //stacksect.saved_stack_ptr = me -> gc_state -> stop_info.stack_ptr;    // SHOULD ENABLE
+    stacksect.prev = BDWGC_SPECIFIC_THREAD_STATE(me) -> traced_stack_sect;
+    BDWGC_SPECIFIC_THREAD_STATE(me) -> thread_blocked = FALSE;
+    BDWGC_SPECIFIC_THREAD_STATE(me) -> traced_stack_sect = &stacksect;
+
+    UNLOCK();
+    client_data = fn(client_data);
+    //    GC_ASSERT(me -> gc_state -> thread_blocked == FALSE);
+    GC_ASSERT(BDWGC_SPECIFIC_THREAD_STATE(me) -> traced_stack_sect == &stacksect);
+
+    /* Restore original "stack section".        */
+    LOCK();
+    BDWGC_SPECIFIC_THREAD_STATE(me) -> traced_stack_sect = stacksect.prev;
+    BDWGC_SPECIFIC_THREAD_STATE(me) -> thread_blocked = (unsigned char)TRUE;
+    //BDWGC_SPECIFIC_THREAD_STATE(me) -> stop_info.stack_ptr = stacksect.saved_stack_ptr; // SHOULD ENABLE
+    UNLOCK();
+
+    return client_data; /* result */
+}
+
+/* It may not be safe to allocate when we register the first thread.    */
+static struct GC_Thread_Rep first_thread;
+
+/* Delete a thread from GC_threads.  We assume it is there.     */
+/* (The code intentionally traps if it wasn't.)                 */
+/* It is safe to delete the main thread.                        */
+STATIC void GC_delete_thread(nk_thread_t* t) { }
+
+
+/* We hold the GC lock.  Wait until an in-progress GC has finished.     */
+/* Repeatedly RELEASES GC LOCK in order to wait.                        */
+/* If wait_for_all is true, then we exit with the GC lock held and no   */
+/* collection in progress; otherwise we just wait for the current GC    */
+/* to finish.                                                           */
+STATIC void GC_wait_for_gc_completion(GC_bool wait_for_all)
+{
+    BDWGC_DEBUG("Waiting for GC Completion");
+    
+    DCL_LOCK_STATE;
+    GC_ASSERT(I_HOLD_LOCK());
+    ASSERT_CANCEL_DISABLED();
+    if (GC_incremental && GC_collection_in_progress()) {
+        word old_gc_no = GC_gc_no;
+
+        /* Make sure that no part of our stack is still on the mark stack, */
+        /* since it's about to be unmapped.                                */
+        while (GC_incremental && GC_collection_in_progress()
+               && (wait_for_all || old_gc_no == GC_gc_no)) {
+            ENTER_GC();
+            GC_in_thread_creation = TRUE;
+            GC_collect_a_little_inner(1);
+            GC_in_thread_creation = FALSE;
+            EXIT_GC();
+            UNLOCK();
+
+            //sched_yield();
+            nk_yield();
+            
+            LOCK();
+        }
+    }
+}
+
+
+STATIC void GC_unregister_my_thread_inner(nk_thread_t* me)
+{
+    BDWGC_DEBUG("Unregistering thread %p\n", get_cur_thread());
+    GC_destroy_thread_local(&(BDWGC_SPECIFIC_THREAD_STATE(me) -> tlfs));
+}
+
+
+GC_API int GC_CALL GC_unregister_my_thread(void)
+{
+    IF_CANCEL(int cancel_state;)
+    DCL_LOCK_STATE;
+
+    LOCK();
+    DISABLE_CANCEL(cancel_state);
+    /* Wait for any GC that may be marking from our stack to    */
+    /* complete before we remove this thread.                   */
+    GC_wait_for_gc_completion(FALSE);
+    GC_unregister_my_thread_inner(get_cur_thread());
+
+    RESTORE_CANCEL(cancel_state);
+    UNLOCK();
+    return GC_SUCCESS;
+}
+
+
+/* It may not be safe to allocate when we register the first thread.    */
+static struct GC_Thread_Rep first_thread;
+
+/* Add a thread to GC_threads.  We assume it wasn't already there.      */
+/* Caller holds allocation lock.                                        */
+STATIC nk_thread_t* GC_new_thread(nk_thread_t* t)
+{
+    BDWGC_DEBUG("Adding a new thread");
+    return t;
+}
+
+
+/* We hold the allocation lock. */
+GC_INNER void GC_thr_init(void)
+{
+  if (GC_thr_initialized) return;
+  GC_thr_initialized = TRUE;
+
+  /* Add the initial thread, so we can stop it. */
+  nk_thread_t* t = get_cur_thread();
+
+  GC_stop_init();
+
+  /* Set GC_nprocs.     */
+  GC_nprocs = GC_get_nprocs();
+  
+  if (GC_nprocs <= 0) {
+    WARN("GC_get_nprocs() returned %" GC_PRIdPTR "\n", GC_nprocs);
+    GC_nprocs = 2; /* assume dual-core */
+  }
+}
+
+
+# if defined(GC_ASSERTIONS)
+    void GC_check_tls_for(GC_tlfs p);
+
+    static void check_thread_tls (nk_thread_t *t)
+    {
+       GC_check_tls_for(&(BDWGC_SPECIFIC_THREAD_STATE(t) -> tlfs));
+    }
+
+    /* Check that all thread-local free-lists are completely marked.    */
+    /* Also check that thread-specific-data structures are marked.      */
+    void GC_check_tls(void)
+    {
+      BDWGC_DEBUG("Checking thread-local free lists are marked\n");
+      nk_sched_map_threads(-1, check_thread_tls,0);
+    }
+
+  /* This is called from thread-local GC_malloc(). */
+  GC_bool GC_is_thread_tsd_valid(void *tsd)
+  {
+    nk_thread_t* me = get_cur_thread();
+
+    //BDWGC_DEBUG("tsd = %d , me->tlfs =  %d\n", (char *)tsd, (char*)&me->tlfs);
+    
+    return (char *)tsd >= (char *)&(BDWGC_SPECIFIC_THREAD_STATE(me) -> tlfs)
+      && (char *)tsd < (char *)&(BDWGC_SPECIFIC_THREAD_STATE(me) -> tlfs) + sizeof(BDWGC_SPECIFIC_THREAD_STATE(me) -> tlfs);
+  }
+# endif // GC_ASSERTIONS
diff --git a/src/gc/bdwgc/new_hblk.c b/src/gc/bdwgc/new_hblk.c
new file mode 100644
index 0000000..6b3d7fe
--- /dev/null
+++ b/src/gc/bdwgc/new_hblk.c
@@ -0,0 +1,193 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+/*
+ * This file contains the functions:
+ *      ptr_t GC_build_flXXX(h, old_fl)
+ *      void GC_new_hblk(size)
+ */
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+#ifndef SMALL_CONFIG
+  /* Build a free list for size 2 (words) cleared objects inside        */
+  /* hblk h.  Set the last link to be ofl.  Return a pointer tpo the    */
+  /* first free list entry.                                             */
+  STATIC ptr_t GC_build_fl_clear2(struct hblk *h, ptr_t ofl)
+  {
+    word * p = (word *)(h -> hb_body);
+    word * lim = (word *)(h + 1);
+
+    p[0] = (word)ofl;
+    p[1] = 0;
+    p[2] = (word)p;
+    p[3] = 0;
+    p += 4;
+    for (; p < lim; p += 4) {
+        p[0] = (word)(p-2);
+        p[1] = 0;
+        p[2] = (word)p;
+        p[3] = 0;
+    };
+    return((ptr_t)(p-2));
+  }
+
+  /* The same for size 4 cleared objects.       */
+  STATIC ptr_t GC_build_fl_clear4(struct hblk *h, ptr_t ofl)
+  {
+    word * p = (word *)(h -> hb_body);
+    word * lim = (word *)(h + 1);
+
+    p[0] = (word)ofl;
+    p[1] = 0;
+    p[2] = 0;
+    p[3] = 0;
+    p += 4;
+    for (; p < lim; p += 4) {
+        PREFETCH_FOR_WRITE((ptr_t)(p+64));
+        p[0] = (word)(p-4);
+        p[1] = 0;
+        CLEAR_DOUBLE(p+2);
+    };
+    return((ptr_t)(p-4));
+  }
+
+  /* The same for size 2 uncleared objects.     */
+  STATIC ptr_t GC_build_fl2(struct hblk *h, ptr_t ofl)
+  {
+    word * p = (word *)(h -> hb_body);
+    word * lim = (word *)(h + 1);
+
+    p[0] = (word)ofl;
+    p[2] = (word)p;
+    p += 4;
+    for (; p < lim; p += 4) {
+        p[0] = (word)(p-2);
+        p[2] = (word)p;
+    };
+    return((ptr_t)(p-2));
+  }
+
+  /* The same for size 4 uncleared objects.     */
+  STATIC ptr_t GC_build_fl4(struct hblk *h, ptr_t ofl)
+  {
+    word * p = (word *)(h -> hb_body);
+    word * lim = (word *)(h + 1);
+
+    p[0] = (word)ofl;
+    p[4] = (word)p;
+    p += 8;
+    for (; p < lim; p += 8) {
+        PREFETCH_FOR_WRITE((ptr_t)(p+64));
+        p[0] = (word)(p-4);
+        p[4] = (word)p;
+    };
+    return((ptr_t)(p-4));
+  }
+#endif /* !SMALL_CONFIG */
+
+/* Build a free list for objects of size sz inside heap block h.        */
+/* Clear objects inside h if clear is set.  Add list to the end of      */
+/* the free list we build.  Return the new free list.                   */
+/* This could be called without the main GC lock, if we ensure that     */
+/* there is no concurrent collection which might reclaim objects that   */
+/* we have not yet allocated.                                           */
+GC_INNER ptr_t GC_build_fl(struct hblk *h, size_t sz, GC_bool clear,
+                           ptr_t list)
+{
+  word *p, *prev;
+  word *last_object;            /* points to last object in new hblk    */
+
+  /* Do a few prefetches here, just because its cheap.          */
+  /* If we were more serious about it, these should go inside   */
+  /* the loops.  But write prefetches usually don't seem to     */
+  /* matter much.                                               */
+    PREFETCH_FOR_WRITE((ptr_t)h);
+    PREFETCH_FOR_WRITE((ptr_t)h + 128);
+    PREFETCH_FOR_WRITE((ptr_t)h + 256);
+    PREFETCH_FOR_WRITE((ptr_t)h + 378);
+# ifndef SMALL_CONFIG
+    /* Handle small objects sizes more efficiently.  For larger objects */
+    /* the difference is less significant.                              */
+    switch (sz) {
+        case 2: if (clear) {
+                    return GC_build_fl_clear2(h, list);
+                } else {
+                    return GC_build_fl2(h, list);
+                }
+        case 4: if (clear) {
+                    return GC_build_fl_clear4(h, list);
+                } else {
+                    return GC_build_fl4(h, list);
+                }
+        default:
+                break;
+    }
+# endif /* !SMALL_CONFIG */
+
+  /* Clear the page if necessary. */
+    if (clear) BZERO(h, HBLKSIZE);
+
+  /* Add objects to free list */
+    p = (word *)(h -> hb_body) + sz;    /* second object in *h  */
+    prev = (word *)(h -> hb_body);              /* One object behind p  */
+    last_object = (word *)((char *)h + HBLKSIZE);
+    last_object -= sz;
+                            /* Last place for last object to start */
+
+  /* make a list of all objects in *h with head as last object */
+    while (p <= last_object) {
+      /* current object's link points to last object */
+        obj_link(p) = (ptr_t)prev;
+        prev = p;
+        p += sz;
+    }
+    p -= sz;                    /* p now points to last object */
+
+  /* Put p (which is now head of list of objects in *h) as first    */
+  /* pointer in the appropriate free list for this size.            */
+    *(ptr_t *)h = list;
+    return ((ptr_t)p);
+}
+
+/* Allocate a new heapblock for small objects of size gran granules.    */
+/* Add all of the heapblock's objects to the free list for objects      */
+/* of that size.  Set all mark bits if objects are uncollectible.       */
+/* Will fail to do anything if we are out of memory.                    */
+GC_INNER void GC_new_hblk(size_t gran, int kind)
+{
+  struct hblk *h;       /* the new heap block */
+  GC_bool clear = GC_obj_kinds[kind].ok_init;
+
+  GC_STATIC_ASSERT((sizeof (struct hblk)) == HBLKSIZE);
+
+  if (GC_debugging_started) clear = TRUE;
+
+  /* Allocate a new heap block */
+    h = GC_allochblk(GRANULES_TO_BYTES(gran), kind, 0);
+    if (h == 0) return;
+
+  /* Mark all objects if appropriate. */
+      if (IS_UNCOLLECTABLE(kind)) GC_set_hdr_marks(HDR(h));
+
+  /* Build the free list */
+      GC_obj_kinds[kind].ok_freelist[gran] =
+        GC_build_fl(h, GRANULES_TO_WORDS(gran), clear,
+                    GC_obj_kinds[kind].ok_freelist[gran]);
+}
diff --git a/src/gc/bdwgc/obj_map.c b/src/gc/bdwgc/obj_map.c
new file mode 100644
index 0000000..ed75ef1
--- /dev/null
+++ b/src/gc/bdwgc/obj_map.c
@@ -0,0 +1,90 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991, 1992 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1999-2001 by Hewlett-Packard Company. All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+/* Routines for maintaining maps describing heap block
+ * layouts for various object sizes.  Allows fast pointer validity checks
+ * and fast location of object start locations on machines (such as SPARC)
+ * with slow division.
+ */
+
+/* Consider pointers that are offset bytes displaced from the beginning */
+/* of an object to be valid.                                            */
+
+GC_API void GC_CALL GC_register_displacement(size_t offset)
+{
+    DCL_LOCK_STATE;
+
+    LOCK();
+    GC_register_displacement_inner(offset);
+    UNLOCK();
+}
+
+GC_INNER void GC_register_displacement_inner(size_t offset)
+{
+    if (offset >= VALID_OFFSET_SZ) {
+        ABORT("Bad argument to GC_register_displacement");
+    }
+    if (!GC_valid_offsets[offset]) {
+      GC_valid_offsets[offset] = TRUE;
+      GC_modws_valid_offsets[offset % sizeof(word)] = TRUE;
+    }
+}
+
+#ifdef MARK_BIT_PER_GRANULE
+  /* Add a heap block map for objects of size granules to obj_map.      */
+  /* Return FALSE on failure.                                           */
+  /* A size of 0 granules is used for large objects.                    */
+  GC_INNER GC_bool GC_add_map_entry(size_t granules)
+  {
+    unsigned displ;
+    short * new_map;
+
+    if (granules > BYTES_TO_GRANULES(MAXOBJBYTES)) granules = 0;
+    if (GC_obj_map[granules] != 0) {
+        return(TRUE);
+    }
+    new_map = (short *)GC_scratch_alloc(MAP_LEN * sizeof(short));
+    if (new_map == 0) return(FALSE);
+    if (GC_print_stats)
+        GC_log_printf("Adding block map for size of %u granules (%u bytes)\n",
+                  (unsigned)granules, (unsigned)(GRANULES_TO_BYTES(granules)));
+    if (granules == 0) {
+      for (displ = 0; displ < BYTES_TO_GRANULES(HBLKSIZE); displ++) {
+        new_map[displ] = 1;  /* Nonzero to get us out of marker fast path. */
+      }
+    } else {
+      for (displ = 0; displ < BYTES_TO_GRANULES(HBLKSIZE); displ++) {
+        new_map[displ] = (short)(displ % granules);
+      }
+    }
+    GC_obj_map[granules] = new_map;
+    return(TRUE);
+  }
+#endif
+
+GC_INNER void GC_initialize_offsets(void)
+{
+  unsigned i;
+  if (GC_all_interior_pointers) {
+    for (i = 0; i < VALID_OFFSET_SZ; ++i)
+      GC_valid_offsets[i] = TRUE;
+  } else {
+    BZERO(GC_valid_offsets, sizeof(GC_valid_offsets));
+    for (i = 0; i < sizeof(word); ++i)
+      GC_modws_valid_offsets[i] = FALSE;
+  }
+}
diff --git a/src/gc/bdwgc/os_dep.c b/src/gc/bdwgc/os_dep.c
new file mode 100644
index 0000000..64056db
--- /dev/null
+++ b/src/gc/bdwgc/os_dep.c
@@ -0,0 +1,4773 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#if defined(LINUX) && !defined(POWERPC)
+# include <linux/version.h>
+# if (LINUX_VERSION_CODE <= 0x10400)
+    /* Ugly hack to get struct sigcontext_struct definition.  Required  */
+    /* for some early 1.3.X releases.  Will hopefully go away soon.     */
+    /* in some later Linux releases, asm/sigcontext.h may have to       */
+    /* be included instead.                                             */
+#   define __KERNEL__
+#   include <asm/signal.h>
+#   undef __KERNEL__
+# else
+    /* Kernels prior to 2.1.1 defined struct sigcontext_struct instead of */
+    /* struct sigcontext.  libc6 (glibc2) uses "struct sigcontext" in     */
+    /* prototypes, so we have to include the top-level sigcontext.h to    */
+    /* make sure the former gets defined to be the latter if appropriate. */
+#   include <features.h>
+#   if 2 <= __GLIBC__
+#     if 2 == __GLIBC__ && 0 == __GLIBC_MINOR__
+        /* glibc 2.1 no longer has sigcontext.h.  But signal.h          */
+        /* has the right declaration for glibc 2.1.                     */
+#       include <sigcontext.h>
+#     endif /* 0 == __GLIBC_MINOR__ */
+#   else /* not 2 <= __GLIBC__ */
+      /* libc5 doesn't have <sigcontext.h>: go directly with the kernel   */
+      /* one.  Check LINUX_VERSION_CODE to see which we should reference. */
+#     include <asm/sigcontext.h>
+#   endif /* 2 <= __GLIBC__ */
+# endif
+#endif
+
+#if !defined(OS2) && !defined(PCR) && !defined(AMIGA) && !defined(MACOS) \
+  && !defined(MSWINCE) && !defined(__CC_ARM) && !defined(NAUT)
+# include <sys/types.h>
+# if !defined(MSWIN32)
+#   include <unistd.h>
+# endif
+#endif
+
+#ifdef NAUT
+# include <nautilus/naut_types.h>
+# include <nautilus/mm.h>
+#else 
+# include <stdio.h>
+#endif
+
+#if defined(MSWINCE) || defined(SN_TARGET_PS3)
+# define SIGSEGV 0 /* value is irrelevant */
+#elif !defined(NAUT)
+# include <signal.h>
+#endif
+
+#if (defined(UNIX_LIKE) || defined(CYGWIN32) || defined(NACL)) && !defined(NAUT)
+# include <fcntl.h>
+#endif
+
+#if defined(LINUX) || defined(LINUX_STACKBOTTOM)
+# include <ctype.h>
+#endif
+
+/* Blatantly OS dependent routines, except for those that are related   */
+/* to dynamic loading.                                                  */
+
+#ifdef AMIGA
+# define GC_AMIGA_DEF
+# include "extra/AmigaOS.c"
+# undef GC_AMIGA_DEF
+#endif
+
+#if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+# ifndef WIN32_LEAN_AND_MEAN
+#   define WIN32_LEAN_AND_MEAN 1
+# endif
+# define NOSERVICE
+# include <windows.h>
+  /* It's not clear this is completely kosher under Cygwin.  But it     */
+  /* allows us to get a working GC_get_stack_base.                      */
+#endif
+
+#ifdef MACOS
+# include <Processes.h>
+#endif
+
+#ifdef IRIX5
+# include <sys/uio.h>
+# include <malloc.h>   /* for locking */
+#endif
+
+#if defined(MMAP_SUPPORTED) || defined(ADD_HEAP_GUARD_PAGES)
+# if defined(USE_MUNMAP) && !defined(USE_MMAP)
+#   error "invalid config - USE_MUNMAP requires USE_MMAP"
+# endif
+# include <sys/types.h>
+# include <sys/mman.h>
+# include <sys/stat.h>
+# include <errno.h>
+#endif
+
+#ifdef DARWIN
+  /* for get_etext and friends */
+# include <mach-o/getsect.h>
+#endif
+
+#ifdef DJGPP
+  /* Apparently necessary for djgpp 2.01.  May cause problems with      */
+  /* other versions.                                                    */
+  typedef long unsigned int caddr_t;
+#endif
+
+#ifdef PCR
+# include "il/PCR_IL.h"
+# include "th/PCR_ThCtl.h"
+# include "mm/PCR_MM.h"
+#endif
+
+#if !defined(NO_EXECUTE_PERMISSION)
+  STATIC GC_bool GC_pages_executable = TRUE;
+#else
+  STATIC GC_bool GC_pages_executable = FALSE;
+#endif
+#define IGNORE_PAGES_EXECUTABLE 1
+                        /* Undefined on GC_pages_executable real use.   */
+
+#ifdef NEED_PROC_MAPS
+/* We need to parse /proc/self/maps, either to find dynamic libraries,  */
+/* and/or to find the register backing store base (IA64).  Do it once   */
+/* here.                                                                */
+
+#define READ read
+
+/* Repeatedly perform a read call until the buffer is filled or */
+/* we encounter EOF.                                            */
+STATIC ssize_t GC_repeat_read(int fd, char *buf, size_t count)
+{
+    size_t num_read = 0;
+    ssize_t result;
+
+    ASSERT_CANCEL_DISABLED();
+    while (num_read < count) {
+        result = READ(fd, buf + num_read, count - num_read);
+        if (result < 0) return result;
+        if (result == 0) break;
+        num_read += result;
+    }
+    return num_read;
+}
+
+#ifdef THREADS
+  /* Determine the length of a file by incrementally reading it into a  */
+  /* This would be silly to use on a file supporting lseek, but Linux   */
+  /* /proc files usually do not.                                        */
+  STATIC size_t GC_get_file_len(int f)
+  {
+    size_t total = 0;
+    ssize_t result;
+#   define GET_FILE_LEN_BUF_SZ 500
+    char buf[GET_FILE_LEN_BUF_SZ];
+
+    do {
+        result = read(f, buf, GET_FILE_LEN_BUF_SZ);
+        if (result == -1) return 0;
+        total += result;
+    } while (result > 0);
+    return total;
+  }
+
+  STATIC size_t GC_get_maps_len(void)
+  {
+    int f = open("/proc/self/maps", O_RDONLY);
+    size_t result;
+    if (f < 0) return 0; /* treat missing file as empty */
+    result = GC_get_file_len(f);
+    close(f);
+    return result;
+  }
+#endif /* THREADS */
+
+/* Copy the contents of /proc/self/maps to a buffer in our address      */
+/* space.  Return the address of the buffer, or zero on failure.        */
+/* This code could be simplified if we could determine its size ahead   */
+/* of time.                                                             */
+GC_INNER char * GC_get_maps(void)
+{
+    int f;
+    ssize_t result;
+    static char *maps_buf = NULL;
+    static size_t maps_buf_sz = 1;
+    size_t maps_size, old_maps_size = 0;
+
+    /* The buffer is essentially static, so there must be a single client. */
+    GC_ASSERT(I_HOLD_LOCK());
+
+    /* Note that in the presence of threads, the maps file can  */
+    /* essentially shrink asynchronously and unexpectedly as    */
+    /* threads that we already think of as dead release their   */
+    /* stacks.  And there is no easy way to read the entire     */
+    /* file atomically.  This is arguably a misfeature of the   */
+    /* /proc/.../maps interface.                                */
+
+    /* Since we don't believe the file can grow                 */
+    /* asynchronously, it should suffice to first determine     */
+    /* the size (using lseek or read), and then to reread the   */
+    /* file.  If the size is inconsistent we have to retry.     */
+    /* This only matters with threads enabled, and if we use    */
+    /* this to locate roots (not the default).                  */
+
+#   ifdef THREADS
+        /* Determine the initial size of /proc/self/maps.       */
+        /* Note that lseek doesn't work, at least as of 2.6.15. */
+        maps_size = GC_get_maps_len();
+        if (0 == maps_size) return 0;
+#   else
+        maps_size = 4000;       /* Guess */
+#   endif
+
+    /* Read /proc/self/maps, growing maps_buf as necessary.     */
+    /* Note that we may not allocate conventionally, and        */
+    /* thus can't use stdio.                                    */
+        do {
+            while (maps_size >= maps_buf_sz) {
+              /* Grow only by powers of 2, since we leak "too small" buffers.*/
+              while (maps_size >= maps_buf_sz) maps_buf_sz *= 2;
+              maps_buf = GC_scratch_alloc(maps_buf_sz);
+#             ifdef THREADS
+                /* Recompute initial length, since we allocated.        */
+                /* This can only happen a few times per program         */
+                /* execution.                                           */
+                maps_size = GC_get_maps_len();
+                if (0 == maps_size) return 0;
+#             endif
+              if (maps_buf == 0) return 0;
+            }
+            GC_ASSERT(maps_buf_sz >= maps_size + 1);
+            f = open("/proc/self/maps", O_RDONLY);
+            if (-1 == f) return 0;
+#           ifdef THREADS
+              old_maps_size = maps_size;
+#           endif
+            maps_size = 0;
+            do {
+                result = GC_repeat_read(f, maps_buf, maps_buf_sz-1);
+                if (result <= 0)
+                  break;
+                maps_size += result;
+            } while ((size_t)result == maps_buf_sz-1);
+            close(f);
+            if (result <= 0)
+              return 0;
+#           ifdef THREADS
+              if (maps_size > old_maps_size) {
+                if (GC_print_stats)
+                  GC_log_printf(
+                        "Unexpected maps size growth from %lu to %lu\n",
+                        (unsigned long)old_maps_size,
+                        (unsigned long)maps_size);
+                ABORT("Unexpected asynchronous /proc/self/maps growth: "
+                      "unregistered thread?");
+              }
+#           endif
+        } while (maps_size >= maps_buf_sz || maps_size < old_maps_size);
+                /* In the single-threaded case, the second clause is false. */
+        maps_buf[maps_size] = '\0';
+
+        /* Apply fn to result.  */
+        return maps_buf;
+}
+
+/*
+ *  GC_parse_map_entry parses an entry from /proc/self/maps so we can
+ *  locate all writable data segments that belong to shared libraries.
+ *  The format of one of these entries and the fields we care about
+ *  is as follows:
+ *  XXXXXXXX-XXXXXXXX r-xp 00000000 30:05 260537     name of mapping...\n
+ *  ^^^^^^^^ ^^^^^^^^ ^^^^          ^^
+ *  start    end      prot          maj_dev
+ *
+ *  Note that since about august 2003 kernels, the columns no longer have
+ *  fixed offsets on 64-bit kernels.  Hence we no longer rely on fixed offsets
+ *  anywhere, which is safer anyway.
+ */
+
+/* Assign various fields of the first line in buf_ptr to (*start),      */
+/* (*end), (*prot), (*maj_dev) and (*mapping_name).  mapping_name may   */
+/* be NULL. (*prot) and (*mapping_name) are assigned pointers into the  */
+/* original buffer.                                                     */
+GC_INNER char *GC_parse_map_entry(char *buf_ptr, ptr_t *start, ptr_t *end,
+                                  char **prot, unsigned int *maj_dev,
+                                  char **mapping_name)
+{
+    unsigned char *start_start, *end_start, *maj_dev_start;
+    unsigned char *p;   /* unsigned for isspace, isxdigit */
+
+    if (buf_ptr == NULL || *buf_ptr == '\0') {
+        return NULL;
+    }
+
+    p = (unsigned char *)buf_ptr;
+    while (isspace(*p)) ++p;
+    start_start = p;
+    GC_ASSERT(isxdigit(*start_start));
+    *start = (ptr_t)strtoul((char *)start_start, (char **)&p, 16);
+    GC_ASSERT(*p=='-');
+
+    ++p;
+    end_start = p;
+    GC_ASSERT(isxdigit(*end_start));
+    *end = (ptr_t)strtoul((char *)end_start, (char **)&p, 16);
+    GC_ASSERT(isspace(*p));
+
+    while (isspace(*p)) ++p;
+    GC_ASSERT(*p == 'r' || *p == '-');
+    *prot = (char *)p;
+    /* Skip past protection field to offset field */
+       while (!isspace(*p)) ++p; while (isspace(*p)) ++p;
+    GC_ASSERT(isxdigit(*p));
+    /* Skip past offset field, which we ignore */
+          while (!isspace(*p)) ++p; while (isspace(*p)) ++p;
+    maj_dev_start = p;
+    GC_ASSERT(isxdigit(*maj_dev_start));
+    *maj_dev = strtoul((char *)maj_dev_start, NULL, 16);
+
+    if (mapping_name == 0) {
+      while (*p && *p++ != '\n');
+    } else {
+      while (*p && *p != '\n' && *p != '/' && *p != '[') p++;
+      *mapping_name = (char *)p;
+      while (*p && *p++ != '\n');
+    }
+    return (char *)p;
+}
+
+#if defined(IA64) || defined(INCLUDE_LINUX_THREAD_DESCR)
+  /* Try to read the backing store base from /proc/self/maps.           */
+  /* Return the bounds of the writable mapping with a 0 major device,   */
+  /* which includes the address passed as data.                         */
+  /* Return FALSE if there is no such mapping.                          */
+  GC_INNER GC_bool GC_enclosing_mapping(ptr_t addr, ptr_t *startp,
+                                        ptr_t *endp)
+  {
+    char *prot;
+    ptr_t my_start, my_end;
+    unsigned int maj_dev;
+    char *maps = GC_get_maps();
+    char *buf_ptr = maps;
+
+    if (0 == maps) return(FALSE);
+    for (;;) {
+      buf_ptr = GC_parse_map_entry(buf_ptr, &my_start, &my_end,
+                                   &prot, &maj_dev, 0);
+
+      if (buf_ptr == NULL) return FALSE;
+      if (prot[1] == 'w' && maj_dev == 0) {
+          if (my_end > addr && my_start <= addr) {
+            *startp = my_start;
+            *endp = my_end;
+            return TRUE;
+          }
+      }
+    }
+    return FALSE;
+  }
+#endif /* IA64 || INCLUDE_LINUX_THREAD_DESCR */
+
+#if defined(REDIRECT_MALLOC)
+  /* Find the text(code) mapping for the library whose name, after      */
+  /* stripping the directory part, starts with nm.                      */
+  GC_INNER GC_bool GC_text_mapping(char *nm, ptr_t *startp, ptr_t *endp)
+  {
+    size_t nm_len = strlen(nm);
+    char *prot;
+    char *map_path;
+    ptr_t my_start, my_end;
+    unsigned int maj_dev;
+    char *maps = GC_get_maps();
+    char *buf_ptr = maps;
+
+    if (0 == maps) return(FALSE);
+    for (;;) {
+      buf_ptr = GC_parse_map_entry(buf_ptr, &my_start, &my_end,
+                                   &prot, &maj_dev, &map_path);
+
+      if (buf_ptr == NULL) return FALSE;
+      if (prot[0] == 'r' && prot[1] == '-' && prot[2] == 'x') {
+          char *p = map_path;
+          /* Set p to point just past last slash, if any. */
+            while (*p != '\0' && *p != '\n' && *p != ' ' && *p != '\t') ++p;
+            while (*p != '/' && p >= map_path) --p;
+            ++p;
+          if (strncmp(nm, p, nm_len) == 0) {
+            *startp = my_start;
+            *endp = my_end;
+            return TRUE;
+          }
+      }
+    }
+    return FALSE;
+  }
+#endif /* REDIRECT_MALLOC */
+
+#ifdef IA64
+  static ptr_t backing_store_base_from_proc(void)
+  {
+    ptr_t my_start, my_end;
+    if (!GC_enclosing_mapping(GC_save_regs_in_stack(), &my_start, &my_end)) {
+        if (GC_print_stats) {
+          GC_log_printf("Failed to find backing store base from /proc\n");
+        }
+        return 0;
+    }
+    return my_start;
+  }
+#endif
+
+#endif /* NEED_PROC_MAPS */
+
+#if defined(SEARCH_FOR_DATA_START)
+  /* The I386 case can be handled without a search.  The Alpha case     */
+  /* used to be handled differently as well, but the rules changed      */
+  /* for recent Linux versions.  This seems to be the easiest way to    */
+  /* cover all versions.                                                */
+
+# if defined(LINUX) || defined(HURD)
+    /* Some Linux distributions arrange to define __data_start.  Some   */
+    /* define data_start as a weak symbol.  The latter is technically   */
+    /* broken, since the user program may define data_start, in which   */
+    /* case we lose.  Nonetheless, we try both, preferring __data_start.*/
+    /* We assume gcc-compatible pragmas.        */
+#   pragma weak __data_start
+    extern int __data_start[];
+#   pragma weak data_start
+    extern int data_start[];
+# endif /* LINUX */
+  extern int _end[];
+
+  ptr_t GC_data_start = NULL;
+
+  ptr_t GC_find_limit(ptr_t, GC_bool);
+
+  GC_INNER void GC_init_linux_data_start(void)
+  {
+
+#   if defined(LINUX) || defined(HURD)
+      /* Try the easy approaches first: */
+      if ((ptr_t)__data_start != 0) {
+          GC_data_start = (ptr_t)(__data_start);
+          return;
+      }
+      if ((ptr_t)data_start != 0) {
+          GC_data_start = (ptr_t)(data_start);
+          return;
+      }
+#   endif /* LINUX */
+    GC_data_start = GC_find_limit((ptr_t)(_end), FALSE);
+  }
+#endif /* SEARCH_FOR_DATA_START */
+
+#ifdef ECOS
+
+# ifndef ECOS_GC_MEMORY_SIZE
+#   define ECOS_GC_MEMORY_SIZE (448 * 1024)
+# endif /* ECOS_GC_MEMORY_SIZE */
+
+  /* FIXME: This is a simple way of allocating memory which is          */
+  /* compatible with ECOS early releases.  Later releases use a more    */
+  /* sophisticated means of allocating memory than this simple static   */
+  /* allocator, but this method is at least bound to work.              */
+  static char ecos_gc_memory[ECOS_GC_MEMORY_SIZE];
+  static char *ecos_gc_brk = ecos_gc_memory;
+
+  static void *tiny_sbrk(ptrdiff_t increment)
+  {
+    void *p = ecos_gc_brk;
+    ecos_gc_brk += increment;
+    if (ecos_gc_brk > ecos_gc_memory + sizeof(ecos_gc_memory)) {
+      ecos_gc_brk -= increment;
+      return NULL;
+    }
+    return p;
+  }
+# define sbrk tiny_sbrk
+#endif /* ECOS */
+
+#if defined(NETBSD) && defined(__ELF__)
+  ptr_t GC_data_start = NULL;
+  ptr_t GC_find_limit(ptr_t, GC_bool);
+
+  extern char **environ;
+
+  GC_INNER void GC_init_netbsd_elf(void)
+  {
+        /* This may need to be environ, without the underscore, for     */
+        /* some versions.                                               */
+    GC_data_start = GC_find_limit((ptr_t)&environ, FALSE);
+  }
+#endif /* NETBSD */
+
+#ifdef OPENBSD
+  static struct sigaction old_segv_act;
+  STATIC sigjmp_buf GC_jmp_buf_openbsd;
+
+# ifdef THREADS
+#   include <sys/syscall.h>
+    extern sigset_t __syscall(quad_t, ...);
+# endif
+
+  /* Don't use GC_find_limit() because siglongjmp() outside of the      */
+  /* signal handler by-passes our userland pthreads lib, leaving        */
+  /* SIGSEGV and SIGPROF masked.  Instead, use this custom one that     */
+  /* works-around the issues.                                           */
+
+  /*ARGSUSED*/
+  STATIC void GC_fault_handler_openbsd(int sig)
+  {
+     siglongjmp(GC_jmp_buf_openbsd, 1);
+  }
+
+  /* Return the first non-addressable location > p or bound.    */
+  /* Requires the allocation lock.                              */
+  STATIC ptr_t GC_find_limit_openbsd(ptr_t p, ptr_t bound)
+  {
+    static volatile ptr_t result;
+             /* Safer if static, since otherwise it may not be  */
+             /* preserved across the longjmp.  Can safely be    */
+             /* static since it's only called with the          */
+             /* allocation lock held.                           */
+
+    struct sigaction act;
+    size_t pgsz = (size_t)sysconf(_SC_PAGESIZE);
+    GC_ASSERT(I_HOLD_LOCK());
+
+    act.sa_handler = GC_fault_handler_openbsd;
+    sigemptyset(&act.sa_mask);
+    act.sa_flags = SA_NODEFER | SA_RESTART;
+    sigaction(SIGSEGV, &act, &old_segv_act);
+
+    if (sigsetjmp(GC_jmp_buf_openbsd, 1) == 0) {
+      result = (ptr_t)((word)p & ~(pgsz-1));
+      for (;;) {
+        result += pgsz;
+        if (result >= bound) {
+          result = bound;
+          break;
+        }
+        GC_noop1((word)(*result));
+      }
+    }
+
+#   ifdef THREADS
+      /* Due to the siglongjump we need to manually unmask SIGPROF.     */
+      __syscall(SYS_sigprocmask, SIG_UNBLOCK, sigmask(SIGPROF));
+#   endif
+
+    sigaction(SIGSEGV, &old_segv_act, 0);
+    return(result);
+  }
+
+  /* Return first addressable location > p or bound.    */
+  /* Requires the allocation lock.                      */
+  STATIC ptr_t GC_skip_hole_openbsd(ptr_t p, ptr_t bound)
+  {
+    static volatile ptr_t result;
+    static volatile int firstpass;
+
+    struct sigaction act;
+    size_t pgsz = (size_t)sysconf(_SC_PAGESIZE);
+    GC_ASSERT(I_HOLD_LOCK());
+
+    act.sa_handler = GC_fault_handler_openbsd;
+    sigemptyset(&act.sa_mask);
+    act.sa_flags = SA_NODEFER | SA_RESTART;
+    sigaction(SIGSEGV, &act, &old_segv_act);
+
+    firstpass = 1;
+    result = (ptr_t)((word)p & ~(pgsz-1));
+    if (sigsetjmp(GC_jmp_buf_openbsd, 1) != 0 || firstpass) {
+      firstpass = 0;
+      result += pgsz;
+      if (result >= bound) {
+        result = bound;
+      } else {
+        GC_noop1((word)(*result));
+      }
+    }
+
+    sigaction(SIGSEGV, &old_segv_act, 0);
+    return(result);
+  }
+#endif /* OPENBSD */
+
+# ifdef OS2
+
+# include <stddef.h>
+
+# if !defined(__IBMC__) && !defined(__WATCOMC__) /* e.g. EMX */
+
+struct exe_hdr {
+    unsigned short      magic_number;
+    unsigned short      padding[29];
+    long                new_exe_offset;
+};
+
+#define E_MAGIC(x)      (x).magic_number
+#define EMAGIC          0x5A4D
+#define E_LFANEW(x)     (x).new_exe_offset
+
+struct e32_exe {
+    unsigned char       magic_number[2];
+    unsigned char       byte_order;
+    unsigned char       word_order;
+    unsigned long       exe_format_level;
+    unsigned short      cpu;
+    unsigned short      os;
+    unsigned long       padding1[13];
+    unsigned long       object_table_offset;
+    unsigned long       object_count;
+    unsigned long       padding2[31];
+};
+
+#define E32_MAGIC1(x)   (x).magic_number[0]
+#define E32MAGIC1       'L'
+#define E32_MAGIC2(x)   (x).magic_number[1]
+#define E32MAGIC2       'X'
+#define E32_BORDER(x)   (x).byte_order
+#define E32LEBO         0
+#define E32_WORDER(x)   (x).word_order
+#define E32LEWO         0
+#define E32_CPU(x)      (x).cpu
+#define E32CPU286       1
+#define E32_OBJTAB(x)   (x).object_table_offset
+#define E32_OBJCNT(x)   (x).object_count
+
+struct o32_obj {
+    unsigned long       size;
+    unsigned long       base;
+    unsigned long       flags;
+    unsigned long       pagemap;
+    unsigned long       mapsize;
+    unsigned long       reserved;
+};
+
+#define O32_FLAGS(x)    (x).flags
+#define OBJREAD         0x0001L
+#define OBJWRITE        0x0002L
+#define OBJINVALID      0x0080L
+#define O32_SIZE(x)     (x).size
+#define O32_BASE(x)     (x).base
+
+# else  /* IBM's compiler */
+
+/* A kludge to get around what appears to be a header file bug */
+# ifndef WORD
+#   define WORD unsigned short
+# endif
+# ifndef DWORD
+#   define DWORD unsigned long
+# endif
+
+# define EXE386 1
+# include <newexe.h>
+# include <exe386.h>
+
+# endif  /* __IBMC__ */
+
+# define INCL_DOSEXCEPTIONS
+# define INCL_DOSPROCESS
+# define INCL_DOSERRORS
+# define INCL_DOSMODULEMGR
+# define INCL_DOSMEMMGR
+# include <os2.h>
+
+# endif /* OS/2 */
+
+/* Find the page size */
+GC_INNER word GC_page_size = 0;
+
+#if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+# ifndef VER_PLATFORM_WIN32_CE
+#   define VER_PLATFORM_WIN32_CE 3
+# endif
+
+# if defined(MSWINCE) && defined(THREADS)
+    GC_INNER GC_bool GC_dont_query_stack_min = FALSE;
+# endif
+
+  GC_INNER void GC_setpagesize(void)
+  {
+    GetSystemInfo(&GC_sysinfo);
+    GC_page_size = GC_sysinfo.dwPageSize;
+#   if defined(MSWINCE) && !defined(_WIN32_WCE_EMULATION)
+      {
+        OSVERSIONINFO verInfo;
+        /* Check the current WinCE version.     */
+        verInfo.dwOSVersionInfoSize = sizeof(OSVERSIONINFO);
+        if (!GetVersionEx(&verInfo))
+          ABORT("GetVersionEx failed");
+        if (verInfo.dwPlatformId == VER_PLATFORM_WIN32_CE &&
+            verInfo.dwMajorVersion < 6) {
+          /* Only the first 32 MB of address space belongs to the       */
+          /* current process (unless WinCE 6.0+ or emulation).          */
+          GC_sysinfo.lpMaximumApplicationAddress = (LPVOID)((word)32 << 20);
+#         ifdef THREADS
+            /* On some old WinCE versions, it's observed that           */
+            /* VirtualQuery calls don't work properly when used to      */
+            /* get thread current stack committed minimum.              */
+            if (verInfo.dwMajorVersion < 5)
+              GC_dont_query_stack_min = TRUE;
+#         endif
+        }
+      }
+#   endif
+  }
+
+# ifndef CYGWIN32
+#   define is_writable(prot) ((prot) == PAGE_READWRITE \
+                            || (prot) == PAGE_WRITECOPY \
+                            || (prot) == PAGE_EXECUTE_READWRITE \
+                            || (prot) == PAGE_EXECUTE_WRITECOPY)
+    /* Return the number of bytes that are writable starting at p.      */
+    /* The pointer p is assumed to be page aligned.                     */
+    /* If base is not 0, *base becomes the beginning of the             */
+    /* allocation region containing p.                                  */
+    STATIC word GC_get_writable_length(ptr_t p, ptr_t *base)
+    {
+      MEMORY_BASIC_INFORMATION buf;
+      word result;
+      word protect;
+
+      result = VirtualQuery(p, &buf, sizeof(buf));
+      if (result != sizeof(buf)) ABORT("Weird VirtualQuery result");
+      if (base != 0) *base = (ptr_t)(buf.AllocationBase);
+      protect = (buf.Protect & ~(PAGE_GUARD | PAGE_NOCACHE));
+      if (!is_writable(protect)) {
+        return(0);
+      }
+      if (buf.State != MEM_COMMIT) return(0);
+      return(buf.RegionSize);
+    }
+
+    GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+    {
+      ptr_t trunc_sp;
+      word size;
+
+      /* Set page size if it is not ready (so client can use this       */
+      /* function even before GC is initialized).                       */
+      if (!GC_page_size) GC_setpagesize();
+
+      trunc_sp = (ptr_t)((word)GC_approx_sp() & ~(GC_page_size - 1));
+      /* FIXME: This won't work if called from a deeply recursive       */
+      /* client code (and the committed stack space has grown).         */
+      size = GC_get_writable_length(trunc_sp, 0);
+      GC_ASSERT(size != 0);
+      sb -> mem_base = trunc_sp + size;
+      return GC_SUCCESS;
+    }
+# else /* CYGWIN32 */
+    /* An alternate version for Cygwin (adapted from Dave Korn's        */
+    /* gcc version of boehm-gc).                                        */
+    GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+    {
+      void * _tlsbase;
+
+      __asm__ ("movl %%fs:4, %0"
+               : "=r" (_tlsbase));
+      sb -> mem_base = _tlsbase;
+      return GC_SUCCESS;
+    }
+# endif /* CYGWIN32 */
+# define HAVE_GET_STACK_BASE
+
+#else /* !MSWIN32 */
+  GC_INNER void GC_setpagesize(void)
+  {
+#   if defined(MPROTECT_VDB) || defined(PROC_VDB) || defined(USE_MMAP)
+      GC_page_size = GETPAGESIZE();
+      if (!GC_page_size) ABORT("getpagesize() failed");
+#   else
+      /* It's acceptable to fake it.    */
+      GC_page_size = HBLKSIZE;
+#   endif
+  }
+#endif /* !MSWIN32 */
+
+#ifdef BEOS
+# include <kernel/OS.h>
+
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+  {
+    thread_info th;
+    get_thread_info(find_thread(NULL),&th);
+    sb->mem_base = th.stack_end;
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* BEOS */
+
+#ifdef OS2
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+  {
+    PTIB ptib; /* thread information block */
+    PPIB ppib;
+    if (DosGetInfoBlocks(&ptib, &ppib) != NO_ERROR) {
+      ABORT("DosGetInfoBlocks failed");
+    }
+    sb->mem_base = ptib->tib_pstacklimit;
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* OS2 */
+
+# ifdef AMIGA
+#   define GC_AMIGA_SB
+#   include "extra/AmigaOS.c"
+#   undef GC_AMIGA_SB
+#   define GET_MAIN_STACKBASE_SPECIAL
+# endif /* AMIGA */
+
+# if (defined(NEED_FIND_LIMIT) || defined(UNIX_LIKE)) && !defined(NAUT)
+    typedef void (*GC_fault_handler_t)(int);
+
+#   if defined(SUNOS5SIGS) || defined(IRIX5) || defined(OSF1) \
+       || defined(HURD) || defined(FREEBSD) || defined(NETBSD)
+        static struct sigaction old_segv_act;
+#       if defined(_sigargs) /* !Irix6.x */ || defined(HPUX) \
+           || defined(HURD) || defined(NETBSD) || defined(FREEBSD)
+            static struct sigaction old_bus_act;
+#       endif
+#   else
+        static GC_fault_handler_t old_segv_handler, old_bus_handler;
+#   endif
+
+    GC_INNER void GC_set_and_save_fault_handler(GC_fault_handler_t h)
+    {
+#       if defined(SUNOS5SIGS) || defined(IRIX5) || defined(OSF1) \
+            || defined(HURD) || defined(FREEBSD) || defined(NETBSD)
+          struct sigaction act;
+
+          act.sa_handler = h;
+#         ifdef SIGACTION_FLAGS_NODEFER_HACK
+            /* Was necessary for Solaris 2.3 and very temporary */
+            /* NetBSD bugs.                                     */
+            act.sa_flags = SA_RESTART | SA_NODEFER;
+#         else
+            act.sa_flags = SA_RESTART;
+#         endif
+
+          (void) sigemptyset(&act.sa_mask);
+#         ifdef GC_IRIX_THREADS
+            /* Older versions have a bug related to retrieving and      */
+            /* and setting a handler at the same time.                  */
+            (void) sigaction(SIGSEGV, 0, &old_segv_act);
+            (void) sigaction(SIGSEGV, &act, 0);
+#         else
+            (void) sigaction(SIGSEGV, &act, &old_segv_act);
+#           if defined(IRIX5) && defined(_sigargs) /* Irix 5.x, not 6.x */ \
+               || defined(HPUX) || defined(HURD) || defined(NETBSD) \
+               || defined(FREEBSD)
+              /* Under Irix 5.x or HP/UX, we may get SIGBUS.    */
+              /* Pthreads doesn't exist under Irix 5.x, so we   */
+              /* don't have to worry in the threads case.       */
+              (void) sigaction(SIGBUS, &act, &old_bus_act);
+#           endif
+#         endif /* !GC_IRIX_THREADS */
+#       else
+          old_segv_handler = signal(SIGSEGV, h);
+#         ifdef SIGBUS
+            old_bus_handler = signal(SIGBUS, h);
+#         endif
+#       endif
+    }
+# endif /* NEED_FIND_LIMIT || UNIX_LIKE */
+
+# if defined(NEED_FIND_LIMIT) \
+     || (defined(USE_PROC_FOR_LIBRARIES) && defined(THREADS))
+  /* Some tools to implement HEURISTIC2 */
+#   define MIN_PAGE_SIZE 256    /* Smallest conceivable page size, bytes */
+
+    /*ARGSUSED*/
+    STATIC void GC_fault_handler(int sig)
+    {
+        LONGJMP(GC_jmp_buf, 1);
+    }
+
+    GC_INNER void GC_setup_temporary_fault_handler(void)
+    {
+        /* Handler is process-wide, so this should only happen in       */
+        /* one thread at a time.                                        */
+        GC_ASSERT(I_HOLD_LOCK());
+        GC_set_and_save_fault_handler(GC_fault_handler);
+    }
+
+    GC_INNER void GC_reset_fault_handler(void)
+    {
+#       if defined(SUNOS5SIGS) || defined(IRIX5) || defined(OSF1) \
+           || defined(HURD) || defined(FREEBSD) || defined(NETBSD)
+          (void) sigaction(SIGSEGV, &old_segv_act, 0);
+#         if defined(IRIX5) && defined(_sigargs) /* Irix 5.x, not 6.x */ \
+             || defined(HPUX) || defined(HURD) || defined(NETBSD) \
+             || defined(FREEBSD)
+              (void) sigaction(SIGBUS, &old_bus_act, 0);
+#         endif
+#       else
+          (void) signal(SIGSEGV, old_segv_handler);
+#         ifdef SIGBUS
+            (void) signal(SIGBUS, old_bus_handler);
+#         endif
+#       endif
+    }
+
+    /* Return the first non-addressable location > p (up) or    */
+    /* the smallest location q s.t. [q,p) is addressable (!up). */
+    /* We assume that p (up) or p-1 (!up) is addressable.       */
+    /* Requires allocation lock.                                */
+    STATIC ptr_t GC_find_limit_with_bound(ptr_t p, GC_bool up, ptr_t bound)
+    {
+        static volatile ptr_t result;
+                /* Safer if static, since otherwise it may not be       */
+                /* preserved across the longjmp.  Can safely be         */
+                /* static since it's only called with the               */
+                /* allocation lock held.                                */
+
+        GC_ASSERT(I_HOLD_LOCK());
+        GC_setup_temporary_fault_handler();
+        if (SETJMP(GC_jmp_buf) == 0) {
+            result = (ptr_t)(((word)(p))
+                              & ~(MIN_PAGE_SIZE-1));
+            for (;;) {
+                if (up) {
+                    result += MIN_PAGE_SIZE;
+                    if (result >= bound) {
+                      result = bound;
+                      break;
+                    }
+                } else {
+                    result -= MIN_PAGE_SIZE;
+                    if (result <= bound) {
+                      result = bound - MIN_PAGE_SIZE;
+                                        /* This is to compensate        */
+                                        /* further result increment (we */
+                                        /* do not modify "up" variable  */
+                                        /* since it might be clobbered  */
+                                        /* by setjmp otherwise).        */
+                      break;
+                    }
+                }
+                GC_noop1((word)(*result));
+            }
+        }
+        GC_reset_fault_handler();
+        if (!up) {
+            result += MIN_PAGE_SIZE;
+        }
+        return(result);
+    }
+
+    ptr_t GC_find_limit(ptr_t p, GC_bool up)
+    {
+        return GC_find_limit_with_bound(p, up, up ? (ptr_t)(word)(-1) : 0);
+    }
+# endif /* NEED_FIND_LIMIT || USE_PROC_FOR_LIBRARIES */
+
+#ifdef HPUX_STACKBOTTOM
+
+#include <sys/param.h>
+#include <sys/pstat.h>
+
+  GC_INNER ptr_t GC_get_register_stack_base(void)
+  {
+    struct pst_vm_status vm_status;
+
+    int i = 0;
+    while (pstat_getprocvm(&vm_status, sizeof(vm_status), 0, i++) == 1) {
+      if (vm_status.pst_type == PS_RSESTACK) {
+        return (ptr_t) vm_status.pst_vaddr;
+      }
+    }
+
+    /* old way to get the register stackbottom */
+    return (ptr_t)(((word)GC_stackbottom - BACKING_STORE_DISPLACEMENT - 1)
+                   & ~(BACKING_STORE_ALIGNMENT - 1));
+  }
+
+#endif /* HPUX_STACK_BOTTOM */
+
+#ifdef LINUX_STACKBOTTOM
+
+# include <sys/types.h>
+# include <sys/stat.h>
+
+# define STAT_SKIP 27   /* Number of fields preceding startstack        */
+                        /* field in /proc/self/stat                     */
+
+# ifdef USE_LIBC_PRIVATES
+#   pragma weak __libc_stack_end
+    extern ptr_t __libc_stack_end;
+# endif
+
+# ifdef IA64
+#   ifdef USE_LIBC_PRIVATES
+#     pragma weak __libc_ia64_register_backing_store_base
+      extern ptr_t __libc_ia64_register_backing_store_base;
+#   endif
+
+    GC_INNER ptr_t GC_get_register_stack_base(void)
+    {
+      ptr_t result;
+
+#     ifdef USE_LIBC_PRIVATES
+        if (0 != &__libc_ia64_register_backing_store_base
+            && 0 != __libc_ia64_register_backing_store_base) {
+          /* Glibc 2.2.4 has a bug such that for dynamically linked     */
+          /* executables __libc_ia64_register_backing_store_base is     */
+          /* defined but uninitialized during constructor calls.        */
+          /* Hence we check for both nonzero address and value.         */
+          return __libc_ia64_register_backing_store_base;
+        }
+#     endif
+      result = backing_store_base_from_proc();
+      if (0 == result) {
+          result = GC_find_limit(GC_save_regs_in_stack(), FALSE);
+          /* Now seems to work better than constant displacement        */
+          /* heuristic used in 6.X versions.  The latter seems to       */
+          /* fail for 2.6 kernels.                                      */
+      }
+      return result;
+    }
+# endif /* IA64 */
+
+  STATIC ptr_t GC_linux_main_stack_base(void)
+  {
+    /* We read the stack base value from /proc/self/stat.  We do this   */
+    /* using direct I/O system calls in order to avoid calling malloc   */
+    /* in case REDIRECT_MALLOC is defined.                              */
+#   ifndef STAT_READ
+      /* Also defined in pthread_support.c. */
+#     define STAT_BUF_SIZE 4096
+#     define STAT_READ read
+#   endif
+          /* Should probably call the real read, if read is wrapped.    */
+    char stat_buf[STAT_BUF_SIZE];
+    int f;
+    word result;
+    int i, buf_offset = 0, len;
+
+    /* First try the easy way.  This should work for glibc 2.2  */
+    /* This fails in a prelinked ("prelink" command) executable */
+    /* since the correct value of __libc_stack_end never        */
+    /* becomes visible to us.  The second test works around     */
+    /* this.                                                    */
+#   ifdef USE_LIBC_PRIVATES
+      if (0 != &__libc_stack_end && 0 != __libc_stack_end ) {
+#       if defined(IA64)
+          /* Some versions of glibc set the address 16 bytes too        */
+          /* low while the initialization code is running.              */
+          if (((word)__libc_stack_end & 0xfff) + 0x10 < 0x1000) {
+            return __libc_stack_end + 0x10;
+          } /* Otherwise it's not safe to add 16 bytes and we fall      */
+            /* back to using /proc.                                     */
+#       elif defined(SPARC)
+          /* Older versions of glibc for 64-bit Sparc do not set
+           * this variable correctly, it gets set to either zero
+           * or one.
+           */
+          if (__libc_stack_end != (ptr_t) (unsigned long)0x1)
+            return __libc_stack_end;
+#       else
+          return __libc_stack_end;
+#       endif
+      }
+#   endif
+    f = open("/proc/self/stat", O_RDONLY);
+    if (f < 0)
+      ABORT("Couldn't read /proc/self/stat");
+    len = STAT_READ(f, stat_buf, STAT_BUF_SIZE);
+    close(f);
+
+    /* Skip the required number of fields.  This number is hopefully    */
+    /* constant across all Linux implementations.                       */
+    for (i = 0; i < STAT_SKIP; ++i) {
+      while (buf_offset < len && isspace(stat_buf[buf_offset++])) {
+        /* empty */
+      }
+      while (buf_offset < len && !isspace(stat_buf[buf_offset++])) {
+        /* empty */
+      }
+    }
+    /* Skip spaces.     */
+    while (buf_offset < len && isspace(stat_buf[buf_offset])) {
+      buf_offset++;
+    }
+    /* Find the end of the number and cut the buffer there.     */
+    for (i = 0; buf_offset + i < len; i++) {
+      if (!isdigit(stat_buf[buf_offset + i])) break;
+    }
+    if (buf_offset + i >= len) ABORT("Could not parse /proc/self/stat");
+    stat_buf[buf_offset + i] = '\0';
+
+    result = (word)STRTOULL(&stat_buf[buf_offset], NULL, 10);
+    if (result < 0x100000 || (result & (sizeof(word) - 1)) != 0)
+      ABORT("Absurd stack bottom value");
+    return (ptr_t)result;
+  }
+#endif /* LINUX_STACKBOTTOM */
+
+#ifdef FREEBSD_STACKBOTTOM
+  /* This uses an undocumented sysctl call, but at least one expert     */
+  /* believes it will stay.                                             */
+
+# include <unistd.h>
+# include <sys/types.h>
+# include <sys/sysctl.h>
+
+  STATIC ptr_t GC_freebsd_main_stack_base(void)
+  {
+    int nm[2] = {CTL_KERN, KERN_USRSTACK};
+    ptr_t base;
+    size_t len = sizeof(ptr_t);
+    int r = sysctl(nm, 2, &base, &len, NULL, 0);
+    if (r) ABORT("Error getting main stack base");
+    return base;
+  }
+#endif /* FREEBSD_STACKBOTTOM */
+
+#if defined(ECOS) || defined(NOSYS)
+  ptr_t GC_get_main_stack_base(void)
+  {
+    return STACKBOTTOM;
+  }
+# define GET_MAIN_STACKBASE_SPECIAL
+#elif !defined(BEOS) && !defined(AMIGA) && !defined(OS2) \
+      && !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32) \
+      && !defined(GC_OPENBSD_THREADS) \
+      && (!defined(GC_SOLARIS_THREADS) || defined(_STRICT_STDC))
+
+# if defined(LINUX) && defined(USE_GET_STACKBASE_FOR_MAIN)
+#   include <pthread.h>
+# elif defined(DARWIN) && !defined(NO_PTHREAD_GET_STACKADDR_NP)
+    /* We could use pthread_get_stackaddr_np even in case of a  */
+    /* single-threaded gclib (there is no -lpthread on Darwin). */
+#   include <pthread.h>
+#   undef STACKBOTTOM
+#   define STACKBOTTOM (ptr_t)pthread_get_stackaddr_nppthread_get_stackaddr_np(pthread_self())
+# elif defined(NAUT)
+#   include <nautilus/thread.h>
+#   define STACKBOTTOM ((ptr_t)(get_cur_thread()->stack + get_cur_thread()->stack_size - sizeof(uint64_t)))
+# endif
+
+  ptr_t GC_get_main_stack_base(void)
+  {
+    ptr_t result;
+#   if defined(LINUX) && !defined(NACL) \
+       && (defined(USE_GET_STACKBASE_FOR_MAIN) \
+           || (defined(THREADS) && !defined(REDIRECT_MALLOC)))
+      pthread_attr_t attr;
+      void *stackaddr;
+      size_t size;
+
+      if (pthread_getattr_np(pthread_self(), &attr) == 0) {
+        if (pthread_attr_getstack(&attr, &stackaddr, &size) == 0
+            && stackaddr != NULL) {
+          pthread_attr_destroy(&attr);
+#         ifdef STACK_GROWS_DOWN
+            stackaddr = (char *)stackaddr + size;
+#         endif
+          return (ptr_t)stackaddr;
+        }
+        pthread_attr_destroy(&attr);
+      }
+      WARN("pthread_getattr_np or pthread_attr_getstack failed"
+           " for main thread\n", 0);
+#   endif
+#   ifdef STACKBOTTOM
+      result = STACKBOTTOM;
+#   else
+#     define STACKBOTTOM_ALIGNMENT_M1 ((word)STACK_GRAN - 1)
+#     ifdef HEURISTIC1
+#       ifdef STACK_GROWS_DOWN
+          result = (ptr_t)(((word)GC_approx_sp() + STACKBOTTOM_ALIGNMENT_M1)
+                           & ~STACKBOTTOM_ALIGNMENT_M1);
+#       else
+          result = (ptr_t)((word)GC_approx_sp() & ~STACKBOTTOM_ALIGNMENT_M1);
+#       endif
+#     endif /* HEURISTIC1 */
+#     ifdef LINUX_STACKBOTTOM
+         result = GC_linux_main_stack_base();
+#     endif
+#     ifdef FREEBSD_STACKBOTTOM
+         result = GC_freebsd_main_stack_base();
+#     endif
+#     ifdef HEURISTIC2
+        {
+          ptr_t sp = GC_approx_sp();
+#         ifdef STACK_GROWS_DOWN
+            result = GC_find_limit(sp, TRUE);
+#           ifdef HEURISTIC2_LIMIT
+              if (result > HEURISTIC2_LIMIT
+                  && sp < HEURISTIC2_LIMIT) {
+                result = HEURISTIC2_LIMIT;
+              }
+#           endif
+#         else
+            result = GC_find_limit(sp, FALSE);
+#           ifdef HEURISTIC2_LIMIT
+              if (result < HEURISTIC2_LIMIT
+                  && sp > HEURISTIC2_LIMIT) {
+                result = HEURISTIC2_LIMIT;
+              }
+#           endif
+#         endif
+        }
+#     endif /* HEURISTIC2 */
+#     ifdef STACK_GROWS_DOWN
+        if (result == 0)
+          result = (ptr_t)(signed_word)(-sizeof(ptr_t));
+#     endif
+#   endif
+        GC_ASSERT(GC_approx_sp() HOTTER_THAN result);
+    return(result);
+  }
+# define GET_MAIN_STACKBASE_SPECIAL
+#endif /* !AMIGA, !BEOS, !OPENBSD, !OS2, !Windows */
+
+#if (defined(GC_LINUX_THREADS) || defined(PLATFORM_ANDROID)) && !defined(NACL)
+
+# include <pthread.h>
+  /* extern int pthread_getattr_np(pthread_t, pthread_attr_t *); */
+
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *b)
+  {
+    pthread_attr_t attr;
+    size_t size;
+#   ifdef IA64
+      DCL_LOCK_STATE;
+#   endif
+
+    if (pthread_getattr_np(pthread_self(), &attr) != 0) {
+        WARN("pthread_getattr_np failed\n", 0);
+        return GC_UNIMPLEMENTED;
+    }
+    if (pthread_attr_getstack(&attr, &(b -> mem_base), &size) != 0) {
+        ABORT("pthread_attr_getstack failed");
+    }
+    pthread_attr_destroy(&attr);
+#   ifdef STACK_GROWS_DOWN
+        b -> mem_base = (char *)(b -> mem_base) + size;
+#   endif
+#   ifdef IA64
+      /* We could try backing_store_base_from_proc, but that's safe     */
+      /* only if no mappings are being asynchronously created.          */
+      /* Subtracting the size from the stack base doesn't work for at   */
+      /* least the main thread.                                         */
+      LOCK();
+      {
+        IF_CANCEL(int cancel_state;)
+        ptr_t bsp;
+        ptr_t next_stack;
+
+        DISABLE_CANCEL(cancel_state);
+        bsp = GC_save_regs_in_stack();
+        next_stack = GC_greatest_stack_base_below(bsp);
+        if (0 == next_stack) {
+          b -> reg_base = GC_find_limit(bsp, FALSE);
+        } else {
+          /* Avoid walking backwards into preceding memory stack and    */
+          /* growing it.                                                */
+          b -> reg_base = GC_find_limit_with_bound(bsp, FALSE, next_stack);
+        }
+        RESTORE_CANCEL(cancel_state);
+      }
+      UNLOCK();
+#   endif
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* GC_LINUX_THREADS */
+
+#if defined(GC_DARWIN_THREADS) && !defined(NO_PTHREAD_GET_STACKADDR_NP)
+# include <pthread.h>
+
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *b)
+  {
+    /* pthread_get_stackaddr_np() should return stack bottom (highest   */
+    /* stack address plus 1).                                           */
+    b->mem_base = pthread_get_stackaddr_np(pthread_self());
+    GC_ASSERT((void *)GC_approx_sp() HOTTER_THAN b->mem_base);
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* GC_DARWIN_THREADS */
+
+#ifdef GC_OPENBSD_THREADS
+# include <sys/signal.h>
+# include <pthread.h>
+# include <pthread_np.h>
+
+  /* Find the stack using pthread_stackseg_np(). */
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+  {
+    stack_t stack;
+    if (pthread_stackseg_np(pthread_self(), &stack))
+      ABORT("pthread_stackseg_np(self) failed");
+    sb->mem_base = stack.ss_sp;
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* GC_OPENBSD_THREADS */
+
+#if defined(GC_SOLARIS_THREADS) && !defined(_STRICT_STDC)
+
+# include <thread.h>
+# include <signal.h>
+# include <pthread.h>
+
+  /* These variables are used to cache ss_sp value for the primordial   */
+  /* thread (it's better not to call thr_stksegment() twice for this    */
+  /* thread - see JDK bug #4352906).                                    */
+  static pthread_t stackbase_main_self = 0;
+                        /* 0 means stackbase_main_ss_sp value is unset. */
+  static void *stackbase_main_ss_sp = NULL;
+
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *b)
+  {
+    stack_t s;
+    pthread_t self = pthread_self();
+
+    if (self == stackbase_main_self)
+      {
+        /* If the client calls GC_get_stack_base() from the main thread */
+        /* then just return the cached value.                           */
+        b -> mem_base = stackbase_main_ss_sp;
+        GC_ASSERT(b -> mem_base != NULL);
+        return GC_SUCCESS;
+      }
+
+    if (thr_stksegment(&s)) {
+      /* According to the manual, the only failure error code returned  */
+      /* is EAGAIN meaning "the information is not available due to the */
+      /* thread is not yet completely initialized or it is an internal  */
+      /* thread" - this shouldn't happen here.                          */
+      ABORT("thr_stksegment failed");
+    }
+    /* s.ss_sp holds the pointer to the stack bottom. */
+    GC_ASSERT((void *)GC_approx_sp() HOTTER_THAN s.ss_sp);
+
+    if (!stackbase_main_self && thr_main() != 0)
+      {
+        /* Cache the stack base value for the primordial thread (this   */
+        /* is done during GC_init, so there is no race).                */
+        stackbase_main_ss_sp = s.ss_sp;
+        stackbase_main_self = self;
+      }
+
+    b -> mem_base = s.ss_sp;
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* GC_SOLARIS_THREADS */
+
+#ifdef GC_RTEMS_PTHREADS
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *sb)
+  {
+    sb->mem_base = rtems_get_stack_bottom();
+    return GC_SUCCESS;
+  }
+# define HAVE_GET_STACK_BASE
+#endif /* GC_RTEMS_PTHREADS */
+
+#ifndef HAVE_GET_STACK_BASE
+  /* Retrieve stack base.                                               */
+  /* Using the GC_find_limit version is risky.                          */
+  /* On IA64, for example, there is no guard page between the           */
+  /* stack of one thread and the register backing store of the          */
+  /* next.  Thus this is likely to identify way too large a             */
+  /* "stack" and thus at least result in disastrous performance.        */
+  /* FIXME - Implement better strategies here.                          */
+  GC_API int GC_CALL GC_get_stack_base(struct GC_stack_base *b)
+  {
+#   ifdef NEED_FIND_LIMIT
+      IF_CANCEL(int cancel_state;)
+      DCL_LOCK_STATE;
+
+      LOCK();
+      DISABLE_CANCEL(cancel_state);  /* May be unnecessary? */
+#     ifdef STACK_GROWS_DOWN
+        b -> mem_base = GC_find_limit(GC_approx_sp(), TRUE);
+#       ifdef IA64
+          b -> reg_base = GC_find_limit(GC_save_regs_in_stack(), FALSE);
+#       endif
+#     else
+        b -> mem_base = GC_find_limit(GC_approx_sp(), FALSE);
+#     endif
+      RESTORE_CANCEL(cancel_state);
+      UNLOCK();
+      return GC_SUCCESS;
+#   else
+      return GC_UNIMPLEMENTED;
+#   endif
+  }
+#endif /* !HAVE_GET_STACK_BASE */
+
+#ifndef GET_MAIN_STACKBASE_SPECIAL
+  /* This is always called from the main thread.  Default implementation. */
+  ptr_t GC_get_main_stack_base(void)
+  {
+    struct GC_stack_base sb;
+
+    if (GC_get_stack_base(&sb) != GC_SUCCESS)
+      ABORT("GC_get_stack_base failed");
+    GC_ASSERT((void *)GC_approx_sp() HOTTER_THAN sb.mem_base);
+    return (ptr_t)sb.mem_base;
+  }
+#endif /* !GET_MAIN_STACKBASE_SPECIAL */
+
+/* Register static data segment(s) as roots.  If more data segments are */
+/* added later then they need to be registered at that point (as we do  */
+/* with SunOS dynamic loading), or GC_mark_roots needs to check for     */
+/* them (as we do with PCR).  Called with allocator lock held.          */
+# ifdef OS2
+#include<signal.h>
+void GC_register_data_segments(void)
+{
+    PTIB ptib;
+    PPIB ppib;
+    HMODULE module_handle;
+#   define PBUFSIZ 512
+    UCHAR path[PBUFSIZ];
+    FILE * myexefile;
+    struct exe_hdr hdrdos;      /* MSDOS header.        */
+    struct e32_exe hdr386;      /* Real header for my executable */
+    struct o32_obj seg;         /* Current segment */
+    int nsegs;
+    /** tmp */
+#error dd
+
+    if (DosGetInfoBlocks(&ptib, &ppib) != NO_ERROR) {
+        ABORT("DosGetInfoBlocks failed");
+    }
+    module_handle = ppib -> pib_hmte;
+    if (DosQueryModuleName(module_handle, PBUFSIZ, path) != NO_ERROR) {
+        GC_err_printf("DosQueryModuleName failed\n");
+        ABORT("DosGetInfoBlocks failed");
+    }
+    myexefile = fopen(path, "rb");
+    if (myexefile == 0) {
+        if (GC_print_stats) {
+            GC_err_puts("Couldn't open executable ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Failed to open executable");
+    }
+    if (fread((char *)(&hdrdos), 1, sizeof(hdrdos), myexefile)
+          < sizeof(hdrdos)) {
+        if (GC_print_stats) {
+            GC_err_puts("Couldn't read MSDOS header from ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Couldn't read MSDOS header");
+    }
+    if (E_MAGIC(hdrdos) != EMAGIC) {
+        if (GC_print_stats) {
+            GC_err_puts("Executable has wrong DOS magic number: ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Bad DOS magic number");
+    }
+    if (fseek(myexefile, E_LFANEW(hdrdos), SEEK_SET) != 0) {
+        if (GC_print_stats) {
+            GC_err_puts("Seek to new header failed in ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Bad DOS magic number");
+    }
+    if (fread((char *)(&hdr386), 1, sizeof(hdr386), myexefile)
+          < sizeof(hdr386)) {
+        if (GC_print_stats) {
+            GC_err_puts("Couldn't read MSDOS header from ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Couldn't read OS/2 header");
+    }
+    if (E32_MAGIC1(hdr386) != E32MAGIC1 || E32_MAGIC2(hdr386) != E32MAGIC2) {
+        if (GC_print_stats) {
+            GC_err_puts("Executable has wrong OS/2 magic number: ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Bad OS/2 magic number");
+    }
+    if (E32_BORDER(hdr386) != E32LEBO || E32_WORDER(hdr386) != E32LEWO) {
+        if (GC_print_stats) {
+            GC_err_puts("Executable has wrong byte order: ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Bad byte order");
+    }
+    if (E32_CPU(hdr386) == E32CPU286) {
+        if (GC_print_stats) {
+            GC_err_puts("GC can't handle 80286 executables: ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Intel 80286 executables are unsupported");
+    }
+    if (fseek(myexefile, E_LFANEW(hdrdos) + E32_OBJTAB(hdr386),
+              SEEK_SET) != 0) {
+        if (GC_print_stats) {
+            GC_err_puts("Seek to object table failed: ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Seek to object table failed");
+    }
+    for (nsegs = E32_OBJCNT(hdr386); nsegs > 0; nsegs--) {
+      int flags;
+      if (fread((char *)(&seg), 1, sizeof(seg), myexefile) < sizeof(seg)) {
+        if (GC_print_stats) {
+            GC_err_puts("Couldn't read obj table entry from ");
+            GC_err_puts(path);
+            GC_err_puts("\n");
+        }
+        ABORT("Couldn't read obj table entry");
+      }
+      flags = O32_FLAGS(seg);
+      if (!(flags & OBJWRITE)) continue;
+      if (!(flags & OBJREAD)) continue;
+      if (flags & OBJINVALID) {
+          GC_err_printf("Object with invalid pages?\n");
+          continue;
+      }
+      GC_add_roots_inner((ptr_t)O32_BASE(seg),
+                         (ptr_t)(O32_BASE(seg)+O32_SIZE(seg)), FALSE);
+    }
+    (void)fclose(myexefile);
+}
+
+# else /* !OS2 */
+
+# if defined(GWW_VDB)
+#   ifndef MEM_WRITE_WATCH
+#     define MEM_WRITE_WATCH 0x200000
+#   endif
+#   ifndef WRITE_WATCH_FLAG_RESET
+#     define WRITE_WATCH_FLAG_RESET 1
+#   endif
+
+    /* Since we can't easily check whether ULONG_PTR and SIZE_T are     */
+    /* defined in Win32 basetsd.h, we define own ULONG_PTR.             */
+#   define GC_ULONG_PTR word
+
+    typedef UINT (WINAPI * GetWriteWatch_type)(
+                                DWORD, PVOID, GC_ULONG_PTR /* SIZE_T */,
+                                PVOID *, GC_ULONG_PTR *, PULONG);
+    static GetWriteWatch_type GetWriteWatch_func;
+    static DWORD GetWriteWatch_alloc_flag;
+
+#   define GC_GWW_AVAILABLE() (GetWriteWatch_func != NULL)
+
+    static void detect_GetWriteWatch(void)
+    {
+      static GC_bool done;
+      HMODULE hK32;
+      if (done)
+        return;
+
+#     if defined(MPROTECT_VDB)
+        {
+          char * str = GETENV("GC_USE_GETWRITEWATCH");
+#         if defined(GC_PREFER_MPROTECT_VDB)
+            if (str == NULL || (*str == '0' && *(str + 1) == '\0')) {
+              /* GC_USE_GETWRITEWATCH is unset or set to "0".           */
+              done = TRUE; /* falling back to MPROTECT_VDB strategy.    */
+              /* This should work as if GWW_VDB is undefined. */
+              return;
+            }
+#         else
+            if (str != NULL && *str == '0' && *(str + 1) == '\0') {
+              /* GC_USE_GETWRITEWATCH is set "0".                       */
+              done = TRUE; /* falling back to MPROTECT_VDB strategy.    */
+              return;
+            }
+#         endif
+        }
+#     endif
+
+      hK32 = GetModuleHandle(TEXT("kernel32.dll"));
+      if (hK32 != (HMODULE)0 &&
+          (GetWriteWatch_func = (GetWriteWatch_type)GetProcAddress(hK32,
+                                                "GetWriteWatch")) != NULL) {
+        /* Also check whether VirtualAlloc accepts MEM_WRITE_WATCH,   */
+        /* as some versions of kernel32.dll have one but not the      */
+        /* other, making the feature completely broken.               */
+        void * page = VirtualAlloc(NULL, GC_page_size,
+                                    MEM_WRITE_WATCH | MEM_RESERVE,
+                                    PAGE_READWRITE);
+        if (page != NULL) {
+          PVOID pages[16];
+          GC_ULONG_PTR count = 16;
+          DWORD page_size;
+          /* Check that it actually works.  In spite of some            */
+          /* documentation it actually seems to exist on W2K.           */
+          /* This test may be unnecessary, but ...                      */
+          if (GetWriteWatch_func(WRITE_WATCH_FLAG_RESET,
+                                 page, GC_page_size,
+                                 pages,
+                                 &count,
+                                 &page_size) != 0) {
+            /* GetWriteWatch always fails. */
+            GetWriteWatch_func = NULL;
+          } else {
+            GetWriteWatch_alloc_flag = MEM_WRITE_WATCH;
+          }
+          VirtualFree(page, 0 /* dwSize */, MEM_RELEASE);
+        } else {
+          /* GetWriteWatch will be useless. */
+          GetWriteWatch_func = NULL;
+        }
+      }
+      if (GC_print_stats) {
+        if (GetWriteWatch_func == NULL) {
+          GC_log_printf("Did not find a usable GetWriteWatch()\n");
+        } else {
+          GC_log_printf("Using GetWriteWatch()\n");
+        }
+      }
+      done = TRUE;
+    }
+
+# else
+#   define GetWriteWatch_alloc_flag 0
+# endif /* !GWW_VDB */
+
+# if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+
+# ifdef MSWIN32
+  /* Unfortunately, we have to handle win32s very differently from NT,  */
+  /* Since VirtualQuery has very different semantics.  In particular,   */
+  /* under win32s a VirtualQuery call on an unmapped page returns an    */
+  /* invalid result.  Under NT, GC_register_data_segments is a no-op    */
+  /* and all real work is done by GC_register_dynamic_libraries.  Under */
+  /* win32s, we cannot find the data segments associated with dll's.    */
+  /* We register the main data segment here.                            */
+  GC_INNER GC_bool GC_no_win32_dlls = FALSE;
+        /* This used to be set for gcc, to avoid dealing with           */
+        /* the structured exception handling issues.  But we now have   */
+        /* assembly code to do that right.                              */
+
+  GC_INNER GC_bool GC_wnt = FALSE;
+         /* This is a Windows NT derivative, i.e. NT, W2K, XP or later. */
+
+  GC_INNER void GC_init_win32(void)
+  {
+    /* Set GC_wnt.  If we're running under win32s, assume that no DLLs  */
+    /* will be loaded.  I doubt anyone still runs win32s, but...        */
+    DWORD v = GetVersion();
+    GC_wnt = !(v & 0x80000000);
+    GC_no_win32_dlls |= ((!GC_wnt) && (v & 0xff) <= 3);
+#   ifdef USE_MUNMAP
+      if (GC_no_win32_dlls) {
+        /* Turn off unmapping for safety (since may not work well with  */
+        /* GlobalAlloc).                                                */
+        GC_unmap_threshold = 0;
+      }
+#   endif
+  }
+
+  /* Return the smallest address a such that VirtualQuery               */
+  /* returns correct results for all addresses between a and start.     */
+  /* Assumes VirtualQuery returns correct information for start.        */
+  STATIC ptr_t GC_least_described_address(ptr_t start)
+  {
+    MEMORY_BASIC_INFORMATION buf;
+    size_t result;
+    LPVOID limit;
+    ptr_t p;
+    LPVOID q;
+
+    limit = GC_sysinfo.lpMinimumApplicationAddress;
+    p = (ptr_t)((word)start & ~(GC_page_size - 1));
+    for (;;) {
+        q = (LPVOID)(p - GC_page_size);
+        if ((ptr_t)q > (ptr_t)p /* underflow */ || q < limit) break;
+        result = VirtualQuery(q, &buf, sizeof(buf));
+        if (result != sizeof(buf) || buf.AllocationBase == 0) break;
+        p = (ptr_t)(buf.AllocationBase);
+    }
+    return p;
+  }
+# endif /* MSWIN32 */
+
+# ifndef REDIRECT_MALLOC
+  /* We maintain a linked list of AllocationBase values that we know    */
+  /* correspond to malloc heap sections.  Currently this is only called */
+  /* during a GC.  But there is some hope that for long running         */
+  /* programs we will eventually see most heap sections.                */
+
+  /* In the long run, it would be more reliable to occasionally walk    */
+  /* the malloc heap with HeapWalk on the default heap.  But that       */
+  /* apparently works only for NT-based Windows.                        */
+
+  STATIC size_t GC_max_root_size = 100000; /* Appr. largest root size.  */
+
+# ifndef CYGWIN32
+  /* In the long run, a better data structure would also be nice ...    */
+  STATIC struct GC_malloc_heap_list {
+    void * allocation_base;
+    struct GC_malloc_heap_list *next;
+  } *GC_malloc_heap_l = 0;
+
+  /* Is p the base of one of the malloc heap sections we already know   */
+  /* about?                                                             */
+  STATIC GC_bool GC_is_malloc_heap_base(ptr_t p)
+  {
+    struct GC_malloc_heap_list *q = GC_malloc_heap_l;
+
+    while (0 != q) {
+      if (q -> allocation_base == p) return TRUE;
+      q = q -> next;
+    }
+    return FALSE;
+  }
+
+  STATIC void *GC_get_allocation_base(void *p)
+  {
+    MEMORY_BASIC_INFORMATION buf;
+    size_t result = VirtualQuery(p, &buf, sizeof(buf));
+    if (result != sizeof(buf)) {
+      ABORT("Weird VirtualQuery result");
+    }
+    return buf.AllocationBase;
+  }
+
+  GC_INNER void GC_add_current_malloc_heap(void)
+  {
+    struct GC_malloc_heap_list *new_l =
+                 malloc(sizeof(struct GC_malloc_heap_list));
+    void * candidate = GC_get_allocation_base(new_l);
+
+    if (new_l == 0) return;
+    if (GC_is_malloc_heap_base(candidate)) {
+      /* Try a little harder to find malloc heap.                       */
+        size_t req_size = 10000;
+        do {
+          void *p = malloc(req_size);
+          if (0 == p) {
+            free(new_l);
+            return;
+          }
+          candidate = GC_get_allocation_base(p);
+          free(p);
+          req_size *= 2;
+        } while (GC_is_malloc_heap_base(candidate)
+                 && req_size < GC_max_root_size/10 && req_size < 500000);
+        if (GC_is_malloc_heap_base(candidate)) {
+          free(new_l);
+          return;
+        }
+    }
+    if (GC_print_stats)
+      GC_log_printf("Found new system malloc AllocationBase at %p\n",
+                    candidate);
+    new_l -> allocation_base = candidate;
+    new_l -> next = GC_malloc_heap_l;
+    GC_malloc_heap_l = new_l;
+  }
+# endif /* !CYGWIN32 */
+
+# endif /* !REDIRECT_MALLOC */
+
+  STATIC word GC_n_heap_bases = 0;      /* See GC_heap_bases.   */
+
+  /* Is p the start of either the malloc heap, or of one of our */
+  /* heap sections?                                             */
+  GC_INNER GC_bool GC_is_heap_base(ptr_t p)
+  {
+     unsigned i;
+#    ifndef REDIRECT_MALLOC
+       if (GC_root_size > GC_max_root_size) GC_max_root_size = GC_root_size;
+#      ifndef CYGWIN32
+         if (GC_is_malloc_heap_base(p)) return TRUE;
+#      endif
+#    endif
+     for (i = 0; i < GC_n_heap_bases; i++) {
+         if (GC_heap_bases[i] == p) return TRUE;
+     }
+     return FALSE;
+  }
+
+#ifdef MSWIN32
+  STATIC void GC_register_root_section(ptr_t static_root)
+  {
+      MEMORY_BASIC_INFORMATION buf;
+      size_t result;
+      DWORD protect;
+      LPVOID p;
+      char * base;
+      char * limit, * new_limit;
+
+      if (!GC_no_win32_dlls) return;
+      p = base = limit = GC_least_described_address(static_root);
+      while (p < GC_sysinfo.lpMaximumApplicationAddress) {
+        result = VirtualQuery(p, &buf, sizeof(buf));
+        if (result != sizeof(buf) || buf.AllocationBase == 0
+            || GC_is_heap_base(buf.AllocationBase)) break;
+        new_limit = (char *)p + buf.RegionSize;
+        protect = buf.Protect;
+        if (buf.State == MEM_COMMIT
+            && is_writable(protect)) {
+            if ((char *)p == limit) {
+                limit = new_limit;
+            } else {
+                if (base != limit) GC_add_roots_inner(base, limit, FALSE);
+                base = p;
+                limit = new_limit;
+            }
+        }
+        if (p > (LPVOID)new_limit /* overflow */) break;
+        p = (LPVOID)new_limit;
+      }
+      if (base != limit) GC_add_roots_inner(base, limit, FALSE);
+  }
+#endif /* MSWIN32 */
+
+  void GC_register_data_segments(void)
+  {
+#   ifdef MSWIN32
+      GC_register_root_section((ptr_t)&GC_pages_executable);
+                            /* any other GC global variable would fit too. */
+#   endif
+  }
+
+# else /* !OS2 && !Windows */
+
+# if (defined(SVR4) || defined(AUX) || defined(DGUX) \
+      || (defined(LINUX) && defined(SPARC))) && !defined(PCR)
+  ptr_t GC_SysVGetDataStart(size_t max_page_size, ptr_t etext_addr)
+  {
+    word text_end = ((word)(etext_addr) + sizeof(word) - 1)
+                    & ~(sizeof(word) - 1);
+        /* etext rounded to word boundary       */
+    word next_page = ((text_end + (word)max_page_size - 1)
+                      & ~((word)max_page_size - 1));
+    word page_offset = (text_end & ((word)max_page_size - 1));
+    char * volatile result = (char *)(next_page + page_offset);
+    /* Note that this isn't equivalent to just adding           */
+    /* max_page_size to &etext if &etext is at a page boundary  */
+
+    GC_setup_temporary_fault_handler();
+    if (SETJMP(GC_jmp_buf) == 0) {
+        /* Try writing to the address.  */
+        *result = *result;
+        GC_reset_fault_handler();
+    } else {
+        GC_reset_fault_handler();
+        /* We got here via a longjmp.  The address is not readable.     */
+        /* This is known to happen under Solaris 2.4 + gcc, which place */
+        /* string constants in the text segment, but after etext.       */
+        /* Use plan B.  Note that we now know there is a gap between    */
+        /* text and data segments, so plan A bought us something.       */
+        result = (char *)GC_find_limit((ptr_t)(DATAEND), FALSE);
+    }
+    return((ptr_t)result);
+  }
+# endif
+
+# if defined(FREEBSD) && !defined(PCR) && (defined(I386) || defined(X86_64) \
+                                || defined(powerpc) || defined(__powerpc__))
+
+/* Its unclear whether this should be identical to the above, or        */
+/* whether it should apply to non-X86 architectures.                    */
+/* For now we don't assume that there is always an empty page after     */
+/* etext.  But in some cases there actually seems to be slightly more.  */
+/* This also deals with holes between read-only data and writable data. */
+ptr_t GC_FreeBSDGetDataStart(size_t max_page_size, ptr_t etext_addr)
+{
+    word text_end = ((word)(etext_addr) + sizeof(word) - 1)
+                     & ~(sizeof(word) - 1);
+        /* etext rounded to word boundary       */
+    volatile word next_page = (text_end + (word)max_page_size - 1)
+                              & ~((word)max_page_size - 1);
+    volatile ptr_t result = (ptr_t)text_end;
+    GC_setup_temporary_fault_handler();
+    if (SETJMP(GC_jmp_buf) == 0) {
+        /* Try reading at the address.                          */
+        /* This should happen before there is another thread.   */
+        for (; next_page < (word)(DATAEND); next_page += (word)max_page_size)
+            *(volatile char *)next_page;
+        GC_reset_fault_handler();
+    } else {
+        GC_reset_fault_handler();
+        /* As above, we go to plan B    */
+        result = GC_find_limit((ptr_t)(DATAEND), FALSE);
+    }
+    return(result);
+}
+
+# endif /* FREEBSD */
+
+
+#ifdef AMIGA
+
+#  define GC_AMIGA_DS
+#  include "extra/AmigaOS.c"
+#  undef GC_AMIGA_DS
+
+#elif defined(OPENBSD)
+
+/* Depending on arch alignment, there can be multiple holes     */
+/* between DATASTART and DATAEND.  Scan in DATASTART .. DATAEND */
+/* and register each region.                                    */
+void GC_register_data_segments(void)
+{
+  ptr_t region_start = DATASTART;
+  ptr_t region_end;
+
+  for (;;) {
+    region_end = GC_find_limit_openbsd(region_start, DATAEND);
+    GC_add_roots_inner(region_start, region_end, FALSE);
+    if (region_end >= DATAEND)
+      break;
+    region_start = GC_skip_hole_openbsd(region_end, DATAEND);
+  }
+}
+
+# else /* !OS2 && !Windows && !AMIGA && !OPENBSD */
+
+void GC_register_data_segments(void)
+{
+#   if !defined(PCR) && !defined(MACOS)
+#     if defined(REDIRECT_MALLOC) && defined(GC_SOLARIS_THREADS)
+        /* As of Solaris 2.3, the Solaris threads implementation        */
+        /* allocates the data structure for the initial thread with     */
+        /* sbrk at process startup.  It needs to be scanned, so that    */
+        /* we don't lose some malloc allocated data structures          */
+        /* hanging from it.  We're on thin ice here ...                 */
+        extern caddr_t sbrk(int);
+
+        GC_add_roots_inner(DATASTART, (ptr_t)sbrk(0), FALSE);
+#     else
+        GC_add_roots_inner(DATASTART, (ptr_t)(DATAEND), FALSE);
+#       if defined(DATASTART2)
+          GC_add_roots_inner(DATASTART2, (ptr_t)(DATAEND2), FALSE);
+#       endif
+#     endif
+#   endif
+#   if defined(MACOS)
+    {
+#   if defined(THINK_C)
+        extern void* GC_MacGetDataStart(void);
+        /* globals begin above stack and end at a5. */
+        GC_add_roots_inner((ptr_t)GC_MacGetDataStart(),
+                           (ptr_t)LMGetCurrentA5(), FALSE);
+#   else
+#     if defined(__MWERKS__)
+#       if !__POWERPC__
+          extern void* GC_MacGetDataStart(void);
+          /* MATTHEW: Function to handle Far Globals (CW Pro 3) */
+#         if __option(far_data)
+          extern void* GC_MacGetDataEnd(void);
+#         endif
+          /* globals begin above stack and end at a5. */
+          GC_add_roots_inner((ptr_t)GC_MacGetDataStart(),
+                             (ptr_t)LMGetCurrentA5(), FALSE);
+          /* MATTHEW: Handle Far Globals */
+#         if __option(far_data)
+      /* Far globals follow he QD globals: */
+          GC_add_roots_inner((ptr_t)LMGetCurrentA5(),
+                             (ptr_t)GC_MacGetDataEnd(), FALSE);
+#         endif
+#       else
+          extern char __data_start__[], __data_end__[];
+          GC_add_roots_inner((ptr_t)&__data_start__,
+                             (ptr_t)&__data_end__, FALSE);
+#       endif /* __POWERPC__ */
+#     endif /* __MWERKS__ */
+#   endif /* !THINK_C */
+    }
+#   endif /* MACOS */
+
+    /* Dynamic libraries are added at every collection, since they may  */
+    /* change.                                                          */
+}
+
+# endif  /* ! AMIGA */
+# endif  /* ! MSWIN32 && ! MSWINCE*/
+# endif  /* ! OS2 */
+
+/*
+ * Auxiliary routines for obtaining memory from OS.
+ */
+
+# if !defined(OS2) && !defined(PCR) && !defined(AMIGA) && !defined(MSWIN32) \
+     && !defined(MSWINCE) && !defined(MACOS) && !defined(DOS4GW) \
+     && !defined(NONSTOP) && !defined(SN_TARGET_PS3) && !defined(RTEMS) \
+     && !defined(__CC_ARM)
+
+# define SBRK_ARG_T ptrdiff_t
+
+#if defined(MMAP_SUPPORTED)
+
+#ifdef USE_MMAP_FIXED
+#   define GC_MMAP_FLAGS MAP_FIXED | MAP_PRIVATE
+        /* Seems to yield better performance on Solaris 2, but can      */
+        /* be unreliable if something is already mapped at the address. */
+#else
+#   define GC_MMAP_FLAGS MAP_PRIVATE
+#endif
+
+#ifdef USE_MMAP_ANON
+# define zero_fd -1
+# if defined(MAP_ANONYMOUS)
+#   define OPT_MAP_ANON MAP_ANONYMOUS
+# else
+#   define OPT_MAP_ANON MAP_ANON
+# endif
+#else
+  static int zero_fd;
+# define OPT_MAP_ANON 0
+#endif
+
+#ifndef HEAP_START
+#   define HEAP_START ((ptr_t)0)
+#endif
+
+STATIC ptr_t GC_unix_mmap_get_mem(word bytes)
+{
+    void *result;
+    static ptr_t last_addr = HEAP_START;
+
+#   ifndef USE_MMAP_ANON
+      static GC_bool initialized = FALSE;
+
+      if (!initialized) {
+          zero_fd = open("/dev/zero", O_RDONLY);
+          if (zero_fd == -1)
+            ABORT("Could not open /dev/zero");
+
+          fcntl(zero_fd, F_SETFD, FD_CLOEXEC);
+          initialized = TRUE;
+      }
+#   endif
+
+    if (bytes & (GC_page_size - 1)) ABORT("Bad GET_MEM arg");
+    result = mmap(last_addr, bytes, (PROT_READ | PROT_WRITE)
+                                    | (GC_pages_executable ? PROT_EXEC : 0),
+                  GC_MMAP_FLAGS | OPT_MAP_ANON, zero_fd, 0/* offset */);
+#   undef IGNORE_PAGES_EXECUTABLE
+
+    if (result == MAP_FAILED) return(0);
+    last_addr = (ptr_t)result + bytes + GC_page_size - 1;
+    last_addr = (ptr_t)((word)last_addr & ~(GC_page_size - 1));
+#   if !defined(LINUX)
+      if (last_addr == 0) {
+        /* Oops.  We got the end of the address space.  This isn't      */
+        /* usable by arbitrary C code, since one-past-end pointers      */
+        /* don't work, so we discard it and try again.                  */
+        munmap(result, (size_t)(-GC_page_size) - (size_t)result);
+                        /* Leave last page mapped, so we can't repeat.  */
+        return GC_unix_mmap_get_mem(bytes);
+      }
+#   else
+      GC_ASSERT(last_addr != 0);
+#   endif
+    return((ptr_t)result);
+}
+
+# endif  /* MMAP_SUPPORTED */
+
+#if defined(USE_MMAP)
+  ptr_t GC_unix_get_mem(word bytes)
+  {
+    return GC_unix_mmap_get_mem(bytes);
+  }
+#else /* !USE_MMAP */
+
+#ifndef NAUT 
+STATIC ptr_t GC_unix_sbrk_get_mem(word bytes)
+{
+  ptr_t result;
+# ifdef IRIX5
+    /* Bare sbrk isn't thread safe.  Play by malloc rules.      */
+    /* The equivalent may be needed on other systems as well.   */
+    __LOCK_MALLOC();
+# endif
+  {
+    ptr_t cur_brk = (ptr_t)sbrk(0);
+    SBRK_ARG_T lsbs = (word)cur_brk & (GC_page_size-1);
+
+    if ((SBRK_ARG_T)bytes < 0) {
+        result = 0; /* too big */
+        goto out;
+    }
+    if (lsbs != 0) {
+        if((ptr_t)sbrk(GC_page_size - lsbs) == (ptr_t)(-1)) {
+            result = 0;
+            goto out;
+        }
+    }
+#   ifdef ADD_HEAP_GUARD_PAGES
+      /* This is useful for catching severe memory overwrite problems that */
+      /* span heap sections.  It shouldn't otherwise be turned on.         */
+      {
+        ptr_t guard = (ptr_t)sbrk((SBRK_ARG_T)GC_page_size);
+        if (mprotect(guard, GC_page_size, PROT_NONE) != 0)
+            ABORT("ADD_HEAP_GUARD_PAGES: mprotect failed");
+      }
+#   endif /* ADD_HEAP_GUARD_PAGES */
+    result = (ptr_t)sbrk((SBRK_ARG_T)bytes);
+    if (result == (ptr_t)(-1)) result = 0;
+  }
+ out:
+# ifdef IRIX5
+    __UNLOCK_MALLOC();
+# endif
+  return(result);
+}
+#endif
+
+#ifndef NAUT
+ptr_t GC_unix_get_mem(word bytes)
+{
+# if defined(MMAP_SUPPORTED)
+    /* By default, we try both sbrk and mmap, in that order.    */
+    static GC_bool sbrk_failed = FALSE;
+    ptr_t result = 0;
+
+    if (!sbrk_failed) result = GC_unix_sbrk_get_mem(bytes);
+    if (0 == result) {
+        sbrk_failed = TRUE;
+        result = GC_unix_mmap_get_mem(bytes);
+    }
+    if (0 == result) {
+        /* Try sbrk again, in case sbrk memory became available.        */
+        result = GC_unix_sbrk_get_mem(bytes);
+    }
+    return result;
+# else /* !MMAP_SUPPORTED */
+    return GC_unix_sbrk_get_mem(bytes);
+# endif
+}
+#endif
+
+#endif /* !USE_MMAP */
+
+# endif /* UN*X */
+
+# ifdef OS2
+
+void * os2_alloc(size_t bytes)
+{
+    void * result;
+
+    if (DosAllocMem(&result, bytes, (PAG_READ | PAG_WRITE | PAG_COMMIT)
+                                    | (GC_pages_executable ? PAG_EXECUTE : 0))
+                    != NO_ERROR) {
+        return(0);
+    }
+    /* FIXME: What's the purpose of this recursion?  (Probably, if      */
+    /* DosAllocMem returns memory at 0 address then just retry once.)   */
+    if (result == 0) return(os2_alloc(bytes));
+    return(result);
+}
+
+# endif /* OS2 */
+
+# if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+    GC_INNER SYSTEM_INFO GC_sysinfo;
+# endif
+
+#ifdef MSWIN32
+
+# ifdef USE_GLOBAL_ALLOC
+#   define GLOBAL_ALLOC_TEST 1
+# else
+#   define GLOBAL_ALLOC_TEST GC_no_win32_dlls
+# endif
+
+# ifdef GC_USE_MEM_TOP_DOWN
+    STATIC DWORD GC_mem_top_down = MEM_TOP_DOWN;
+                           /* Use GC_USE_MEM_TOP_DOWN for better 64-bit */
+                           /* testing.  Otherwise all addresses tend to */
+                           /* end up in first 4GB, hiding bugs.         */
+# else
+    STATIC DWORD GC_mem_top_down = 0;
+# endif
+
+#endif /* MSWIN32 */
+
+#if defined(MSWIN32) || defined(CYGWIN32)
+  ptr_t GC_win32_get_mem(word bytes)
+  {
+    ptr_t result;
+
+# ifdef CYGWIN32
+    result = GC_unix_get_mem(bytes);
+# else
+    if (GLOBAL_ALLOC_TEST) {
+        /* VirtualAlloc doesn't like PAGE_EXECUTE_READWRITE.    */
+        /* There are also unconfirmed rumors of other           */
+        /* problems, so we dodge the issue.                     */
+        result = (ptr_t) GlobalAlloc(0, bytes + HBLKSIZE);
+        result = (ptr_t)(((word)result + HBLKSIZE - 1) & ~(HBLKSIZE-1));
+    } else {
+        /* VirtualProtect only works on regions returned by a   */
+        /* single VirtualAlloc call.  Thus we allocate one      */
+        /* extra page, which will prevent merging of blocks     */
+        /* in separate regions, and eliminate any temptation    */
+        /* to call VirtualProtect on a range spanning regions.  */
+        /* This wastes a small amount of memory, and risks      */
+        /* increased fragmentation.  But better alternatives    */
+        /* would require effort.                                */
+#       ifdef MPROTECT_VDB
+          /* We can't check for GC_incremental here (because    */
+          /* GC_enable_incremental() might be called some time  */
+          /* later after the GC initialization).                */
+#         ifdef GWW_VDB
+#           define VIRTUAL_ALLOC_PAD (GC_GWW_AVAILABLE() ? 0 : 1)
+#         else
+#           define VIRTUAL_ALLOC_PAD 1
+#         endif
+#       else
+#         define VIRTUAL_ALLOC_PAD 0
+#       endif
+        /* Pass the MEM_WRITE_WATCH only if GetWriteWatch-based */
+        /* VDBs are enabled and the GetWriteWatch function is   */
+        /* available.  Otherwise we waste resources or possibly */
+        /* cause VirtualAlloc to fail (observed in Windows 2000 */
+        /* SP2).                                                */
+        result = (ptr_t) VirtualAlloc(NULL, bytes + VIRTUAL_ALLOC_PAD,
+                                GetWriteWatch_alloc_flag
+                                | (MEM_COMMIT | MEM_RESERVE)
+                                | GC_mem_top_down,
+                                GC_pages_executable ? PAGE_EXECUTE_READWRITE :
+                                                      PAGE_READWRITE);
+#       undef IGNORE_PAGES_EXECUTABLE
+    }
+# endif /* !CYGWIN32 */
+    if (HBLKDISPL(result) != 0) ABORT("Bad VirtualAlloc result");
+        /* If I read the documentation correctly, this can      */
+        /* only happen if HBLKSIZE > 64k or not a power of 2.   */
+    if (GC_n_heap_bases >= MAX_HEAP_SECTS) ABORT("Too many heap sections");
+    if (0 != result) GC_heap_bases[GC_n_heap_bases++] = result;
+    return(result);
+  }
+
+  GC_API void GC_CALL GC_win32_free_heap(void)
+  {
+#   ifndef CYGWIN32
+      if (GLOBAL_ALLOC_TEST)
+#   endif
+    {
+      while (GC_n_heap_bases-- > 0) {
+#       ifdef CYGWIN32
+          /* FIXME: Is it ok to use non-GC free() here? */
+#       else
+          GlobalFree(GC_heap_bases[GC_n_heap_bases]);
+#       endif
+        GC_heap_bases[GC_n_heap_bases] = 0;
+      }
+    } /* else */
+#   ifndef CYGWIN32
+      else {
+        /* Avoiding VirtualAlloc leak. */
+        while (GC_n_heap_bases > 0) {
+          VirtualFree(GC_heap_bases[--GC_n_heap_bases], 0, MEM_RELEASE);
+          GC_heap_bases[GC_n_heap_bases] = 0;
+        }
+      }
+#   endif
+  }
+#endif /* MSWIN32 || CYGWIN32 */
+
+#ifdef AMIGA
+# define GC_AMIGA_AM
+# include "extra/AmigaOS.c"
+# undef GC_AMIGA_AM
+#endif
+
+
+#ifdef MSWINCE
+  ptr_t GC_wince_get_mem(word bytes)
+  {
+    ptr_t result = 0; /* initialized to prevent warning. */
+    word i;
+
+    /* Round up allocation size to multiple of page size */
+    bytes = (bytes + GC_page_size-1) & ~(GC_page_size-1);
+
+    /* Try to find reserved, uncommitted pages */
+    for (i = 0; i < GC_n_heap_bases; i++) {
+        if (((word)(-(signed_word)GC_heap_lengths[i])
+             & (GC_sysinfo.dwAllocationGranularity-1))
+            >= bytes) {
+            result = GC_heap_bases[i] + GC_heap_lengths[i];
+            break;
+        }
+    }
+
+    if (i == GC_n_heap_bases) {
+        /* Reserve more pages */
+        word res_bytes = (bytes + GC_sysinfo.dwAllocationGranularity-1)
+                         & ~(GC_sysinfo.dwAllocationGranularity-1);
+        /* If we ever support MPROTECT_VDB here, we will probably need to    */
+        /* ensure that res_bytes is strictly > bytes, so that VirtualProtect */
+        /* never spans regions.  It seems to be OK for a VirtualFree         */
+        /* argument to span regions, so we should be OK for now.             */
+        result = (ptr_t) VirtualAlloc(NULL, res_bytes,
+                                MEM_RESERVE | MEM_TOP_DOWN,
+                                GC_pages_executable ? PAGE_EXECUTE_READWRITE :
+                                                      PAGE_READWRITE);
+        if (HBLKDISPL(result) != 0) ABORT("Bad VirtualAlloc result");
+            /* If I read the documentation correctly, this can  */
+            /* only happen if HBLKSIZE > 64k or not a power of 2.       */
+        if (GC_n_heap_bases >= MAX_HEAP_SECTS) ABORT("Too many heap sections");
+        if (result == NULL) return NULL;
+        GC_heap_bases[GC_n_heap_bases] = result;
+        GC_heap_lengths[GC_n_heap_bases] = 0;
+        GC_n_heap_bases++;
+    }
+
+    /* Commit pages */
+    result = (ptr_t) VirtualAlloc(result, bytes, MEM_COMMIT,
+                              GC_pages_executable ? PAGE_EXECUTE_READWRITE :
+                                                    PAGE_READWRITE);
+#   undef IGNORE_PAGES_EXECUTABLE
+
+    if (result != NULL) {
+        if (HBLKDISPL(result) != 0) ABORT("Bad VirtualAlloc result");
+        GC_heap_lengths[i] += bytes;
+    }
+
+    return(result);
+  }
+#endif
+
+#ifdef USE_MUNMAP
+
+/* For now, this only works on Win32/WinCE and some Unix-like   */
+/* systems.  If you have something else, don't define           */
+/* USE_MUNMAP.                                                  */
+
+#if !defined(MSWIN32) && !defined(MSWINCE)
+
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+
+#endif
+
+/* Compute a page aligned starting address for the unmap        */
+/* operation on a block of size bytes starting at start.        */
+/* Return 0 if the block is too small to make this feasible.    */
+STATIC ptr_t GC_unmap_start(ptr_t start, size_t bytes)
+{
+    ptr_t result;
+    /* Round start to next page boundary.       */
+    result = (ptr_t)((word)(start + GC_page_size - 1) & ~(GC_page_size - 1));
+    if (result + GC_page_size > start + bytes) return 0;
+    return result;
+}
+
+/* Compute end address for an unmap operation on the indicated  */
+/* block.                                                       */
+STATIC ptr_t GC_unmap_end(ptr_t start, size_t bytes)
+{
+    return (ptr_t)((word)(start + bytes) & ~(GC_page_size - 1));
+}
+
+/* Under Win32/WinCE we commit (map) and decommit (unmap)       */
+/* memory using VirtualAlloc and VirtualFree.  These functions  */
+/* work on individual allocations of virtual memory, made       */
+/* previously using VirtualAlloc with the MEM_RESERVE flag.     */
+/* The ranges we need to (de)commit may span several of these   */
+/* allocations; therefore we use VirtualQuery to check          */
+/* allocation lengths, and split up the range as necessary.     */
+
+/* We assume that GC_remap is called on exactly the same range  */
+/* as a previous call to GC_unmap.  It is safe to consistently  */
+/* round the endpoints in both places.                          */
+GC_INNER void GC_unmap(ptr_t start, size_t bytes)
+{
+    ptr_t start_addr = GC_unmap_start(start, bytes);
+    ptr_t end_addr = GC_unmap_end(start, bytes);
+    word len = end_addr - start_addr;
+    if (0 == start_addr) return;
+#   if defined(MSWIN32) || defined(MSWINCE)
+      while (len != 0) {
+          MEMORY_BASIC_INFORMATION mem_info;
+          GC_word free_len;
+          if (VirtualQuery(start_addr, &mem_info, sizeof(mem_info))
+              != sizeof(mem_info))
+              ABORT("Weird VirtualQuery result");
+          free_len = (len < mem_info.RegionSize) ? len : mem_info.RegionSize;
+          if (!VirtualFree(start_addr, free_len, MEM_DECOMMIT))
+              ABORT("VirtualFree failed");
+          GC_unmapped_bytes += free_len;
+          start_addr += free_len;
+          len -= free_len;
+      }
+#   else
+      /* We immediately remap it to prevent an intervening mmap from    */
+      /* accidentally grabbing the same address space.                  */
+      {
+        void * result;
+        result = mmap(start_addr, len, PROT_NONE,
+                      MAP_PRIVATE | MAP_FIXED | OPT_MAP_ANON,
+                      zero_fd, 0/* offset */);
+        if (result != (void *)start_addr)
+          ABORT("mmap(PROT_NONE) failed");
+      }
+      GC_unmapped_bytes += len;
+#   endif
+}
+
+GC_INNER void GC_remap(ptr_t start, size_t bytes)
+{
+    ptr_t start_addr = GC_unmap_start(start, bytes);
+    ptr_t end_addr = GC_unmap_end(start, bytes);
+    word len = end_addr - start_addr;
+
+    /* FIXME: Handle out-of-memory correctly (at least for Win32)       */
+#   if defined(MSWIN32) || defined(MSWINCE)
+      ptr_t result;
+
+      if (0 == start_addr) return;
+      while (len != 0) {
+          MEMORY_BASIC_INFORMATION mem_info;
+          GC_word alloc_len;
+          if (VirtualQuery(start_addr, &mem_info, sizeof(mem_info))
+              != sizeof(mem_info))
+              ABORT("Weird VirtualQuery result");
+          alloc_len = (len < mem_info.RegionSize) ? len : mem_info.RegionSize;
+          result = VirtualAlloc(start_addr, alloc_len, MEM_COMMIT,
+                                GC_pages_executable ? PAGE_EXECUTE_READWRITE :
+                                                      PAGE_READWRITE);
+          if (result != start_addr) {
+              if (GetLastError() == ERROR_NOT_ENOUGH_MEMORY ||
+                  GetLastError() == ERROR_OUTOFMEMORY) {
+                  ABORT("Not enough memory to process remapping");
+              } else {
+                  ABORT("VirtualAlloc remapping failed");
+              }
+          }
+          GC_unmapped_bytes -= alloc_len;
+          start_addr += alloc_len;
+          len -= alloc_len;
+      }
+#   else
+      /* It was already remapped with PROT_NONE. */
+      int result;
+      if (0 == start_addr) return;
+
+#     ifndef NACL
+        result = mprotect(start_addr, len, (PROT_READ | PROT_WRITE)
+                                    | (GC_pages_executable ? PROT_EXEC : 0));
+#     else
+        {
+          /* NaCl does not expose mprotect, but mmap should work fine.  */
+          void *mmap_result = mmap(start_addr, len, (PROT_READ | PROT_WRITE)
+                                    | (GC_pages_executable ? PROT_EXEC : 0),
+                                   MAP_PRIVATE | MAP_FIXED | OPT_MAP_ANON,
+                                   zero_fd, 0 /* offset */);
+          if (mmap_result != (void *)start_addr)
+            ABORT("mmap as mprotect failed");
+          /* Fake the return value as if mprotect succeeded.    */
+          result = 0;
+        }
+#     endif /* NACL */
+#     undef IGNORE_PAGES_EXECUTABLE
+
+      if (result != 0) {
+        if (GC_print_stats)
+          GC_log_printf("Mprotect failed at %p (length %lu) with errno %d\n",
+                        start_addr, (unsigned long)len, errno);
+        ABORT("mprotect remapping failed");
+      }
+      GC_unmapped_bytes -= len;
+#   endif
+}
+
+/* Two adjacent blocks have already been unmapped and are about to      */
+/* be merged.  Unmap the whole block.  This typically requires          */
+/* that we unmap a small section in the middle that was not previously  */
+/* unmapped due to alignment constraints.                               */
+GC_INNER void GC_unmap_gap(ptr_t start1, size_t bytes1, ptr_t start2,
+                           size_t bytes2)
+{
+    ptr_t start1_addr = GC_unmap_start(start1, bytes1);
+    ptr_t end1_addr = GC_unmap_end(start1, bytes1);
+    ptr_t start2_addr = GC_unmap_start(start2, bytes2);
+    ptr_t start_addr = end1_addr;
+    ptr_t end_addr = start2_addr;
+    size_t len;
+    GC_ASSERT(start1 + bytes1 == start2);
+    if (0 == start1_addr) start_addr = GC_unmap_start(start1, bytes1 + bytes2);
+    if (0 == start2_addr) end_addr = GC_unmap_end(start1, bytes1 + bytes2);
+    if (0 == start_addr) return;
+    len = end_addr - start_addr;
+#   if defined(MSWIN32) || defined(MSWINCE)
+      while (len != 0) {
+          MEMORY_BASIC_INFORMATION mem_info;
+          GC_word free_len;
+          if (VirtualQuery(start_addr, &mem_info, sizeof(mem_info))
+              != sizeof(mem_info))
+              ABORT("Weird VirtualQuery result");
+          free_len = (len < mem_info.RegionSize) ? len : mem_info.RegionSize;
+          if (!VirtualFree(start_addr, free_len, MEM_DECOMMIT))
+              ABORT("VirtualFree failed");
+          GC_unmapped_bytes += free_len;
+          start_addr += free_len;
+          len -= free_len;
+      }
+#   else
+      if (len != 0) {
+        /* Immediately remap as above. */
+        void * result;
+        result = mmap(start_addr, len, PROT_NONE,
+                      MAP_PRIVATE | MAP_FIXED | OPT_MAP_ANON,
+                      zero_fd, 0/* offset */);
+        if (result != (void *)start_addr)
+          ABORT("mmap(PROT_NONE) failed");
+      }
+      GC_unmapped_bytes += len;
+#   endif
+}
+
+#endif /* USE_MUNMAP */
+
+/* Routine for pushing any additional roots.  In THREADS        */
+/* environment, this is also responsible for marking from       */
+/* thread stacks.                                               */
+#ifndef THREADS
+  void (*GC_push_other_roots)(void) = 0;
+#else /* THREADS */
+
+# ifdef PCR
+PCR_ERes GC_push_thread_stack(PCR_Th_T *t, PCR_Any dummy)
+{
+    struct PCR_ThCtl_TInfoRep info;
+    PCR_ERes result;
+
+    info.ti_stkLow = info.ti_stkHi = 0;
+    result = PCR_ThCtl_GetInfo(t, &info);
+    GC_push_all_stack((ptr_t)(info.ti_stkLow), (ptr_t)(info.ti_stkHi));
+    return(result);
+}
+
+/* Push the contents of an old object. We treat this as stack   */
+/* data only because that makes it robust against mark stack    */
+/* overflow.                                                    */
+PCR_ERes GC_push_old_obj(void *p, size_t size, PCR_Any data)
+{
+    GC_push_all_stack((ptr_t)p, (ptr_t)p + size);
+    return(PCR_ERes_okay);
+}
+
+extern struct PCR_MM_ProcsRep * GC_old_allocator;
+                                        /* defined in pcr_interface.c.  */
+
+STATIC void GC_default_push_other_roots(void)
+{
+    /* Traverse data allocated by previous memory managers.             */
+          if ((*(GC_old_allocator->mmp_enumerate))(PCR_Bool_false,
+                                                   GC_push_old_obj, 0)
+              != PCR_ERes_okay) {
+              ABORT("Old object enumeration failed");
+          }
+    /* Traverse all thread stacks. */
+        if (PCR_ERes_IsErr(
+                PCR_ThCtl_ApplyToAllOtherThreads(GC_push_thread_stack,0))
+            || PCR_ERes_IsErr(GC_push_thread_stack(PCR_Th_CurrThread(), 0))) {
+          ABORT("Thread stack marking failed");
+        }
+}
+
+# endif /* PCR */
+
+# if defined(GC_PTHREADS) || defined(GC_WIN32_THREADS) || defined(NAUT_THREADS)
+    STATIC void GC_default_push_other_roots(void)
+    {
+      GC_push_all_stacks();
+    }
+# endif /* GC_WIN32_THREADS || GC_PTHREADS */
+
+# ifdef SN_TARGET_PS3
+    STATIC void GC_default_push_other_roots(void)
+    {
+      ABORT("GC_default_push_other_roots is not implemented");
+    }
+
+    void GC_push_thread_structures(void)
+    {
+      ABORT("GC_push_thread_structures is not implemented");
+    }
+# endif /* SN_TARGET_PS3 */
+
+  void (*GC_push_other_roots)(void) = GC_default_push_other_roots;
+#endif /* THREADS */
+
+/*
+ * Routines for accessing dirty bits on virtual pages.
+ * There are six ways to maintain this information:
+ * DEFAULT_VDB: A simple dummy implementation that treats every page
+ *              as possibly dirty.  This makes incremental collection
+ *              useless, but the implementation is still correct.
+ * MANUAL_VDB:  Stacks and static data are always considered dirty.
+ *              Heap pages are considered dirty if GC_dirty(p) has been
+ *              called on some pointer p pointing to somewhere inside
+ *              an object on that page.  A GC_dirty() call on a large
+ *              object directly dirties only a single page, but for
+ *              MANUAL_VDB we are careful to treat an object with a dirty
+ *              page as completely dirty.
+ *              In order to avoid races, an object must be marked dirty
+ *              after it is written, and a reference to the object
+ *              must be kept on a stack or in a register in the interim.
+ *              With threads enabled, an object directly reachable from the
+ *              stack at the time of a collection is treated as dirty.
+ *              In single-threaded mode, it suffices to ensure that no
+ *              collection can take place between the pointer assignment
+ *              and the GC_dirty() call.
+ * PCR_VDB:     Use PPCRs virtual dirty bit facility.
+ * PROC_VDB:    Use the /proc facility for reading dirty bits.  Only
+ *              works under some SVR4 variants.  Even then, it may be
+ *              too slow to be entirely satisfactory.  Requires reading
+ *              dirty bits for entire address space.  Implementations tend
+ *              to assume that the client is a (slow) debugger.
+ * MPROTECT_VDB:Protect pages and then catch the faults to keep track of
+ *              dirtied pages.  The implementation (and implementability)
+ *              is highly system dependent.  This usually fails when system
+ *              calls write to a protected page.  We prevent the read system
+ *              call from doing so.  It is the clients responsibility to
+ *              make sure that other system calls are similarly protected
+ *              or write only to the stack.
+ * GWW_VDB:     Use the Win32 GetWriteWatch functions, if available, to
+ *              read dirty bits.  In case it is not available (because we
+ *              are running on Windows 95, Windows 2000 or earlier),
+ *              MPROTECT_VDB may be defined as a fallback strategy.
+ */
+#ifndef GC_DISABLE_INCREMENTAL
+  GC_INNER GC_bool GC_dirty_maintained = FALSE;
+#endif
+
+#if defined(PROC_VDB) || defined(GWW_VDB)
+  /* Add all pages in pht2 to pht1 */
+  STATIC void GC_or_pages(page_hash_table pht1, page_hash_table pht2)
+  {
+    register unsigned i;
+    for (i = 0; i < PHT_SIZE; i++) pht1[i] |= pht2[i];
+  }
+
+# ifdef MPROTECT_VDB
+    STATIC GC_bool GC_gww_page_was_dirty(struct hblk * h)
+# else
+    GC_INNER GC_bool GC_page_was_dirty(struct hblk * h)
+# endif
+  {
+    register word index;
+    if (HDR(h) == 0)
+      return TRUE;
+    index = PHT_HASH(h);
+    return get_pht_entry_from_index(GC_grungy_pages, index);
+  }
+
+# if defined(CHECKSUMS) || defined(PROC_VDB)
+    /* Used only if GWW_VDB. */
+#   ifdef MPROTECT_VDB
+      STATIC GC_bool GC_gww_page_was_ever_dirty(struct hblk * h)
+#   else
+      GC_INNER GC_bool GC_page_was_ever_dirty(struct hblk * h)
+#   endif
+    {
+      register word index;
+      if (HDR(h) == 0)
+        return TRUE;
+      index = PHT_HASH(h);
+      return get_pht_entry_from_index(GC_written_pages, index);
+    }
+# endif /* CHECKSUMS || PROC_VDB */
+
+# ifndef MPROTECT_VDB
+    /* Ignore write hints.  They don't help us here.    */
+    /*ARGSUSED*/
+    GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                       GC_bool is_ptrfree) {}
+# endif
+
+#endif /* PROC_VDB || GWW_VDB */
+
+#ifdef GWW_VDB
+
+# define GC_GWW_BUF_LEN (MAXHINCR * HBLKSIZE / 4096 /* X86 page size */)
+  /* Still susceptible to overflow, if there are very large allocations, */
+  /* and everything is dirty.                                            */
+  static PVOID gww_buf[GC_GWW_BUF_LEN];
+
+# ifdef MPROTECT_VDB
+    GC_INNER GC_bool GC_gww_dirty_init(void)
+    {
+      detect_GetWriteWatch();
+      return GC_GWW_AVAILABLE();
+    }
+# else
+    GC_INNER void GC_dirty_init(void)
+    {
+      detect_GetWriteWatch();
+      GC_dirty_maintained = GC_GWW_AVAILABLE();
+    }
+# endif /* !MPROTECT_VDB */
+
+# ifdef MPROTECT_VDB
+    STATIC void GC_gww_read_dirty(void)
+# else
+    GC_INNER void GC_read_dirty(void)
+# endif
+  {
+    word i;
+
+    BZERO(GC_grungy_pages, sizeof(GC_grungy_pages));
+
+    for (i = 0; i != GC_n_heap_sects; ++i) {
+      GC_ULONG_PTR count;
+
+      do {
+        PVOID * pages, * pages_end;
+        DWORD page_size;
+
+        pages = gww_buf;
+        count = GC_GWW_BUF_LEN;
+        /* GetWriteWatch is documented as returning non-zero when it    */
+        /* fails, but the documentation doesn't explicitly say why it   */
+        /* would fail or what its behaviour will be if it fails.        */
+        /* It does appear to fail, at least on recent W2K instances, if */
+        /* the underlying memory was not allocated with the appropriate */
+        /* flag.  This is common if GC_enable_incremental is called     */
+        /* shortly after GC initialization.  To avoid modifying the     */
+        /* interface, we silently work around such a failure, it only   */
+        /* affects the initial (small) heap allocation. If there are    */
+        /* more dirty pages than will fit in the buffer, this is not    */
+        /* treated as a failure; we must check the page count in the    */
+        /* loop condition. Since each partial call will reset the       */
+        /* status of some pages, this should eventually terminate even  */
+        /* in the overflow case.                                        */
+        if (GetWriteWatch_func(WRITE_WATCH_FLAG_RESET,
+                               GC_heap_sects[i].hs_start,
+                               GC_heap_sects[i].hs_bytes,
+                               pages,
+                               &count,
+                               &page_size) != 0) {
+          static int warn_count = 0;
+          unsigned j;
+          struct hblk * start = (struct hblk *)GC_heap_sects[i].hs_start;
+          static struct hblk *last_warned = 0;
+          size_t nblocks = divHBLKSZ(GC_heap_sects[i].hs_bytes);
+
+          if ( i != 0 && last_warned != start && warn_count++ < 5) {
+            last_warned = start;
+            WARN(
+              "GC_gww_read_dirty unexpectedly failed at %p: "
+              "Falling back to marking all pages dirty\n", start);
+          }
+          for (j = 0; j < nblocks; ++j) {
+              word hash = PHT_HASH(start + j);
+              set_pht_entry_from_index(GC_grungy_pages, hash);
+          }
+          count = 1;  /* Done with this section. */
+        } else /* succeeded */ {
+          pages_end = pages + count;
+          while (pages != pages_end) {
+            struct hblk * h = (struct hblk *) *pages++;
+            struct hblk * h_end = (struct hblk *) ((char *) h + page_size);
+            do
+              set_pht_entry_from_index(GC_grungy_pages, PHT_HASH(h));
+            while (++h < h_end);
+          }
+        }
+      } while (count == GC_GWW_BUF_LEN);
+      /* FIXME: It's unclear from Microsoft's documentation if this loop */
+      /* is useful.  We suspect the call just fails if the buffer fills  */
+      /* up.  But that should still be handled correctly.                */
+    }
+
+    GC_or_pages(GC_written_pages, GC_grungy_pages);
+  }
+#endif /* GWW_VDB */
+
+#ifdef DEFAULT_VDB
+  /* All of the following assume the allocation lock is held.   */
+
+  /* The client asserts that unallocated pages in the heap are never    */
+  /* written.                                                           */
+
+  /* Initialize virtual dirty bit implementation.       */
+  GC_INNER void GC_dirty_init(void)
+  {
+    if (GC_print_stats == VERBOSE)
+      GC_log_printf("Initializing DEFAULT_VDB...\n");
+    GC_dirty_maintained = TRUE;
+  }
+
+  /* Retrieve system dirty bits for heap to a local buffer.     */
+  /* Restore the systems notion of which pages are dirty.       */
+  GC_INNER void GC_read_dirty(void) {}
+
+  /* Is the HBLKSIZE sized page at h marked dirty in the local buffer?  */
+  /* If the actual page size is different, this returns TRUE if any     */
+  /* of the pages overlapping h are dirty.  This routine may err on the */
+  /* side of labeling pages as dirty (and this implementation does).    */
+  /*ARGSUSED*/
+  GC_INNER GC_bool GC_page_was_dirty(struct hblk *h)
+  {
+    return(TRUE);
+  }
+
+  /* The following two routines are typically less crucial.             */
+  /* They matter most with large dynamic libraries, or if we can't      */
+  /* accurately identify stacks, e.g. under Solaris 2.X.  Otherwise the */
+  /* following default versions are adequate.                           */
+# ifdef CHECKSUMS
+    /* Could any valid GC heap pointer ever have been written to this page? */
+    /*ARGSUSED*/
+    GC_INNER GC_bool GC_page_was_ever_dirty(struct hblk *h)
+    {
+      return(TRUE);
+    }
+# endif /* CHECKSUMS */
+
+  /* A call that:                                         */
+  /* I) hints that [h, h+nblocks) is about to be written. */
+  /* II) guarantees that protection is removed.           */
+  /* (I) may speed up some dirty bit implementations.     */
+  /* (II) may be essential if we need to ensure that      */
+  /* pointer-free system call buffers in the heap are     */
+  /* not protected.                                       */
+  /*ARGSUSED*/
+  GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                     GC_bool is_ptrfree) {}
+#endif /* DEFAULT_VDB */
+
+#ifdef MANUAL_VDB
+  /* Initialize virtual dirty bit implementation.       */
+  GC_INNER void GC_dirty_init(void)
+  {
+    if (GC_print_stats == VERBOSE)
+      GC_log_printf("Initializing MANUAL_VDB...\n");
+    /* GC_dirty_pages and GC_grungy_pages are already cleared.  */
+    GC_dirty_maintained = TRUE;
+  }
+
+  /* Retrieve system dirty bits for heap to a local buffer.     */
+  /* Restore the systems notion of which pages are dirty.       */
+  GC_INNER void GC_read_dirty(void)
+  {
+    BCOPY((word *)GC_dirty_pages, GC_grungy_pages,
+          (sizeof GC_dirty_pages));
+    BZERO((word *)GC_dirty_pages, (sizeof GC_dirty_pages));
+  }
+
+  /* Is the HBLKSIZE sized page at h marked dirty in the local buffer?  */
+  /* If the actual page size is different, this returns TRUE if any     */
+  /* of the pages overlapping h are dirty.  This routine may err on the */
+  /* side of labeling pages as dirty (and this implementation does).    */
+  GC_INNER GC_bool GC_page_was_dirty(struct hblk *h)
+  {
+    register word index = PHT_HASH(h);
+    return(HDR(h) == 0 || get_pht_entry_from_index(GC_grungy_pages, index));
+  }
+
+# define async_set_pht_entry_from_index(db, index) \
+                        set_pht_entry_from_index(db, index) /* for now */
+
+  /* Mark the page containing p as dirty.  Logically, this dirties the  */
+  /* entire object.                                                     */
+  void GC_dirty(ptr_t p)
+  {
+    word index = PHT_HASH(p);
+    async_set_pht_entry_from_index(GC_dirty_pages, index);
+  }
+
+  /*ARGSUSED*/
+  GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                     GC_bool is_ptrfree) {}
+
+# ifdef CHECKSUMS
+    /* Could any valid GC heap pointer ever have been written to this page? */
+    /*ARGSUSED*/
+    GC_INNER GC_bool GC_page_was_ever_dirty(struct hblk *h)
+    {
+      /* FIXME - implement me.  */
+      return(TRUE);
+    }
+# endif /* CHECKSUMS */
+
+#endif /* MANUAL_VDB */
+
+#ifdef MPROTECT_VDB
+  /* See DEFAULT_VDB for interface descriptions.        */
+
+  /*
+   * This implementation maintains dirty bits itself by catching write
+   * faults and keeping track of them.  We assume nobody else catches
+   * SIGBUS or SIGSEGV.  We assume no write faults occur in system calls.
+   * This means that clients must ensure that system calls don't write
+   * to the write-protected heap.  Probably the best way to do this is to
+   * ensure that system calls write at most to pointer-free objects in the
+   * heap, and do even that only if we are on a platform on which those
+   * are not protected.  Another alternative is to wrap system calls
+   * (see example for read below), but the current implementation holds
+   * applications.
+   * We assume the page size is a multiple of HBLKSIZE.
+   * We prefer them to be the same.  We avoid protecting pointer-free
+   * objects only if they are the same.
+   */
+# ifdef DARWIN
+    /* Using vm_protect (mach syscall) over mprotect (BSD syscall) seems to
+       decrease the likelihood of some of the problems described below. */
+#   include <mach/vm_map.h>
+    STATIC mach_port_t GC_task_self = 0;
+#   define PROTECT(addr,len) \
+        if(vm_protect(GC_task_self,(vm_address_t)(addr),(vm_size_t)(len), \
+                      FALSE, VM_PROT_READ \
+                             | (GC_pages_executable ? VM_PROT_EXECUTE : 0)) \
+                != KERN_SUCCESS) { \
+            ABORT("vm_protect(PROTECT) failed"); \
+        }
+#   define UNPROTECT(addr,len) \
+        if(vm_protect(GC_task_self,(vm_address_t)(addr),(vm_size_t)(len), \
+                      FALSE, (VM_PROT_READ | VM_PROT_WRITE) \
+                             | (GC_pages_executable ? VM_PROT_EXECUTE : 0)) \
+                != KERN_SUCCESS) { \
+            ABORT("vm_protect(UNPROTECT) failed"); \
+        }
+
+# elif !defined(MSWIN32) && !defined(MSWINCE)
+#   include <sys/mman.h>
+#   include <signal.h>
+#   include <sys/syscall.h>
+
+#   define PROTECT(addr, len) \
+        if (mprotect((caddr_t)(addr), (size_t)(len), \
+                     PROT_READ \
+                     | (GC_pages_executable ? PROT_EXEC : 0)) < 0) { \
+          ABORT("mprotect failed"); \
+        }
+#   define UNPROTECT(addr, len) \
+        if (mprotect((caddr_t)(addr), (size_t)(len), \
+                     (PROT_READ | PROT_WRITE) \
+                     | (GC_pages_executable ? PROT_EXEC : 0)) < 0) { \
+          ABORT(GC_pages_executable ? "un-mprotect executable page" \
+                                      " failed (probably disabled by OS)" : \
+                              "un-mprotect failed"); \
+        }
+#   undef IGNORE_PAGES_EXECUTABLE
+
+# else /* MSWIN32 */
+#   ifndef MSWINCE
+#     include <signal.h>
+#   endif
+
+    static DWORD protect_junk;
+#   define PROTECT(addr, len) \
+        if (!VirtualProtect((addr), (len), \
+                            GC_pages_executable ? PAGE_EXECUTE_READ : \
+                                                  PAGE_READONLY, \
+                            &protect_junk)) { \
+          if (GC_print_stats) \
+            GC_log_printf("Last error code: 0x%lx\n", (long)GetLastError()); \
+          ABORT("VirtualProtect failed"); \
+        }
+#   define UNPROTECT(addr, len) \
+        if (!VirtualProtect((addr), (len), \
+                            GC_pages_executable ? PAGE_EXECUTE_READWRITE : \
+                                                  PAGE_READWRITE, \
+                            &protect_junk)) { \
+          ABORT("un-VirtualProtect failed"); \
+        }
+# endif /* MSWIN32 || MSWINCE || DARWIN */
+
+# if defined(MSWIN32)
+    typedef LPTOP_LEVEL_EXCEPTION_FILTER SIG_HNDLR_PTR;
+#   undef SIG_DFL
+#   define SIG_DFL (LPTOP_LEVEL_EXCEPTION_FILTER)((signed_word)-1)
+# elif defined(MSWINCE)
+    typedef LONG (WINAPI *SIG_HNDLR_PTR)(struct _EXCEPTION_POINTERS *);
+#   undef SIG_DFL
+#   define SIG_DFL (SIG_HNDLR_PTR) (-1)
+# elif defined(DARWIN)
+    typedef void (* SIG_HNDLR_PTR)();
+# else
+    typedef void (* SIG_HNDLR_PTR)(int, siginfo_t *, void *);
+    typedef void (* PLAIN_HNDLR_PTR)(int);
+# endif
+
+# if defined(__GLIBC__)
+#   if __GLIBC__ < 2 || __GLIBC__ == 2 && __GLIBC_MINOR__ < 2
+#       error glibc too old?
+#   endif
+# endif
+
+#ifndef DARWIN
+  STATIC SIG_HNDLR_PTR GC_old_segv_handler = 0;
+                        /* Also old MSWIN32 ACCESS_VIOLATION filter */
+# if !defined(MSWIN32) && !defined(MSWINCE)
+    STATIC SIG_HNDLR_PTR GC_old_bus_handler = 0;
+    STATIC GC_bool GC_old_bus_handler_used_si = FALSE;
+    STATIC GC_bool GC_old_segv_handler_used_si = FALSE;
+# endif
+#endif /* !DARWIN */
+
+#if defined(THREADS)
+/* We need to lock around the bitmap update in the write fault handler  */
+/* in order to avoid the risk of losing a bit.  We do this with a       */
+/* test-and-set spin lock if we know how to do that.  Otherwise we      */
+/* check whether we are already in the handler and use the dumb but     */
+/* safe fallback algorithm of setting all bits in the word.             */
+/* Contention should be very rare, so we do the minimum to handle it    */
+/* correctly.                                                           */
+#ifdef AO_HAVE_test_and_set_acquire
+  GC_INNER volatile AO_TS_t GC_fault_handler_lock = AO_TS_INITIALIZER;
+  static void async_set_pht_entry_from_index(volatile page_hash_table db,
+                                             size_t index)
+  {
+    while (AO_test_and_set_acquire(&GC_fault_handler_lock) == AO_TS_SET) {
+      /* empty */
+    }
+    /* Could also revert to set_pht_entry_from_index_safe if initial    */
+    /* GC_test_and_set fails.                                           */
+    set_pht_entry_from_index(db, index);
+    AO_CLEAR(&GC_fault_handler_lock);
+  }
+#else /* !AO_HAVE_test_and_set_acquire */
+# error No test_and_set operation: Introduces a race.
+  /* THIS WOULD BE INCORRECT!                                           */
+  /* The dirty bit vector may be temporarily wrong,                     */
+  /* just before we notice the conflict and correct it. We may end up   */
+  /* looking at it while it's wrong.  But this requires contention      */
+  /* exactly when a GC is triggered, which seems far less likely to     */
+  /* fail than the old code, which had no reported failures.  Thus we   */
+  /* leave it this way while we think of something better, or support   */
+  /* GC_test_and_set on the remaining platforms.                        */
+  static volatile word currently_updating = 0;
+  static void async_set_pht_entry_from_index(volatile page_hash_table db,
+                                             size_t index)
+  {
+    unsigned int update_dummy;
+    currently_updating = (word)(&update_dummy);
+    set_pht_entry_from_index(db, index);
+    /* If we get contention in the 10 or so instruction window here,    */
+    /* and we get stopped by a GC between the two updates, we lose!     */
+    if (currently_updating != (word)(&update_dummy)) {
+        set_pht_entry_from_index_safe(db, index);
+        /* We claim that if two threads concurrently try to update the  */
+        /* dirty bit vector, the first one to execute UPDATE_START      */
+        /* will see it changed when UPDATE_END is executed.  (Note that */
+        /* &update_dummy must differ in two distinct threads.)  It      */
+        /* will then execute set_pht_entry_from_index_safe, thus        */
+        /* returning us to a safe state, though not soon enough.        */
+    }
+  }
+#endif /* !AO_HAVE_test_and_set_acquire */
+#else /* !THREADS */
+# define async_set_pht_entry_from_index(db, index) \
+                        set_pht_entry_from_index(db, index)
+#endif /* !THREADS */
+
+#ifdef CHECKSUMS
+  void GC_record_fault(struct hblk * h); /* from checksums.c */
+#endif
+
+#ifndef DARWIN
+
+# if !defined(MSWIN32) && !defined(MSWINCE)
+#   include <errno.h>
+#   if defined(FREEBSD) || defined(HURD) || defined(HPUX)
+#     define SIG_OK (sig == SIGBUS || sig == SIGSEGV)
+#   else
+#     define SIG_OK (sig == SIGSEGV)
+#   endif
+#   if defined(FREEBSD)
+#     ifndef SEGV_ACCERR
+#       define SEGV_ACCERR 2
+#     endif
+#     if defined(POWERPC)
+#      define AIM      /* Pretend that we're AIM. */
+#      include <machine/trap.h>
+#       define CODE_OK (si -> si_code == EXC_DSI \
+          || si -> si_code == SEGV_ACCERR)
+#     else
+#       define CODE_OK (si -> si_code == BUS_PAGE_FAULT \
+          || si -> si_code == SEGV_ACCERR)
+#     endif
+#   elif defined(OSF1)
+#     define CODE_OK (si -> si_code == 2 /* experimentally determined */)
+#   elif defined(IRIX5)
+#     define CODE_OK (si -> si_code == EACCES)
+#   elif defined(HURD)
+#     define CODE_OK TRUE
+#   elif defined(LINUX)
+#     define CODE_OK TRUE
+      /* Empirically c.trapno == 14, on IA32, but is that useful?       */
+      /* Should probably consider alignment issues on other             */
+      /* architectures.                                                 */
+#   elif defined(HPUX)
+#     define CODE_OK (si -> si_code == SEGV_ACCERR \
+                      || si -> si_code == BUS_ADRERR \
+                      || si -> si_code == BUS_UNKNOWN \
+                      || si -> si_code == SEGV_UNKNOWN \
+                      || si -> si_code == BUS_OBJERR)
+#   elif defined(SUNOS5SIGS)
+#     define CODE_OK (si -> si_code == SEGV_ACCERR)
+#   endif
+#   ifndef NO_GETCONTEXT
+#     include <ucontext.h>
+#   endif
+    /*ARGSUSED*/
+    STATIC void GC_write_fault_handler(int sig, siginfo_t *si, void *raw_sc)
+# else
+#   define SIG_OK (exc_info -> ExceptionRecord -> ExceptionCode \
+                     == STATUS_ACCESS_VIOLATION)
+#   define CODE_OK (exc_info -> ExceptionRecord -> ExceptionInformation[0] \
+                      == 1) /* Write fault */
+    STATIC LONG WINAPI GC_write_fault_handler(
+                                struct _EXCEPTION_POINTERS *exc_info)
+# endif /* MSWIN32 || MSWINCE */
+  {
+#   if !defined(MSWIN32) && !defined(MSWINCE)
+        char *addr = si -> si_addr;
+#   else
+        char * addr = (char *) (exc_info -> ExceptionRecord
+                                -> ExceptionInformation[1]);
+#   endif
+    unsigned i;
+
+    if (SIG_OK && CODE_OK) {
+        register struct hblk * h =
+                        (struct hblk *)((word)addr & ~(GC_page_size-1));
+        GC_bool in_allocd_block;
+#       ifdef CHECKSUMS
+          GC_record_fault(h);
+#       endif
+
+#       ifdef SUNOS5SIGS
+            /* Address is only within the correct physical page.        */
+            in_allocd_block = FALSE;
+            for (i = 0; i < divHBLKSZ(GC_page_size); i++) {
+              if (HDR(h+i) != 0) {
+                in_allocd_block = TRUE;
+                break;
+              }
+            }
+#       else
+            in_allocd_block = (HDR(addr) != 0);
+#       endif
+        if (!in_allocd_block) {
+            /* FIXME - We should make sure that we invoke the   */
+            /* old handler with the appropriate calling         */
+            /* sequence, which often depends on SA_SIGINFO.     */
+
+            /* Heap blocks now begin and end on page boundaries */
+            SIG_HNDLR_PTR old_handler;
+
+#           if defined(MSWIN32) || defined(MSWINCE)
+                old_handler = GC_old_segv_handler;
+#           else
+                GC_bool used_si;
+
+                if (sig == SIGSEGV) {
+                   old_handler = GC_old_segv_handler;
+                   used_si = GC_old_segv_handler_used_si;
+                } else {
+                   old_handler = GC_old_bus_handler;
+                   used_si = GC_old_bus_handler_used_si;
+                }
+#           endif
+
+            if (old_handler == (SIG_HNDLR_PTR)SIG_DFL) {
+#               if !defined(MSWIN32) && !defined(MSWINCE)
+                    if (GC_print_stats)
+                      GC_log_printf("Unexpected segfault at %p\n", addr);
+                    ABORT("Unexpected bus error or segmentation fault");
+#               else
+                    return(EXCEPTION_CONTINUE_SEARCH);
+#               endif
+            } else {
+                /*
+                 * FIXME: This code should probably check if the
+                 * old signal handler used the traditional style and
+                 * if so call it using that style.
+                 */
+#               if defined(MSWIN32) || defined(MSWINCE)
+                    return((*old_handler)(exc_info));
+#               else
+                    if (used_si)
+                      ((SIG_HNDLR_PTR)old_handler) (sig, si, raw_sc);
+                    else
+                      /* FIXME: should pass nonstandard args as well. */
+                      ((PLAIN_HNDLR_PTR)old_handler) (sig);
+                    return;
+#               endif
+            }
+        }
+        UNPROTECT(h, GC_page_size);
+        /* We need to make sure that no collection occurs between       */
+        /* the UNPROTECT and the setting of the dirty bit.  Otherwise   */
+        /* a write by a third thread might go unnoticed.  Reversing     */
+        /* the order is just as bad, since we would end up unprotecting */
+        /* a page in a GC cycle during which it's not marked.           */
+        /* Currently we do this by disabling the thread stopping        */
+        /* signals while this handler is running.  An alternative might */
+        /* be to record the fact that we're about to unprotect, or      */
+        /* have just unprotected a page in the GC's thread structure,   */
+        /* and then to have the thread stopping code set the dirty      */
+        /* flag, if necessary.                                          */
+        for (i = 0; i < divHBLKSZ(GC_page_size); i++) {
+            size_t index = PHT_HASH(h+i);
+
+            async_set_pht_entry_from_index(GC_dirty_pages, index);
+        }
+        /* The write may not take place before dirty bits are read.     */
+        /* But then we'll fault again ...                               */
+#       if defined(MSWIN32) || defined(MSWINCE)
+            return(EXCEPTION_CONTINUE_EXECUTION);
+#       else
+            return;
+#       endif
+    }
+#   if defined(MSWIN32) || defined(MSWINCE)
+      return EXCEPTION_CONTINUE_SEARCH;
+#   else
+      if (GC_print_stats)
+        GC_log_printf("Unexpected segfault at %p\n", addr);
+      ABORT("Unexpected bus error or segmentation fault");
+#   endif
+  }
+
+# ifdef GC_WIN32_THREADS
+    GC_INNER void GC_set_write_fault_handler(void)
+    {
+      SetUnhandledExceptionFilter(GC_write_fault_handler);
+    }
+# endif
+#endif /* !DARWIN */
+
+/* We hold the allocation lock.  We expect block h to be written        */
+/* shortly.  Ensure that all pages containing any part of the n hblks   */
+/* starting at h are no longer protected.  If is_ptrfree is false, also */
+/* ensure that they will subsequently appear to be dirty.  Not allowed  */
+/* to call GC_printf (and the friends) here, see Win32 GC_stop_world()  */
+/* for the information.                                                 */
+GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                   GC_bool is_ptrfree)
+{
+    struct hblk * h_trunc;  /* Truncated to page boundary */
+    struct hblk * h_end;    /* Page boundary following block end */
+    struct hblk * current;
+
+#   if defined(GWW_VDB)
+      if (GC_GWW_AVAILABLE()) return;
+#   endif
+    if (!GC_dirty_maintained) return;
+    h_trunc = (struct hblk *)((word)h & ~(GC_page_size-1));
+    h_end = (struct hblk *)(((word)(h + nblocks) + GC_page_size-1)
+                            & ~(GC_page_size-1));
+    if (h_end == h_trunc + 1 &&
+        get_pht_entry_from_index(GC_dirty_pages, PHT_HASH(h_trunc))) {
+        /* already marked dirty, and hence unprotected. */
+        return;
+    }
+    for (current = h_trunc; current < h_end; ++current) {
+        size_t index = PHT_HASH(current);
+        if (!is_ptrfree || current < h || current >= h + nblocks) {
+            async_set_pht_entry_from_index(GC_dirty_pages, index);
+        }
+    }
+    UNPROTECT(h_trunc, (ptr_t)h_end - (ptr_t)h_trunc);
+}
+
+#if !defined(DARWIN)
+  GC_INNER void GC_dirty_init(void)
+  {
+#   if !defined(MSWIN32) && !defined(MSWINCE)
+      struct sigaction  act, oldact;
+      act.sa_flags      = SA_RESTART | SA_SIGINFO;
+      act.sa_sigaction = GC_write_fault_handler;
+      (void)sigemptyset(&act.sa_mask);
+#     ifdef SIG_SUSPEND
+        /* Arrange to postpone SIG_SUSPEND while we're in a write fault */
+        /* handler.  This effectively makes the handler atomic w.r.t.   */
+        /* stopping the world for GC.                                   */
+        (void)sigaddset(&act.sa_mask, SIG_SUSPEND);
+#     endif /* SIG_SUSPEND */
+#   endif
+    if (GC_print_stats == VERBOSE)
+      GC_log_printf(
+                "Initializing mprotect virtual dirty bit implementation\n");
+    GC_dirty_maintained = TRUE;
+    if (GC_page_size % HBLKSIZE != 0) {
+        ABORT("Page size not multiple of HBLKSIZE");
+    }
+#   if !defined(MSWIN32) && !defined(MSWINCE)
+#     if defined(GC_IRIX_THREADS)
+        sigaction(SIGSEGV, 0, &oldact);
+        sigaction(SIGSEGV, &act, 0);
+#     else
+        {
+          int res = sigaction(SIGSEGV, &act, &oldact);
+          if (res != 0) ABORT("Sigaction failed");
+        }
+#     endif
+      if (oldact.sa_flags & SA_SIGINFO) {
+        GC_old_segv_handler = oldact.sa_sigaction;
+        GC_old_segv_handler_used_si = TRUE;
+      } else {
+        GC_old_segv_handler = (SIG_HNDLR_PTR)oldact.sa_handler;
+        GC_old_segv_handler_used_si = FALSE;
+      }
+      if (GC_old_segv_handler == (SIG_HNDLR_PTR)SIG_IGN) {
+        if (GC_print_stats)
+          GC_err_printf("Previously ignored segmentation violation!?\n");
+        GC_old_segv_handler = (SIG_HNDLR_PTR)SIG_DFL;
+      }
+      if (GC_old_segv_handler != (SIG_HNDLR_PTR)SIG_DFL) {
+        if (GC_print_stats == VERBOSE)
+          GC_log_printf("Replaced other SIGSEGV handler\n");
+      }
+#   if defined(HPUX) || defined(LINUX) || defined(HURD) \
+      || (defined(FREEBSD) && defined(SUNOS5SIGS))
+      sigaction(SIGBUS, &act, &oldact);
+      if (oldact.sa_flags & SA_SIGINFO) {
+        GC_old_bus_handler = oldact.sa_sigaction;
+        GC_old_bus_handler_used_si = TRUE;
+      } else {
+        GC_old_bus_handler = (SIG_HNDLR_PTR)oldact.sa_handler;
+        GC_old_bus_handler_used_si = FALSE;
+      }
+      if (GC_old_bus_handler == (SIG_HNDLR_PTR)SIG_IGN) {
+        if (GC_print_stats)
+          GC_err_printf("Previously ignored bus error!?\n");
+        GC_old_bus_handler = (SIG_HNDLR_PTR)SIG_DFL;
+      }
+      if (GC_old_bus_handler != (SIG_HNDLR_PTR)SIG_DFL) {
+        if (GC_print_stats == VERBOSE)
+          GC_log_printf("Replaced other SIGBUS handler\n");
+      }
+#   endif /* HPUX || LINUX || HURD || (FREEBSD && SUNOS5SIGS) */
+#   endif /* ! MS windows */
+#   if defined(GWW_VDB)
+      if (GC_gww_dirty_init())
+        return;
+#   endif
+#   if defined(MSWIN32)
+      GC_old_segv_handler = SetUnhandledExceptionFilter(GC_write_fault_handler);
+      if (GC_old_segv_handler != NULL) {
+        if (GC_print_stats)
+          GC_log_printf("Replaced other UnhandledExceptionFilter\n");
+      } else {
+          GC_old_segv_handler = SIG_DFL;
+      }
+#   elif defined(MSWINCE)
+      /* MPROTECT_VDB is unsupported for WinCE at present.      */
+      /* FIXME: implement it (if possible). */
+#   endif
+  }
+#endif /* !DARWIN */
+
+GC_API int GC_CALL GC_incremental_protection_needs(void)
+{
+    if (GC_page_size == HBLKSIZE) {
+        return GC_PROTECTS_POINTER_HEAP;
+    } else {
+        return GC_PROTECTS_POINTER_HEAP | GC_PROTECTS_PTRFREE_HEAP;
+    }
+}
+#define HAVE_INCREMENTAL_PROTECTION_NEEDS
+
+#define IS_PTRFREE(hhdr) ((hhdr)->hb_descr == 0)
+#define PAGE_ALIGNED(x) !((word)(x) & (GC_page_size - 1))
+
+STATIC void GC_protect_heap(void)
+{
+    ptr_t start;
+    size_t len;
+    struct hblk * current;
+    struct hblk * current_start;  /* Start of block to be protected. */
+    struct hblk * limit;
+    unsigned i;
+    GC_bool protect_all =
+          (0 != (GC_incremental_protection_needs() & GC_PROTECTS_PTRFREE_HEAP));
+    for (i = 0; i < GC_n_heap_sects; i++) {
+        start = GC_heap_sects[i].hs_start;
+        len = GC_heap_sects[i].hs_bytes;
+        if (protect_all) {
+          PROTECT(start, len);
+        } else {
+          GC_ASSERT(PAGE_ALIGNED(len));
+          GC_ASSERT(PAGE_ALIGNED(start));
+          current_start = current = (struct hblk *)start;
+          limit = (struct hblk *)(start + len);
+          while (current < limit) {
+            hdr * hhdr;
+            word nhblks;
+            GC_bool is_ptrfree;
+
+            GC_ASSERT(PAGE_ALIGNED(current));
+            GET_HDR(current, hhdr);
+            if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+              /* This can happen only if we're at the beginning of a    */
+              /* heap segment, and a block spans heap segments.         */
+              /* We will handle that block as part of the preceding     */
+              /* segment.                                               */
+              GC_ASSERT(current_start == current);
+              current_start = ++current;
+              continue;
+            }
+            if (HBLK_IS_FREE(hhdr)) {
+              GC_ASSERT(PAGE_ALIGNED(hhdr -> hb_sz));
+              nhblks = divHBLKSZ(hhdr -> hb_sz);
+              is_ptrfree = TRUE;        /* dirty on alloc */
+            } else {
+              nhblks = OBJ_SZ_TO_BLOCKS(hhdr -> hb_sz);
+              is_ptrfree = IS_PTRFREE(hhdr);
+            }
+            if (is_ptrfree) {
+              if (current_start < current) {
+                PROTECT(current_start, (ptr_t)current - (ptr_t)current_start);
+              }
+              current_start = (current += nhblks);
+            } else {
+              current += nhblks;
+            }
+          }
+          if (current_start < current) {
+            PROTECT(current_start, (ptr_t)current - (ptr_t)current_start);
+          }
+        }
+    }
+}
+
+/* We assume that either the world is stopped or its OK to lose dirty   */
+/* bits while this is happening (as in GC_enable_incremental).          */
+GC_INNER void GC_read_dirty(void)
+{
+#   if defined(GWW_VDB)
+      if (GC_GWW_AVAILABLE()) {
+        GC_gww_read_dirty();
+        return;
+      }
+#   endif
+    BCOPY((word *)GC_dirty_pages, GC_grungy_pages,
+          (sizeof GC_dirty_pages));
+    BZERO((word *)GC_dirty_pages, (sizeof GC_dirty_pages));
+    GC_protect_heap();
+}
+
+GC_INNER GC_bool GC_page_was_dirty(struct hblk *h)
+{
+    register word index;
+
+#   if defined(GWW_VDB)
+      if (GC_GWW_AVAILABLE())
+        return GC_gww_page_was_dirty(h);
+#   endif
+
+    index = PHT_HASH(h);
+    return(HDR(h) == 0 || get_pht_entry_from_index(GC_grungy_pages, index));
+}
+
+/*
+ * Acquiring the allocation lock here is dangerous, since this
+ * can be called from within GC_call_with_alloc_lock, and the cord
+ * package does so.  On systems that allow nested lock acquisition, this
+ * happens to work.
+ * On other systems, SET_LOCK_HOLDER and friends must be suitably defined.
+ */
+
+#if 0
+static GC_bool syscall_acquired_lock = FALSE;   /* Protected by GC lock. */
+
+void GC_begin_syscall(void)
+{
+    /* FIXME: Resurrecting this code would require fixing the   */
+    /* test, which can spuriously return TRUE.                  */
+    if (!I_HOLD_LOCK()) {
+        LOCK();
+        syscall_acquired_lock = TRUE;
+    }
+}
+
+void GC_end_syscall(void)
+{
+    if (syscall_acquired_lock) {
+        syscall_acquired_lock = FALSE;
+        UNLOCK();
+    }
+}
+
+void GC_unprotect_range(ptr_t addr, word len)
+{
+    struct hblk * start_block;
+    struct hblk * end_block;
+    register struct hblk *h;
+    ptr_t obj_start;
+
+    if (!GC_dirty_maintained) return;
+    obj_start = GC_base(addr);
+    if (obj_start == 0) return;
+    if (GC_base(addr + len - 1) != obj_start) {
+        ABORT("GC_unprotect_range(range bigger than object)");
+    }
+    start_block = (struct hblk *)((word)addr & ~(GC_page_size - 1));
+    end_block = (struct hblk *)((word)(addr + len - 1) & ~(GC_page_size - 1));
+    end_block += GC_page_size/HBLKSIZE - 1;
+    for (h = start_block; h <= end_block; h++) {
+        register word index = PHT_HASH(h);
+
+        async_set_pht_entry_from_index(GC_dirty_pages, index);
+    }
+    UNPROTECT(start_block,
+              ((ptr_t)end_block - (ptr_t)start_block) + HBLKSIZE);
+}
+
+
+/* We no longer wrap read by default, since that was causing too many   */
+/* problems.  It is preferred that the client instead avoids writing    */
+/* to the write-protected heap with a system call.                      */
+/* This still serves as sample code if you do want to wrap system calls.*/
+
+#if !defined(MSWIN32) && !defined(MSWINCE) && !defined(GC_USE_LD_WRAP)
+/* Replacement for UNIX system call.                                    */
+/* Other calls that write to the heap should be handled similarly.      */
+/* Note that this doesn't work well for blocking reads:  It will hold   */
+/* the allocation lock for the entire duration of the call.             */
+/* Multi-threaded clients should really ensure that it won't block,     */
+/* either by setting the descriptor non-blocking, or by calling select  */
+/* or poll first, to make sure that input is available.                 */
+/* Another, preferred alternative is to ensure that system calls never  */
+/* write to the protected heap (see above).                             */
+# include <unistd.h>
+# include <sys/uio.h>
+ssize_t read(int fd, void *buf, size_t nbyte)
+{
+    int result;
+
+    GC_begin_syscall();
+    GC_unprotect_range(buf, (word)nbyte);
+#   if defined(IRIX5) || defined(GC_LINUX_THREADS)
+        /* Indirect system call may not always be easily available.     */
+        /* We could call _read, but that would interfere with the       */
+        /* libpthread interception of read.                             */
+        /* On Linux, we have to be careful with the linuxthreads        */
+        /* read interception.                                           */
+        {
+            struct iovec iov;
+
+            iov.iov_base = buf;
+            iov.iov_len = nbyte;
+            result = readv(fd, &iov, 1);
+        }
+#   else
+#     if defined(HURD)
+        result = __read(fd, buf, nbyte);
+#     else
+        /* The two zero args at the end of this list are because one
+           IA-64 syscall() implementation actually requires six args
+           to be passed, even though they aren't always used. */
+        result = syscall(SYS_read, fd, buf, nbyte, 0, 0);
+#     endif /* !HURD */
+#   endif
+    GC_end_syscall();
+    return(result);
+}
+#endif /* !MSWIN32 && !MSWINCE && !GC_LINUX_THREADS */
+
+#if defined(GC_USE_LD_WRAP) && !defined(THREADS)
+    /* We use the GNU ld call wrapping facility.                        */
+    /* I'm not sure that this actually wraps whatever version of read   */
+    /* is called by stdio.  That code also mentions __read.             */
+#   include <unistd.h>
+    ssize_t __wrap_read(int fd, void *buf, size_t nbyte)
+    {
+        int result;
+
+        GC_begin_syscall();
+        GC_unprotect_range(buf, (word)nbyte);
+        result = __real_read(fd, buf, nbyte);
+        GC_end_syscall();
+        return(result);
+    }
+
+    /* We should probably also do this for __read, or whatever stdio    */
+    /* actually calls.                                                  */
+#endif
+#endif /* 0 */
+
+# ifdef CHECKSUMS
+    /*ARGSUSED*/
+    GC_INNER GC_bool GC_page_was_ever_dirty(struct hblk *h)
+    {
+#     if defined(GWW_VDB)
+        if (GC_GWW_AVAILABLE())
+          return GC_gww_page_was_ever_dirty(h);
+#     endif
+      return(TRUE);
+    }
+# endif /* CHECKSUMS */
+
+#endif /* MPROTECT_VDB */
+
+#ifdef PROC_VDB
+/* See DEFAULT_VDB for interface descriptions.  */
+
+/* This implementation assumes a Solaris 2.X like /proc                 */
+/* pseudo-file-system from which we can read page modified bits.  This  */
+/* facility is far from optimal (e.g. we would like to get the info for */
+/* only some of the address space), but it avoids intercepting system   */
+/* calls.                                                               */
+
+# include <errno.h>
+# include <sys/types.h>
+# include <sys/signal.h>
+# include <sys/fault.h>
+# include <sys/syscall.h>
+# include <sys/procfs.h>
+# include <sys/stat.h>
+
+# define INITIAL_BUF_SZ 16384
+  STATIC word GC_proc_buf_size = INITIAL_BUF_SZ;
+  STATIC char *GC_proc_buf = NULL;
+  STATIC int GC_proc_fd = 0;
+
+GC_INNER void GC_dirty_init(void)
+{
+    int fd;
+    char buf[30];
+
+    if (GC_bytes_allocd != 0 || GC_bytes_allocd_before_gc != 0) {
+      memset(GC_written_pages, 0xff, sizeof(page_hash_table));
+      if (GC_print_stats == VERBOSE)
+        GC_log_printf("Allocated bytes:%lu:all pages may have been written\n",
+                      (unsigned long)(GC_bytes_allocd
+                                      + GC_bytes_allocd_before_gc));
+    }
+
+    sprintf(buf, "/proc/%ld", (long)getpid());
+    fd = open(buf, O_RDONLY);
+    if (fd < 0) {
+        ABORT("/proc open failed");
+    }
+    GC_proc_fd = syscall(SYS_ioctl, fd, PIOCOPENPD, 0);
+    close(fd);
+    syscall(SYS_fcntl, GC_proc_fd, F_SETFD, FD_CLOEXEC);
+    if (GC_proc_fd < 0) {
+        WARN("/proc ioctl(PIOCOPENPD) failed", 0);
+        return;
+    }
+
+    GC_dirty_maintained = TRUE;
+    GC_proc_buf = GC_scratch_alloc(GC_proc_buf_size);
+    if (GC_proc_buf == NULL)
+      ABORT("Insufficient space for /proc read");
+}
+
+# define READ read
+
+GC_INNER void GC_read_dirty(void)
+{
+    int nmaps;
+    unsigned long npages;
+    unsigned pagesize;
+    ptr_t vaddr, limit;
+    struct prasmap * map;
+    char * bufp;
+    int i;
+
+    BZERO(GC_grungy_pages, sizeof(GC_grungy_pages));
+    bufp = GC_proc_buf;
+    if (READ(GC_proc_fd, bufp, GC_proc_buf_size) <= 0) {
+        /* Retry with larger buffer.    */
+        word new_size = 2 * GC_proc_buf_size;
+        char *new_buf;
+        if (GC_print_stats)
+          GC_err_printf("/proc read failed: GC_proc_buf_size = %lu\n",
+                        (unsigned long)GC_proc_buf_size);
+
+        new_buf = GC_scratch_alloc(new_size);
+        if (new_buf != 0) {
+            GC_proc_buf = bufp = new_buf;
+            GC_proc_buf_size = new_size;
+        }
+        if (READ(GC_proc_fd, bufp, GC_proc_buf_size) <= 0) {
+            WARN("Insufficient space for /proc read\n", 0);
+            /* Punt:        */
+            memset(GC_grungy_pages, 0xff, sizeof (page_hash_table));
+            memset(GC_written_pages, 0xff, sizeof(page_hash_table));
+            return;
+        }
+    }
+
+    /* Copy dirty bits into GC_grungy_pages     */
+    nmaps = ((struct prpageheader *)bufp) -> pr_nmap;
+#   ifdef DEBUG_DIRTY_BITS
+      GC_log_printf("Proc VDB read: pr_nmap= %u, pr_npage= %lu\n",
+                    nmaps, ((struct prpageheader *)bufp)->pr_npage);
+
+#   endif
+    bufp += sizeof(struct prpageheader);
+    for (i = 0; i < nmaps; i++) {
+        map = (struct prasmap *)bufp;
+        vaddr = (ptr_t)(map -> pr_vaddr);
+        npages = map -> pr_npage;
+        pagesize = map -> pr_pagesize;
+#       ifdef DEBUG_DIRTY_BITS
+          GC_log_printf(
+                "pr_vaddr= %p, npage= %lu, mflags= 0x%x, pagesize= 0x%x\n",
+                vaddr, npages, map->pr_mflags, pagesize);
+#       endif
+
+        bufp += sizeof(struct prasmap);
+        limit = vaddr + pagesize * npages;
+        for (; vaddr < limit; vaddr += pagesize) {
+            if ((*bufp++) & PG_MODIFIED) {
+                register struct hblk * h;
+                ptr_t next_vaddr = vaddr + pagesize;
+#               ifdef DEBUG_DIRTY_BITS
+                  GC_log_printf("dirty page at: %p\n", vaddr);
+#               endif
+                for (h = (struct hblk *)vaddr; (ptr_t)h < next_vaddr; h++) {
+                    register word index = PHT_HASH(h);
+                    set_pht_entry_from_index(GC_grungy_pages, index);
+                }
+            }
+        }
+        bufp = (char *)(((word)bufp + (sizeof(long)-1)) & ~(sizeof(long)-1));
+    }
+#   ifdef DEBUG_DIRTY_BITS
+      GC_log_printf("Proc VDB read done.\n");
+#   endif
+
+    /* Update GC_written_pages. */
+    GC_or_pages(GC_written_pages, GC_grungy_pages);
+}
+
+# undef READ
+#endif /* PROC_VDB */
+
+#ifdef PCR_VDB
+
+# include "vd/PCR_VD.h"
+
+# define NPAGES (32*1024)       /* 128 MB */
+
+PCR_VD_DB GC_grungy_bits[NPAGES];
+
+STATIC ptr_t GC_vd_base = NULL;
+                        /* Address corresponding to GC_grungy_bits[0]   */
+                        /* HBLKSIZE aligned.                            */
+
+GC_INNER void GC_dirty_init(void)
+{
+    GC_dirty_maintained = TRUE;
+    /* For the time being, we assume the heap generally grows up */
+    GC_vd_base = GC_heap_sects[0].hs_start;
+    if (GC_vd_base == 0) {
+        ABORT("Bad initial heap segment");
+    }
+    if (PCR_VD_Start(HBLKSIZE, GC_vd_base, NPAGES*HBLKSIZE)
+        != PCR_ERes_okay) {
+        ABORT("Dirty bit initialization failed");
+    }
+}
+
+GC_INNER void GC_read_dirty(void)
+{
+    /* lazily enable dirty bits on newly added heap sects */
+    {
+        static int onhs = 0;
+        int nhs = GC_n_heap_sects;
+        for(; onhs < nhs; onhs++) {
+            PCR_VD_WriteProtectEnable(
+                    GC_heap_sects[onhs].hs_start,
+                    GC_heap_sects[onhs].hs_bytes );
+        }
+    }
+
+    if (PCR_VD_Clear(GC_vd_base, NPAGES*HBLKSIZE, GC_grungy_bits)
+        != PCR_ERes_okay) {
+        ABORT("Dirty bit read failed");
+    }
+}
+
+GC_INNER GC_bool GC_page_was_dirty(struct hblk *h)
+{
+    if((ptr_t)h < GC_vd_base || (ptr_t)h >= GC_vd_base + NPAGES*HBLKSIZE) {
+        return(TRUE);
+    }
+    return(GC_grungy_bits[h - (struct hblk *)GC_vd_base] & PCR_VD_DB_dirtyBit);
+}
+
+/*ARGSUSED*/
+GC_INNER void GC_remove_protection(struct hblk *h, word nblocks,
+                                   GC_bool is_ptrfree)
+{
+    PCR_VD_WriteProtectDisable(h, nblocks*HBLKSIZE);
+    PCR_VD_WriteProtectEnable(h, nblocks*HBLKSIZE);
+}
+
+#endif /* PCR_VDB */
+
+#if defined(MPROTECT_VDB) && defined(DARWIN)
+/* The following sources were used as a "reference" for this exception
+   handling code:
+      1. Apple's mach/xnu documentation
+      2. Timothy J. Wood's "Mach Exception Handlers 101" post to the
+         omnigroup's macosx-dev list.
+         www.omnigroup.com/mailman/archive/macosx-dev/2000-June/014178.html
+      3. macosx-nat.c from Apple's GDB source code.
+*/
+
+/* The bug that caused all this trouble should now be fixed. This should
+   eventually be removed if all goes well. */
+
+/* #define BROKEN_EXCEPTION_HANDLING */
+
+#include <mach/mach.h>
+#include <mach/mach_error.h>
+#include <mach/thread_status.h>
+#include <mach/exception.h>
+#include <mach/task.h>
+#include <pthread.h>
+
+/* These are not defined in any header, although they are documented */
+extern boolean_t
+exc_server(mach_msg_header_t *, mach_msg_header_t *);
+
+extern kern_return_t
+exception_raise(mach_port_t, mach_port_t, mach_port_t, exception_type_t,
+                exception_data_t, mach_msg_type_number_t);
+
+extern kern_return_t
+exception_raise_state(mach_port_t, mach_port_t, mach_port_t, exception_type_t,
+                      exception_data_t, mach_msg_type_number_t,
+                      thread_state_flavor_t*, thread_state_t,
+                      mach_msg_type_number_t, thread_state_t,
+                      mach_msg_type_number_t*);
+
+extern kern_return_t
+exception_raise_state_identity(mach_port_t, mach_port_t, mach_port_t,
+                               exception_type_t, exception_data_t,
+                               mach_msg_type_number_t, thread_state_flavor_t*,
+                               thread_state_t, mach_msg_type_number_t,
+                               thread_state_t, mach_msg_type_number_t*);
+
+GC_API_OSCALL kern_return_t
+catch_exception_raise(mach_port_t exception_port, mach_port_t thread,
+                      mach_port_t task, exception_type_t exception,
+                      exception_data_t code, mach_msg_type_number_t code_count);
+
+/* These should never be called, but just in case...  */
+GC_API_OSCALL kern_return_t
+catch_exception_raise_state(mach_port_name_t exception_port, int exception,
+                            exception_data_t code,
+                            mach_msg_type_number_t codeCnt, int flavor,
+                            thread_state_t old_state, int old_stateCnt,
+                            thread_state_t new_state, int new_stateCnt)
+{
+  ABORT("Unexpected catch_exception_raise_state invocation");
+  return(KERN_INVALID_ARGUMENT);
+}
+
+GC_API_OSCALL kern_return_t
+catch_exception_raise_state_identity(mach_port_name_t exception_port,
+                                     mach_port_t thread, mach_port_t task,
+                                     int exception, exception_data_t code,
+                                     mach_msg_type_number_t codeCnt, int flavor,
+                                     thread_state_t old_state, int old_stateCnt,
+                                     thread_state_t new_state, int new_stateCnt)
+{
+  ABORT("Unexpected catch_exception_raise_state_identity invocation");
+  return(KERN_INVALID_ARGUMENT);
+}
+
+#define MAX_EXCEPTION_PORTS 16
+
+static struct {
+  mach_msg_type_number_t count;
+  exception_mask_t      masks[MAX_EXCEPTION_PORTS];
+  exception_handler_t   ports[MAX_EXCEPTION_PORTS];
+  exception_behavior_t  behaviors[MAX_EXCEPTION_PORTS];
+  thread_state_flavor_t flavors[MAX_EXCEPTION_PORTS];
+} GC_old_exc_ports;
+
+STATIC struct {
+  void (*volatile os_callback[3])(void);
+  mach_port_t exception;
+# if defined(THREADS)
+    mach_port_t reply;
+# endif
+} GC_ports = {
+  {
+    /* This is to prevent stripping these routines as dead.     */
+    (void (*)(void))catch_exception_raise,
+    (void (*)(void))catch_exception_raise_state,
+    (void (*)(void))catch_exception_raise_state_identity
+  },
+  0
+};
+
+typedef struct {
+    mach_msg_header_t head;
+} GC_msg_t;
+
+typedef enum {
+    GC_MP_NORMAL,
+    GC_MP_DISCARDING,
+    GC_MP_STOPPED
+} GC_mprotect_state_t;
+
+#ifdef THREADS
+  /* FIXME: 1 and 2 seem to be safe to use in the msgh_id field,        */
+  /* but it isn't documented. Use the source and see if they            */
+  /* should be ok.                                                      */
+# define ID_STOP 1
+# define ID_RESUME 2
+
+  /* This value is only used on the reply port. */
+# define ID_ACK 3
+
+  STATIC GC_mprotect_state_t GC_mprotect_state = 0;
+
+  /* The following should ONLY be called when the world is stopped.     */
+  STATIC void GC_mprotect_thread_notify(mach_msg_id_t id)
+  {
+    struct {
+      GC_msg_t msg;
+      mach_msg_trailer_t trailer;
+    } buf;
+    mach_msg_return_t r;
+
+    /* remote, local */
+    buf.msg.head.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND, 0);
+    buf.msg.head.msgh_size = sizeof(buf.msg);
+    buf.msg.head.msgh_remote_port = GC_ports.exception;
+    buf.msg.head.msgh_local_port = MACH_PORT_NULL;
+    buf.msg.head.msgh_id = id;
+
+    r = mach_msg(&buf.msg.head, MACH_SEND_MSG | MACH_RCV_MSG | MACH_RCV_LARGE,
+                 sizeof(buf.msg), sizeof(buf), GC_ports.reply,
+                 MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
+    if (r != MACH_MSG_SUCCESS)
+      ABORT("mach_msg failed in GC_mprotect_thread_notify");
+    if (buf.msg.head.msgh_id != ID_ACK)
+      ABORT("Invalid ack in GC_mprotect_thread_notify");
+  }
+
+  /* Should only be called by the mprotect thread */
+  STATIC void GC_mprotect_thread_reply(void)
+  {
+    GC_msg_t msg;
+    mach_msg_return_t r;
+    /* remote, local */
+
+    msg.head.msgh_bits = MACH_MSGH_BITS(MACH_MSG_TYPE_MAKE_SEND, 0);
+    msg.head.msgh_size = sizeof(msg);
+    msg.head.msgh_remote_port = GC_ports.reply;
+    msg.head.msgh_local_port = MACH_PORT_NULL;
+    msg.head.msgh_id = ID_ACK;
+
+    r = mach_msg(&msg.head, MACH_SEND_MSG, sizeof(msg), 0, MACH_PORT_NULL,
+                 MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
+    if (r != MACH_MSG_SUCCESS)
+      ABORT("mach_msg failed in GC_mprotect_thread_reply");
+  }
+
+  GC_INNER void GC_mprotect_stop(void)
+  {
+    GC_mprotect_thread_notify(ID_STOP);
+  }
+
+  GC_INNER void GC_mprotect_resume(void)
+  {
+    GC_mprotect_thread_notify(ID_RESUME);
+  }
+
+# ifndef GC_NO_THREADS_DISCOVERY
+    GC_INNER void GC_darwin_register_mach_handler_thread(mach_port_t thread);
+# endif
+
+#else
+  /* The compiler should optimize away any GC_mprotect_state computations */
+# define GC_mprotect_state GC_MP_NORMAL
+#endif /* !THREADS */
+
+STATIC void *GC_mprotect_thread(void *arg)
+{
+  mach_msg_return_t r;
+  /* These two structures contain some private kernel data.  We don't   */
+  /* need to access any of it so we don't bother defining a proper      */
+  /* struct.  The correct definitions are in the xnu source code.       */
+  struct {
+    mach_msg_header_t head;
+    char data[256];
+  } reply;
+  struct {
+    mach_msg_header_t head;
+    mach_msg_body_t msgh_body;
+    char data[1024];
+  } msg;
+  mach_msg_id_t id;
+
+# if defined(THREADS) && !defined(GC_NO_THREADS_DISCOVERY)
+    GC_darwin_register_mach_handler_thread(mach_thread_self());
+# endif
+
+  for(;;) {
+    r = mach_msg(&msg.head, MACH_RCV_MSG | MACH_RCV_LARGE |
+                 (GC_mprotect_state == GC_MP_DISCARDING ? MACH_RCV_TIMEOUT : 0),
+                 0, sizeof(msg), GC_ports.exception,
+                 GC_mprotect_state == GC_MP_DISCARDING ? 0
+                 : MACH_MSG_TIMEOUT_NONE, MACH_PORT_NULL);
+    id = r == MACH_MSG_SUCCESS ? msg.head.msgh_id : -1;
+
+#   if defined(THREADS)
+      if(GC_mprotect_state == GC_MP_DISCARDING) {
+        if(r == MACH_RCV_TIMED_OUT) {
+          GC_mprotect_state = GC_MP_STOPPED;
+          GC_mprotect_thread_reply();
+          continue;
+        }
+        if(r == MACH_MSG_SUCCESS && (id == ID_STOP || id == ID_RESUME))
+          ABORT("Out of order mprotect thread request");
+      }
+#   endif /* THREADS */
+
+    if (r != MACH_MSG_SUCCESS) {
+      if (GC_print_stats)
+        GC_log_printf("mach_msg failed with code %d: %s\n", (int)r,
+                      mach_error_string(r));
+      ABORT("mach_msg failed");
+    }
+
+    switch(id) {
+#     if defined(THREADS)
+        case ID_STOP:
+          if(GC_mprotect_state != GC_MP_NORMAL)
+            ABORT("Called mprotect_stop when state wasn't normal");
+          GC_mprotect_state = GC_MP_DISCARDING;
+          break;
+        case ID_RESUME:
+          if(GC_mprotect_state != GC_MP_STOPPED)
+            ABORT("Called mprotect_resume when state wasn't stopped");
+          GC_mprotect_state = GC_MP_NORMAL;
+          GC_mprotect_thread_reply();
+          break;
+#     endif /* THREADS */
+        default:
+          /* Handle the message (calls catch_exception_raise) */
+          if(!exc_server(&msg.head, &reply.head))
+            ABORT("exc_server failed");
+          /* Send the reply */
+          r = mach_msg(&reply.head, MACH_SEND_MSG, reply.head.msgh_size, 0,
+                       MACH_PORT_NULL, MACH_MSG_TIMEOUT_NONE,
+                       MACH_PORT_NULL);
+          if(r != MACH_MSG_SUCCESS) {
+            /* This will fail if the thread dies, but the thread */
+            /* shouldn't die... */
+#           ifdef BROKEN_EXCEPTION_HANDLING
+              GC_err_printf("mach_msg failed with %d %s while sending "
+                            "exc reply\n", (int)r, mach_error_string(r));
+#           else
+              ABORT("mach_msg failed while sending exception reply");
+#           endif
+          }
+    } /* switch */
+  } /* for(;;) */
+    /* NOT REACHED */
+  return NULL;
+}
+
+/* All this SIGBUS code shouldn't be necessary. All protection faults should
+   be going through the mach exception handler. However, it seems a SIGBUS is
+   occasionally sent for some unknown reason. Even more odd, it seems to be
+   meaningless and safe to ignore. */
+#ifdef BROKEN_EXCEPTION_HANDLING
+
+  /* Updates to this aren't atomic, but the SIGBUS'es seem pretty rare.    */
+  /* Even if this doesn't get updated property, it isn't really a problem. */
+  STATIC int GC_sigbus_count = 0;
+
+  STATIC void GC_darwin_sigbus(int num, siginfo_t *sip, void *context)
+  {
+    if (num != SIGBUS)
+      ABORT("Got a non-sigbus signal in the sigbus handler");
+
+    /* Ugh... some seem safe to ignore, but too many in a row probably means
+       trouble. GC_sigbus_count is reset for each mach exception that is
+       handled */
+    if (GC_sigbus_count >= 8) {
+      ABORT("Got more than 8 SIGBUSs in a row!");
+    } else {
+      GC_sigbus_count++;
+      WARN("Ignoring SIGBUS.\n", 0);
+    }
+  }
+#endif /* BROKEN_EXCEPTION_HANDLING */
+
+GC_INNER void GC_dirty_init(void)
+{
+  kern_return_t r;
+  mach_port_t me;
+  pthread_t thread;
+  pthread_attr_t attr;
+  exception_mask_t mask;
+
+# ifdef CAN_HANDLE_FORK
+    if (GC_handle_fork) {
+      /* To both support GC incremental mode and GC functions usage in  */
+      /* the forked child, pthread_atfork should be used to install     */
+      /* handlers that switch off GC_dirty_maintained in the child      */
+      /* gracefully (unprotecting all pages and clearing                */
+      /* GC_mach_handler_thread).  For now, we just disable incremental */
+      /* mode if fork() handling is requested by the client.            */
+      if (GC_print_stats)
+        GC_log_printf(
+            "GC incremental mode disabled since fork() handling requested\n");
+      return;
+    }
+# endif
+
+  if (GC_print_stats == VERBOSE)
+    GC_log_printf(
+      "Initializing mach/darwin mprotect virtual dirty bit implementation\n");
+# ifdef BROKEN_EXCEPTION_HANDLING
+    WARN("Enabling workarounds for various darwin "
+         "exception handling bugs.\n", 0);
+# endif
+  GC_dirty_maintained = TRUE;
+  if (GC_page_size % HBLKSIZE != 0) {
+    ABORT("Page size not multiple of HBLKSIZE");
+  }
+
+  GC_task_self = me = mach_task_self();
+
+  r = mach_port_allocate(me, MACH_PORT_RIGHT_RECEIVE, &GC_ports.exception);
+  if (r != KERN_SUCCESS)
+    ABORT("mach_port_allocate failed (exception port)");
+
+  r = mach_port_insert_right(me, GC_ports.exception, GC_ports.exception,
+                             MACH_MSG_TYPE_MAKE_SEND);
+  if (r != KERN_SUCCESS)
+    ABORT("mach_port_insert_right failed (exception port)");
+
+#  if defined(THREADS)
+     r = mach_port_allocate(me, MACH_PORT_RIGHT_RECEIVE, &GC_ports.reply);
+     if(r != KERN_SUCCESS)
+       ABORT("mach_port_allocate failed (reply port)");
+#  endif
+
+  /* The exceptions we want to catch */
+  mask = EXC_MASK_BAD_ACCESS;
+
+  r = task_get_exception_ports(me, mask, GC_old_exc_ports.masks,
+                               &GC_old_exc_ports.count, GC_old_exc_ports.ports,
+                               GC_old_exc_ports.behaviors,
+                               GC_old_exc_ports.flavors);
+  if (r != KERN_SUCCESS)
+    ABORT("task_get_exception_ports failed");
+
+  r = task_set_exception_ports(me, mask, GC_ports.exception, EXCEPTION_DEFAULT,
+                               GC_MACH_THREAD_STATE);
+  if (r != KERN_SUCCESS)
+    ABORT("task_set_exception_ports failed");
+  if (pthread_attr_init(&attr) != 0)
+    ABORT("pthread_attr_init failed");
+  if (pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED) != 0)
+    ABORT("pthread_attr_setdetachedstate failed");
+
+# undef pthread_create
+  /* This will call the real pthread function, not our wrapper */
+  if (pthread_create(&thread, &attr, GC_mprotect_thread, NULL) != 0)
+    ABORT("pthread_create failed");
+  pthread_attr_destroy(&attr);
+
+  /* Setup the sigbus handler for ignoring the meaningless SIGBUSs */
+# ifdef BROKEN_EXCEPTION_HANDLING
+    {
+      struct sigaction sa, oldsa;
+      sa.sa_handler = (SIG_HNDLR_PTR)GC_darwin_sigbus;
+      sigemptyset(&sa.sa_mask);
+      sa.sa_flags = SA_RESTART|SA_SIGINFO;
+      if (sigaction(SIGBUS, &sa, &oldsa) < 0)
+        ABORT("sigaction failed");
+      if ((SIG_HNDLR_PTR)oldsa.sa_handler != SIG_DFL) {
+        if (GC_print_stats == VERBOSE)
+          GC_err_printf("Replaced other SIGBUS handler\n");
+      }
+    }
+# endif /* BROKEN_EXCEPTION_HANDLING  */
+}
+
+/* The source code for Apple's GDB was used as a reference for the      */
+/* exception forwarding code.  This code is similar to be GDB code only */
+/* because there is only one way to do it.                              */
+STATIC kern_return_t GC_forward_exception(mach_port_t thread, mach_port_t task,
+                                          exception_type_t exception,
+                                          exception_data_t data,
+                                          mach_msg_type_number_t data_count)
+{
+  unsigned int i;
+  kern_return_t r;
+  mach_port_t port;
+  exception_behavior_t behavior;
+  thread_state_flavor_t flavor;
+
+  thread_state_data_t thread_state;
+  mach_msg_type_number_t thread_state_count = THREAD_STATE_MAX;
+
+  for (i=0; i < GC_old_exc_ports.count; i++)
+    if (GC_old_exc_ports.masks[i] & (1 << exception))
+      break;
+  if (i == GC_old_exc_ports.count)
+    ABORT("No handler for exception!");
+
+  port = GC_old_exc_ports.ports[i];
+  behavior = GC_old_exc_ports.behaviors[i];
+  flavor = GC_old_exc_ports.flavors[i];
+
+  if (behavior == EXCEPTION_STATE || behavior == EXCEPTION_STATE_IDENTITY) {
+    r = thread_get_state(thread, flavor, thread_state, &thread_state_count);
+    if(r != KERN_SUCCESS)
+      ABORT("thread_get_state failed in forward_exception");
+    }
+
+  switch(behavior) {
+    case EXCEPTION_STATE:
+      r = exception_raise_state(port, thread, task, exception, data, data_count,
+                                &flavor, thread_state, thread_state_count,
+                                thread_state, &thread_state_count);
+      break;
+    case EXCEPTION_STATE_IDENTITY:
+      r = exception_raise_state_identity(port, thread, task, exception, data,
+                                         data_count, &flavor, thread_state,
+                                         thread_state_count, thread_state,
+                                         &thread_state_count);
+      break;
+    /* case EXCEPTION_DEFAULT: */ /* default signal handlers */
+    default: /* user-supplied signal handlers */
+      r = exception_raise(port, thread, task, exception, data, data_count);
+  }
+
+  if (behavior == EXCEPTION_STATE || behavior == EXCEPTION_STATE_IDENTITY) {
+    r = thread_set_state(thread, flavor, thread_state, thread_state_count);
+    if (r != KERN_SUCCESS)
+      ABORT("thread_set_state failed in forward_exception");
+  }
+  return r;
+}
+
+#define FWD() GC_forward_exception(thread, task, exception, code, code_count)
+
+#ifdef ARM32
+# define DARWIN_EXC_STATE         ARM_EXCEPTION_STATE
+# define DARWIN_EXC_STATE_COUNT   ARM_EXCEPTION_STATE_COUNT
+# define DARWIN_EXC_STATE_T       arm_exception_state_t
+# define DARWIN_EXC_STATE_DAR     THREAD_FLD(far)
+#elif defined(POWERPC)
+# if CPP_WORDSZ == 32
+#   define DARWIN_EXC_STATE       PPC_EXCEPTION_STATE
+#   define DARWIN_EXC_STATE_COUNT PPC_EXCEPTION_STATE_COUNT
+#   define DARWIN_EXC_STATE_T     ppc_exception_state_t
+# else
+#   define DARWIN_EXC_STATE       PPC_EXCEPTION_STATE64
+#   define DARWIN_EXC_STATE_COUNT PPC_EXCEPTION_STATE64_COUNT
+#   define DARWIN_EXC_STATE_T     ppc_exception_state64_t
+# endif
+# define DARWIN_EXC_STATE_DAR     THREAD_FLD(dar)
+#elif defined(I386) || defined(X86_64)
+# if CPP_WORDSZ == 32
+#   define DARWIN_EXC_STATE       x86_EXCEPTION_STATE32
+#   define DARWIN_EXC_STATE_COUNT x86_EXCEPTION_STATE32_COUNT
+#   define DARWIN_EXC_STATE_T     x86_exception_state32_t
+# else
+#   define DARWIN_EXC_STATE       x86_EXCEPTION_STATE64
+#   define DARWIN_EXC_STATE_COUNT x86_EXCEPTION_STATE64_COUNT
+#   define DARWIN_EXC_STATE_T     x86_exception_state64_t
+# endif
+# define DARWIN_EXC_STATE_DAR     THREAD_FLD(faultvaddr)
+#else
+# error FIXME for non-arm/ppc/x86 darwin
+#endif
+
+/* This violates the namespace rules but there isn't anything that can  */
+/* be done about it.  The exception handling stuff is hard coded to     */
+/* call this.  catch_exception_raise, catch_exception_raise_state and   */
+/* and catch_exception_raise_state_identity are called from OS.         */
+GC_API_OSCALL kern_return_t
+catch_exception_raise(mach_port_t exception_port, mach_port_t thread,
+                      mach_port_t task, exception_type_t exception,
+                      exception_data_t code, mach_msg_type_number_t code_count)
+{
+  kern_return_t r;
+  char *addr;
+  struct hblk *h;
+  unsigned int i;
+  thread_state_flavor_t flavor = DARWIN_EXC_STATE;
+  mach_msg_type_number_t exc_state_count = DARWIN_EXC_STATE_COUNT;
+  DARWIN_EXC_STATE_T exc_state;
+
+  if (exception != EXC_BAD_ACCESS || code[0] != KERN_PROTECTION_FAILURE) {
+#   ifdef DEBUG_EXCEPTION_HANDLING
+      /* We aren't interested, pass it on to the old handler */
+      GC_log_printf("Exception: 0x%x Code: 0x%x 0x%x in catch...\n",
+                    exception, code_count > 0 ? code[0] : -1,
+                    code_count > 1 ? code[1] : -1);
+#   endif
+    return FWD();
+  }
+
+  r = thread_get_state(thread, flavor, (natural_t*)&exc_state,
+                       &exc_state_count);
+  if(r != KERN_SUCCESS) {
+    /* The thread is supposed to be suspended while the exception       */
+    /* handler is called.  This shouldn't fail.                         */
+#   ifdef BROKEN_EXCEPTION_HANDLING
+      GC_err_printf("thread_get_state failed in catch_exception_raise\n");
+      return KERN_SUCCESS;
+#   else
+      ABORT("thread_get_state failed in catch_exception_raise");
+#   endif
+  }
+
+  /* This is the address that caused the fault */
+  addr = (char*) exc_state.DARWIN_EXC_STATE_DAR;
+  if (HDR(addr) == 0) {
+    /* Ugh... just like the SIGBUS problem above, it seems we get       */
+    /* a bogus KERN_PROTECTION_FAILURE every once and a while.  We wait */
+    /* till we get a bunch in a row before doing anything about it.     */
+    /* If a "real" fault ever occurs it'll just keep faulting over and  */
+    /* over and we'll hit the limit pretty quickly.                     */
+#   ifdef BROKEN_EXCEPTION_HANDLING
+      static char *last_fault;
+      static int last_fault_count;
+
+      if(addr != last_fault) {
+        last_fault = addr;
+        last_fault_count = 0;
+      }
+      if(++last_fault_count < 32) {
+        if(last_fault_count == 1)
+          WARN("Ignoring KERN_PROTECTION_FAILURE at %p\n", addr);
+        return KERN_SUCCESS;
+      }
+
+      GC_err_printf(
+        "Unexpected KERN_PROTECTION_FAILURE at %p; aborting...\n", addr);
+      /* Can't pass it along to the signal handler because that is      */
+      /* ignoring SIGBUS signals.  We also shouldn't call ABORT here as */
+      /* signals don't always work too well from the exception handler. */
+      exit(EXIT_FAILURE);
+#   else /* BROKEN_EXCEPTION_HANDLING */
+      /* Pass it along to the next exception handler
+         (which should call SIGBUS/SIGSEGV) */
+      return FWD();
+#   endif /* !BROKEN_EXCEPTION_HANDLING */
+  }
+
+# ifdef BROKEN_EXCEPTION_HANDLING
+    /* Reset the number of consecutive SIGBUSs */
+    GC_sigbus_count = 0;
+# endif
+
+  if (GC_mprotect_state == GC_MP_NORMAL) { /* common case */
+    h = (struct hblk*)((word)addr & ~(GC_page_size-1));
+    UNPROTECT(h, GC_page_size);
+    for (i = 0; i < divHBLKSZ(GC_page_size); i++) {
+      register int index = PHT_HASH(h+i);
+      async_set_pht_entry_from_index(GC_dirty_pages, index);
+    }
+  } else if (GC_mprotect_state == GC_MP_DISCARDING) {
+    /* Lie to the thread for now. No sense UNPROTECT()ing the memory
+       when we're just going to PROTECT() it again later. The thread
+       will just fault again once it resumes */
+  } else {
+    /* Shouldn't happen, i don't think */
+    GC_err_printf("KERN_PROTECTION_FAILURE while world is stopped\n");
+    return FWD();
+  }
+  return KERN_SUCCESS;
+}
+#undef FWD
+
+#ifndef NO_DESC_CATCH_EXCEPTION_RAISE
+  /* These symbols should have REFERENCED_DYNAMICALLY (0x10) bit set to */
+  /* let strip know they are not to be stripped.                        */
+  __asm__(".desc _catch_exception_raise, 0x10");
+  __asm__(".desc _catch_exception_raise_state, 0x10");
+  __asm__(".desc _catch_exception_raise_state_identity, 0x10");
+#endif
+
+#endif /* DARWIN && MPROTECT_VDB */
+
+#ifndef HAVE_INCREMENTAL_PROTECTION_NEEDS
+  GC_API int GC_CALL GC_incremental_protection_needs(void)
+  {
+    return GC_PROTECTS_NONE;
+  }
+#endif /* !HAVE_INCREMENTAL_PROTECTION_NEEDS */
+
+#ifdef ECOS
+  /* Undo sbrk() redirection. */
+# undef sbrk
+#endif
+
+/* If value is non-zero then allocate executable memory.        */
+GC_API void GC_CALL GC_set_pages_executable(int value)
+{
+  GC_ASSERT(!GC_is_initialized);
+  /* Even if IGNORE_PAGES_EXECUTABLE is defined, GC_pages_executable is */
+  /* touched here to prevent a compiler warning.                        */
+  GC_pages_executable = (GC_bool)(value != 0);
+}
+
+/* Returns non-zero if the GC-allocated memory is executable.   */
+/* GC_get_pages_executable is defined after all the places      */
+/* where GC_get_pages_executable is undefined.                  */
+GC_API int GC_CALL GC_get_pages_executable(void)
+{
+# ifdef IGNORE_PAGES_EXECUTABLE
+    return 1;   /* Always allocate executable memory. */
+# else
+    return (int)GC_pages_executable;
+# endif
+}
+
+/* Call stack save code for debugging.  Should probably be in           */
+/* mach_dep.c, but that requires reorganization.                        */
+
+/* I suspect the following works for most X86 *nix variants, so         */
+/* long as the frame pointer is explicitly stored.  In the case of gcc, */
+/* compiler flags (e.g. -fomit-frame-pointer) determine whether it is.  */
+#if defined(I386) && defined(LINUX) && defined(SAVE_CALL_CHAIN)
+#   include <features.h>
+
+    struct frame {
+        struct frame *fr_savfp;
+        long    fr_savpc;
+        long    fr_arg[NARGS];  /* All the arguments go here.   */
+    };
+#endif
+
+#if defined(SPARC)
+#  if defined(LINUX)
+#    include <features.h>
+
+     struct frame {
+        long    fr_local[8];
+        long    fr_arg[6];
+        struct frame *fr_savfp;
+        long    fr_savpc;
+#       ifndef __arch64__
+          char  *fr_stret;
+#       endif
+        long    fr_argd[6];
+        long    fr_argx[0];
+     };
+#  elif defined (DRSNX)
+#    include <sys/sparc/frame.h>
+#  elif defined(OPENBSD)
+#    include <frame.h>
+#  elif defined(FREEBSD) || defined(NETBSD)
+#    include <machine/frame.h>
+#  else
+#    include <sys/frame.h>
+#  endif
+#  if NARGS > 6
+#    error We only know how to get the first 6 arguments
+#  endif
+#endif /* SPARC */
+
+#ifdef NEED_CALLINFO
+/* Fill in the pc and argument information for up to NFRAMES of my      */
+/* callers.  Ignore my frame and my callers frame.                      */
+
+#ifdef LINUX
+#   include <unistd.h>
+#endif
+
+#endif /* NEED_CALLINFO */
+
+#if defined(GC_HAVE_BUILTIN_BACKTRACE)
+# ifdef _MSC_VER
+#  include "private/msvc_dbg.h"
+# else
+#  include <execinfo.h>
+# endif
+#endif
+
+#ifdef SAVE_CALL_CHAIN
+
+#if NARGS == 0 && NFRAMES % 2 == 0 /* No padding */ \
+    && defined(GC_HAVE_BUILTIN_BACKTRACE)
+
+#ifdef REDIRECT_MALLOC
+  /* Deal with possible malloc calls in backtrace by omitting   */
+  /* the infinitely recursing backtrace.                        */
+# ifdef THREADS
+    __thread    /* If your compiler doesn't understand this */
+                /* you could use something like pthread_getspecific.    */
+# endif
+  GC_in_save_callers = FALSE;
+#endif
+
+GC_INNER void GC_save_callers(struct callinfo info[NFRAMES])
+{
+  void * tmp_info[NFRAMES + 1];
+  int npcs, i;
+# define IGNORE_FRAMES 1
+
+  /* We retrieve NFRAMES+1 pc values, but discard the first, since it   */
+  /* points to our own frame.                                           */
+# ifdef REDIRECT_MALLOC
+    if (GC_in_save_callers) {
+      info[0].ci_pc = (word)(&GC_save_callers);
+      for (i = 1; i < NFRAMES; ++i) info[i].ci_pc = 0;
+      return;
+    }
+    GC_in_save_callers = TRUE;
+# endif
+  GC_STATIC_ASSERT(sizeof(struct callinfo) == sizeof(void *));
+  npcs = backtrace((void **)tmp_info, NFRAMES + IGNORE_FRAMES);
+  BCOPY(tmp_info+IGNORE_FRAMES, info, (npcs - IGNORE_FRAMES) * sizeof(void *));
+  for (i = npcs - IGNORE_FRAMES; i < NFRAMES; ++i) info[i].ci_pc = 0;
+# ifdef REDIRECT_MALLOC
+    GC_in_save_callers = FALSE;
+# endif
+}
+
+#else /* No builtin backtrace; do it ourselves */
+
+#if (defined(OPENBSD) || defined(NETBSD) || defined(FREEBSD)) && defined(SPARC)
+#  define FR_SAVFP fr_fp
+#  define FR_SAVPC fr_pc
+#else
+#  define FR_SAVFP fr_savfp
+#  define FR_SAVPC fr_savpc
+#endif
+
+#if defined(SPARC) && (defined(__arch64__) || defined(__sparcv9))
+#   define BIAS 2047
+#else
+#   define BIAS 0
+#endif
+
+GC_INNER void GC_save_callers(struct callinfo info[NFRAMES])
+{
+  struct frame *frame;
+  struct frame *fp;
+  int nframes = 0;
+# ifdef I386
+    /* We assume this is turned on only with gcc as the compiler. */
+    asm("movl %%ebp,%0" : "=r"(frame));
+    fp = frame;
+# else
+    frame = (struct frame *)GC_save_regs_in_stack();
+    fp = (struct frame *)((long) frame -> FR_SAVFP + BIAS);
+#endif
+
+   for (; (!(fp HOTTER_THAN frame) && !(GC_stackbottom HOTTER_THAN (ptr_t)fp)
+           && (nframes < NFRAMES));
+       fp = (struct frame *)((long) fp -> FR_SAVFP + BIAS), nframes++) {
+      register int i;
+
+      info[nframes].ci_pc = fp->FR_SAVPC;
+#     if NARGS > 0
+        for (i = 0; i < NARGS; i++) {
+          info[nframes].ci_arg[i] = ~(fp->fr_arg[i]);
+        }
+#     endif /* NARGS > 0 */
+  }
+  if (nframes < NFRAMES) info[nframes].ci_pc = 0;
+}
+
+#endif /* No builtin backtrace */
+
+#endif /* SAVE_CALL_CHAIN */
+
+#ifdef NEED_CALLINFO
+
+/* Print info to stderr.  We do NOT hold the allocation lock */
+GC_INNER void GC_print_callers(struct callinfo info[NFRAMES])
+{
+    int i;
+    static int reentry_count = 0;
+    GC_bool stop = FALSE;
+    DCL_LOCK_STATE;
+
+    /* FIXME: This should probably use a different lock, so that we     */
+    /* become callable with or without the allocation lock.             */
+    LOCK();
+      ++reentry_count;
+    UNLOCK();
+
+#   if NFRAMES == 1
+      GC_err_printf("\tCaller at allocation:\n");
+#   else
+      GC_err_printf("\tCall chain at allocation:\n");
+#   endif
+    for (i = 0; i < NFRAMES && !stop; i++) {
+        if (info[i].ci_pc == 0) break;
+#       if NARGS > 0
+        {
+          int j;
+
+          GC_err_printf("\t\targs: ");
+          for (j = 0; j < NARGS; j++) {
+            if (j != 0) GC_err_printf(", ");
+            GC_err_printf("%d (0x%X)", ~(info[i].ci_arg[j]),
+                                        ~(info[i].ci_arg[j]));
+          }
+          GC_err_printf("\n");
+        }
+#       endif
+        if (reentry_count > 1) {
+            /* We were called during an allocation during       */
+            /* a previous GC_print_callers call; punt.          */
+            GC_err_printf("\t\t##PC##= 0x%lx\n", info[i].ci_pc);
+            continue;
+        }
+        {
+#         ifdef LINUX
+            FILE *pipe;
+#         endif
+#         if defined(GC_HAVE_BUILTIN_BACKTRACE) \
+             && !defined(GC_BACKTRACE_SYMBOLS_BROKEN)
+            char **sym_name =
+              backtrace_symbols((void **)(&(info[i].ci_pc)), 1);
+            char *name = sym_name[0];
+#         else
+            char buf[40];
+            char *name = buf;
+            sprintf(buf, "##PC##= 0x%lx", info[i].ci_pc);
+#         endif
+#         if defined(LINUX) && !defined(SMALL_CONFIG)
+            /* Try for a line number. */
+            {
+#               define EXE_SZ 100
+                static char exe_name[EXE_SZ];
+#               define CMD_SZ 200
+                char cmd_buf[CMD_SZ];
+#               define RESULT_SZ 200
+                static char result_buf[RESULT_SZ];
+                size_t result_len;
+                char *old_preload;
+#               define PRELOAD_SZ 200
+                char preload_buf[PRELOAD_SZ];
+                static GC_bool found_exe_name = FALSE;
+                static GC_bool will_fail = FALSE;
+                int ret_code;
+                /* Try to get it via a hairy and expensive scheme.      */
+                /* First we get the name of the executable:             */
+                if (will_fail) goto out;
+                if (!found_exe_name) {
+                  ret_code = readlink("/proc/self/exe", exe_name, EXE_SZ);
+                  if (ret_code < 0 || ret_code >= EXE_SZ
+                      || exe_name[0] != '/') {
+                    will_fail = TRUE;   /* Don't try again. */
+                    goto out;
+                  }
+                  exe_name[ret_code] = '\0';
+                  found_exe_name = TRUE;
+                }
+                /* Then we use popen to start addr2line -e <exe> <addr> */
+                /* There are faster ways to do this, but hopefully this */
+                /* isn't time critical.                                 */
+                sprintf(cmd_buf, "/usr/bin/addr2line -f -e %s 0x%lx", exe_name,
+                                 (unsigned long)info[i].ci_pc);
+                old_preload = GETENV("LD_PRELOAD");
+                if (0 != old_preload) {
+                  if (strlen (old_preload) >= PRELOAD_SZ) {
+                    will_fail = TRUE;
+                    goto out;
+                  }
+                  strcpy (preload_buf, old_preload);
+                  unsetenv ("LD_PRELOAD");
+                }
+                pipe = popen(cmd_buf, "r");
+                if (0 != old_preload
+                    && 0 != setenv ("LD_PRELOAD", preload_buf, 0)) {
+                  WARN("Failed to reset LD_PRELOAD\n", 0);
+                }
+                if (pipe == NULL
+                    || (result_len = fread(result_buf, 1, RESULT_SZ - 1, pipe))
+                       == 0) {
+                  if (pipe != NULL) pclose(pipe);
+                  will_fail = TRUE;
+                  goto out;
+                }
+                if (result_buf[result_len - 1] == '\n') --result_len;
+                result_buf[result_len] = 0;
+                if (result_buf[0] == '?'
+                    || (result_buf[result_len-2] == ':'
+                        && result_buf[result_len-1] == '0')) {
+                    pclose(pipe);
+                    goto out;
+                }
+                /* Get rid of embedded newline, if any.  Test for "main" */
+                {
+                   char * nl = strchr(result_buf, '\n');
+                   if (nl != NULL && nl < result_buf + result_len) {
+                     *nl = ':';
+                   }
+                   if (strncmp(result_buf, "main", nl - result_buf) == 0) {
+                     stop = TRUE;
+                   }
+                }
+                if (result_len < RESULT_SZ - 25) {
+                  /* Add in hex address */
+                    sprintf(result_buf + result_len, " [0x%lx]",
+                          (unsigned long)info[i].ci_pc);
+                }
+                name = result_buf;
+                pclose(pipe);
+                out:;
+            }
+#         endif /* LINUX */
+          GC_err_printf("\t\t%s\n", name);
+#         if defined(GC_HAVE_BUILTIN_BACKTRACE) \
+             && !defined(GC_BACKTRACE_SYMBOLS_BROKEN)
+            free(sym_name);  /* May call GC_free; that's OK */
+#         endif
+        }
+    }
+    LOCK();
+      --reentry_count;
+    UNLOCK();
+}
+
+#endif /* NEED_CALLINFO */
+
+#if defined(LINUX) && defined(__ELF__) && !defined(SMALL_CONFIG)
+  /* Dump /proc/self/maps to GC_stderr, to enable looking up names for  */
+  /* addresses in FIND_LEAK output.                                     */
+  void GC_print_address_map(void)
+  {
+    char *maps;
+
+    GC_err_printf("---------- Begin address map ----------\n");
+    maps = GC_get_maps();
+    GC_err_puts(maps != NULL ? maps : "Failed to get map!\n");
+    GC_err_printf("---------- End address map ----------\n");
+  }
+#endif /* LINUX && ELF */
diff --git a/src/gc/bdwgc/pcr_interface.c b/src/gc/bdwgc/pcr_interface.c
new file mode 100644
index 0000000..77bddf8
--- /dev/null
+++ b/src/gc/bdwgc/pcr_interface.c
@@ -0,0 +1,179 @@
+/* 
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+# include "private/gc_priv.h"
+
+# ifdef PCR
+/*
+ * Note that POSIX PCR requires an ANSI C compiler.  Hence we are allowed
+ * to make the same assumption here.
+ * We wrap all of the allocator functions to avoid questions of
+ * compatibility between the prototyped and nonprototyped versions of the f
+ */
+# include "config/PCR_StdTypes.h"
+# include "mm/PCR_MM.h"
+# include <errno.h>
+
+# define MY_MAGIC 17L
+# define MY_DEBUGMAGIC 42L
+
+void * GC_AllocProc(size_t size, PCR_Bool ptrFree, PCR_Bool clear )
+{
+    if (ptrFree) {
+        void * result = (void *)GC_malloc_atomic(size);
+        if (clear && result != 0) BZERO(result, size);
+        return(result);
+    } else {
+        return((void *)GC_malloc(size));
+    }
+}
+
+void * GC_DebugAllocProc(size_t size, PCR_Bool ptrFree, PCR_Bool clear )
+{
+    if (ptrFree) {
+        void * result = (void *)GC_debug_malloc_atomic(size, __FILE__,
+        						     __LINE__);
+        if (clear && result != 0) BZERO(result, size);
+        return(result);
+    } else {
+        return((void *)GC_debug_malloc(size, __FILE__, __LINE__));
+    }
+}
+
+# define GC_ReallocProc GC_realloc
+void * GC_DebugReallocProc(void * old_object, size_t new_size_in_bytes)
+{
+    return(GC_debug_realloc(old_object, new_size_in_bytes, __FILE__, __LINE__));
+}
+
+# define GC_FreeProc GC_free
+# define GC_DebugFreeProc GC_debug_free
+
+typedef struct {
+  PCR_ERes (*ed_proc)(void *p, size_t size, PCR_Any data);
+  GC_bool ed_pointerfree;
+  PCR_ERes ed_fail_code;
+  PCR_Any ed_client_data;
+} enumerate_data;
+
+void GC_enumerate_block(struct hblk *h; enumerate_data * ed)
+{
+    register hdr * hhdr;
+    register int sz;
+    ptr_t p;
+    ptr_t lim;
+    word descr;
+#   error This code was updated without testing.
+#   error and its precursor was clearly broken.
+    
+    hhdr = HDR(h);
+    descr = hhdr -> hb_descr;
+    sz = hhdr -> hb_sz;
+    if (descr != 0 && ed -> ed_pointerfree
+    	|| descr == 0 && !(ed -> ed_pointerfree)) return;
+    lim = (ptr_t)(h+1) - sz;
+    p = (ptr_t)h;
+    do {
+        if (PCR_ERes_IsErr(ed -> ed_fail_code)) return;
+        ed -> ed_fail_code =
+            (*(ed -> ed_proc))(p, sz, ed -> ed_client_data);
+        p+= sz;
+    } while (p <= lim);
+}
+
+struct PCR_MM_ProcsRep * GC_old_allocator = 0;
+
+PCR_ERes GC_EnumerateProc(
+    PCR_Bool ptrFree,
+    PCR_ERes (*proc)(void *p, size_t size, PCR_Any data),
+    PCR_Any data
+)
+{
+    enumerate_data ed;
+    
+    ed.ed_proc = proc;
+    ed.ed_pointerfree = ptrFree;
+    ed.ed_fail_code = PCR_ERes_okay;
+    ed.ed_client_data = data;
+    GC_apply_to_all_blocks(GC_enumerate_block, &ed);
+    if (ed.ed_fail_code != PCR_ERes_okay) {
+        return(ed.ed_fail_code);
+    } else {
+    	/* Also enumerate objects allocated by my predecessors */
+    	return((*(GC_old_allocator->mmp_enumerate))(ptrFree, proc, data));
+    }
+}
+
+void GC_DummyFreeProc(void *p) {}
+
+void GC_DummyShutdownProc(void) {}
+
+struct PCR_MM_ProcsRep GC_Rep = {
+	MY_MAGIC,
+	GC_AllocProc,
+	GC_ReallocProc,
+	GC_DummyFreeProc,  	/* mmp_free */
+	GC_FreeProc,  		/* mmp_unsafeFree */
+	GC_EnumerateProc,
+	GC_DummyShutdownProc	/* mmp_shutdown */
+};
+
+struct PCR_MM_ProcsRep GC_DebugRep = {
+	MY_DEBUGMAGIC,
+	GC_DebugAllocProc,
+	GC_DebugReallocProc,
+	GC_DummyFreeProc,  	/* mmp_free */
+	GC_DebugFreeProc,  		/* mmp_unsafeFree */
+	GC_EnumerateProc,
+	GC_DummyShutdownProc	/* mmp_shutdown */
+};
+
+GC_bool GC_use_debug = 0;
+
+void GC_pcr_install()
+{
+    PCR_MM_Install((GC_use_debug? &GC_DebugRep : &GC_Rep), &GC_old_allocator);
+}
+
+PCR_ERes
+PCR_GC_Setup(void)
+{
+    return PCR_ERes_okay;
+}
+
+PCR_ERes
+PCR_GC_Run(void)
+{
+
+    if( !PCR_Base_TestPCRArg("-nogc") ) {
+        GC_quiet = ( PCR_Base_TestPCRArg("-gctrace") ? 0 : 1 );
+        GC_use_debug = (GC_bool)PCR_Base_TestPCRArg("-debug_alloc");
+        GC_init();
+        if( !PCR_Base_TestPCRArg("-nogc_incremental") ) {
+            /*
+             * awful hack to test whether VD is implemented ...
+             */
+            if( PCR_VD_Start( 0, NIL, 0) != PCR_ERes_FromErr(ENOSYS) ) {
+	        GC_enable_incremental();
+	    }
+	}
+    }
+    return PCR_ERes_okay;
+}
+
+void GC_push_thread_structures(void)
+{
+    /* PCR doesn't work unless static roots are pushed.  Can't get here. */
+    ABORT("In GC_push_thread_structures()");
+}
+
+# endif
diff --git a/src/gc/bdwgc/ptr_chck.c b/src/gc/bdwgc/ptr_chck.c
new file mode 100644
index 0000000..f76739f
--- /dev/null
+++ b/src/gc/bdwgc/ptr_chck.c
@@ -0,0 +1,278 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_pmark.h"
+
+/*
+ * These are checking routines calls to which could be inserted by a
+ * preprocessor to validate C pointer arithmetic.
+ */
+
+STATIC void GC_CALLBACK GC_default_same_obj_print_proc(void * p, void * q)
+{
+    GC_err_printf("%p and %p are not in the same object\n", p, q);
+    ABORT("GC_same_obj test failed");
+}
+
+void (GC_CALLBACK *GC_same_obj_print_proc) (void *, void *)
+                = GC_default_same_obj_print_proc;
+
+/* Check that p and q point to the same object.  Call           */
+/* *GC_same_obj_print_proc if they don't.                       */
+/* Returns the first argument.  (Return value may be hard       */
+/* to use,due to typing issues.  But if we had a suitable       */
+/* preprocessor ...)                                            */
+/* Succeeds if neither p nor q points to the heap.              */
+/* We assume this is performance critical.  (It shouldn't       */
+/* be called by production code, but this can easily make       */
+/* debugging intolerably slow.)                                 */
+GC_API void * GC_CALL GC_same_obj(void *p, void *q)
+{
+    struct hblk *h;
+    hdr *hhdr;
+    ptr_t base, limit;
+    word sz;
+
+    if (!GC_is_initialized) GC_init();
+    hhdr = HDR((word)p);
+    if (hhdr == 0) {
+        if (divHBLKSZ((word)p) != divHBLKSZ((word)q)
+            && HDR((word)q) != 0) {
+            goto fail;
+        }
+        return(p);
+    }
+    /* If it's a pointer to the middle of a large object, move it       */
+    /* to the beginning.                                                */
+    if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+        h = HBLKPTR(p) - (word)hhdr;
+        hhdr = HDR(h);
+        while (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+           h = FORWARDED_ADDR(h, hhdr);
+           hhdr = HDR(h);
+        }
+        limit = (ptr_t)h + hhdr -> hb_sz;
+        if ((ptr_t)p >= limit || (ptr_t)q >= limit || (ptr_t)q < (ptr_t)h ) {
+            goto fail;
+        }
+        return(p);
+    }
+    sz = hhdr -> hb_sz;
+    if (sz > MAXOBJBYTES) {
+      base = (ptr_t)HBLKPTR(p);
+      limit = base + sz;
+      if ((ptr_t)p >= limit) {
+        goto fail;
+      }
+    } else {
+      size_t offset;
+      size_t pdispl = HBLKDISPL(p);
+
+      offset = pdispl % sz;
+      if (HBLKPTR(p) != HBLKPTR(q)) goto fail;
+                /* W/o this check, we might miss an error if    */
+                /* q points to the first object on a page, and  */
+                /* points just before the page.                 */
+      base = (ptr_t)p - offset;
+      limit = base + sz;
+    }
+    /* [base, limit) delimits the object containing p, if any.  */
+    /* If p is not inside a valid object, then either q is      */
+    /* also outside any valid object, or it is outside          */
+    /* [base, limit).                                           */
+    if ((ptr_t)q >= limit || (ptr_t)q < base) {
+        goto fail;
+    }
+    return(p);
+fail:
+    (*GC_same_obj_print_proc)((ptr_t)p, (ptr_t)q);
+    return(p);
+}
+
+STATIC void GC_CALLBACK GC_default_is_valid_displacement_print_proc (void *p)
+{
+    GC_err_printf("%p does not point to valid object displacement\n", p);
+    ABORT("GC_is_valid_displacement test failed");
+}
+
+void (GC_CALLBACK *GC_is_valid_displacement_print_proc)(void *) =
+        GC_default_is_valid_displacement_print_proc;
+
+/* Check that if p is a pointer to a heap page, then it points to       */
+/* a valid displacement within a heap object.                           */
+/* Uninteresting with GC_all_interior_pointers.                         */
+/* Always returns its argument.                                         */
+/* Note that we don't lock, since nothing relevant about the header     */
+/* should change while we have a valid object pointer to the block.     */
+GC_API void * GC_CALL GC_is_valid_displacement(void *p)
+{
+    hdr *hhdr;
+    word pdispl;
+    word offset;
+    struct hblk *h;
+    word sz;
+
+    if (!GC_is_initialized) GC_init();
+    hhdr = HDR((word)p);
+    if (hhdr == 0) return(p);
+    h = HBLKPTR(p);
+    if (GC_all_interior_pointers) {
+        while (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+           h = FORWARDED_ADDR(h, hhdr);
+           hhdr = HDR(h);
+        }
+    }
+    if (IS_FORWARDING_ADDR_OR_NIL(hhdr)) {
+        goto fail;
+    }
+    sz = hhdr -> hb_sz;
+    pdispl = HBLKDISPL(p);
+    offset = pdispl % sz;
+    if ((sz > MAXOBJBYTES && (ptr_t)p >= (ptr_t)h + sz)
+        || !GC_valid_offsets[offset]
+        || (ptr_t)p - offset + sz > (ptr_t)(h + 1)) {
+        goto fail;
+    }
+    return(p);
+fail:
+    (*GC_is_valid_displacement_print_proc)((ptr_t)p);
+    return(p);
+}
+
+STATIC void GC_CALLBACK GC_default_is_visible_print_proc(void * p)
+{
+    GC_err_printf("%p is not a GC visible pointer location\n", p);
+    ABORT("GC_is_visible test failed");
+}
+
+void (GC_CALLBACK *GC_is_visible_print_proc)(void * p) =
+                GC_default_is_visible_print_proc;
+
+#ifndef THREADS
+/* Could p be a stack address? */
+   STATIC GC_bool GC_on_stack(ptr_t p)
+   {
+#       ifdef STACK_GROWS_DOWN
+            if ((ptr_t)p >= GC_approx_sp() && (ptr_t)p < GC_stackbottom) {
+                return(TRUE);
+            }
+#       else
+            if ((ptr_t)p <= GC_approx_sp() && (ptr_t)p > GC_stackbottom) {
+                return(TRUE);
+            }
+#       endif
+        return(FALSE);
+   }
+#endif
+
+/* Check that p is visible                                              */
+/* to the collector as a possibly pointer containing location.          */
+/* If it isn't invoke *GC_is_visible_print_proc.                        */
+/* Returns the argument in all cases.  May erroneously succeed          */
+/* in hard cases.  (This is intended for debugging use with             */
+/* untyped allocations.  The idea is that it should be possible, though */
+/* slow, to add such a call to all indirect pointer stores.)            */
+/* Currently useless for the multi-threaded worlds.                     */
+GC_API void * GC_CALL GC_is_visible(void *p)
+{
+    hdr *hhdr;
+
+    if ((word)p & (ALIGNMENT - 1)) goto fail;
+    if (!GC_is_initialized) GC_init();
+#   ifdef THREADS
+        hhdr = HDR((word)p);
+        if (hhdr != 0 && GC_base(p) == 0) {
+            goto fail;
+        } else {
+            /* May be inside thread stack.  We can't do much. */
+            return(p);
+        }
+#   else
+        /* Check stack first: */
+          if (GC_on_stack(p)) return(p);
+        hhdr = HDR((word)p);
+        if (hhdr == 0) {
+            if (GC_is_static_root(p)) return(p);
+            /* Else do it again correctly:      */
+#           if defined(DYNAMIC_LOADING) || defined(MSWIN32) \
+                || defined(MSWINCE) || defined(CYGWIN32) || defined(PCR)
+              GC_register_dynamic_libraries();
+              if (GC_is_static_root(p))
+                return(p);
+#           endif
+            goto fail;
+        } else {
+            /* p points to the heap. */
+            word descr;
+            ptr_t base = GC_base(p);    /* Should be manually inlined? */
+
+            if (base == 0) goto fail;
+            if (HBLKPTR(base) != HBLKPTR(p)) hhdr = HDR((word)p);
+            descr = hhdr -> hb_descr;
+    retry:
+            switch(descr & GC_DS_TAGS) {
+                case GC_DS_LENGTH:
+                    if ((word)((ptr_t)p - (ptr_t)base) > (word)descr) goto fail;
+                    break;
+                case GC_DS_BITMAP:
+                    if ((word)((ptr_t)p - (ptr_t)base)
+                         >= WORDS_TO_BYTES(BITMAP_BITS)
+                         || ((word)p & (sizeof(word) - 1))) goto fail;
+                    if (!(((word)1 << (WORDSZ - ((ptr_t)p - (ptr_t)base) - 1))
+                          & descr)) goto fail;
+                    break;
+                case GC_DS_PROC:
+                    /* We could try to decipher this partially.         */
+                    /* For now we just punt.                            */
+                    break;
+                case GC_DS_PER_OBJECT:
+                    if ((signed_word)descr >= 0) {
+                      descr = *(word *)((ptr_t)base + (descr & ~GC_DS_TAGS));
+                    } else {
+                      ptr_t type_descr = *(ptr_t *)base;
+                      descr = *(word *)(type_descr
+                              - (descr - (word)(GC_DS_PER_OBJECT
+                                          - GC_INDIR_PER_OBJ_BIAS)));
+                    }
+                    goto retry;
+            }
+            return(p);
+        }
+#   endif
+fail:
+    (*GC_is_visible_print_proc)((ptr_t)p);
+    return(p);
+}
+
+GC_API void * GC_CALL GC_pre_incr (void **p, ptrdiff_t how_much)
+{
+    void * initial = *p;
+    void * result = GC_same_obj((void *)((ptr_t)initial + how_much), initial);
+
+    if (!GC_all_interior_pointers) {
+        (void) GC_is_valid_displacement(result);
+    }
+    return (*p = result);
+}
+
+GC_API void * GC_CALL GC_post_incr (void **p, ptrdiff_t how_much)
+{
+    void * initial = *p;
+    void * result = GC_same_obj((void *)((ptr_t)initial + how_much), initial);
+
+    if (!GC_all_interior_pointers) {
+        (void) GC_is_valid_displacement(result);
+    }
+    *p = result;
+    return(initial);
+}
diff --git a/src/gc/bdwgc/reclaim.c b/src/gc/bdwgc/reclaim.c
new file mode 100644
index 0000000..a4722e5
--- /dev/null
+++ b/src/gc/bdwgc/reclaim.c
@@ -0,0 +1,631 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1996 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1999-2004 Hewlett-Packard Development Company, L.P.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+GC_INNER signed_word GC_bytes_found = 0;
+                        /* Number of bytes of memory reclaimed     */
+                        /* minus the number of bytes originally    */
+                        /* on free lists which we had to drop.     */
+
+#if defined(PARALLEL_MARK)
+  GC_INNER word GC_fl_builder_count = 0;
+        /* Number of threads currently building free lists without      */
+        /* holding GC lock.  It is not safe to collect if this is       */
+        /* nonzero.                                                     */
+#endif /* PARALLEL_MARK */
+
+/* We defer printing of leaked objects until we're done with the GC     */
+/* cycle, since the routine for printing objects needs to run outside   */
+/* the collector, e.g. without the allocation lock.                     */
+#ifndef MAX_LEAKED
+# define MAX_LEAKED 40
+#endif
+STATIC ptr_t GC_leaked[MAX_LEAKED] = { NULL };
+STATIC unsigned GC_n_leaked = 0;
+
+GC_INNER GC_bool GC_have_errors = FALSE;
+
+GC_INLINE void GC_add_leaked(ptr_t leaked)
+{
+#  ifndef SHORT_DBG_HDRS
+     if (GC_findleak_delay_free && !GC_check_leaked(leaked))
+       return;
+#  endif
+
+    GC_have_errors = TRUE;
+    if (GC_n_leaked < MAX_LEAKED) {
+      GC_leaked[GC_n_leaked++] = leaked;
+      /* Make sure it's not reclaimed this cycle */
+      GC_set_mark_bit(leaked);
+    }
+}
+
+/* Print all objects on the list after printing any smashed objects.    */
+/* Clear both lists.  Called without the allocation lock held.          */
+GC_INNER void GC_print_all_errors(void)
+{
+    static GC_bool printing_errors = FALSE;
+    GC_bool have_errors;
+    unsigned i, n_leaked;
+    ptr_t leaked[MAX_LEAKED];
+    DCL_LOCK_STATE;
+
+    LOCK();
+    if (printing_errors) {
+        UNLOCK();
+        return;
+    }
+    have_errors = GC_have_errors;
+    printing_errors = TRUE;
+    n_leaked = GC_n_leaked;
+    GC_ASSERT(n_leaked <= MAX_LEAKED);
+    BCOPY(GC_leaked, leaked, n_leaked * sizeof(ptr_t));
+    GC_n_leaked = 0;
+    BZERO(GC_leaked, n_leaked * sizeof(ptr_t));
+    UNLOCK();
+
+    if (GC_debugging_started) {
+      GC_print_all_smashed();
+    } else {
+      have_errors = FALSE;
+    }
+
+    for (i = 0; i < n_leaked; i++) {
+        ptr_t p = leaked[i];
+        if (HDR(p) -> hb_obj_kind == PTRFREE) {
+            GC_err_printf("LEAKDETECT: Leaked atomic object at ");
+        } else {
+            GC_err_printf("LEAKDETECT: Leaked composite object at ");
+        }
+        GC_print_heap_obj(p);
+        GC_err_printf("\n");
+        GC_free(p);
+        have_errors = TRUE;
+    }
+
+    if (have_errors
+#       ifndef GC_ABORT_ON_LEAK
+          && GETENV("GC_ABORT_ON_LEAK") != NULL
+#       endif
+        ) {
+      ABORT("Leaked or smashed objects encountered");
+    }
+
+    LOCK();
+    printing_errors = FALSE;
+    UNLOCK();
+}
+
+
+/*
+ * reclaim phase
+ *
+ */
+
+/* Test whether a block is completely empty, i.e. contains no marked    */
+/* objects.  This does not require the block to be in physical memory.  */
+GC_INNER GC_bool GC_block_empty(hdr *hhdr)
+{
+    return (hhdr -> hb_n_marks == 0);
+}
+
+STATIC GC_bool GC_block_nearly_full(hdr *hhdr)
+{
+    return (hhdr -> hb_n_marks > 7 * HBLK_OBJS(hhdr -> hb_sz)/8);
+}
+
+/* FIXME: This should perhaps again be specialized for USE_MARK_BYTES   */
+/* and USE_MARK_BITS cases.                                             */
+
+/*
+ * Restore unmarked small objects in h of size sz to the object
+ * free list.  Returns the new list.
+ * Clears unmarked objects.  Sz is in bytes.
+ */
+STATIC ptr_t GC_reclaim_clear(struct hblk *hbp, hdr *hhdr, size_t sz,
+                              ptr_t list, signed_word *count)
+{
+    word bit_no = 0;
+    word *p, *q, *plim;
+    signed_word n_bytes_found = 0;
+
+    GC_ASSERT(hhdr == GC_find_header((ptr_t)hbp));
+    GC_ASSERT(sz == hhdr -> hb_sz);
+    GC_ASSERT((sz & (BYTES_PER_WORD-1)) == 0);
+    p = (word *)(hbp->hb_body);
+    plim = (word *)(hbp->hb_body + HBLKSIZE - sz);
+
+    /* go through all words in block */
+        while (p <= plim) {
+            if( mark_bit_from_hdr(hhdr, bit_no) ) {
+                p = (word *)((ptr_t)p + sz);
+            } else {
+                n_bytes_found += sz;
+                /* object is available - put on list */
+                    obj_link(p) = list;
+                    list = ((ptr_t)p);
+                /* Clear object, advance p to next object in the process */
+                    q = (word *)((ptr_t)p + sz);
+#                   ifdef USE_MARK_BYTES
+                      GC_ASSERT(!(sz & 1)
+                                && !((word)p & (2 * sizeof(word) - 1)));
+                      p[1] = 0;
+                      p += 2;
+                      while (p < q) {
+                        CLEAR_DOUBLE(p);
+                        p += 2;
+                      }
+#                   else
+                      p++; /* Skip link field */
+                      while (p < q) {
+                        *p++ = 0;
+                      }
+#                   endif
+            }
+            bit_no += MARK_BIT_OFFSET(sz);
+        }
+    *count += n_bytes_found;
+    return(list);
+}
+
+/* The same thing, but don't clear objects: */
+STATIC ptr_t GC_reclaim_uninit(struct hblk *hbp, hdr *hhdr, size_t sz,
+                               ptr_t list, signed_word *count)
+{
+    word bit_no = 0;
+    word *p, *plim;
+    signed_word n_bytes_found = 0;
+
+    GC_ASSERT(sz == hhdr -> hb_sz);
+    p = (word *)(hbp->hb_body);
+    plim = (word *)((ptr_t)hbp + HBLKSIZE - sz);
+
+    /* go through all words in block */
+        while (p <= plim) {
+            if( !mark_bit_from_hdr(hhdr, bit_no) ) {
+                n_bytes_found += sz;
+                /* object is available - put on list */
+                    obj_link(p) = list;
+                    list = ((ptr_t)p);
+            }
+            p = (word *)((ptr_t)p + sz);
+            bit_no += MARK_BIT_OFFSET(sz);
+        }
+    *count += n_bytes_found;
+    return(list);
+}
+
+/* Don't really reclaim objects, just check for unmarked ones: */
+STATIC void GC_reclaim_check(struct hblk *hbp, hdr *hhdr, word sz)
+{
+    word bit_no;
+    ptr_t p, plim;
+    GC_ASSERT(sz == hhdr -> hb_sz);
+
+    /* go through all words in block */
+    p = hbp->hb_body;
+    plim = p + HBLKSIZE - sz;
+    for (bit_no = 0; p <= plim; p += sz, bit_no += MARK_BIT_OFFSET(sz)) {
+      if (!mark_bit_from_hdr(hhdr, bit_no)) {
+        GC_add_leaked(p);
+      }
+    }
+}
+
+/*
+ * Generic procedure to rebuild a free list in hbp.
+ * Also called directly from GC_malloc_many.
+ * Sz is now in bytes.
+ */
+GC_INNER ptr_t GC_reclaim_generic(struct hblk * hbp, hdr *hhdr, size_t sz,
+                                  GC_bool init, ptr_t list,
+                                  signed_word *count)
+{
+    ptr_t result;
+
+    GC_ASSERT(GC_find_header((ptr_t)hbp) == hhdr);
+#   ifndef GC_DISABLE_INCREMENTAL
+      GC_remove_protection(hbp, 1, (hhdr)->hb_descr == 0 /* Pointer-free? */);
+#   endif
+    if (init || GC_debugging_started) {
+      result = GC_reclaim_clear(hbp, hhdr, sz, list, count);
+    } else {
+      GC_ASSERT((hhdr)->hb_descr == 0 /* Pointer-free block */);
+      result = GC_reclaim_uninit(hbp, hhdr, sz, list, count);
+    }
+    if (IS_UNCOLLECTABLE(hhdr -> hb_obj_kind)) GC_set_hdr_marks(hhdr);
+    return result;
+}
+
+/*
+ * Restore unmarked small objects in the block pointed to by hbp
+ * to the appropriate object free list.
+ * If entirely empty blocks are to be completely deallocated, then
+ * caller should perform that check.
+ */
+STATIC void GC_reclaim_small_nonempty_block(struct hblk *hbp,
+                                            GC_bool report_if_found)
+{
+    hdr *hhdr = HDR(hbp);
+    size_t sz = hhdr -> hb_sz;
+    struct obj_kind * ok = &GC_obj_kinds[hhdr -> hb_obj_kind];
+    void **flh = &(ok -> ok_freelist[BYTES_TO_GRANULES(sz)]);
+
+    hhdr -> hb_last_reclaimed = (unsigned short) GC_gc_no;
+
+    if (report_if_found) {
+        GC_reclaim_check(hbp, hhdr, sz);
+    } else {
+        *flh = GC_reclaim_generic(hbp, hhdr, sz, ok -> ok_init,
+                                  *flh, &GC_bytes_found);
+    }
+}
+
+/*
+ * Restore an unmarked large object or an entirely empty blocks of small objects
+ * to the heap block free list.
+ * Otherwise enqueue the block for later processing
+ * by GC_reclaim_small_nonempty_block.
+ * If report_if_found is TRUE, then process any block immediately, and
+ * simply report free objects; do not actually reclaim them.
+ */
+STATIC void GC_reclaim_block(struct hblk *hbp, word report_if_found)
+{
+    hdr * hhdr = HDR(hbp);
+    size_t sz = hhdr -> hb_sz;  /* size of objects in current block     */
+    struct obj_kind * ok = &GC_obj_kinds[hhdr -> hb_obj_kind];
+    struct hblk ** rlh;
+
+    if( sz > MAXOBJBYTES ) {  /* 1 big object */
+        if( !mark_bit_from_hdr(hhdr, 0) ) {
+            if (report_if_found) {
+              GC_add_leaked((ptr_t)hbp);
+            } else {
+              size_t blocks = OBJ_SZ_TO_BLOCKS(sz);
+              if (blocks > 1) {
+                GC_large_allocd_bytes -= blocks * HBLKSIZE;
+              }
+              GC_bytes_found += sz;
+              GC_freehblk(hbp);
+            }
+        } else {
+            if (hhdr -> hb_descr != 0) {
+              GC_composite_in_use += sz;
+            } else {
+              GC_atomic_in_use += sz;
+            }
+        }
+    } else {
+        GC_bool empty = GC_block_empty(hhdr);
+#       ifdef PARALLEL_MARK
+          /* Count can be low or one too high because we sometimes      */
+          /* have to ignore decrements.  Objects can also potentially   */
+          /* be repeatedly marked by each marker.                       */
+          /* Here we assume two markers, but this is extremely          */
+          /* unlikely to fail spuriously with more.  And if it does, it */
+          /* should be looked at.                                       */
+          GC_ASSERT(hhdr -> hb_n_marks <= 2 * (HBLKSIZE/sz + 1) + 16);
+#       else
+          GC_ASSERT(sz * hhdr -> hb_n_marks <= HBLKSIZE);
+#       endif
+        if (hhdr -> hb_descr != 0) {
+          GC_composite_in_use += sz * hhdr -> hb_n_marks;
+        } else {
+          GC_atomic_in_use += sz * hhdr -> hb_n_marks;
+        }
+        if (report_if_found) {
+          GC_reclaim_small_nonempty_block(hbp, TRUE /* report_if_found */);
+        } else if (empty) {
+          GC_bytes_found += HBLKSIZE;
+          GC_freehblk(hbp);
+        } else if (GC_find_leak || !GC_block_nearly_full(hhdr)) {
+          /* group of smaller objects, enqueue the real work */
+          rlh = &(ok -> ok_reclaim_list[BYTES_TO_GRANULES(sz)]);
+          hhdr -> hb_next = *rlh;
+          *rlh = hbp;
+        } /* else not worth salvaging. */
+        /* We used to do the nearly_full check later, but we    */
+        /* already have the right cache context here.  Also     */
+        /* doing it here avoids some silly lock contention in   */
+        /* GC_malloc_many.                                      */
+    }
+}
+
+#if !defined(NO_DEBUGGING)
+/* Routines to gather and print heap block info         */
+/* intended for debugging.  Otherwise should be called  */
+/* with lock.                                           */
+
+struct Print_stats
+{
+        size_t number_of_blocks;
+        size_t total_bytes;
+};
+
+#ifdef USE_MARK_BYTES
+
+/* Return the number of set mark bits in the given header.      */
+/* Remains externally visible as used by GNU GCJ currently.     */
+int GC_n_set_marks(hdr *hhdr)
+{
+    int result = 0;
+    int i;
+    size_t sz = hhdr -> hb_sz;
+    int offset = (int)MARK_BIT_OFFSET(sz);
+    int limit = (int)FINAL_MARK_BIT(sz);
+
+    for (i = 0; i < limit; i += offset) {
+        result += hhdr -> hb_marks[i];
+    }
+    GC_ASSERT(hhdr -> hb_marks[limit]);
+    return(result);
+}
+
+#else
+
+/* Number of set bits in a word.  Not performance critical.     */
+static int set_bits(word n)
+{
+    word m = n;
+    int result = 0;
+
+    while (m > 0) {
+        if (m & 1) result++;
+        m >>= 1;
+    }
+    return(result);
+}
+
+int GC_n_set_marks(hdr *hhdr)
+{
+    int result = 0;
+    int i;
+    int n_mark_words;
+#   ifdef MARK_BIT_PER_OBJ
+      int n_objs = (int)HBLK_OBJS(hhdr -> hb_sz);
+
+      if (0 == n_objs) n_objs = 1;
+      n_mark_words = divWORDSZ(n_objs + WORDSZ - 1);
+#   else /* MARK_BIT_PER_GRANULE */
+      n_mark_words = MARK_BITS_SZ;
+#   endif
+    for (i = 0; i < n_mark_words - 1; i++) {
+        result += set_bits(hhdr -> hb_marks[i]);
+    }
+#   ifdef MARK_BIT_PER_OBJ
+      result += set_bits((hhdr -> hb_marks[n_mark_words - 1])
+                         << (n_mark_words * WORDSZ - n_objs));
+#   else
+      result += set_bits(hhdr -> hb_marks[n_mark_words - 1]);
+#   endif
+    return(result - 1);
+}
+
+#endif /* !USE_MARK_BYTES  */
+
+STATIC void GC_print_block_descr(struct hblk *h,
+                                 word /* struct PrintStats */ raw_ps)
+{
+    hdr * hhdr = HDR(h);
+    size_t bytes = hhdr -> hb_sz;
+    struct Print_stats *ps;
+    unsigned n_marks = GC_n_set_marks(hhdr);
+
+    if (hhdr -> hb_n_marks != n_marks) {
+      GC_printf("(%u:%u,%u!=%u)", hhdr -> hb_obj_kind, (unsigned)bytes,
+                (unsigned)hhdr -> hb_n_marks, n_marks);
+    } else {
+      GC_printf("(%u:%u,%u)", hhdr -> hb_obj_kind,
+                (unsigned)bytes, n_marks);
+    }
+    bytes += HBLKSIZE-1;
+    bytes &= ~(HBLKSIZE-1);
+
+    ps = (struct Print_stats *)raw_ps;
+    ps->total_bytes += bytes;
+    ps->number_of_blocks++;
+}
+
+void GC_print_block_list(void)
+{
+    struct Print_stats pstats;
+
+    GC_printf("(kind(0=ptrfree,1=normal,2=unc.):size_in_bytes, #_marks_set)\n");
+    pstats.number_of_blocks = 0;
+    pstats.total_bytes = 0;
+    GC_apply_to_all_blocks(GC_print_block_descr, (word)&pstats);
+    GC_printf("\nblocks = %lu, bytes = %lu\n",
+              (unsigned long)pstats.number_of_blocks,
+              (unsigned long)pstats.total_bytes);
+}
+
+/* Currently for debugger use only: */
+void GC_print_free_list(int kind, size_t sz_in_granules)
+{
+    struct obj_kind * ok = &GC_obj_kinds[kind];
+    ptr_t flh = ok -> ok_freelist[sz_in_granules];
+    struct hblk *lastBlock = 0;
+    int n;
+
+    for (n = 1; flh; n++) {
+        struct hblk *block = HBLKPTR(flh);
+        if (block != lastBlock) {
+          GC_printf("\nIn heap block at %p:\n\t", block);
+          lastBlock = block;
+        }
+        GC_printf("%d: %p;", n, flh);
+        flh = obj_link(flh);
+    }
+}
+
+#endif /* !NO_DEBUGGING */
+
+/*
+ * Clear all obj_link pointers in the list of free objects *flp.
+ * Clear *flp.
+ * This must be done before dropping a list of free gcj-style objects,
+ * since may otherwise end up with dangling "descriptor" pointers.
+ * It may help for other pointer-containing objects.
+ */
+STATIC void GC_clear_fl_links(void **flp)
+{
+    void *next = *flp;
+
+    while (0 != next) {
+       *flp = 0;
+       flp = &(obj_link(next));
+       next = *flp;
+    }
+}
+
+/*
+ * Perform GC_reclaim_block on the entire heap, after first clearing
+ * small object free lists (if we are not just looking for leaks).
+ */
+GC_INNER void GC_start_reclaim(GC_bool report_if_found)
+{
+    unsigned kind;
+
+#   if defined(PARALLEL_MARK)
+      GC_ASSERT(0 == GC_fl_builder_count);
+#   endif
+    /* Reset in use counters.  GC_reclaim_block recomputes them. */
+      GC_composite_in_use = 0;
+      GC_atomic_in_use = 0;
+    /* Clear reclaim- and free-lists */
+      for (kind = 0; kind < GC_n_kinds; kind++) {
+        void **fop;
+        void **lim;
+        struct hblk ** rlist = GC_obj_kinds[kind].ok_reclaim_list;
+        GC_bool should_clobber = (GC_obj_kinds[kind].ok_descriptor != 0);
+
+        if (rlist == 0) continue;       /* This kind not used.  */
+        if (!report_if_found) {
+            lim = &(GC_obj_kinds[kind].ok_freelist[MAXOBJGRANULES+1]);
+            for( fop = GC_obj_kinds[kind].ok_freelist; fop < lim; fop++ ) {
+              if (*fop != 0) {
+                if (should_clobber) {
+                  GC_clear_fl_links(fop);
+                } else {
+                  *fop = 0;
+                }
+              }
+            }
+        } /* otherwise free list objects are marked,    */
+          /* and its safe to leave them                 */
+        BZERO(rlist, (MAXOBJGRANULES + 1) * sizeof(void *));
+      }
+
+
+  /* Go through all heap blocks (in hblklist) and reclaim unmarked objects */
+  /* or enqueue the block for later processing.                            */
+    GC_apply_to_all_blocks(GC_reclaim_block, (word)report_if_found);
+
+# ifdef EAGER_SWEEP
+    /* This is a very stupid thing to do.  We make it possible anyway,  */
+    /* so that you can convince yourself that it really is very stupid. */
+    GC_reclaim_all((GC_stop_func)0, FALSE);
+# endif
+# if defined(PARALLEL_MARK)
+    GC_ASSERT(0 == GC_fl_builder_count);
+# endif
+
+}
+
+/*
+ * Sweep blocks of the indicated object size and kind until either the
+ * appropriate free list is nonempty, or there are no more blocks to
+ * sweep.
+ */
+GC_INNER void GC_continue_reclaim(size_t sz /* granules */, int kind)
+{
+    hdr * hhdr;
+    struct hblk * hbp;
+    struct obj_kind * ok = &(GC_obj_kinds[kind]);
+    struct hblk ** rlh = ok -> ok_reclaim_list;
+    void **flh = &(ok -> ok_freelist[sz]);
+
+    if (rlh == 0) return;       /* No blocks of this kind.      */
+    rlh += sz;
+    while ((hbp = *rlh) != 0) {
+        hhdr = HDR(hbp);
+        *rlh = hhdr -> hb_next;
+        GC_reclaim_small_nonempty_block(hbp, FALSE);
+        if (*flh != 0) break;
+    }
+}
+
+/*
+ * Reclaim all small blocks waiting to be reclaimed.
+ * Abort and return FALSE when/if (*stop_func)() returns TRUE.
+ * If this returns TRUE, then it's safe to restart the world
+ * with incorrectly cleared mark bits.
+ * If ignore_old is TRUE, then reclaim only blocks that have been
+ * recently reclaimed, and discard the rest.
+ * Stop_func may be 0.
+ */
+GC_INNER GC_bool GC_reclaim_all(GC_stop_func stop_func, GC_bool ignore_old)
+{
+    word sz;
+    unsigned kind;
+    hdr * hhdr;
+    struct hblk * hbp;
+    struct obj_kind * ok;
+    struct hblk ** rlp;
+    struct hblk ** rlh;
+#   ifndef SMALL_CONFIG
+      CLOCK_TYPE start_time = 0; /* initialized to prevent warning. */
+      CLOCK_TYPE done_time;
+
+      if (GC_print_stats == VERBOSE)
+        GET_TIME(start_time);
+#   endif
+
+    for (kind = 0; kind < GC_n_kinds; kind++) {
+        ok = &(GC_obj_kinds[kind]);
+        rlp = ok -> ok_reclaim_list;
+        if (rlp == 0) continue;
+        for (sz = 1; sz <= MAXOBJGRANULES; sz++) {
+            rlh = rlp + sz;
+            while ((hbp = *rlh) != 0) {
+                if (stop_func != (GC_stop_func)0 && (*stop_func)()) {
+                    return(FALSE);
+                }
+                hhdr = HDR(hbp);
+                *rlh = hhdr -> hb_next;
+                if (!ignore_old || hhdr -> hb_last_reclaimed == GC_gc_no - 1) {
+                    /* It's likely we'll need it this time, too */
+                    /* It's been touched recently, so this      */
+                    /* shouldn't trigger paging.                */
+                    GC_reclaim_small_nonempty_block(hbp, FALSE);
+                }
+            }
+        }
+    }
+#   ifndef SMALL_CONFIG
+      if (GC_print_stats == VERBOSE) {
+        GET_TIME(done_time);
+        GC_log_printf("Disposing of reclaim lists took %lu msecs\n",
+                      MS_TIME_DIFF(done_time,start_time));
+      }
+#   endif
+    return(TRUE);
+}
diff --git a/src/gc/bdwgc/sparc_mach_dep.S b/src/gc/bdwgc/sparc_mach_dep.S
new file mode 100644
index 0000000..d204dc4
--- /dev/null
+++ b/src/gc/bdwgc/sparc_mach_dep.S
@@ -0,0 +1,61 @@
+!	SPARCompiler 3.0 and later apparently no longer handles
+!	asm outside functions.  So we need a separate .s file
+!	This is only set up for SunOS 5, not SunOS 4.
+!	Assumes this is called before the stack contents are
+!	examined.
+
+	.seg 	"text"
+	.globl	GC_save_regs_in_stack
+GC_save_regs_in_stack:
+#if defined(__arch64__) || defined(__sparcv9)
+	save	%sp,-128,%sp
+	flushw
+	ret
+	  restore %sp,2047+128,%o0
+#else /* 32 bit SPARC */
+	ta	0x3   ! ST_FLUSH_WINDOWS
+	mov	%sp,%o0
+	retl
+	nop
+#endif /* 32 bit SPARC */
+.GC_save_regs_in_stack_end:
+	.size GC_save_regs_in_stack,.GC_save_regs_in_stack_end-GC_save_regs_in_stack
+
+! GC_clear_stack_inner(arg, limit) clears stack area up to limit and
+! returns arg.  Stack clearing is crucial on SPARC, so we supply
+! an assembly version that s more careful.  Assumes limit is hotter
+! than sp, and limit is 8 byte aligned.
+	.globl	GC_clear_stack_inner
+GC_clear_stack_inner:
+#if defined(__arch64__) || defined(__sparcv9)
+	mov %sp,%o2		! Save sp
+	add %sp,2047-8,%o3	! p = sp+bias-8
+	add %o1,-2047-192,%sp	! Move sp out of the way,
+  				! so that traps still work.
+  				! Includes some extra words
+  				! so we can be sloppy below.
+loop:
+	stx %g0,[%o3]		! *(long *)p = 0
+	cmp %o3,%o1
+	bgu,pt %xcc, loop	! if (p > limit) goto loop
+          add %o3,-8,%o3	! p -= 8 (delay slot)
+	retl
+    	  mov %o2,%sp		! Restore sp., delay slot
+#else  /* 32 bit SPARC */
+	mov	%sp,%o2		! Save sp
+	add	%sp,-8,%o3	! p = sp-8
+	clr	%g1		! [g0,g1] = 0
+	add	%o1,-0x60,%sp	! Move sp out of the way,
+				! so that traps still work.
+				! Includes some extra words
+				! so we can be sloppy below.
+loop:
+	std	%g0,[%o3]	! *(long long *)p = 0
+	cmp	%o3,%o1
+	bgu	loop		! if (p > limit) goto loop
+	  add	%o3,-8,%o3	! p -= 8 (delay slot)
+	retl
+	  mov	%o2,%sp		! Restore sp., delay slot
+#endif  /* 32 bit SPARC */
+.GC_clear_stack_inner_end:
+      	.size GC_clear_stack_inner,.GC_clear_stack_inner_end-GC_clear_stack_inner
diff --git a/src/gc/bdwgc/specific.c b/src/gc/bdwgc/specific.c
new file mode 100644
index 0000000..98d86c3
--- /dev/null
+++ b/src/gc/bdwgc/specific.c
@@ -0,0 +1,162 @@
+/*
+ * Copyright (c) 2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"    /* For configuration, pthreads.h. */
+#include "private/thread_local_alloc.h"
+                /* To determine type of tsd impl.       */
+                /* Includes private/specific.h          */
+                /* if needed.                           */
+
+#if defined(USE_CUSTOM_SPECIFIC)
+
+#include "atomic_ops.h"
+
+static tse invalid_tse = {INVALID_QTID, 0, 0, INVALID_THREADID};
+            /* A thread-specific data entry which will never    */
+            /* appear valid to a reader.  Used to fill in empty */
+            /* cache entries to avoid a check for 0.            */
+
+int PREFIXED(key_create) (tsd ** key_ptr, void (* destructor)(void *)) {
+    int i;
+    tsd * result = (tsd *)MALLOC_CLEAR(sizeof(tsd));
+
+    /* A quick alignment check, since we need atomic stores */
+    GC_ASSERT((unsigned long)(&invalid_tse.next) % sizeof(tse *) == 0);
+    if (0 == result) return ENOMEM;
+    pthread_mutex_init(&(result -> lock), NULL);
+    for (i = 0; i < TS_CACHE_SIZE; ++i) {
+      result -> cache[i] = &invalid_tse;
+    }
+#   ifdef GC_ASSERTIONS
+      for (i = 0; i < TS_HASH_SIZE; ++i) {
+        GC_ASSERT(result -> hash[i] == 0);
+      }
+#   endif
+    *key_ptr = result;
+    return 0;
+}
+
+int PREFIXED(setspecific) (tsd * key, void * value) {
+    pthread_t self = pthread_self();
+    int hash_val = HASH(self);
+    volatile tse * entry = (volatile tse *)MALLOC_CLEAR(sizeof (tse));
+
+    GC_ASSERT(self != INVALID_THREADID);
+    if (0 == entry) return ENOMEM;
+    pthread_mutex_lock(&(key -> lock));
+    /* Could easily check for an existing entry here.   */
+    entry -> next = key -> hash[hash_val];
+    entry -> thread = self;
+    entry -> value = value;
+    GC_ASSERT(entry -> qtid == INVALID_QTID);
+    /* There can only be one writer at a time, but this needs to be     */
+    /* atomic with respect to concurrent readers.                       */
+    AO_store_release((volatile AO_t *)(key -> hash + hash_val), (AO_t)entry);
+    pthread_mutex_unlock(&(key -> lock));
+    return 0;
+}
+
+/* Remove thread-specific data for this thread.  Should be called on    */
+/* thread exit.                                                         */
+void PREFIXED(remove_specific) (tsd * key) {
+    pthread_t self = pthread_self();
+    unsigned hash_val = HASH(self);
+    tse *entry;
+    tse **link = key -> hash + hash_val;
+
+    pthread_mutex_lock(&(key -> lock));
+    entry = *link;
+    while (entry != NULL && entry -> thread != self) {
+      link = &(entry -> next);
+      entry = *link;
+    }
+    /* Invalidate qtid field, since qtids may be reused, and a later    */
+    /* cache lookup could otherwise find this entry.                    */
+    if (entry != NULL) {
+      entry -> qtid = INVALID_QTID;
+      *link = entry -> next;
+      /* Atomic! concurrent accesses still work.        */
+      /* They must, since readers don't lock.           */
+      /* We shouldn't need a volatile access here,      */
+      /* since both this and the preceding write        */
+      /* should become visible no later than            */
+      /* the pthread_mutex_unlock() call.               */
+    }
+    /* If we wanted to deallocate the entry, we'd first have to clear   */
+    /* any cache entries pointing to it.  That probably requires        */
+    /* additional synchronization, since we can't prevent a concurrent  */
+    /* cache lookup, which should still be examining deallocated memory.*/
+    /* This can only happen if the concurrent access is from another    */
+    /* thread, and hence has missed the cache, but still...             */
+
+    /* With GC, we're done, since the pointers from the cache will      */
+    /* be overwritten, all local pointers to the entries will be        */
+    /* dropped, and the entry will then be reclaimed.                   */
+    pthread_mutex_unlock(&(key -> lock));
+}
+
+/* Note that even the slow path doesn't lock.   */
+void * PREFIXED(slow_getspecific) (tsd * key, unsigned long qtid,
+                tse * volatile * cache_ptr) {
+    pthread_t self = pthread_self();
+    unsigned hash_val = HASH(self);
+    tse *entry = key -> hash[hash_val];
+
+    GC_ASSERT(qtid != INVALID_QTID);
+    while (entry != NULL && entry -> thread != self) {
+      entry = entry -> next;
+    }
+    if (entry == NULL) return NULL;
+    /* Set cache_entry. */
+    entry -> qtid = (AO_t)qtid;
+        /* It's safe to do this asynchronously.  Either value   */
+        /* is safe, though may produce spurious misses.         */
+        /* We're replacing one qtid with another one for the    */
+        /* same thread.                                         */
+    *cache_ptr = entry;
+        /* Again this is safe since pointer assignments are     */
+        /* presumed atomic, and either pointer is valid.        */
+    return entry -> value;
+}
+
+#ifdef GC_ASSERTIONS
+  /* Check that that all elements of the data structure associated  */
+  /* with key are marked.                                           */
+  void PREFIXED(check_tsd_marks) (tsd *key)
+  {
+    int i;
+    tse *p;
+
+    if (!GC_is_marked(GC_base(key))) {
+      ABORT("Unmarked thread-specific-data table");
+    }
+    for (i = 0; i < TS_HASH_SIZE; ++i) {
+      for (p = key -> hash[i]; p != 0; p = p -> next) {
+        if (!GC_is_marked(GC_base(p))) {
+          GC_err_printf("Thread-specific-data entry at %p not marked\n", p);
+          ABORT("Unmarked tse");
+        }
+      }
+    }
+    for (i = 0; i < TS_CACHE_SIZE; ++i) {
+      p = key -> cache[i];
+      if (p != &invalid_tse && !GC_is_marked(GC_base(p))) {
+        GC_err_printf("Cached thread-specific-data entry at %p not marked\n",
+                      p);
+        ABORT("Unmarked cached tse");
+      }
+    }
+  }
+#endif /* GC_ASSERTIONS */
+
+#endif /* USE_CUSTOM_SPECIFIC */
diff --git a/src/gc/bdwgc/stubborn.c b/src/gc/bdwgc/stubborn.c
new file mode 100644
index 0000000..b1991f1
--- /dev/null
+++ b/src/gc/bdwgc/stubborn.c
@@ -0,0 +1,59 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#if defined(MANUAL_VDB)
+
+  /* Stubborn object (hard to change, nearly immutable) allocation.     */
+  /* This interface is deprecated.  We mostly emulate it using          */
+  /* MANUAL_VDB.  But that imposes the additional constraint that       */
+  /* written, but not yet GC_dirty()ed objects must be referenced       */
+  /* by a stack.                                                        */
+
+  void GC_dirty(ptr_t p);
+
+  GC_API void * GC_CALL GC_malloc_stubborn(size_t lb)
+  {
+    return(GC_malloc(lb));
+  }
+
+  GC_API void GC_CALL GC_end_stubborn_change(void *p)
+  {
+    GC_dirty(p);
+  }
+
+  /*ARGSUSED*/
+  GC_API void GC_CALL GC_change_stubborn(void *p)
+  {
+  }
+
+#else /* !MANUAL_VDB */
+
+  GC_API void * GC_CALL GC_malloc_stubborn(size_t lb)
+  {
+    return(GC_malloc(lb));
+  }
+
+  /*ARGSUSED*/
+  GC_API void GC_CALL GC_end_stubborn_change(void *p)
+  {
+  }
+
+  /*ARGSUSED*/
+  GC_API void GC_CALL GC_change_stubborn(void *p)
+  {
+  }
+
+#endif /* !MANUAL_VDB */
diff --git a/src/gc/bdwgc/tests/CMakeLists.txt b/src/gc/bdwgc/tests/CMakeLists.txt
new file mode 100644
index 0000000..0f4d455
--- /dev/null
+++ b/src/gc/bdwgc/tests/CMakeLists.txt
@@ -0,0 +1,19 @@
+#
+# Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+# Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+# Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+# Copyright (c) 2000-2010 by Hewlett-Packard Company.  All rights reserved.
+##
+# THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+# OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+##
+# Permission is hereby granted to use or copy this program
+# for any purpose,  provided the above notices are retained on all copies.
+# Permission to modify the code and to distribute modified code is granted,
+# provided the above notices are retained, and a notice that the code was
+# modified is included with the above copyright notice.
+##
+
+ADD_DEFINITIONS(-DGC_NOT_DLL)
+ADD_EXECUTABLE(gctest WIN32 test.c)
+TARGET_LINK_LIBRARIES(gctest gc-lib)
diff --git a/src/gc/bdwgc/tests/Makefile b/src/gc/bdwgc/tests/Makefile
new file mode 100644
index 0000000..22af511
--- /dev/null
+++ b/src/gc/bdwgc/tests/Makefile
@@ -0,0 +1,16 @@
+CFLAGS += -Isrc/gc/bdwgc/include \
+	-DNAUT \
+	-Ulinux \
+	-U__linux__ \
+	-U__GNU__ \
+	-U__GLIBC__ \
+	-DNO_CLOCK \
+	-DSMALL_CONFIG \
+	-DGC_DISABLE_INCREMENTAL \
+	-DNO_GETCONTEXT \
+	-DNO_DEBUGGING
+obj-y += test.o \
+	     huge_test.o \
+         realloc_test.o \
+         leak_test.o 
+#		 setjmp_t.o
diff --git a/src/gc/bdwgc/tests/huge_test.c b/src/gc/bdwgc/tests/huge_test.c
new file mode 100644
index 0000000..0468389
--- /dev/null
+++ b/src/gc/bdwgc/tests/huge_test.c
@@ -0,0 +1,54 @@
+
+
+#include <nautilus/libccompat.h>
+#include <nautilus/limits.h>
+#include <nautilus/printk.h>
+
+
+#ifndef GC_IGNORE_WARN
+/* Ignore misleading "Out of Memory!" warning (which is printed on    */
+/* every GC_MALLOC(LONG_MAX) call) by defining this macro before      */
+/* "gc.h" inclusion.                                                  */
+# define GC_IGNORE_WARN
+#endif
+
+#include "test.h"
+#include "gc.h"
+
+/*
+ * Check that very large allocation requests fail.  "Success" would usually
+ * indicate that the size was somehow converted to a negative
+ * number.  Clients shouldn't do this, but we should fail in the
+ * expected manner.
+ */
+
+int huge_test(void)
+{
+    GC_INIT();
+
+    GC_set_max_heap_size(100*1024*1024);
+        /* Otherwise heap expansion aborts when deallocating large block. */
+        /* That's OK.  We test this corner case mostly to make sure that  */
+        /* it fails predictably.                                          */
+    GC_expand_hp(1024*1024*5);
+    if (sizeof(long) == sizeof(void *)) {
+        void *r = GC_MALLOC(LONG_MAX-1024);
+        if (0 != r) {
+            printk("Size LONG_MAX-1024 allocation unexpectedly succeeded\n");
+            exit(1);
+        }
+        r = GC_MALLOC(LONG_MAX);
+        if (0 != r) {
+            printk("Size LONG_MAX allocation unexpectedly succeeded\n");
+            exit(1);
+        }
+        r = GC_MALLOC((size_t)LONG_MAX + 1024);
+        if (0 != r) {
+            printk("Size LONG_MAX+1024 allocation unexpectedly succeeded\n");
+            exit(1);
+        }
+    }
+    printk("Successfully passed huge test!\n");
+
+    return 0;
+}
diff --git a/src/gc/bdwgc/tests/initsecondarythread.c b/src/gc/bdwgc/tests/initsecondarythread.c
new file mode 100644
index 0000000..9b2ca60
--- /dev/null
+++ b/src/gc/bdwgc/tests/initsecondarythread.c
@@ -0,0 +1,100 @@
+/*
+ * Copyright (C) 2011 Ludovic Courtes <ludo@gnu.org>
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED. ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose, provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* Make sure 'GC_INIT' can be called from threads other than the initial
+ * thread.
+ */
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+#ifndef GC_THREADS
+# define GC_THREADS
+#endif
+
+#define GC_NO_THREAD_REDIRECTS 1
+                /* Do not redirect thread creation and join calls.      */
+
+#include "gc.h"
+
+#ifdef GC_PTHREADS
+# include <pthread.h>
+#else
+# include <windows.h>
+#endif
+
+#include <stdlib.h>
+#include <stdio.h>
+
+#ifdef GC_PTHREADS
+  static void *thread(void *arg)
+#else
+  static DWORD WINAPI thread(LPVOID arg)
+#endif
+{
+  GC_INIT();
+  (void)GC_MALLOC(123);
+  (void)GC_MALLOC(12345);
+# ifdef GC_PTHREADS
+    return arg;
+# else
+    return (DWORD)(GC_word)arg;
+# endif
+}
+
+#include "private/gcconfig.h"
+
+int main(void)
+{
+# ifdef GC_PTHREADS
+    int code;
+    pthread_t t;
+# else
+    HANDLE t;
+    DWORD thread_id;
+# endif
+# if !(defined(BEOS) || defined(MSWIN32) || defined(MSWINCE) \
+       || defined(CYGWIN32) || defined(GC_OPENBSD_THREADS) \
+       || (defined(DARWIN) && !defined(NO_PTHREAD_GET_STACKADDR_NP)) \
+       || (defined(LINUX) && !defined(NACL)) \
+       || (defined(GC_SOLARIS_THREADS) && !defined(_STRICT_STDC)) \
+       || (!defined(STACKBOTTOM) && (defined(HEURISTIC1) \
+          || (!defined(LINUX_STACKBOTTOM) && !defined(FREEBSD_STACKBOTTOM)))))
+    /* GC_INIT() must be called from main thread only. */
+    GC_INIT();
+# endif
+# ifdef GC_PTHREADS
+    if ((code = pthread_create (&t, NULL, thread, NULL)) != 0) {
+      printf("Thread creation failed %d\n", code);
+      return 1;
+    }
+    if ((code = pthread_join (t, NULL)) != 0) {
+      printf("Thread join failed %d\n", code);
+      return 1;
+    }
+# else
+    t = CreateThread(NULL, 0, thread, 0, 0, &thread_id);
+    if (t == NULL) {
+      printf("Thread creation failed %d\n", (int)GetLastError());
+      return 1;
+    }
+    if (WaitForSingleObject(t, INFINITE) != WAIT_OBJECT_0) {
+      printf("Thread join failed %d\n", (int)GetLastError());
+      CloseHandle(t);
+      return 1;
+    }
+    CloseHandle(t);
+# endif
+  return 0;
+}
diff --git a/src/gc/bdwgc/tests/leak_test.c b/src/gc/bdwgc/tests/leak_test.c
new file mode 100644
index 0000000..51eca27
--- /dev/null
+++ b/src/gc/bdwgc/tests/leak_test.c
@@ -0,0 +1,44 @@
+
+
+#include <nautilus/printk.h>
+
+#include "gc.h"
+#include "test.h"
+
+//#include "leak_detector.h"
+
+int leak_test(void) {
+    int *p[10];
+    int i;
+
+    // Explicitly trigger full collection before mode change to clean
+    // up previous tests
+    GC_gcollect();
+    
+    GC_set_find_leak(1); /* for new collect versions not compiled       */
+                         /* with -DFIND_LEAK.                           */
+    GC_INIT();  /* Needed if thread-local allocation is enabled.        */
+                /* FIXME: This is not ideal.                            */
+
+    printk("Check for any pre-test leaks (should be none!)\n");
+    GC_gcollect();
+    
+    for (i = 0; i < 10; ++i) {
+        p[i] = GC_MALLOC(sizeof(int)+i);
+    }
+
+    for (i = 1; i < 10; ++i) {
+        GC_FREE(p[i]);
+    }
+
+    // Leak p[0] by overwriting it without freeing 
+    for (i = 0; i < 10; ++i) {
+        p[i] = GC_MALLOC(sizeof(int)+i);
+    }
+
+    printk("Check for any leaks after overwriting pointer (should be 1!)\n");
+    GC_gcollect();
+
+    GC_set_find_leak(0); // Not sure if this is allowed 
+    return 0;
+}
diff --git a/src/gc/bdwgc/tests/middle.c b/src/gc/bdwgc/tests/middle.c
new file mode 100644
index 0000000..ff0a235
--- /dev/null
+++ b/src/gc/bdwgc/tests/middle.c
@@ -0,0 +1,25 @@
+/*
+ * Test at the boundary between small and large objects.
+ * Inspired by a test case from Zoltan Varga.
+ */
+#include "gc.h"
+#include <stdio.h>
+
+int main (void)
+{
+  int i;
+
+  GC_set_all_interior_pointers(0);
+  GC_INIT();
+
+  for (i = 0; i < 20000; ++i) {
+    GC_malloc_atomic (4096);
+    GC_malloc (4096);
+  }
+  for (i = 0; i < 20000; ++i) {
+    GC_malloc_atomic (2048);
+    GC_malloc (2048);
+  }
+  printf("Final heap size is %lu\n", (unsigned long)GC_get_heap_size());
+  return 0;
+}
diff --git a/src/gc/bdwgc/tests/realloc_test.c b/src/gc/bdwgc/tests/realloc_test.c
new file mode 100644
index 0000000..e72a616
--- /dev/null
+++ b/src/gc/bdwgc/tests/realloc_test.c
@@ -0,0 +1,38 @@
+
+
+#include <nautilus/printk.h>
+#include <nautilus/libccompat.h>
+
+#include "test.h"
+#include "gc.h"
+
+#define COUNT 10000000
+
+int realloc_test(void) {
+  int i;
+  unsigned long last_heap_size = 0;
+
+  GC_INIT();
+
+  for (i = 0; i < COUNT; i++) {
+    int **p = GC_MALLOC(sizeof(int *));
+    int *q = GC_MALLOC_ATOMIC(sizeof(int));
+
+    if (p == 0 || *p != 0) {
+      printk("GC_malloc returned garbage (or NULL)\n");
+      exit(1);
+    }
+
+    *p = GC_REALLOC(q, 2 * sizeof(int));
+
+    if (i % 10 == 0) {
+      unsigned long heap_size = (unsigned long)GC_get_heap_size();
+      if (heap_size != last_heap_size) {
+        printk("Heap size: %lu\n", heap_size);
+        last_heap_size = heap_size;
+      }
+    }
+  }
+  printk("Succesfully passed realloc test!\n");
+  return 0;
+}
diff --git a/src/gc/bdwgc/tests/setjmp_t.c b/src/gc/bdwgc/tests/setjmp_t.c
new file mode 100644
index 0000000..bd0633c
--- /dev/null
+++ b/src/gc/bdwgc/tests/setjmp_t.c
@@ -0,0 +1,147 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+/* Check whether setjmp actually saves registers in jmp_buf. */
+/* If it doesn't, the generic mark_regs code won't work.     */
+/* Compilers vary as to whether they will put x in a         */
+/* (callee-save) register without -O.  The code is           */
+/* contrived such that any decent compiler should put x in   */
+/* a callee-save register with -O.  Thus it is               */
+/* recommended that this be run optimized.  (If the machine  */
+/* has no callee-save registers, then the generic code is    */
+/* safe, but this will not be noticed by this piece of       */
+/* code.)  This test appears to be far from perfect.         */
+
+
+#ifdef NAUT
+# include <nautilus/printk.h>
+# include <nautilus/setjmp.h>
+#else
+# include <stdio.h>
+# include <setjmp.h>
+# include <string.h>
+#endif
+
+# include "private/gc_priv.h"
+
+#ifdef OS2
+/* GETPAGESIZE() is set to getpagesize() by default, but that   */
+/* doesn't really exist, and the collector doesn't need it.     */
+#define INCL_DOSFILEMGR
+#define INCL_DOSMISC
+#define INCL_DOSERRORS
+#include <os2.h>
+
+int getpagesize(void)
+{
+    ULONG result[1];
+
+    if (DosQuerySysInfo(QSV_PAGE_SIZE, QSV_PAGE_SIZE,
+                        (void *)result, sizeof(ULONG)) != NO_ERROR) {
+        fprintf(stderr, "DosQuerySysInfo failed\n");
+        result[0] = 4096;
+    }
+    return((int)(result[0]));
+}
+#endif
+
+struct {
+  char a_a;
+  char * a_b;
+} a;
+
+int * nested_sp(void)
+{
+    volatile int sp;
+    sp = (int)&sp;
+    return (int *)sp;
+}
+
+
+int bdwgc_test_setjmp(void)
+{
+    volatile word sp;
+    long ps = GETPAGESIZE();
+    jmp_buf b;
+    register int x = (int)strlen("a");  /* 1, slightly disguised */
+    static int y = 0;
+
+    sp = (word)(&sp);
+    printf("This appears to be a %s running %s\n", MACH_TYPE, OS_TYPE);
+
+    if (nested_sp() < (int *)sp) {
+      printf("Stack appears to grow down, which is the default.\n");
+      printf("A good guess for STACKBOTTOM on this machine is 0x%lx.\n",
+             ((unsigned long)sp + ps) & ~(ps-1));
+    } else {
+      printf("Stack appears to grow up.\n");
+      printf("Define STACK_GROWS_UP in gc_private.h\n");
+      printf("A good guess for STACKBOTTOM on this machine is 0x%lx.\n",
+             ((unsigned long)sp + ps) & ~(ps-1));
+    }
+    printf("Note that this may vary between machines of ostensibly\n");
+    printf("the same architecture (e.g. Sun 3/50s and 3/80s).\n");
+    printf("On many machines the value is not fixed.\n");
+    printf("A good guess for ALIGNMENT on this machine is %ld.\n",
+           (unsigned long)(&(a.a_b))-(unsigned long)(&a));
+
+    printf("The following is a very dubious test of one root marking"
+           " strategy.\n");
+    printf("Results may not be accurate/useful:\n");
+    /* Encourage the compiler to keep x in a callee-save register */
+    x = 2*x-1;
+    printf("");
+    x = 2*x-1;
+    setjmp(b);
+    if (y == 1) {
+      if (x == 2) {
+        printf("Setjmp-based generic mark_regs code probably wont work.\n");
+        printf("But we rarely try that anymore.  If you have getcontect()\n");
+        printf("this probably doesn't matter.\n");
+      } else if (x == 1) {
+          printf("Setjmp-based register marking code may work.\n");
+      } else {
+          printf("Very strange setjmp implementation.\n");
+      }
+    }
+    y++;
+    x = 2;
+    if (y == 1) longjmp(b,1);
+    printf("Some GC internal configuration stuff: \n");
+    printf("\tWORDSZ = %d, ALIGNMENT = %d, GC_GRANULE_BYTES = %d\n",
+           WORDSZ, ALIGNMENT, GC_GRANULE_BYTES);
+    printf("\tUsing one mark ");
+#   if defined(USE_MARK_BYTES)
+      printf("byte");
+#   else
+      printf("bit");
+#   endif
+    printf(" per ");
+#   if defined(MARK_BIT_PER_OBJ)
+      printf("object.\n");
+#   elif defined(MARK_BIT_PER_GRANULE)
+      printf("granule.\n");
+#   endif
+#   ifdef THREAD_LOCAL_ALLOC
+      printf("Thread local allocation enabled.\n");
+#   endif
+#   ifdef PARALLEL_MARK
+      printf("Parallel marking enabled.\n");
+#   endif
+    return(0);
+}
+
+int g(int x)
+{
+    return(x);
+}
diff --git a/src/gc/bdwgc/tests/smash_test.c b/src/gc/bdwgc/tests/smash_test.c
new file mode 100644
index 0000000..0e8b1f0
--- /dev/null
+++ b/src/gc/bdwgc/tests/smash_test.c
@@ -0,0 +1,28 @@
+/*
+ * Test that overwrite error detection works reasonably.
+ */
+#define GC_DEBUG
+#include "gc.h"
+
+#include <stdio.h>
+
+#define COUNT 7000
+#define SIZE  40
+
+char * A[COUNT];
+
+int main(void)
+{
+  int i;
+  char *p;
+
+  GC_INIT();
+
+  for (i = 0; i < COUNT; ++i) {
+     A[i] = p = GC_MALLOC(SIZE);
+
+     if (i%3000 == 0) GC_gcollect();
+     if (i%5678 == 0 && p != 0) p[SIZE + i/2000] = 42;
+  }
+  return 0;
+}
diff --git a/src/gc/bdwgc/tests/staticrootslib.c b/src/gc/bdwgc/tests/staticrootslib.c
new file mode 100644
index 0000000..2a8fcd4
--- /dev/null
+++ b/src/gc/bdwgc/tests/staticrootslib.c
@@ -0,0 +1,33 @@
+
+/* This test file is intended to be compiled into a DLL. */
+
+#include <stdio.h>
+
+#ifndef GC_DEBUG
+# define GC_DEBUG
+#endif
+
+#include "gc.h"
+
+struct treenode {
+    struct treenode *x;
+    struct treenode *y;
+} * root[10];
+
+struct treenode * libsrl_mktree(int i)
+{
+  struct treenode * r = GC_MALLOC(sizeof(struct treenode));
+  if (0 == i) return 0;
+  if (1 == i) r = GC_MALLOC_ATOMIC(sizeof(struct treenode));
+  if (r) {
+    r -> x = libsrl_mktree(i-1);
+    r -> y = libsrl_mktree(i-1);
+  }
+  return r;
+}
+
+void * libsrl_init(void)
+{
+  GC_INIT();
+  return GC_MALLOC(sizeof(struct treenode));
+}
diff --git a/src/gc/bdwgc/tests/staticrootstest.c b/src/gc/bdwgc/tests/staticrootstest.c
new file mode 100644
index 0000000..222f209
--- /dev/null
+++ b/src/gc/bdwgc/tests/staticrootstest.c
@@ -0,0 +1,57 @@
+
+#include <stdio.h>
+#include <string.h>
+
+#ifndef GC_DEBUG
+# define GC_DEBUG
+#endif
+
+#include "gc.h"
+#include "gc_backptr.h"
+
+struct treenode {
+    struct treenode *x;
+    struct treenode *y;
+} * root[10];
+
+static char *staticroot = 0;
+
+extern struct treenode * libsrl_mktree(int i);
+extern void * libsrl_init(void);
+
+/*
+struct treenode * mktree(int i) {
+  struct treenode * r = GC_MALLOC(sizeof(struct treenode));
+  if (0 == i) return 0;
+  if (1 == i) r = GC_MALLOC_ATOMIC(sizeof(struct treenode));
+  r -> x = mktree(i-1);
+  r -> y = mktree(i-1);
+  return r;
+}*/
+
+int main(void)
+{
+  int i;
+  /*GC_INIT();
+  staticroot = GC_MALLOC(sizeof(struct treenode));*/
+  staticroot = libsrl_init();
+  memset(staticroot, 0x42, sizeof(struct treenode));
+  GC_gcollect();
+  for (i = 0; i < 10; ++i) {
+    root[i] = libsrl_mktree(12);
+    GC_gcollect();
+  }
+  for (i = 0; i < (int)sizeof(struct treenode); ++i) {
+    if (staticroot[i] != 0x42)
+      return -1;
+  }
+  for (i = 0; i < 10; ++i) {
+    root[i] = libsrl_mktree(12);
+    GC_gcollect();
+  }
+  for (i = 0; i < (int)sizeof(struct treenode); ++i) {
+    if (staticroot[i] != 0x42)
+      return -1;
+  }
+  return 0;
+}
diff --git a/src/gc/bdwgc/tests/test.c b/src/gc/bdwgc/tests/test.c
new file mode 100644
index 0000000..ac8e2da
--- /dev/null
+++ b/src/gc/bdwgc/tests/test.c
@@ -0,0 +1,1677 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+/* An incomplete test for the garbage collector.                */
+/* Some more obscure entry points are not tested at all.        */
+/* This must be compiled with the same flags used to build the  */
+/* GC.  It uses GC internals to allow more precise results      */
+/* checking for some of the tests.                              */
+
+# ifdef HAVE_CONFIG_H
+#   include "private/config.h"
+# endif
+
+# undef GC_BUILD
+
+#if (defined(DBG_HDRS_ALL) || defined(MAKE_BACK_GRAPH)) && !defined(GC_DEBUG)
+#  define GC_DEBUG
+#endif
+
+#include "test.h"
+#include "gc.h"
+
+#ifndef NTHREADS /* Number of additional threads to fork. */
+#  define NTHREADS 1 /** Tested working on up to 10 threads */
+        /* Not respected by PCR test. */
+#endif
+
+#if defined(mips) && defined(SYSTYPE_BSD43)
+    /* MIPS RISCOS 4 */
+#elif !defined(NAUT)
+#   include <stdlib.h>
+#endif
+
+#ifndef NAUT
+# include <stdio.h>
+#endif
+
+/* # if defined(_WIN32_WCE) && !defined(__GNUC__) */
+/* #   include <winbase.h> */
+/* /\* #   define assert ASSERT *\/ */
+/* # else */
+/* #   include <assert.h>        /\* Not normally used, but handy for debugging. *\/ */
+/* # endif */
+
+
+# include "gc_typed.h"
+# include "private/gc_priv.h"   /* For output, locking, MIN_WORDS,      */
+                                /* some statistics and gcconfig.h.      */
+
+/* # if defined(MSWIN32) || defined(MSWINCE) */
+/* #   include <windows.h> */
+/* # endif */
+
+#ifdef GC_PRINT_VERBOSE_STATS
+# define print_stats VERBOSE
+# define INIT_PRINT_STATS /* empty */
+#else
+  /* Use own variable as GC_print_stats might not be exported.  */
+  static int print_stats = 0;
+# ifdef GC_READ_ENV_FILE
+    /* GETENV uses GC internal function in this case.   */
+#   define INIT_PRINT_STATS /* empty */
+# else
+#   define INIT_PRINT_STATS \
+        { \
+          if (0 != GETENV("GC_PRINT_VERBOSE_STATS")) \
+            print_stats = VERBOSE; \
+          else if (0 != GETENV("GC_PRINT_STATS")) \
+            print_stats = 1; \
+        }
+# endif
+#endif /* !GC_PRINT_VERBOSE_STATS */
+
+
+# if defined(GC_PTHREADS)
+#   include <pthread.h>
+# endif
+
+# if (!defined(THREADS) || !defined(HANDLE_FORK) \
+      || (defined(DARWIN) && defined(MPROTECT_VDB) \
+          && !defined(NO_INCREMENTAL) && !defined(MAKE_BACK_GRAPH))) \
+     && !defined(NO_TEST_HANDLE_FORK)
+#   define NO_TEST_HANDLE_FORK
+# endif
+
+# ifdef NAUT
+#   define NO_TEST_HANDLE_FORK
+# endif 
+
+
+# ifndef NO_TEST_HANDLE_FORK
+#   include <unistd.h>
+#   define INIT_FORK_SUPPORT GC_set_handle_fork(1)
+# else
+#   define INIT_FORK_SUPPORT /* empty */
+# endif
+
+# if defined(GC_WIN32_THREADS) && !defined(GC_PTHREADS)
+    static CRITICAL_SECTION incr_cs;
+# endif
+
+/* # include <stdarg.h> */
+
+#ifndef GC_ALPHA_VERSION
+# define GC_ALPHA_VERSION GC_TMP_ALPHA_VERSION
+#endif
+
+#define CHECH_GCLIB_VERSION \
+            if (GC_get_version() != ((GC_VERSION_MAJOR<<16) \
+                                    | (GC_VERSION_MINOR<<8) \
+                                    | GC_ALPHA_VERSION)) { \
+              GC_printf("libgc version mismatch\n"); \
+              exit(1); \
+            }
+
+/* Call GC_INIT only on platforms on which we think we really need it,  */
+/* so that we can test automatic initialization on the rest.            */
+#if defined(CYGWIN32) || defined (AIX) || defined(DARWIN) \
+        || defined(THREAD_LOCAL_ALLOC) \
+        || (defined(MSWINCE) && !defined(GC_WINMAIN_REDIRECT))
+#  define GC_OPT_INIT GC_INIT()
+#else
+#  define GC_OPT_INIT /* empty */
+#endif
+
+#define GC_COND_INIT() \
+    INIT_FORK_SUPPORT; GC_OPT_INIT; CHECH_GCLIB_VERSION; INIT_PRINT_STATS
+
+#define CHECK_OUT_OF_MEMORY(p) \
+            if ((p) == NULL) { \
+              printk("Out of memory\n"); \
+              panic("Out of memory"); \
+            }
+
+/* Allocation Statistics.  Incremented without synchronization. */
+/* FIXME: We should be using synchronization.                   */
+int stubborn_count = 0;
+int uncollectable_count = 0;
+int collectable_count = 0;
+int atomic_count = 0;
+int realloc_count = 0;
+
+#if defined(GC_AMIGA_FASTALLOC) && defined(AMIGA)
+
+  void GC_amiga_free_all_mem(void);
+  void Amiga_Fail(void){GC_amiga_free_all_mem();abort();}
+# define FAIL (void)Amiga_Fail()
+  void *GC_amiga_gctest_malloc_explicitly_typed(size_t lb, GC_descr d){
+    void *ret=GC_malloc_explicitly_typed(lb,d);
+    if(ret==NULL){
+                if(!GC_dont_gc){
+              GC_gcollect();
+              ret=GC_malloc_explicitly_typed(lb,d);
+                }
+      if(ret==NULL){
+        GC_printf("Out of memory, (typed allocations are not directly "
+                      "supported with the GC_AMIGA_FASTALLOC option.)\n");
+        FAIL;
+      }
+    }
+    return ret;
+  }
+  void *GC_amiga_gctest_calloc_explicitly_typed(size_t a,size_t lb, GC_descr d){
+    void *ret=GC_calloc_explicitly_typed(a,lb,d);
+    if(ret==NULL){
+                if(!GC_dont_gc){
+              GC_gcollect();
+              ret=GC_calloc_explicitly_typed(a,lb,d);
+                }
+      if(ret==NULL){
+        GC_printf("Out of memory, (typed allocations are not directly "
+                      "supported with the GC_AMIGA_FASTALLOC option.)\n");
+        FAIL;
+      }
+    }
+    return ret;
+  }
+# define GC_malloc_explicitly_typed(a,b) GC_amiga_gctest_malloc_explicitly_typed(a,b)
+# define GC_calloc_explicitly_typed(a,b,c) GC_amiga_gctest_calloc_explicitly_typed(a,b,c)
+
+#else /* !AMIGA_FASTALLOC */
+
+# if defined(PCR) || defined(LINT2)
+#   define FAIL (void)abort()
+# else
+//#   define FAIL panic("Test failed")
+#   define FAIL BDWGC_ERROR("Test failed\n")
+# endif
+
+#endif /* !AMIGA_FASTALLOC */
+
+/* AT_END may be defined to exercise the interior pointer test  */
+/* if the collector is configured with ALL_INTERIOR_POINTERS.   */
+/* As it stands, this test should succeed with either           */
+/* configuration.  In the FIND_LEAK configuration, it should    */
+/* find lots of leaks, since we free almost nothing.            */
+
+struct SEXPR {
+    struct SEXPR * sexpr_car;
+    struct SEXPR * sexpr_cdr;
+};
+
+
+typedef struct SEXPR * sexpr;
+
+# define INT_TO_SEXPR(x) ((sexpr)(GC_word)(x))
+# define SEXPR_TO_INT(x) ((int)(GC_word)(x))
+
+# undef nil
+# define nil (INT_TO_SEXPR(0))
+# define car(x) ((x) -> sexpr_car)
+# define cdr(x) ((x) -> sexpr_cdr)
+# define is_nil(x) ((x) == nil)
+
+
+int extra_count = 0;        /* Amount of space wasted in cons node */
+
+/* Silly implementation of Lisp cons. Intentionally wastes lots of space */
+/* to test collector.                                                    */
+# ifdef VERY_SMALL_CONFIG
+#   define cons small_cons
+# else
+sexpr cons (sexpr x, sexpr y)
+{
+    sexpr r;
+    int *p;
+    int my_extra = extra_count;
+
+    stubborn_count++;
+    r = (sexpr) GC_malloc(sizeof(struct SEXPR) + my_extra);
+    CHECK_OUT_OF_MEMORY(r);
+    for (p = (int *)r;
+         ((char *)p) < ((char *)r) + my_extra + sizeof(struct SEXPR); p++) {
+        if (*p) {
+            GC_printf("Found nonzero at %p - allocator is broken\n", p);
+            FAIL;
+        }
+        *p = (int)((13 << 12) + ((p - (int *)r) & 0xfff));
+    }
+#   ifdef AT_END
+        r = (sexpr)((char *)r + (my_extra & ~7));
+#   endif
+    r -> sexpr_car = x;
+    r -> sexpr_cdr = y;
+    my_extra++;
+    if ( my_extra >= 5000 ) {
+        extra_count = 0;
+    } else {
+        extra_count = my_extra;
+    }
+    GC_END_STUBBORN_CHANGE((char *)r);
+    return(r);
+}
+# endif
+
+#ifdef GC_GCJ_SUPPORT
+
+#include "gc_mark.h"
+#include "gc_gcj.h"
+
+/* The following struct emulates the vtable in gcj.     */
+/* This assumes the default value of MARK_DESCR_OFFSET. */
+struct fake_vtable {
+  void * dummy;         /* class pointer in real gcj.   */
+  GC_word descr;
+};
+
+struct fake_vtable gcj_class_struct1 = { 0, sizeof(struct SEXPR)
+                                            + sizeof(struct fake_vtable *) };
+                        /* length based descriptor.     */
+struct fake_vtable gcj_class_struct2 =
+                        { 0, ((GC_word)3 << (CPP_WORDSZ - 3)) | GC_DS_BITMAP};
+                        /* Bitmap based descriptor.     */
+
+struct GC_ms_entry * fake_gcj_mark_proc(word * addr,
+                                        struct GC_ms_entry *mark_stack_ptr,
+                                        struct GC_ms_entry *mark_stack_limit,
+                                        word env   )
+{
+    sexpr x;
+    if (1 == env) {
+        /* Object allocated with debug allocator.       */
+        addr = (word *)GC_USR_PTR_FROM_BASE(addr);
+    }
+    x = (sexpr)(addr + 1); /* Skip the vtable pointer. */
+    mark_stack_ptr = GC_MARK_AND_PUSH(
+                              (void *)(x -> sexpr_cdr), mark_stack_ptr,
+                              mark_stack_limit, (void * *)&(x -> sexpr_cdr));
+    mark_stack_ptr = GC_MARK_AND_PUSH(
+                              (void *)(x -> sexpr_car), mark_stack_ptr,
+                              mark_stack_limit, (void * *)&(x -> sexpr_car));
+    return(mark_stack_ptr);
+}
+
+#endif /* GC_GCJ_SUPPORT */
+
+
+sexpr small_cons (sexpr x, sexpr y)
+{
+    sexpr r;
+
+    collectable_count++;
+    r = (sexpr) GC_MALLOC(sizeof(struct SEXPR));
+    CHECK_OUT_OF_MEMORY(r);
+    r -> sexpr_car = x;
+    r -> sexpr_cdr = y;
+    return(r);
+}
+
+sexpr small_cons_uncollectable (sexpr x, sexpr y)
+{
+    sexpr r;
+
+    uncollectable_count++;
+    r = (sexpr) GC_MALLOC_UNCOLLECTABLE(sizeof(struct SEXPR));
+    CHECK_OUT_OF_MEMORY(r);
+    r -> sexpr_car = x;
+    r -> sexpr_cdr = (sexpr)(~(GC_word)y);
+    return(r);
+}
+
+#ifdef GC_GCJ_SUPPORT
+
+
+sexpr gcj_cons(sexpr x, sexpr y)
+{
+    GC_word * r;
+    sexpr result;
+
+    r = (GC_word *) GC_GCJ_MALLOC(sizeof(struct SEXPR)
+                                  + sizeof(struct fake_vtable*),
+                                   &gcj_class_struct2);
+    CHECK_OUT_OF_MEMORY(r);
+    result = (sexpr)(r + 1);
+    result -> sexpr_car = x;
+    result -> sexpr_cdr = y;
+    return(result);
+}
+#endif
+
+/* Return reverse(x) concatenated with y */
+sexpr reverse1(sexpr x, sexpr y)
+{
+    if (is_nil(x)) {
+        return(y);
+    } else {
+        return( reverse1(cdr(x), cons(car(x), y)) );
+    }
+}
+
+sexpr reverse(sexpr x)
+{
+#   ifdef TEST_WITH_SYSTEM_MALLOC
+      malloc(100000);
+#   endif
+    return( reverse1(x, nil) );
+}
+
+sexpr ints(int low, int up)
+{
+    if (low > up) {
+        return(nil);
+    } else {
+        return(small_cons(small_cons(INT_TO_SEXPR(low), nil), ints(low+1, up)));
+    }
+}
+
+#ifdef GC_GCJ_SUPPORT
+/* Return reverse(x) concatenated with y */
+sexpr gcj_reverse1(sexpr x, sexpr y)
+{
+    if (is_nil(x)) {
+        return(y);
+    } else {
+        return( gcj_reverse1(cdr(x), gcj_cons(car(x), y)) );
+    }
+}
+
+sexpr gcj_reverse(sexpr x)
+{
+    return( gcj_reverse1(x, nil) );
+}
+
+sexpr gcj_ints(int low, int up)
+{
+    if (low > up) {
+        return(nil);
+    } else {
+        return(gcj_cons(gcj_cons(INT_TO_SEXPR(low), nil), gcj_ints(low+1, up)));
+    }
+}
+#endif /* GC_GCJ_SUPPORT */
+
+/* To check uncollectible allocation we build lists with disguised cdr  */
+/* pointers, and make sure they don't go away.                          */
+sexpr uncollectable_ints(int low, int up)
+{
+    if (low > up) {
+        return(nil);
+    } else {
+        return(small_cons_uncollectable(small_cons(INT_TO_SEXPR(low), nil),
+               uncollectable_ints(low+1, up)));
+    }
+}
+
+void check_ints(sexpr list, int low, int up)
+{
+    if (SEXPR_TO_INT(car(car(list))) != low) {
+        GC_printf(
+           "List reversal produced incorrect list - collector is broken\n");
+        FAIL;
+    }
+    if (low == up) {
+        if (cdr(list) != nil) {
+           GC_printf("List too long - collector is broken\n");
+           FAIL;
+        }
+    } else {
+        check_ints(cdr(list), low+1, up);
+    }
+}
+
+# define UNCOLLECTABLE_CDR(x) (sexpr)(~(GC_word)(cdr(x)))
+
+void check_uncollectable_ints(sexpr list, int low, int up)
+{
+  //BDWGC_DEBUG("check_uncollectable1 %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    if (SEXPR_TO_INT(car(car(list))) != low) {
+      //      BDWGC_DEBUG("check_uncollectable1a %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+        GC_printf("Uncollectable list corrupted - collector is broken\n");
+        FAIL;
+    }
+    if (low == up) {
+      //BDWGC_DEBUG("check_uncollectable2a %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+      if (UNCOLLECTABLE_CDR(list) != nil) {
+        GC_printf("Uncollectable list too long - collector is broken\n");
+        FAIL;
+      }
+    }
+    else {
+      //BDWGC_DEBUG("check_uncollectable2b %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+        check_uncollectable_ints(UNCOLLECTABLE_CDR(list), low+1, up);
+    }
+}
+
+/* Not used, but useful for debugging: */
+void print_int_list(sexpr x)
+{
+    if (is_nil(x)) {
+        GC_printf("NIL\n");
+    } else {
+        GC_printf("(%d)", SEXPR_TO_INT(car(car(x))));
+        if (!is_nil(cdr(x))) {
+            GC_printf(", ");
+            print_int_list(cdr(x));
+        } else {
+            GC_printf("\n");
+        }
+    }
+}
+
+/* ditto: */
+void check_marks_int_list(sexpr x)
+{
+    if (!GC_is_marked((ptr_t)x))
+        GC_printf("[unm:%p]", x);
+    else
+        GC_printf("[mkd:%p]", x);
+    if (is_nil(x)) {
+        GC_printf("NIL\n");
+    } else {
+        if (!GC_is_marked((ptr_t)car(x)))
+          GC_printf("[unm car:%p]", car(x));
+        GC_printf("(%d)", SEXPR_TO_INT(car(car(x))));
+        if (!is_nil(cdr(x))) {
+            GC_printf(", ");
+            check_marks_int_list(cdr(x));
+        } else {
+            GC_printf("\n");
+        }
+    }
+}
+
+/*
+ * A tiny list reversal test to check thread creation.
+ */
+#ifdef THREADS
+
+# ifdef VERY_SMALL_CONFIG
+#   define TINY_REVERSE_UPPER_VALUE 4
+# else
+#   define TINY_REVERSE_UPPER_VALUE 10
+# endif
+
+# if defined(GC_WIN32_THREADS) && !defined(GC_PTHREADS)
+    DWORD  __stdcall tiny_reverse_test(void * arg)
+# else
+    void * tiny_reverse_test(void * arg)
+# endif
+{
+    int i;
+    for (i = 0; i < 5; ++i) {
+      check_ints(reverse(reverse(ints(1, TINY_REVERSE_UPPER_VALUE))),
+                 1, TINY_REVERSE_UPPER_VALUE);
+    }
+    return 0;
+}
+
+# if defined(GC_PTHREADS)
+    void fork_a_thread(void)
+    {
+      pthread_t t;
+      int code;
+      if ((code = pthread_create(&t, 0, tiny_reverse_test, 0)) != 0) {
+        GC_printf("Small thread creation failed %d\n", code);
+        FAIL;
+      }
+      if ((code = pthread_join(t, 0)) != 0) {
+        GC_printf("Small thread join failed %d\n", code);
+        FAIL;
+      }
+    }
+
+# elif defined(GC_WIN32_THREADS)
+    void fork_a_thread(void)
+    {
+        DWORD thread_id;
+        HANDLE h;
+        h = GC_CreateThread((SECURITY_ATTRIBUTES *)NULL, (word)0,
+                            tiny_reverse_test, NULL, (DWORD)0, &thread_id);
+                                /* Explicitly specify types of the      */
+                                /* arguments to test the prototype.     */
+        if (h == (HANDLE)NULL) {
+            GC_printf("Small thread creation failed %d\n",
+                          (int)GetLastError());
+            FAIL;
+        }
+        if (WaitForSingleObject(h, INFINITE) != WAIT_OBJECT_0) {
+            GC_printf("Small thread wait failed %d\n",
+                          (int)GetLastError());
+            FAIL;
+        }
+    }
+
+# endif
+
+#endif
+
+/* Try to force a to be strangely aligned */
+struct {
+  char dummy;
+  sexpr aa;
+} A;
+#define a A.aa
+
+/*
+ * Repeatedly reverse lists built out of very different sized cons cells.
+ * Check that we didn't lose anything.
+ */
+void *GC_CALLBACK reverse_test_inner(void *data)
+{
+
+    BDWGC_DEBUG("r1 reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    int i;
+    sexpr b;
+    sexpr c;
+    sexpr d;
+    sexpr e;
+    sexpr *f, *g, *h;
+
+    if (data == 0) {
+      /* This stack frame is not guaranteed to be scanned. */
+      BDWGC_DEBUG("r2a reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+      return GC_call_with_gc_active(reverse_test_inner, (void*)(word)1);
+    }
+    BDWGC_DEBUG("r2b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+
+#   if /*defined(MSWIN32) ||*/ defined(MACOS)
+      /* Win32S only allows 128K stacks */
+#     define BIG 1000
+#   elif defined(PCR)
+      /* PCR default stack is 100K.  Stack frames are up to 120 bytes. */
+#     define BIG 700
+#   elif defined(MSWINCE) || defined(RTEMS)
+      /* WinCE only allows 64K stacks */
+#     define BIG 500
+#   elif defined(OSF1)
+      /* OSF has limited stack space by default, and large frames. */
+#     define BIG 200
+#   elif defined(__MACH__) && defined(__ppc64__)
+#     define BIG 2500
+#   else
+#     define BIG 4500
+#   endif
+    BDWGC_DEBUG("r3b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    A.dummy = 17;
+    BDWGC_DEBUG("r3.1b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    a = ints(1, 49);
+    BDWGC_DEBUG("r3.11b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    b = ints(1, 50);
+    BDWGC_DEBUG("r3.12b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    c = ints(1, BIG);
+    BDWGC_DEBUG("r3.2b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    d = uncollectable_ints(1, 100);
+    BDWGC_DEBUG("r3.3b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    e = uncollectable_ints(1, 1);
+    /* Check that realloc updates object descriptors correctly */
+    collectable_count++;
+    BDWGC_DEBUG("r4b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    f = (sexpr *)GC_MALLOC(4 * sizeof(sexpr));
+    realloc_count++;
+    f = (sexpr *)GC_REALLOC((void *)f, 6 * sizeof(sexpr));
+    CHECK_OUT_OF_MEMORY(f);
+    f[5] = ints(1,17);
+    collectable_count++;
+    g = (sexpr *)GC_MALLOC(513 * sizeof(sexpr));
+    realloc_count++;
+    g = (sexpr *)GC_REALLOC((void *)g, 800 * sizeof(sexpr));
+    CHECK_OUT_OF_MEMORY(g);
+    g[799] = ints(1,18);
+    collectable_count++;
+    h = (sexpr *)GC_MALLOC(1025 * sizeof(sexpr));
+    realloc_count++;
+    h = (sexpr *)GC_REALLOC((void *)h, 2000 * sizeof(sexpr));
+    BDWGC_DEBUG("r5b reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    CHECK_OUT_OF_MEMORY(h);
+    BDWGC_DEBUG("r5b1 reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    h[1999] = ints(1,200);
+    BDWGC_DEBUG("r5b2 reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    /* Try to force some collections and reuse of small list elements */
+    for (i = 0; i < 10; i++) {
+      BDWGC_DEBUG("r5.%d reverse test %p (tid %d)\n", i, get_cur_thread(), get_cur_thread()->tid);
+      (void)ints(1, BIG);
+    }
+    /* Superficially test interior pointer recognition on stack */
+    c = (sexpr)((char *)c + sizeof(char *));
+    d = (sexpr)((char *)d + sizeof(char *));
+    GC_FREE((void *)e);
+    check_ints(b,1,50);
+    check_ints(a,1,49);
+    for (i = 0; i < 50; i++) {
+        check_ints(b,1,50);
+        b = reverse(reverse(b));
+    }
+    check_ints(b,1,50);
+    check_ints(a,1,49);
+    for (i = 0; i < 60; i++) {
+#       if defined(GC_PTHREADS) || defined(GC_WIN32_THREADS)
+            if (i % 10 == 0) fork_a_thread();
+#       endif
+        /* This maintains the invariant that a always points to a list of */
+        /* 49 integers.  Thus this is thread safe without locks,          */
+        /* assuming atomic pointer assignments.                           */
+        a = reverse(reverse(a));
+#       if !defined(AT_END) && !defined(THREADS)
+          /* This is not thread safe, since realloc explicitly deallocates */
+          if (i & 1) {
+            a = (sexpr)GC_REALLOC((void *)a, 500);
+          } else {
+            a = (sexpr)GC_REALLOC((void *)a, 8200);
+          }
+#       endif
+    }
+    check_ints(a,1,49);
+    check_ints(b,1,50);
+    /* Restore c and d values. */
+    c = (sexpr)((char *)c - sizeof(char *));
+    d = (sexpr)((char *)d - sizeof(char *));
+    check_ints(c,1,BIG);
+    check_uncollectable_ints(d, 1, 100);
+    check_ints(f[5], 1,17);
+    check_ints(g[799], 1,18);
+    check_ints(h[1999], 1,200);
+#   ifndef THREADS
+        a = 0;
+#   endif
+    *(sexpr volatile *)&b = 0;
+    *(sexpr volatile *)&c = 0;
+
+    return 0;
+}
+
+void reverse_test(void)
+{
+    /* Test GC_do_blocking/GC_call_with_gc_active. */
+    BDWGC_DEBUG("Running reverse test %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+    (void)GC_do_blocking(reverse_test_inner, 0);
+}
+
+#undef a
+
+/*
+ * The rest of this builds balanced binary trees, checks that they don't
+ * disappear, and tests finalization.
+ */
+typedef struct treenode {
+    int level;
+    struct treenode * lchild;
+    struct treenode * rchild;
+} tn;
+
+int finalizable_count = 0;
+int finalized_count = 0;
+volatile int dropped_something = 0;
+
+void GC_CALLBACK finalizer(void * obj, void * client_data)
+{
+  tn * t = (tn *)obj;
+
+# ifdef PCR
+     PCR_ThCrSec_EnterSys();
+# endif
+# if defined(GC_PTHREADS)
+    static pthread_mutex_t incr_lock = PTHREAD_MUTEX_INITIALIZER;
+    pthread_mutex_lock(&incr_lock);
+# elif defined(GC_WIN32_THREADS)
+    EnterCriticalSection(&incr_cs);
+# endif
+  if ((int)(GC_word)client_data != t -> level) {
+     GC_printf("Wrong finalization data - collector is broken\n");
+     FAIL;
+  }
+  finalized_count++;
+  t -> level = -1;      /* detect duplicate finalization immediately */
+# ifdef PCR
+    PCR_ThCrSec_ExitSys();
+# endif
+# if defined(GC_PTHREADS)
+    pthread_mutex_unlock(&incr_lock);
+# elif defined(GC_WIN32_THREADS)
+    LeaveCriticalSection(&incr_cs);
+# endif
+}
+
+size_t counter = 0;
+
+# define MAX_FINALIZED (NTHREADS*4000)
+
+# if !defined(MACOS)
+  GC_FAR GC_word live_indicators[MAX_FINALIZED] = {0};
+#else
+  /* Too big for THINK_C. have to allocate it dynamically. */
+  GC_word *live_indicators = 0;
+#endif
+
+int live_indicators_count = 0;
+
+tn * mktree(int n)
+{
+
+    tn * result = (tn *)GC_MALLOC(sizeof(tn));
+    //memset(result, 111, sizeof(tn)*2);
+    
+    
+    collectable_count++;
+    if (n == 0) return(0);
+    CHECK_OUT_OF_MEMORY(result);
+    result -> level = n;
+    result -> lchild = mktree(n-1);
+    result -> rchild = mktree(n-1);
+    if (counter++ % 17 == 0 && n >= 2) {
+        tn * tmp;
+
+        CHECK_OUT_OF_MEMORY(result->lchild);
+        tmp = result -> lchild -> rchild;
+        CHECK_OUT_OF_MEMORY(result->rchild);
+        result -> lchild -> rchild = result -> rchild -> lchild;
+        result -> rchild -> lchild = tmp;
+    }
+
+    if (counter++ % 119 == 0) {
+        int my_index;
+
+        {
+#         ifdef PCR
+            PCR_ThCrSec_EnterSys();
+#         endif
+#         if defined(GC_PTHREADS)
+            static pthread_mutex_t incr_lock = PTHREAD_MUTEX_INITIALIZER;
+            pthread_mutex_lock(&incr_lock);
+#         elif defined(GC_WIN32_THREADS)
+            EnterCriticalSection(&incr_cs);
+#         endif
+                /* Losing a count here causes erroneous report of failure. */
+          finalizable_count++;
+          my_index = live_indicators_count++;
+#         ifdef PCR
+            PCR_ThCrSec_ExitSys();
+#         endif
+#         if defined(GC_PTHREADS)
+            pthread_mutex_unlock(&incr_lock);
+#         elif defined(GC_WIN32_THREADS)
+            LeaveCriticalSection(&incr_cs);
+#         endif
+        }
+        GC_REGISTER_FINALIZER((void *)result, finalizer, (void *)(GC_word)n,
+                              (GC_finalization_proc *)0, (void * *)0);
+        if (my_index >= MAX_FINALIZED) {
+          GC_printf("live_indicators overflowed\n");
+          FAIL;
+        }
+        live_indicators[my_index] = 13;
+        if (GC_GENERAL_REGISTER_DISAPPEARING_LINK(
+                (void * *)(&(live_indicators[my_index])),
+                (void *)result) != 0) {
+                GC_printf("GC_general_register_disappearing_link failed\n");
+                FAIL;
+        }
+        if (GC_unregister_disappearing_link(
+                (void * *)
+                   (&(live_indicators[my_index]))) == 0) {
+                GC_printf("GC_unregister_disappearing_link failed\n");
+                FAIL;
+        }
+        if (GC_GENERAL_REGISTER_DISAPPEARING_LINK(
+                (void * *)(&(live_indicators[my_index])),
+                (void *)result) != 0) {
+                GC_printf("GC_general_register_disappearing_link failed 2\n");
+                FAIL;
+        }
+        GC_reachable_here(result);
+    }
+    return(result);
+}
+
+void chktree(tn *t, int n)
+{
+    if (n == 0 && t != 0) {
+        GC_printf("Clobbered a leaf - collector is broken\n");
+        FAIL;
+    }
+    if (n == 0) return;
+    if (t -> level != n) {
+        GC_printf("Lost a node at level %d - collector is broken\n", n);
+        FAIL;
+    }
+    if (counter++ % 373 == 0) {
+        collectable_count++;
+        (void) GC_MALLOC(counter%5001);
+    }
+    chktree(t -> lchild, n-1);
+    if (counter++ % 73 == 0) {
+        collectable_count++;
+        (void) GC_MALLOC(counter%373);
+    }
+    chktree(t -> rchild, n-1);
+}
+
+
+#if defined(GC_PTHREADS) || defined(NAUT_THREADS)
+
+# ifdef NAUT
+  nk_tls_key_t fl_key;
+# else 
+  pthread_key_t fl_key;
+# endif
+
+void * alloc8bytes(void)
+{
+# if defined(SMALL_CONFIG) || defined(GC_DEBUG)
+    collectable_count++;
+    return(GC_MALLOC(8));
+# else
+    void ** my_free_list_ptr;
+    void * my_free_list;
+
+    my_free_list_ptr = (void **)pthread_getspecific(fl_key);
+    if (my_free_list_ptr == 0) {
+        uncollectable_count++;
+        my_free_list_ptr = GC_NEW_UNCOLLECTABLE(void *);
+        CHECK_OUT_OF_MEMORY(my_free_list_ptr);
+        if (pthread_setspecific(fl_key, my_free_list_ptr) != 0) {
+            GC_printf("pthread_setspecific failed\n");
+            FAIL;
+        }
+    }
+    my_free_list = *my_free_list_ptr;
+    if (my_free_list == 0) {
+        my_free_list = GC_malloc_many(8);
+        CHECK_OUT_OF_MEMORY(my_free_list);
+    }
+    *my_free_list_ptr = GC_NEXT(my_free_list);
+    GC_NEXT(my_free_list) = 0;
+    collectable_count++;
+    return(my_free_list);
+# endif
+}
+
+#else
+#   define alloc8bytes() GC_MALLOC_ATOMIC(8)
+#endif
+
+void alloc_small(int n)
+{
+    int i;
+
+    for (i = 0; i < n; i += 8) {
+        atomic_count++;
+        if (alloc8bytes() == 0) {
+            BDWGC_DEBUG("Out of memory\n");
+            FAIL;
+        }
+    }
+}
+
+/* # if defined(THREADS) && defined(GC_DEBUG) */
+/* #   ifdef VERY_SMALL_CONFIG */
+/* #     define TREE_HEIGHT 12 */
+/* #   else */
+/* #     define TREE_HEIGHT 15 */
+/* #   endif */
+/* # else */
+/* #   ifdef VERY_SMALL_CONFIG */
+/* #     define TREE_HEIGHT 13 */
+/* #   else */
+/* #     define TREE_HEIGHT 16 */
+/* #   endif */
+/* # endif */
+
+
+#define TREE_HEIGHT 12
+
+void tree_test(void)
+{
+    tn * root;
+    BDWGC_DEBUG("tt1: run_one_test %p\n", get_cur_thread());
+    root = mktree(TREE_HEIGHT);
+#   ifndef VERY_SMALL_CONFIG
+      alloc_small(5000000);
+#   endif
+
+    BDWGC_DEBUG("tt2: run_one_test %p\n", get_cur_thread());
+    chktree(root, TREE_HEIGHT);
+    BDWGC_DEBUG("tt3: run_one_test %p\n", get_cur_thread());
+    if (finalized_count && ! dropped_something) {
+        GC_printf("Premature finalization - collector is broken\n");
+        FAIL;
+    }
+    dropped_something = 1;
+    GC_noop1((word)root);       /* Root needs to remain live until      */
+                                /* dropped_something is set.            */
+    root = mktree(TREE_HEIGHT);
+    BDWGC_DEBUG("tt4: run_one_test %p\n", get_cur_thread());
+    chktree(root, TREE_HEIGHT);
+    BDWGC_DEBUG("tt5: run_one_test %p\n", get_cur_thread());
+    for (int i = TREE_HEIGHT; i >= 0; i--) {
+        root = mktree(i);
+        chktree(root, i);
+    }
+    BDWGC_DEBUG("tt6: run_one_test %p\n", get_cur_thread());
+#   ifndef VERY_SMALL_CONFIG
+      alloc_small(5000000);
+#   endif
+}
+
+unsigned n_tests = 0;
+
+GC_word bm_huge[10] = {
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0xffffffff,
+    0x00ffffff,
+};
+
+/* A very simple test of explicitly typed allocation    */
+void typed_test(void)
+{
+  return;
+  
+  #ifdef NAUT
+  printk("Running typed_test\n");
+  #endif
+    GC_word * old, * new;
+    GC_word bm3 = 0x3;
+    GC_word bm2 = 0x2;
+    GC_word bm_large = 0xf7ff7fff;
+    GC_descr d1 = GC_make_descriptor(&bm3, 2);
+    GC_descr d2 = GC_make_descriptor(&bm2, 2);
+    GC_descr d3 = GC_make_descriptor(&bm_large, 32);
+    GC_descr d4 = GC_make_descriptor(bm_huge, 320);
+    GC_word * x = (GC_word *)GC_malloc_explicitly_typed(2000, d4);
+    int i;
+
+#   ifndef LINT
+      (void)GC_make_descriptor(&bm_large, 32);
+#   endif
+    collectable_count++;
+    old = 0;
+    for (i = 0; i < 4000; i++) {
+        collectable_count++;
+        new = (GC_word *) GC_malloc_explicitly_typed(4 * sizeof(GC_word), d1);
+        CHECK_OUT_OF_MEMORY(new);
+        if (0 != new[0] || 0 != new[1]) {
+            GC_printf("Bad initialization by GC_malloc_explicitly_typed\n");
+            FAIL;
+        }
+        new[0] = 17;
+        new[1] = (GC_word)old;
+        old = new;
+        collectable_count++;
+        new = (GC_word *) GC_malloc_explicitly_typed(4 * sizeof(GC_word), d2);
+        CHECK_OUT_OF_MEMORY(new);
+        new[0] = 17;
+        new[1] = (GC_word)old;
+        old = new;
+        collectable_count++;
+        new = (GC_word *) GC_malloc_explicitly_typed(33 * sizeof(GC_word), d3);
+        CHECK_OUT_OF_MEMORY(new);
+        new[0] = 17;
+        new[1] = (GC_word)old;
+        old = new;
+        collectable_count++;
+        new = (GC_word *) GC_calloc_explicitly_typed(4, 2 * sizeof(GC_word),
+                                                     d1);
+        CHECK_OUT_OF_MEMORY(new);
+        new[0] = 17;
+        new[1] = (GC_word)old;
+        old = new;
+        collectable_count++;
+        if (i & 0xff) {
+          new = (GC_word *) GC_calloc_explicitly_typed(7, 3 * sizeof(GC_word),
+                                                     d2);
+        } else {
+          new = (GC_word *) GC_calloc_explicitly_typed(1001,
+                                                       3 * sizeof(GC_word),
+                                                       d2);
+          if (new && (0 != new[0] || 0 != new[1])) {
+            GC_printf("Bad initialization by GC_malloc_explicitly_typed\n");
+            FAIL;
+          }
+        }
+        CHECK_OUT_OF_MEMORY(new);
+        new[0] = 17;
+        new[1] = (GC_word)old;
+        old = new;
+    }
+    printk("Running typed_test_2\n");
+    for (i = 0; i < 20000; i++) {
+        if (new[0] != 17) {
+            GC_printf("typed alloc failed at %lu\n", (unsigned long)i);
+            FAIL;
+        }
+        new[0] = 0;
+        old = new;
+        new = (GC_word *)(old[1]);
+    }
+    printk("Running typed_test_2\n");
+
+    GC_gcollect();
+    printk("Running typed_test_3\n");
+    GC_noop1((word)x);
+}
+
+int fail_count = 0;
+
+/*ARGSUSED*/
+void GC_CALLBACK fail_proc1(void * x)
+{
+    fail_count++;
+}
+
+static void uniq(void *p, ...) {
+  va_list a;
+  void *q[100];
+  int n = 0, i, j;
+  q[n++] = p;
+  va_start(a,p);
+  for (;(q[n] = va_arg(a,void *)) != NULL;n++) ;
+  va_end(a);
+  for (i=0; i<n; i++)
+    for (j=0; j<i; j++)
+      if (q[i] == q[j]) {
+        GC_printf(
+              "Apparently failed to mark from some function arguments.\n"
+              "Perhaps GC_push_regs was configured incorrectly?\n"
+        );
+        FAIL;
+      }
+}
+
+#ifdef THREADS
+#   define TEST_FAIL_COUNT(n) 1
+#else
+#   define TEST_FAIL_COUNT(n) (fail_count >= (n))
+#endif
+
+void * GC_CALLBACK inc_int_counter(void *pcounter)
+{
+ ++(*(int *)pcounter);
+ return NULL;
+}
+
+void run_one_test(void)
+{
+#   ifndef DBG_HDRS_ALL
+        char *x;
+        char **z;
+#       ifdef LINT
+            char *y = 0;
+#       else
+            char *y = (char *)(GC_word)fail_proc1;
+#       endif
+        CLOCK_TYPE typed_time;
+#   endif
+    CLOCK_TYPE start_time;
+    CLOCK_TYPE reverse_time;
+    CLOCK_TYPE tree_time;
+    unsigned long time_diff;
+#   ifdef FIND_LEAK
+        GC_printf(
+              "This test program is not designed for leak detection mode\n");
+        GC_printf("Expect lots of problems\n");
+#   endif
+    GC_FREE(0);
+
+#   ifndef DBG_HDRS_ALL
+
+      collectable_count += 3;
+
+      GC_malloc(7);
+      
+      if ((GC_size(GC_malloc(7)) != 8 &&
+           GC_size(GC_malloc(7)) != MIN_WORDS * sizeof(GC_word))
+           || GC_size(GC_malloc(15)) != 16) {
+        GC_printf("GC_size produced unexpected results\n");
+        FAIL;
+      }
+      collectable_count += 1;
+      if (GC_size(GC_malloc(0)) != MIN_WORDS * sizeof(GC_word)) {
+        GC_printf("GC_malloc(0) failed: GC_size returns %ld\n",
+                      (unsigned long)GC_size(GC_malloc(0)));
+        FAIL;
+      }
+
+      collectable_count += 1;
+      if (GC_size(GC_malloc_uncollectable(0)) != MIN_WORDS * sizeof(GC_word)) {
+        GC_printf("GC_malloc_uncollectable(0) failed\n");
+        FAIL;
+      }
+      GC_is_valid_displacement_print_proc = fail_proc1;
+      GC_is_visible_print_proc = fail_proc1;
+      collectable_count += 1;
+      x = GC_malloc(16);
+      if (GC_base(GC_PTR_ADD(x, 13)) != x) {
+        GC_printf("GC_base(heap ptr) produced incorrect result\n");
+        FAIL;
+      }
+      (void)GC_PRE_INCR(x, 0);
+      (void)GC_POST_INCR(x);
+      (void)GC_POST_DECR(x);
+      if (GC_base(x) != x) {
+        GC_printf("Bad INCR/DECR result\n");
+        FAIL;
+      }
+#     ifndef PCR
+        if (GC_base(y) != 0) {
+          GC_printf("GC_base(fn_ptr) produced incorrect result\n");
+          FAIL;
+        }
+#     endif
+
+        BDWGC_DEBUG("b4: run_one_test %p\n", get_cur_thread());
+      if (GC_same_obj(x+5, x) != x + 5) {
+        GC_printf("GC_same_obj produced incorrect result\n");
+        FAIL;
+      }
+      if (GC_is_visible(y) != y || GC_is_visible(x) != x) {
+        GC_printf("GC_is_visible produced incorrect result\n");
+        FAIL;
+      }
+        BDWGC_DEBUG("b5: run_one_test %p\n", get_cur_thread());
+      z = GC_malloc(8);
+      
+        BDWGC_DEBUG("b6: run_one_test %p\n", get_cur_thread());
+      
+      CHECK_OUT_OF_MEMORY(z);
+      GC_PTR_STORE(z, x);
+      if (*z != x) {
+        GC_printf("GC_PTR_STORE failed: %p != %p\n", *z, x);
+        FAIL;
+      }
+      if (!TEST_FAIL_COUNT(1)) {
+#       if!(defined(POWERPC) || defined(IA64)) || defined(M68K)
+          /* On POWERPCs function pointers point to a descriptor in the */
+          /* data segment, so there should have been no failures.       */
+          /* The same applies to IA64.  Something similar seems to      */
+          /* be going on with NetBSD/M68K.                              */
+          GC_printf("GC_is_visible produced wrong failure indication\n");
+          FAIL;
+#       endif
+      }
+
+        BDWGC_DEBUG("b7: run_one_test %p\n", get_cur_thread());
+      
+      if (GC_is_valid_displacement(y) != y
+        || GC_is_valid_displacement(x) != x
+        || GC_is_valid_displacement(x + 3) != x + 3) {
+        GC_printf("GC_is_valid_displacement produced incorrect result\n");
+        FAIL;
+      }
+        {
+          size_t i;
+
+          GC_malloc(17);
+          for (i = sizeof(GC_word); i < 512; i *= 2) {
+            GC_word result = (GC_word) GC_memalign(i, 17);
+            if (result % i != 0 || result == 0 || *(int *)result != 0) FAIL;
+          }
+        }
+          BDWGC_DEBUG("b8: run_one_test %p\n", get_cur_thread());
+#     ifndef ALL_INTERIOR_POINTERS
+#      if defined(RS6000) || defined(POWERPC)
+        if (!TEST_FAIL_COUNT(1))
+#      else
+        if (!TEST_FAIL_COUNT(GC_get_all_interior_pointers() ? 1 : 2))
+#      endif
+        {
+          GC_printf(
+              "GC_is_valid_displacement produced wrong failure indication\n");
+          FAIL;
+        }
+#     endif
+#   endif /* DBG_HDRS_ALL */
+    /* Test floating point alignment */
+        collectable_count += 2;
+        {
+          double *dp = GC_MALLOC(sizeof(double));
+          CHECK_OUT_OF_MEMORY(dp);
+          *dp = 1.0;
+          dp = GC_MALLOC(sizeof(double));
+          CHECK_OUT_OF_MEMORY(dp);
+          *dp = 1.0;
+        }
+    /* Test size 0 allocation a bit more */
+        BDWGC_DEBUG("b9: run_one_test: Test size 0 allocation %p\n", get_cur_thread());
+        {
+           size_t i;
+           for (i = 0; i < 10; ++i) {
+             GC_MALLOC(0);
+             GC_FREE(GC_MALLOC(0));
+             GC_MALLOC_ATOMIC(0);
+             GC_FREE(GC_MALLOC_ATOMIC(0));
+           }
+         }
+        BDWGC_DEBUG("b10: run_one_test %p\n", get_cur_thread());
+        //#   ifndef NAUT
+    /* Make sure that fn arguments are visible to the collector.        */
+      uniq(
+        GC_malloc(12), GC_malloc(12), GC_malloc(12),
+        (GC_gcollect(),GC_malloc(12)),
+        GC_malloc(12), GC_malloc(12), GC_malloc(12),
+        (GC_gcollect(),GC_malloc(12)),
+        GC_malloc(12), GC_malloc(12), GC_malloc(12),
+        (GC_gcollect(),GC_malloc(12)),
+        GC_malloc(12), GC_malloc(12), GC_malloc(12),
+        (GC_gcollect(),GC_malloc(12)),
+        GC_malloc(12), GC_malloc(12), GC_malloc(12),
+        (GC_gcollect(),GC_malloc(12)),
+        (void *)0);
+    /* GC_malloc(0) must return NULL or something we can deallocate. */
+        GC_free(GC_malloc(0));
+        GC_free(GC_malloc_atomic(0));
+        GC_free(GC_malloc(0));
+        GC_free(GC_malloc_atomic(0));
+    /* Repeated list reversal test. */
+        GET_TIME(start_time);
+        reverse_test();
+        if (print_stats) {
+          GET_TIME(reverse_time);
+          time_diff = MS_TIME_DIFF(reverse_time, start_time);
+          GC_log_printf("-------------Finished reverse_test at time %u (%p)\n",
+                        (unsigned) time_diff, &start_time);
+        }
+        //#   endif
+          BDWGC_DEBUG("b11: run_one_test %p\n", get_cur_thread());
+        
+#   ifndef DBG_HDRS_ALL
+      typed_test();
+      if (print_stats) {
+        GET_TIME(typed_time);
+        time_diff = MS_TIME_DIFF(typed_time, start_time);
+        GC_log_printf("-------------Finished typed_test at time %u (%p)\n",
+                      (unsigned) time_diff, &start_time);
+      }
+#   endif /* DBG_HDRS_ALL */
+        BDWGC_DEBUG("b12: run_one_test %p\n", get_cur_thread());
+      tree_test();
+        BDWGC_DEBUG("b13: run_one_test %p\n", get_cur_thread());
+    if (print_stats) {
+      GET_TIME(tree_time);
+      time_diff = MS_TIME_DIFF(tree_time, start_time);
+      GC_log_printf("-------------Finished tree_test at time %u (%p)\n",
+                    (unsigned) time_diff, &start_time);
+    }
+    /* Run reverse_test a second time, so we hopefully notice corruption. */
+      reverse_test();
+        BDWGC_DEBUG("b14: run_one_test %p\n", get_cur_thread());
+      if (print_stats) {
+        GET_TIME(reverse_time);
+        time_diff = MS_TIME_DIFF(reverse_time, start_time);
+        GC_log_printf(
+                "-------------Finished second reverse_test at time %u (%p)\n",
+                (unsigned)time_diff, &start_time);
+      }
+    /* GC_allocate_ml and GC_need_to_lock are no longer exported, and   */
+    /* AO_fetch_and_add1() may be unavailable to update a counter.      */
+    (void)GC_call_with_alloc_lock(inc_int_counter, &n_tests);
+      BDWGC_DEBUG("b15: run_one_test %p\n", get_cur_thread());
+#   ifndef NO_TEST_HANDLE_FORK
+      if (fork() == 0) {
+        GC_gcollect();
+        tiny_reverse_test(0);
+        GC_gcollect();
+        if (print_stats)
+          GC_log_printf("Finished a child process\n");
+        exit(0);
+      }
+#   endif
+    if (print_stats)
+      GC_log_printf("Finished %p\n", &start_time);
+}
+
+
+
+#ifdef NAUT
+int test_bdwgc_main();
+
+int bdwgc_test()
+{
+  #ifdef FIND_LEAK
+    printk("Cannot run bdwgc tests with leak detection enabled!");
+    return -1;
+  #endif
+  printk("\nRunning bdwgc collector tests\n");
+  
+  printk("\nRunning main tests:\n");
+  test_bdwgc_main();
+  
+  //printk("\nRunning huge test:\n");
+  //huge_test();
+  
+  //printk("\nRunning realloc test:\n"); // Enabling this test causes atomic leak to show
+  //realloc_test();
+  //printk("\n");
+
+  return 0;
+}
+
+
+int bdwgc_test_leak_detector()
+{
+  printk("\nRunning bdwgc leak test:\n");
+  leak_test();
+
+  return 0; 
+}
+#endif
+
+#define NUMBER_ROUND_UP(v, bound) ((((v) + (bound) - 1) / (bound)) * (bound))
+
+void check_heap_stats(void)
+{
+    size_t max_heap_sz;
+    int i;
+    int still_live;
+#   ifdef FINALIZE_ON_DEMAND
+        int late_finalize_count = 0;
+#   endif
+
+#   ifdef VERY_SMALL_CONFIG
+    /* The upper bounds are a guess, which has been empirically */
+    /* adjusted.  On low end uniprocessors with incremental GC  */
+    /* these may be particularly dubious, since empirically the */
+    /* heap tends to grow largely as a result of the GC not     */
+    /* getting enough cycles.                                   */
+#     if CPP_WORDSZ == 64
+        max_heap_sz = 4500000;
+#     else
+        max_heap_sz = 2800000;
+#     endif
+#   else
+#     if CPP_WORDSZ == 64
+        max_heap_sz = 23000000;
+#     else
+        max_heap_sz = 16000000;
+#     endif
+#   endif
+#   ifdef GC_DEBUG
+        max_heap_sz *= 2;
+#       ifdef SAVE_CALL_CHAIN
+            max_heap_sz *= 3;
+#           ifdef SAVE_CALL_COUNT
+                max_heap_sz += max_heap_sz * SAVE_CALL_COUNT/4;
+#           endif
+#       endif
+#   endif
+    max_heap_sz *= n_tests;
+#   if defined(USE_MMAP) || defined(MSWIN32)
+      max_heap_sz = NUMBER_ROUND_UP(max_heap_sz, 4 * 1024 * 1024);
+#   endif
+    /* Garbage collect repeatedly so that all inaccessible objects      */
+    /* can be finalized.                                                */
+      while (GC_collect_a_little()) { }
+      for (i = 0; i < 16; i++) {
+        GC_gcollect();
+#   ifdef FINALIZE_ON_DEMAND
+           late_finalize_count +=
+#   endif
+                GC_invoke_finalizers();
+      }
+      if (print_stats) {
+          GC_log_printf("Primordial thread stack bottom: %p\n",
+                        GC_stackbottom);
+      }
+    GC_printf("Completed %u tests\n", n_tests);
+    GC_printf("Allocated %d collectable objects\n", collectable_count);
+    GC_printf("Allocated %d uncollectable objects\n",
+                  uncollectable_count);
+    GC_printf("Allocated %d atomic objects\n", atomic_count);
+    GC_printf("Allocated %d stubborn objects\n", stubborn_count);
+    GC_printf("Finalized %d/%d objects - ",
+                  finalized_count, finalizable_count);
+#   ifdef FINALIZE_ON_DEMAND
+        if (finalized_count != late_finalize_count) {
+            GC_printf("Demand finalization error\n");
+            FAIL;
+        }
+#   endif
+    if (finalized_count > finalizable_count
+        || finalized_count < finalizable_count/2) {
+        GC_printf("finalization is probably broken\n");
+        FAIL;
+    } else {
+        GC_printf("finalization is probably ok\n");
+    }
+    still_live = 0;
+    for (i = 0; i < MAX_FINALIZED; i++) {
+        if (live_indicators[i] != 0) {
+            still_live++;
+        }
+    }
+    i = finalizable_count - finalized_count - still_live;
+    if (0 != i) {
+        GC_printf("%d disappearing links remain and %d more objects "
+                      "were not finalized\n", still_live, i);
+        if (i > 10) {
+            GC_printf("\tVery suspicious!\n");
+        } else {
+            GC_printf("\tSlightly suspicious, but probably OK\n");
+        }
+    }
+    GC_printf("Total number of bytes allocated is %lu\n",
+                  (unsigned long)GC_get_total_bytes());
+    GC_printf("Final heap size is %lu bytes\n",
+                  (unsigned long)GC_get_heap_size());
+    if (GC_get_total_bytes() < (size_t)n_tests *
+#   ifdef VERY_SMALL_CONFIG
+        2700000
+#   else
+        33500000
+#   endif
+        ) {
+      GC_printf("Incorrect execution - missed some allocations\n");
+      FAIL;
+    }
+    if (GC_get_heap_size() + GC_get_unmapped_bytes() > max_heap_sz) {
+        GC_printf("Unexpected heap growth - collector may be broken"
+                  " (heapsize: %lu, expected: %lu)\n",
+            (unsigned long)(GC_get_heap_size() + GC_get_unmapped_bytes()),
+            (unsigned long)max_heap_sz);
+        FAIL;
+    }
+#   ifdef THREADS
+      GC_unregister_my_thread(); /* just to check it works (for main) */
+#   endif
+    GC_printf("Collector appears to work\n");
+}
+
+#if defined(MACOS)
+void SetMinimumStack(long minSize)
+{
+        long newApplLimit;
+
+        if (minSize > LMGetDefltStack())
+        {
+                newApplLimit = (long) GetApplLimit()
+                                - (minSize - LMGetDefltStack());
+                SetApplLimit((Ptr) newApplLimit);
+                MaxApplZone();
+        }
+}
+
+#define cMinStackSpace (512L * 1024L)
+
+#endif
+
+void GC_CALLBACK warn_proc(char *msg, GC_word p)
+{
+    GC_printf(msg, (unsigned long)p);
+    /*FAIL;*/
+}
+
+#if defined(MSWINCE) && defined(UNDER_CE)
+# define WINMAIN_LPTSTR LPWSTR
+#else
+# define WINMAIN_LPTSTR LPSTR
+#endif
+
+
+/* #ifndef NAUT_THREADS */
+
+/* #if !defined(PCR) && !defined(GC_WIN32_THREADS) && !defined(GC_PTHREADS) \ */
+/*     || defined(LINT) */
+/* #if defined(MSWIN32) && !defined(__MINGW32__) || defined(MSWINCE) */
+/*   int APIENTRY WinMain(HINSTANCE instance, HINSTANCE prev, */
+/*                        WINMAIN_LPTSTR cmd, int n) */
+/* #elif defined(RTEMS) */
+/* # include <bsp.h> */
+/* # define CONFIGURE_APPLICATION_NEEDS_CLOCK_DRIVER */
+/* # define CONFIGURE_APPLICATION_NEEDS_CONSOLE_DRIVER */
+/* # define CONFIGURE_RTEMS_INIT_TASKS_TABLE */
+/* # define CONFIGURE_MAXIMUM_TASKS 1 */
+/* # define CONFIGURE_INIT */
+/* # define CONFIGURE_INIT_TASK_STACK_SIZE (64*1024) */
+/* # include <rtems/confdefs.h> */
+/*   rtems_task Init(rtems_task_argument ignord) */
+/* #else */
+/*   int main(void) */
+/* #endif */
+/* { */
+/*     n_tests = 0; */
+/* #   if defined(MACOS) */
+/*         /\* Make sure we have lots and lots of stack space.      *\/ */
+/*         SetMinimumStack(cMinStackSpace); */
+/*         /\* Cheat and let stdio initialize toolbox for us.       *\/ */
+/*         printf("Testing GC Macintosh port\n"); */
+/* #   endif */
+/*     GC_COND_INIT(); */
+/*     GC_set_warn_proc(warn_proc); */
+/* #   if (defined(MPROTECT_VDB) || defined(PROC_VDB) || defined(GWW_VDB)) \ */
+/*           && !defined(MAKE_BACK_GRAPH) && !defined(NO_INCREMENTAL) */
+/*       GC_enable_incremental(); */
+/*       GC_printf("Switched to incremental mode\n"); */
+/* #     if defined(MPROTECT_VDB) */
+/*         GC_printf("Emulating dirty bits with mprotect/signals\n"); */
+/* #     else */
+/* #       ifdef PROC_VDB */
+/*           GC_printf("Reading dirty bits from /proc\n"); */
+/* #       elif defined(GWW_VDB) */
+/*           GC_printf("Using GetWriteWatch-based implementation\n"); */
+/* #       else */
+/*           GC_printf("Using DEFAULT_VDB dirty bit implementation\n"); */
+/* #       endif */
+/* #      endif */
+/* #   endif */
+
+/*    for (int it = 0; it < 5; it++ ) */
+/*      { */
+/*        printk("Running test %d\n", it); */
+/*        run_one_test(); */
+/*      } */
+/*     check_heap_stats(); */
+/* #   if !defined(MSWINCE) && !defined(NAUT) */
+/*       fflush(stdout); */
+/* #   endif */
+/* #   ifdef LINT */
+/*         /\* Entry points we should be testing, but aren't.                  *\/ */
+/*         /\* Some can be tested by defining GC_DEBUG at the top of this file *\/ */
+/*         /\* This is a bit SunOS4 specific.                                  *\/ */
+/*         GC_noop(GC_expand_hp, GC_add_roots, GC_clear_roots, */
+/*                 GC_register_disappearing_link, */
+/*                 GC_register_finalizer_ignore_self, */
+/*                 GC_debug_register_displacement, GC_debug_change_stubborn, */
+/*                 GC_debug_end_stubborn_change, GC_debug_malloc_uncollectable, */
+/*                 GC_debug_free, GC_debug_realloc, */
+/*                 GC_generic_malloc_words_small, GC_init, */
+/*                 GC_malloc_ignore_off_page, GC_malloc_atomic_ignore_off_page, */
+/*                 GC_set_max_heap_size, GC_get_bytes_since_gc, */
+/*                 GC_get_total_bytes, GC_pre_incr, GC_post_incr); */
+/* #   endif */
+/* #   ifdef MSWIN32 */
+/*       GC_win32_free_heap(); */
+/* #   endif */
+/* #   ifdef RTEMS */
+/*       exit(0); */
+/* #   else */
+/*       return(0); */
+/* #   endif */
+/* } */
+/* # endif */
+
+/* #endif */
+
+
+
+#if (defined(GC_PTHREADS) || defined(NAUT_THREADS))
+void * thr_run_one_test(void * arg)
+{
+    run_one_test();
+    return(0);
+}
+
+#ifdef GC_DEBUG
+#  define GC_free GC_debug_free
+#endif
+
+int test_bdwgc_main(void)
+{
+    nk_thread_id_t th[NTHREADS];
+    int code = 0;
+    int i;
+    
+    GC_COND_INIT();
+    n_tests = 0;
+    BDWGC_DEBUG("MAIN_BREAK_1\n");
+    
+#   if (defined(MPROTECT_VDB)) && !defined(REDIRECT_MALLOC) \
+            && !defined(MAKE_BACK_GRAPH) && !defined(USE_PROC_FOR_LIBRARIES) \
+            && !defined(NO_INCREMENTAL)
+        GC_enable_incremental();
+        GC_printf("Switched to incremental mode\n");
+#     if defined(MPROTECT_VDB)
+        GC_printf("Emulating dirty bits with mprotect/signals\n");
+#     else
+#       ifdef PROC_VDB
+          GC_printf("Reading dirty bits from /proc\n");
+#       else
+          GC_printf("Using DEFAULT_VDB dirty bit implementation\n");
+#       endif
+#     endif
+#   endif
+    GC_set_warn_proc(warn_proc);
+
+    for (i = 0; i < NTHREADS; ++i) {
+      if (nk_thread_start((void (*) (void*, void**))thr_run_one_test,
+                          0,
+                          NULL,
+                          0,
+                          1024*1024,
+                          th+i,
+                          CPU_ANY) != 0)
+        {
+          GC_printf("Thread %d creation failed %d\n", i, code);
+          FAIL;
+        }
+    }
+    
+    for (i = 0; i < NTHREADS; ++i) {
+      if ((code = nk_join(th[i], 0)) != 0) {
+        BDWGC_DEBUG("Thread %d failed %d\n", i, code);
+        FAIL;
+      }
+    }
+
+    check_heap_stats();
+    GC_printf("Completed %u collections\n", (unsigned)GC_get_gc_no());
+    return(0);
+}
+
+#endif /* GC_PTHREADS */
diff --git a/src/gc/bdwgc/tests/test_cpp.cc b/src/gc/bdwgc/tests/test_cpp.cc
new file mode 100644
index 0000000..3c3f208
--- /dev/null
+++ b/src/gc/bdwgc/tests/test_cpp.cc
@@ -0,0 +1,303 @@
+/****************************************************************************
+Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+
+THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+
+Permission is hereby granted to use or copy this program for any
+purpose, provided the above notices are retained on all copies.
+Permission to modify the code and to distribute modified code is
+granted, provided the above notices are retained, and a notice that
+the code was modified is included with the above copyright notice.
+****************************************************************************
+Last modified on Mon Jul 10 21:06:03 PDT 1995 by ellis
+     modified on December 20, 1994 7:27 pm PST by boehm
+
+usage: test_cpp number-of-iterations
+
+This program tries to test the specific C++ functionality provided by
+gc_c++.h that isn't tested by the more general test routines of the
+collector.
+
+A recommended value for number-of-iterations is 10, which will take a
+few minutes to complete.
+
+***************************************************************************/
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+#undef GC_BUILD
+
+#include "gc_cpp.h"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define USE_STD_ALLOCATOR
+
+#ifdef USE_STD_ALLOCATOR
+#   include "gc_allocator.h"
+#else
+#   include "new_gc_alloc.h"
+#endif
+
+extern "C" {
+# include "private/gcconfig.h"
+
+# ifndef GC_API_PRIV
+#   define GC_API_PRIV GC_API
+# endif
+  GC_API_PRIV void GC_printf(const char * format, ...);
+  /* Use GC private output to reach the same log file.  */
+  /* Don't include gc_priv.h, since that may include Windows system     */
+  /* header files that don't take kindly to this context.               */
+}
+
+#ifdef MSWIN32
+# include <windows.h>
+#endif
+
+#ifdef GC_NAME_CONFLICT
+# define USE_GC UseGC
+  struct foo * GC;
+#else
+# define USE_GC GC
+#endif
+
+#define my_assert( e ) \
+    if (! (e)) { \
+        GC_printf( "Assertion failure in " __FILE__ ", line %d: " #e "\n", \
+                    __LINE__ ); \
+        exit( 1 ); }
+
+
+class A {public:
+    /* An uncollectible class. */
+
+    A( int iArg ): i( iArg ) {}
+    void Test( int iArg ) {
+        my_assert( i == iArg );}
+    int i;};
+
+
+class B: public gc, public A {public:
+    /* A collectible class. */
+
+    B( int j ): A( j ) {}
+    ~B() {
+        my_assert( deleting );}
+    static void Deleting( int on ) {
+        deleting = on;}
+    static int deleting;};
+
+int B::deleting = 0;
+
+
+class C: public gc_cleanup, public A {public:
+    /* A collectible class with cleanup and virtual multiple inheritance. */
+
+    C( int levelArg ): A( levelArg ), level( levelArg ) {
+        nAllocated++;
+        if (level > 0) {
+            left = new C( level - 1 );
+            right = new C( level - 1 );}
+        else {
+            left = right = 0;}}
+    ~C() {
+        this->A::Test( level );
+        nFreed++;
+        my_assert( level == 0 ?
+                   left == 0 && right == 0 :
+                   level == left->level + 1 && level == right->level + 1 );
+        left = right = 0;
+        level = -123456;}
+    static void Test() {
+        my_assert( nFreed <= nAllocated && nFreed >= .8 * nAllocated );}
+
+    static int nFreed;
+    static int nAllocated;
+    int level;
+    C* left;
+    C* right;};
+
+int C::nFreed = 0;
+int C::nAllocated = 0;
+
+
+class D: public gc {public:
+    /* A collectible class with a static member function to be used as
+    an explicit clean-up function supplied to ::new. */
+
+    D( int iArg ): i( iArg ) {
+        nAllocated++;}
+    static void CleanUp( void* obj, void* data ) {
+        D* self = (D*) obj;
+        nFreed++;
+        my_assert( self->i == (int) (GC_word) data );}
+    static void Test() {
+        my_assert( nFreed >= .8 * nAllocated );}
+
+    int i;
+    static int nFreed;
+    static int nAllocated;};
+
+int D::nFreed = 0;
+int D::nAllocated = 0;
+
+
+class E: public gc_cleanup {public:
+    /* A collectible class with clean-up for use by F. */
+
+    E() {
+        nAllocated++;}
+    ~E() {
+        nFreed++;}
+
+    static int nFreed;
+    static int nAllocated;};
+
+int E::nFreed = 0;
+int E::nAllocated = 0;
+
+
+class F: public E {public:
+    /* A collectible class with clean-up, a base with clean-up, and a
+    member with clean-up. */
+
+    F() {
+        nAllocated++;}
+    ~F() {
+        nFreed++;}
+    static void Test() {
+        my_assert( nFreed >= .8 * nAllocated );
+        my_assert( 2 * nFreed == E::nFreed );}
+
+    E e;
+    static int nFreed;
+    static int nAllocated;};
+
+int F::nFreed = 0;
+int F::nAllocated = 0;
+
+
+GC_word Disguise( void* p ) {
+    return ~ (GC_word) p;}
+
+void* Undisguise( GC_word i ) {
+    return (void*) ~ i;}
+
+#ifdef MSWIN32
+int APIENTRY WinMain(
+    HINSTANCE instance, HINSTANCE prev, LPSTR cmd, int cmdShow )
+{
+    int argc = 0;
+    char* argv[ 3 ];
+
+    if (cmd != 0)
+      for (argc = 1; argc < (int)(sizeof(argv) / sizeof(argv[0])); argc++) {
+        argv[ argc ] = strtok( argc == 1 ? cmd : 0, " \t" );
+        if (0 == argv[ argc ]) break;}
+#elif defined(MACOS)
+  int main() {
+    char* argv_[] = {"test_cpp", "10"}; // MacOS doesn't have a commandline
+    argv = argv_;
+    argc = sizeof(argv_)/sizeof(argv_[0]);
+#else
+  int main( int argc, char* argv[] ) {
+#endif
+
+    GC_set_all_interior_pointers(1);
+                        /* needed due to C++ multiple inheritance used  */
+
+    GC_INIT();
+
+    int i, iters, n;
+#   ifdef USE_STD_ALLOCATOR
+      int *x = gc_allocator<int>().allocate(1);
+      int *xio;
+      xio = gc_allocator_ignore_off_page<int>().allocate(1);
+      int **xptr = traceable_allocator<int *>().allocate(1);
+#   else
+      int *x = (int *)gc_alloc::allocate(sizeof(int));
+#   endif
+    *x = 29;
+#   ifdef USE_STD_ALLOCATOR
+      *xptr = x;
+      x = 0;
+#   endif
+    if (argc != 2 || (0 >= (n = atoi( argv[ 1 ] )))) {
+        GC_printf( "usage: test_cpp number-of-iterations\nAssuming 10 iters\n" );
+        n = 10;}
+
+    for (iters = 1; iters <= n; iters++) {
+        GC_printf( "Starting iteration %d\n", iters );
+
+            /* Allocate some uncollectible As and disguise their pointers.
+            Later we'll check to see if the objects are still there.  We're
+            checking to make sure these objects really are uncollectible. */
+        GC_word as[ 1000 ];
+        GC_word bs[ 1000 ];
+        for (i = 0; i < 1000; i++) {
+            as[ i ] = Disguise( new (NoGC ) A( i ) );
+            bs[ i ] = Disguise( new (NoGC) B( i ) );}
+
+            /* Allocate a fair number of finalizable Cs, Ds, and Fs.
+            Later we'll check to make sure they've gone away. */
+        for (i = 0; i < 1000; i++) {
+            C* c = new C( 2 );
+            C c1( 2 );           /* stack allocation should work too */
+            D* d;
+            F* f;
+            d = ::new (USE_GC, D::CleanUp, (void*)(GC_word)i) D( i );
+            f = new F;
+            if (0 == i % 10) delete c;}
+
+            /* Allocate a very large number of collectible As and Bs and
+            drop the references to them immediately, forcing many
+            collections. */
+        for (i = 0; i < 1000000; i++) {
+            A* a;
+            a = new (USE_GC) A( i );
+            B* b = new B( i );
+            b = new (USE_GC) B( i );
+            if (0 == i % 10) {
+                B::Deleting( 1 );
+                delete b;
+                B::Deleting( 0 );}
+#           ifdef FINALIZE_ON_DEMAND
+              GC_invoke_finalizers();
+#           endif
+            }
+
+            /* Make sure the uncollectible As and Bs are still there. */
+        for (i = 0; i < 1000; i++) {
+            A* a = (A*) Undisguise( as[ i ] );
+            B* b = (B*) Undisguise( bs[ i ] );
+            a->Test( i );
+            delete a;
+            b->Test( i );
+            B::Deleting( 1 );
+            delete b;
+            B::Deleting( 0 );
+#           ifdef FINALIZE_ON_DEMAND
+                 GC_invoke_finalizers();
+#           endif
+            }
+
+            /* Make sure most of the finalizable Cs, Ds, and Fs have
+            gone away. */
+        C::Test();
+        D::Test();
+        F::Test();}
+
+#   ifdef USE_STD_ALLOCATOR
+      x = *xptr;
+#   endif
+    my_assert (29 == x[0]);
+    GC_printf( "The test appears to have succeeded.\n" );
+    return( 0 );
+}
+
diff --git a/src/gc/bdwgc/tests/thread_leak_test.c b/src/gc/bdwgc/tests/thread_leak_test.c
new file mode 100644
index 0000000..0710fc2
--- /dev/null
+++ b/src/gc/bdwgc/tests/thread_leak_test.c
@@ -0,0 +1,86 @@
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+#ifndef GC_THREADS
+# define GC_THREADS
+#endif
+
+#include "leak_detector.h"
+
+#ifdef GC_PTHREADS
+# include <pthread.h>
+#else
+# include <windows.h>
+#endif
+
+#include <stdio.h>
+
+#ifdef GC_PTHREADS
+  void * test(void * arg)
+#else
+  DWORD WINAPI test(LPVOID arg)
+#endif
+{
+    int *p[10];
+    int i;
+    for (i = 0; i < 10; ++i) {
+        p[i] = malloc(sizeof(int)+i);
+    }
+    CHECK_LEAKS();
+    for (i = 1; i < 10; ++i) {
+        free(p[i]);
+    }
+#   ifdef GC_PTHREADS
+      return arg;
+#   else
+      return (DWORD)(GC_word)arg;
+#   endif
+}
+
+#define NTHREADS 5
+
+int main(void) {
+    int i;
+#   ifdef GC_PTHREADS
+      pthread_t t[NTHREADS];
+#   else
+      HANDLE t[NTHREADS];
+      DWORD thread_id;
+#   endif
+    int code;
+
+    GC_set_find_leak(1); /* for new collect versions not compiled       */
+                         /* with -DFIND_LEAK.                           */
+    GC_INIT();
+
+    for (i = 0; i < NTHREADS; ++i) {
+#       ifdef GC_PTHREADS
+          code = pthread_create(t + i, 0, test, 0);
+#       else
+          t[i] = CreateThread(NULL, 0, test, 0, 0, &thread_id);
+          code = t[i] != NULL ? 0 : (int)GetLastError();
+#       endif
+        if (code != 0) {
+            printf("Thread creation failed %d\n", code);
+        }
+    }
+
+    for (i = 0; i < NTHREADS; ++i) {
+#       ifdef GC_PTHREADS
+          code = pthread_join(t[i], 0);
+#       else
+          code = WaitForSingleObject(t[i], INFINITE) == WAIT_OBJECT_0 ? 0 :
+                                                        (int)GetLastError();
+#       endif
+        if (code != 0) {
+            printf("Thread join failed %d\n", code);
+        }
+    }
+
+    CHECK_LEAKS();
+    CHECK_LEAKS();
+    CHECK_LEAKS();
+    return 0;
+}
diff --git a/src/gc/bdwgc/tests/threadkey_test.c b/src/gc/bdwgc/tests/threadkey_test.c
new file mode 100644
index 0000000..7635424
--- /dev/null
+++ b/src/gc/bdwgc/tests/threadkey_test.c
@@ -0,0 +1,99 @@
+
+#ifdef HAVE_CONFIG_H
+# include "private/config.h"
+#endif
+
+#ifndef GC_THREADS
+# define GC_THREADS
+#endif
+
+#define GC_NO_THREAD_REDIRECTS 1
+
+#include "gc.h"
+
+#if (!defined(GC_PTHREADS) || defined(GC_SOLARIS_THREADS) \
+     || defined(__native_client__)) && !defined(SKIP_THREADKEY_TEST)
+  /* FIXME: Skip this test on Solaris for now.  The test may fail on    */
+  /* other targets as well.  Currently, tested only on Linux, Cygwin    */
+  /* and Darwin.                                                        */
+# define SKIP_THREADKEY_TEST
+#endif
+
+#ifdef SKIP_THREADKEY_TEST
+
+#include <stdio.h>
+
+int main (void)
+{
+  printf("threadkey_test skipped\n");
+  return 0;
+}
+
+#else
+
+#include <pthread.h>
+
+pthread_key_t key;
+
+#ifdef GC_SOLARIS_THREADS
+  /* pthread_once_t key_once = { PTHREAD_ONCE_INIT }; */
+#else
+  pthread_once_t key_once = PTHREAD_ONCE_INIT;
+#endif
+
+void * entry (void *arg)
+{
+  pthread_setspecific(key,
+                      (void *)GC_HIDE_POINTER(GC_STRDUP("hello, world")));
+  return arg;
+}
+
+void * GC_CALLBACK on_thread_exit_inner (struct GC_stack_base * sb, void * arg)
+{
+  int res = GC_register_my_thread (sb);
+  pthread_t t;
+  int creation_res;     /* Used to suppress a warning about     */
+                        /* unchecked pthread_create() result.   */
+
+  creation_res = GC_pthread_create (&t, NULL, entry, NULL);
+  if (res == GC_SUCCESS)
+    GC_unregister_my_thread ();
+
+  return (void*)(GC_word)creation_res;
+}
+
+void on_thread_exit (void *v)
+{
+  GC_call_with_stack_base (on_thread_exit_inner, NULL);
+}
+
+void make_key (void)
+{
+  pthread_key_create (&key, on_thread_exit);
+}
+
+#ifndef LIMIT
+# define LIMIT 30
+#endif
+
+int main (void)
+{
+  int i;
+  GC_INIT ();
+
+# ifdef GC_SOLARIS_THREADS
+    pthread_key_create (&key, on_thread_exit);
+# else
+    pthread_once (&key_once, make_key);
+# endif
+  for (i = 0; i < LIMIT; i++) {
+    pthread_t t;
+    void *res;
+    if (GC_pthread_create (&t, NULL, entry, NULL) == 0
+        && (i & 1) != 0)
+      GC_pthread_join (t, &res);
+  }
+  return 0;
+}
+
+#endif /* !SKIP_THREADKEY_TEST */
diff --git a/src/gc/bdwgc/tests/trace_test.c b/src/gc/bdwgc/tests/trace_test.c
new file mode 100644
index 0000000..923e0f8
--- /dev/null
+++ b/src/gc/bdwgc/tests/trace_test.c
@@ -0,0 +1,41 @@
+#include <stdio.h>
+#include <stdlib.h>
+
+#ifndef GC_DEBUG
+# define GC_DEBUG
+#endif
+
+#include "gc.h"
+#include "gc_backptr.h"
+
+struct treenode {
+    struct treenode *x;
+    struct treenode *y;
+} * root[10];
+
+struct treenode * mktree(int i) {
+  struct treenode * r = GC_MALLOC(sizeof(struct treenode));
+  if (0 == i) return 0;
+  if (1 == i) r = GC_MALLOC_ATOMIC(sizeof(struct treenode));
+  if (r == NULL) {
+    printf("Out of memory\n");
+    exit(1);
+  }
+  r -> x = mktree(i-1);
+  r -> y = mktree(i-1);
+  return r;
+}
+
+int main(void)
+{
+  int i;
+  GC_INIT();
+  for (i = 0; i < 10; ++i) {
+    root[i] = mktree(12);
+  }
+  GC_generate_random_backtrace();
+  GC_generate_random_backtrace();
+  GC_generate_random_backtrace();
+  GC_generate_random_backtrace();
+  return 0;
+}
diff --git a/src/gc/bdwgc/thread_local_alloc.c b/src/gc/bdwgc/thread_local_alloc.c
new file mode 100644
index 0000000..602e6c9
--- /dev/null
+++ b/src/gc/bdwgc/thread_local_alloc.c
@@ -0,0 +1,217 @@
+/*
+ * Copyright (c) 2000-2005 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#if defined(THREAD_LOCAL_ALLOC)
+
+#ifndef THREADS
+# error "invalid config - THREAD_LOCAL_ALLOC requires GC_THREADS"
+#endif
+
+#include "private/thread_local_alloc.h"
+
+#ifndef NAUT
+#include <stdlib.h>
+#endif
+
+#if defined(USE_COMPILER_TLS)
+  __thread
+#elif defined(USE_WIN32_COMPILER_TLS)
+  __declspec(thread)
+#endif
+
+/* #ifdef NAUT */
+/*   //extern GC_key_t GC_thread_key; */
+/* #else  */
+/* GC_key_t GC_thread_key; */
+/* #endif */
+
+static GC_bool keys_initialized;
+
+/* Return a single nonempty freelist fl to the global one pointed to    */
+/* by gfl.      */
+
+static void return_single_freelist(void *fl, void **gfl)
+{
+    void *q, **qptr;
+
+    if (*gfl == 0) {
+      *gfl = fl;
+    } else {
+      GC_ASSERT(GC_size(fl) == GC_size(*gfl));
+      /* Concatenate: */
+        qptr = &(obj_link(fl));
+        while ((word)(q = *qptr) >= HBLKSIZE)
+          qptr = &(obj_link(q));
+        GC_ASSERT(0 == q);
+        *qptr = *gfl;
+        *gfl = fl;
+    }
+}
+
+/* Recover the contents of the freelist array fl into the global one gfl.*/
+/* We hold the allocator lock.                                          */
+static void return_freelists(void **fl, void **gfl)
+{
+    int i;
+
+    for (i = 1; i < TINY_FREELISTS; ++i) {
+        if ((word)(fl[i]) >= HBLKSIZE) {
+          return_single_freelist(fl[i], gfl+i);
+        }
+        /* Clear fl[i], since the thread structure may hang around.     */
+        /* Do it in a way that is likely to trap if we access it.       */
+        fl[i] = (ptr_t)HBLKSIZE;
+    }
+    /* The 0 granule freelist really contains 1 granule objects.        */
+#   ifdef GC_GCJ_SUPPORT
+      if (fl[0] == ERROR_FL) return;
+#   endif
+    if ((word)(fl[0]) >= HBLKSIZE) {
+        return_single_freelist(fl[0], gfl+1);
+    }
+}
+
+/* Each thread structure must be initialized.   */
+/* This call must be made from the new thread.  */
+GC_INNER void GC_init_thread_local(GC_tlfs p)
+{
+  #ifndef NAUT
+    int i;
+
+    printk("BDWGC: GC_init_thread_local\n");
+    
+    GC_ASSERT(I_HOLD_LOCK());
+    if (!keys_initialized) {
+        if (0 != GC_key_create(&GC_thread_key, 0)) {
+            ABORT("Failed to create key for local allocator");
+        }
+        keys_initialized = TRUE;
+    }
+    if (0 != GC_setspecific(GC_thread_key, p)) {
+        ABORT("Failed to set thread specific allocation pointers");
+    }
+    for (i = 1; i < TINY_FREELISTS; ++i) {
+        p -> ptrfree_freelists[i] = (void *)(word)1;
+        p -> normal_freelists[i] = (void *)(word)1;
+#       ifdef GC_GCJ_SUPPORT
+          p -> gcj_freelists[i] = (void *)(word)1;
+#       endif
+    }
+    /* Set up the size 0 free lists.    */
+    /* We now handle most of them like regular free lists, to ensure    */
+    /* That explicit deallocation works.  However, allocation of a      */
+    /* size 0 "gcj" object is always an error.                          */
+    p -> ptrfree_freelists[0] = (void *)(word)1;
+    p -> normal_freelists[0] = (void *)(word)1;
+#   ifdef GC_GCJ_SUPPORT
+        p -> gcj_freelists[0] = ERROR_FL;
+#   endif
+
+#endif
+}
+
+/* We hold the allocator lock.  */
+GC_INNER void GC_destroy_thread_local(GC_tlfs p)
+{
+    /* We currently only do this from the thread itself or from */
+    /* the fork handler for a child process.                    */
+    return_freelists(p -> ptrfree_freelists, GC_aobjfreelist);
+    return_freelists(p -> normal_freelists, GC_objfreelist);
+}
+
+
+GC_bool GC_is_thread_tsd_valid(void *tsd);
+
+GC_API void * GC_CALL GC_malloc(size_t bytes)
+{
+  //  BDWGC_DEBUG("Allocating %d bytes for thread %p\n", bytes, get_cur_thread());
+    size_t granules = ROUNDED_UP_GRANULES(bytes);
+    void *result;
+
+    //if (EXPECT(0 == &(get_cur_thread()-> gc_state -> tlfs), FALSE)) {
+    if (0 == (void *)&(BDWGC_THREAD_STATE() -> tlfs))
+      {
+        BDWGC_DEBUG("ENTER CORE MALLOC %p (tid %d)\n", get_cur_thread(), get_cur_thread()->tid);
+        result =  GC_core_malloc(bytes);
+        sti();
+        return result; 
+      }
+    
+    GC_ASSERT(GC_is_initialized);
+
+    GC_FAST_MALLOC_GRANS(result,
+                         granules,
+                         BDWGC_THREAD_STATE() -> tlfs.normal_freelists,
+                         DIRECT_GRANULES,
+                         NORMAL,
+                         GC_core_malloc(bytes),
+                         obj_link(result)=0);
+
+#   ifdef LOG_ALLOCS
+      GC_err_printf("GC_malloc(%u) = %p : %u\n",
+                        (unsigned)bytes, result, (unsigned)GC_gc_no);
+#   endif
+    return result;
+}
+
+
+GC_API void * GC_CALL GC_malloc_atomic(size_t bytes)
+{
+    size_t granules = ROUNDED_UP_GRANULES(bytes);
+    void *tsd = &(BDWGC_THREAD_STATE() -> tlfs); 
+    void *result;
+    void **tiny_fl;
+
+    GC_ASSERT(GC_is_initialized);
+    tiny_fl = ((GC_tlfs)tsd) -> ptrfree_freelists;
+    GC_FAST_MALLOC_GRANS(result, granules, tiny_fl, DIRECT_GRANULES, PTRFREE,
+                         GC_core_malloc_atomic(bytes), (void)0 /* no init */);
+    return result;
+}
+
+
+/* The thread support layer must arrange to mark thread-local   */
+/* free lists explicitly, since the link field is often         */
+/* invisible to the marker.  It knows how to find all threads;  */
+/* we take care of an individual thread freelist structure.     */
+GC_INNER void GC_mark_thread_local_fls_for(GC_tlfs p)
+{
+    ptr_t q;
+    int j;
+
+    for (j = 0; j < TINY_FREELISTS; ++j) {
+      q = p->ptrfree_freelists[j];
+      if ((word)q > HBLKSIZE) GC_set_fl_marks(q);
+      q = p->normal_freelists[j];
+      if ((word)q > HBLKSIZE) GC_set_fl_marks(q);
+    }
+}
+
+#if defined(GC_ASSERTIONS)
+    /* Check that all thread-local free-lists in p are completely marked. */
+    void GC_check_tls_for(GC_tlfs p)
+    {
+        BDWGC_DEBUG("Checking %p is marked\n", p);
+
+        int j;
+
+        for (j = 1; j < TINY_FREELISTS; ++j) {
+          GC_check_fl_marks(&p->ptrfree_freelists[j]);
+          GC_check_fl_marks(&p->normal_freelists[j]);
+        }
+    }
+#endif /* GC_ASSERTIONS */
+
+#endif /* THREAD_LOCAL_ALLOC */
diff --git a/src/gc/bdwgc/typd_mlc.c b/src/gc/bdwgc/typd_mlc.c
new file mode 100644
index 0000000..47fc977
--- /dev/null
+++ b/src/gc/bdwgc/typd_mlc.c
@@ -0,0 +1,726 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1999-2000 by Hewlett-Packard Company.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ */
+
+#include "private/gc_pmark.h"
+
+/*
+ * Some simple primitives for allocation with explicit type information.
+ * Simple objects are allocated such that they contain a GC_descr at the
+ * end (in the last allocated word).  This descriptor may be a procedure
+ * which then examines an extended descriptor passed as its environment.
+ *
+ * Arrays are treated as simple objects if they have sufficiently simple
+ * structure.  Otherwise they are allocated from an array kind that supplies
+ * a special mark procedure.  These arrays contain a pointer to a
+ * complex_descriptor as their last word.
+ * This is done because the environment field is too small, and the collector
+ * must trace the complex_descriptor.
+ *
+ * Note that descriptors inside objects may appear cleared, if we encounter a
+ * false reference to an object on a free list.  In the GC_descr case, this
+ * is OK, since a 0 descriptor corresponds to examining no fields.
+ * In the complex_descriptor case, we explicitly check for that case.
+ *
+ * MAJOR PARTS OF THIS CODE HAVE NOT BEEN TESTED AT ALL and are not testable,
+ * since they are not accessible through the current interface.
+ */
+
+#include "gc_typed.h"
+
+#define TYPD_EXTRA_BYTES (sizeof(word) - EXTRA_BYTES)
+
+STATIC GC_bool GC_explicit_typing_initialized = FALSE;
+
+STATIC int GC_explicit_kind = 0;
+                        /* Object kind for objects with indirect        */
+                        /* (possibly extended) descriptors.             */
+
+STATIC int GC_array_kind = 0;
+                        /* Object kind for objects with complex         */
+                        /* descriptors and GC_array_mark_proc.          */
+
+/* Extended descriptors.  GC_typed_mark_proc understands these. */
+/* These are used for simple objects that are larger than what  */
+/* can be described by a BITMAP_BITS sized bitmap.              */
+typedef struct {
+        word ed_bitmap; /* lsb corresponds to first word.       */
+        GC_bool ed_continued;   /* next entry is continuation.  */
+} ext_descr;
+
+/* Array descriptors.  GC_array_mark_proc understands these.    */
+/* We may eventually need to add provisions for headers and     */
+/* trailers.  Hence we provide for tree structured descriptors, */
+/* though we don't really use them currently.                   */
+typedef union ComplexDescriptor {
+    struct LeafDescriptor {     /* Describes simple array       */
+        word ld_tag;
+#       define LEAF_TAG 1
+        size_t ld_size;         /* bytes per element    */
+                                /* multiple of ALIGNMENT        */
+        size_t ld_nelements;    /* Number of elements.  */
+        GC_descr ld_descriptor; /* A simple length, bitmap,     */
+                                /* or procedure descriptor.     */
+    } ld;
+    struct ComplexArrayDescriptor {
+        word ad_tag;
+#       define ARRAY_TAG 2
+        size_t ad_nelements;
+        union ComplexDescriptor * ad_element_descr;
+    } ad;
+    struct SequenceDescriptor {
+        word sd_tag;
+#       define SEQUENCE_TAG 3
+        union ComplexDescriptor * sd_first;
+        union ComplexDescriptor * sd_second;
+    } sd;
+} complex_descriptor;
+#define TAG ld.ld_tag
+
+STATIC ext_descr * GC_ext_descriptors = NULL;
+                                        /* Points to array of extended  */
+                                        /* descriptors.                 */
+
+STATIC size_t GC_ed_size = 0;   /* Current size of above arrays.        */
+#define ED_INITIAL_SIZE 100
+
+STATIC size_t GC_avail_descr = 0;       /* Next available slot.         */
+
+STATIC int GC_typed_mark_proc_index = 0; /* Indices of my mark          */
+STATIC int GC_array_mark_proc_index = 0; /* procedures.                 */
+
+STATIC void GC_push_typed_structures_proc(void)
+{
+  GC_push_all((ptr_t)&GC_ext_descriptors,
+              (ptr_t)&GC_ext_descriptors + sizeof(word));
+}
+
+/* Add a multiword bitmap to GC_ext_descriptors arrays.  Return */
+/* starting index.                                              */
+/* Returns -1 on failure.                                       */
+/* Caller does not hold allocation lock.                        */
+STATIC signed_word GC_add_ext_descriptor(GC_bitmap bm, word nbits)
+{
+    size_t nwords = divWORDSZ(nbits + WORDSZ-1);
+    signed_word result;
+    size_t i;
+    word last_part;
+    size_t extra_bits;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    while (GC_avail_descr + nwords >= GC_ed_size) {
+        ext_descr * new;
+        size_t new_size;
+        word ed_size = GC_ed_size;
+
+        if (ed_size == 0) {
+            GC_push_typed_structures = GC_push_typed_structures_proc;
+            UNLOCK();
+            new_size = ED_INITIAL_SIZE;
+        } else {
+            UNLOCK();
+            new_size = 2 * ed_size;
+            if (new_size > MAX_ENV) return(-1);
+        }
+        new = (ext_descr *) GC_malloc_atomic(new_size * sizeof(ext_descr));
+        if (new == 0) return(-1);
+        LOCK();
+        if (ed_size == GC_ed_size) {
+            if (GC_avail_descr != 0) {
+                BCOPY(GC_ext_descriptors, new,
+                      GC_avail_descr * sizeof(ext_descr));
+            }
+            GC_ed_size = new_size;
+            GC_ext_descriptors = new;
+        }  /* else another thread already resized it in the meantime */
+    }
+    result = GC_avail_descr;
+    for (i = 0; i < nwords-1; i++) {
+        GC_ext_descriptors[result + i].ed_bitmap = bm[i];
+        GC_ext_descriptors[result + i].ed_continued = TRUE;
+    }
+    last_part = bm[i];
+    /* Clear irrelevant bits. */
+    extra_bits = nwords * WORDSZ - nbits;
+    last_part <<= extra_bits;
+    last_part >>= extra_bits;
+    GC_ext_descriptors[result + i].ed_bitmap = last_part;
+    GC_ext_descriptors[result + i].ed_continued = FALSE;
+    GC_avail_descr += nwords;
+    UNLOCK();
+    return(result);
+}
+
+/* Table of bitmap descriptors for n word long all pointer objects.     */
+STATIC GC_descr GC_bm_table[WORDSZ/2];
+
+/* Return a descriptor for the concatenation of 2 nwords long objects,  */
+/* each of which is described by descriptor.                            */
+/* The result is known to be short enough to fit into a bitmap          */
+/* descriptor.                                                          */
+/* Descriptor is a GC_DS_LENGTH or GC_DS_BITMAP descriptor.             */
+STATIC GC_descr GC_double_descr(GC_descr descriptor, word nwords)
+{
+    if ((descriptor & GC_DS_TAGS) == GC_DS_LENGTH) {
+        descriptor = GC_bm_table[BYTES_TO_WORDS((word)descriptor)];
+    };
+    descriptor |= (descriptor & ~GC_DS_TAGS) >> nwords;
+    return(descriptor);
+}
+
+STATIC complex_descriptor *
+GC_make_sequence_descriptor(complex_descriptor *first,
+                            complex_descriptor *second);
+
+/* Build a descriptor for an array with nelements elements,     */
+/* each of which can be described by a simple descriptor.       */
+/* We try to optimize some common cases.                        */
+/* If the result is COMPLEX, then a complex_descr* is returned  */
+/* in *complex_d.                                                       */
+/* If the result is LEAF, then we built a LeafDescriptor in     */
+/* the structure pointed to by leaf.                            */
+/* The tag in the leaf structure is not set.                    */
+/* If the result is SIMPLE, then a GC_descr                     */
+/* is returned in *simple_d.                                    */
+/* If the result is NO_MEM, then                                */
+/* we failed to allocate the descriptor.                        */
+/* The implementation knows that GC_DS_LENGTH is 0.             */
+/* *leaf, *complex_d, and *simple_d may be used as temporaries  */
+/* during the construction.                                     */
+#define COMPLEX 2
+#define LEAF    1
+#define SIMPLE  0
+#define NO_MEM  (-1)
+STATIC int GC_make_array_descriptor(size_t nelements, size_t size,
+                                    GC_descr descriptor, GC_descr *simple_d,
+                                    complex_descriptor **complex_d,
+                                    struct LeafDescriptor * leaf)
+{
+#   define OPT_THRESHOLD 50
+        /* For larger arrays, we try to combine descriptors of adjacent */
+        /* descriptors to speed up marking, and to reduce the amount    */
+        /* of space needed on the mark stack.                           */
+    if ((descriptor & GC_DS_TAGS) == GC_DS_LENGTH) {
+      if (descriptor == (GC_descr)size) {
+        *simple_d = nelements * descriptor;
+        return(SIMPLE);
+      } else if ((word)descriptor == 0) {
+        *simple_d = (GC_descr)0;
+        return(SIMPLE);
+      }
+    }
+    if (nelements <= OPT_THRESHOLD) {
+      if (nelements <= 1) {
+        if (nelements == 1) {
+            *simple_d = descriptor;
+            return(SIMPLE);
+        } else {
+            *simple_d = (GC_descr)0;
+            return(SIMPLE);
+        }
+      }
+    } else if (size <= BITMAP_BITS/2
+               && (descriptor & GC_DS_TAGS) != GC_DS_PROC
+               && (size & (sizeof(word)-1)) == 0) {
+      int result =
+          GC_make_array_descriptor(nelements/2, 2*size,
+                                   GC_double_descr(descriptor,
+                                                   BYTES_TO_WORDS(size)),
+                                   simple_d, complex_d, leaf);
+      if ((nelements & 1) == 0) {
+          return(result);
+      } else {
+          struct LeafDescriptor * one_element =
+              (struct LeafDescriptor *)
+                GC_malloc_atomic(sizeof(struct LeafDescriptor));
+
+          if (result == NO_MEM || one_element == 0) return(NO_MEM);
+          one_element -> ld_tag = LEAF_TAG;
+          one_element -> ld_size = size;
+          one_element -> ld_nelements = 1;
+          one_element -> ld_descriptor = descriptor;
+          switch(result) {
+            case SIMPLE:
+            {
+              struct LeafDescriptor * beginning =
+                (struct LeafDescriptor *)
+                  GC_malloc_atomic(sizeof(struct LeafDescriptor));
+              if (beginning == 0) return(NO_MEM);
+              beginning -> ld_tag = LEAF_TAG;
+              beginning -> ld_size = size;
+              beginning -> ld_nelements = 1;
+              beginning -> ld_descriptor = *simple_d;
+              *complex_d = GC_make_sequence_descriptor(
+                                (complex_descriptor *)beginning,
+                                (complex_descriptor *)one_element);
+              break;
+            }
+            case LEAF:
+            {
+              struct LeafDescriptor * beginning =
+                (struct LeafDescriptor *)
+                  GC_malloc_atomic(sizeof(struct LeafDescriptor));
+              if (beginning == 0) return(NO_MEM);
+              beginning -> ld_tag = LEAF_TAG;
+              beginning -> ld_size = leaf -> ld_size;
+              beginning -> ld_nelements = leaf -> ld_nelements;
+              beginning -> ld_descriptor = leaf -> ld_descriptor;
+              *complex_d = GC_make_sequence_descriptor(
+                                (complex_descriptor *)beginning,
+                                (complex_descriptor *)one_element);
+              break;
+            }
+            case COMPLEX:
+              *complex_d = GC_make_sequence_descriptor(
+                                *complex_d,
+                                (complex_descriptor *)one_element);
+              break;
+          }
+          return(COMPLEX);
+      }
+    }
+
+    leaf -> ld_size = size;
+    leaf -> ld_nelements = nelements;
+    leaf -> ld_descriptor = descriptor;
+    return(LEAF);
+}
+
+STATIC complex_descriptor *
+GC_make_sequence_descriptor(complex_descriptor *first,
+                            complex_descriptor *second)
+{
+    struct SequenceDescriptor * result =
+        (struct SequenceDescriptor *)
+                GC_malloc(sizeof(struct SequenceDescriptor));
+    /* Can't result in overly conservative marking, since tags are      */
+    /* very small integers. Probably faster than maintaining type       */
+    /* info.                                                            */
+    if (result != 0) {
+        result -> sd_tag = SEQUENCE_TAG;
+        result -> sd_first = first;
+        result -> sd_second = second;
+    }
+    return((complex_descriptor *)result);
+}
+
+#ifdef UNDEFINED
+  complex_descriptor * GC_make_complex_array_descriptor(word nelements,
+                                                complex_descriptor *descr)
+  {
+    struct ComplexArrayDescriptor * result =
+        (struct ComplexArrayDescriptor *)
+                GC_malloc(sizeof(struct ComplexArrayDescriptor));
+
+    if (result != 0) {
+        result -> ad_tag = ARRAY_TAG;
+        result -> ad_nelements = nelements;
+        result -> ad_element_descr = descr;
+    }
+    return((complex_descriptor *)result);
+  }
+#endif
+
+STATIC ptr_t * GC_eobjfreelist = NULL;
+
+STATIC ptr_t * GC_arobjfreelist = NULL;
+
+STATIC mse * GC_typed_mark_proc(word * addr, mse * mark_stack_ptr,
+                                mse * mark_stack_limit, word env);
+
+STATIC mse * GC_array_mark_proc(word * addr, mse * mark_stack_ptr,
+                                mse * mark_stack_limit, word env);
+
+/* Caller does not hold allocation lock. */
+STATIC void GC_init_explicit_typing(void)
+{
+    register unsigned i;
+    DCL_LOCK_STATE;
+
+    GC_STATIC_ASSERT(sizeof(struct LeafDescriptor) % sizeof(word) == 0);
+    LOCK();
+    if (GC_explicit_typing_initialized) {
+      UNLOCK();
+      return;
+    }
+    GC_explicit_typing_initialized = TRUE;
+    /* Set up object kind with simple indirect descriptor. */
+      GC_eobjfreelist = (ptr_t *)GC_new_free_list_inner();
+      GC_explicit_kind = GC_new_kind_inner(
+                            (void **)GC_eobjfreelist,
+                            (((word)WORDS_TO_BYTES(-1)) | GC_DS_PER_OBJECT),
+                            TRUE, TRUE);
+                /* Descriptors are in the last word of the object. */
+      GC_typed_mark_proc_index = GC_new_proc_inner(GC_typed_mark_proc);
+    /* Set up object kind with array descriptor. */
+      GC_arobjfreelist = (ptr_t *)GC_new_free_list_inner();
+      GC_array_mark_proc_index = GC_new_proc_inner(GC_array_mark_proc);
+      GC_array_kind = GC_new_kind_inner(
+                            (void **)GC_arobjfreelist,
+                            GC_MAKE_PROC(GC_array_mark_proc_index, 0),
+                            FALSE, TRUE);
+      for (i = 0; i < WORDSZ/2; i++) {
+          GC_descr d = (((word)(-1)) >> (WORDSZ - i)) << (WORDSZ - i);
+          d |= GC_DS_BITMAP;
+          GC_bm_table[i] = d;
+      }
+    UNLOCK();
+}
+
+STATIC mse * GC_typed_mark_proc(word * addr, mse * mark_stack_ptr,
+                                mse * mark_stack_limit, word env)
+{
+    word bm = GC_ext_descriptors[env].ed_bitmap;
+    word * current_p = addr;
+    word current;
+    ptr_t greatest_ha = GC_greatest_plausible_heap_addr;
+    ptr_t least_ha = GC_least_plausible_heap_addr;
+    DECLARE_HDR_CACHE;
+
+    INIT_HDR_CACHE;
+    for (; bm != 0; bm >>= 1, current_p++) {
+        if (bm & 1) {
+            current = *current_p;
+            FIXUP_POINTER(current);
+            if ((ptr_t)current >= least_ha && (ptr_t)current <= greatest_ha) {
+                PUSH_CONTENTS((ptr_t)current, mark_stack_ptr,
+                              mark_stack_limit, (ptr_t)current_p, exit1);
+            }
+        }
+    }
+    if (GC_ext_descriptors[env].ed_continued) {
+        /* Push an entry with the rest of the descriptor back onto the  */
+        /* stack.  Thus we never do too much work at once.  Note that   */
+        /* we also can't overflow the mark stack unless we actually     */
+        /* mark something.                                              */
+        mark_stack_ptr++;
+        if (mark_stack_ptr >= mark_stack_limit) {
+            mark_stack_ptr = GC_signal_mark_stack_overflow(mark_stack_ptr);
+        }
+        mark_stack_ptr -> mse_start = (ptr_t)(addr + WORDSZ);
+        mark_stack_ptr -> mse_descr =
+                GC_MAKE_PROC(GC_typed_mark_proc_index, env+1);
+    }
+    return(mark_stack_ptr);
+}
+
+/* Return the size of the object described by d.  It would be faster to */
+/* store this directly, or to compute it as part of                     */
+/* GC_push_complex_descriptor, but hopefully it doesn't matter.         */
+STATIC word GC_descr_obj_size(complex_descriptor *d)
+{
+    switch(d -> TAG) {
+      case LEAF_TAG:
+        return(d -> ld.ld_nelements * d -> ld.ld_size);
+      case ARRAY_TAG:
+        return(d -> ad.ad_nelements
+               * GC_descr_obj_size(d -> ad.ad_element_descr));
+      case SEQUENCE_TAG:
+        return(GC_descr_obj_size(d -> sd.sd_first)
+               + GC_descr_obj_size(d -> sd.sd_second));
+      default:
+        ABORT("Bad complex descriptor");
+        /*NOTREACHED*/ return 0; /*NOTREACHED*/
+    }
+}
+
+/* Push descriptors for the object at addr with complex descriptor d    */
+/* onto the mark stack.  Return 0 if the mark stack overflowed.         */
+STATIC mse * GC_push_complex_descriptor(word *addr, complex_descriptor *d,
+                                        mse *msp, mse *msl)
+{
+    register ptr_t current = (ptr_t) addr;
+    register word nelements;
+    register word sz;
+    register word i;
+
+    switch(d -> TAG) {
+      case LEAF_TAG:
+        {
+          register GC_descr descr = d -> ld.ld_descriptor;
+
+          nelements = d -> ld.ld_nelements;
+          if (msl - msp <= (ptrdiff_t)nelements) return(0);
+          sz = d -> ld.ld_size;
+          for (i = 0; i < nelements; i++) {
+              msp++;
+              msp -> mse_start = current;
+              msp -> mse_descr = descr;
+              current += sz;
+          }
+          return(msp);
+        }
+      case ARRAY_TAG:
+        {
+          register complex_descriptor *descr = d -> ad.ad_element_descr;
+
+          nelements = d -> ad.ad_nelements;
+          sz = GC_descr_obj_size(descr);
+          for (i = 0; i < nelements; i++) {
+              msp = GC_push_complex_descriptor((word *)current, descr,
+                                                msp, msl);
+              if (msp == 0) return(0);
+              current += sz;
+          }
+          return(msp);
+        }
+      case SEQUENCE_TAG:
+        {
+          sz = GC_descr_obj_size(d -> sd.sd_first);
+          msp = GC_push_complex_descriptor((word *)current, d -> sd.sd_first,
+                                           msp, msl);
+          if (msp == 0) return(0);
+          current += sz;
+          msp = GC_push_complex_descriptor((word *)current, d -> sd.sd_second,
+                                           msp, msl);
+          return(msp);
+        }
+      default:
+        ABORT("Bad complex descriptor");
+        /*NOTREACHED*/ return 0; /*NOTREACHED*/
+   }
+}
+
+/*ARGSUSED*/
+STATIC mse * GC_array_mark_proc(word * addr, mse * mark_stack_ptr,
+                                mse * mark_stack_limit, word env)
+{
+    hdr * hhdr = HDR(addr);
+    size_t sz = hhdr -> hb_sz;
+    size_t nwords = BYTES_TO_WORDS(sz);
+    complex_descriptor * descr = (complex_descriptor *)(addr[nwords-1]);
+    mse * orig_mark_stack_ptr = mark_stack_ptr;
+    mse * new_mark_stack_ptr;
+
+    if (descr == 0) {
+        /* Found a reference to a free list entry.  Ignore it. */
+        return(orig_mark_stack_ptr);
+    }
+    /* In use counts were already updated when array descriptor was     */
+    /* pushed.  Here we only replace it by subobject descriptors, so    */
+    /* no update is necessary.                                          */
+    new_mark_stack_ptr = GC_push_complex_descriptor(addr, descr,
+                                                    mark_stack_ptr,
+                                                    mark_stack_limit-1);
+    if (new_mark_stack_ptr == 0) {
+        /* Doesn't fit.  Conservatively push the whole array as a unit  */
+        /* and request a mark stack expansion.                          */
+        /* This cannot cause a mark stack overflow, since it replaces   */
+        /* the original array entry.                                    */
+        GC_mark_stack_too_small = TRUE;
+        new_mark_stack_ptr = orig_mark_stack_ptr + 1;
+        new_mark_stack_ptr -> mse_start = (ptr_t)addr;
+        new_mark_stack_ptr -> mse_descr = sz | GC_DS_LENGTH;
+    } else {
+        /* Push descriptor itself */
+        new_mark_stack_ptr++;
+        new_mark_stack_ptr -> mse_start = (ptr_t)(addr + nwords - 1);
+        new_mark_stack_ptr -> mse_descr = sizeof(word) | GC_DS_LENGTH;
+    }
+    return new_mark_stack_ptr;
+}
+
+GC_API GC_descr GC_CALL GC_make_descriptor(GC_bitmap bm, size_t len)
+{
+    signed_word last_set_bit = len - 1;
+    GC_descr result;
+    signed_word i;
+#   define HIGH_BIT (((word)1) << (WORDSZ - 1))
+
+    if (!GC_explicit_typing_initialized) GC_init_explicit_typing();
+    while (last_set_bit >= 0 && !GC_get_bit(bm, last_set_bit))
+      last_set_bit--;
+    if (last_set_bit < 0) return(0 /* no pointers */);
+#   if ALIGNMENT == CPP_WORDSZ/8
+    {
+      register GC_bool all_bits_set = TRUE;
+      for (i = 0; i < last_set_bit; i++) {
+        if (!GC_get_bit(bm, i)) {
+            all_bits_set = FALSE;
+            break;
+        }
+      }
+      if (all_bits_set) {
+        /* An initial section contains all pointers.  Use length descriptor. */
+        return (WORDS_TO_BYTES(last_set_bit+1) | GC_DS_LENGTH);
+      }
+    }
+#   endif
+    if ((word)last_set_bit < BITMAP_BITS) {
+        /* Hopefully the common case.                   */
+        /* Build bitmap descriptor (with bits reversed) */
+        result = HIGH_BIT;
+        for (i = last_set_bit - 1; i >= 0; i--) {
+            result >>= 1;
+            if (GC_get_bit(bm, i)) result |= HIGH_BIT;
+        }
+        result |= GC_DS_BITMAP;
+        return(result);
+    } else {
+        signed_word index;
+
+        index = GC_add_ext_descriptor(bm, (word)last_set_bit+1);
+        if (index == -1) return(WORDS_TO_BYTES(last_set_bit+1) | GC_DS_LENGTH);
+                                /* Out of memory: use conservative      */
+                                /* approximation.                       */
+        result = GC_MAKE_PROC(GC_typed_mark_proc_index, (word)index);
+        return result;
+    }
+}
+
+GC_API void * GC_CALL GC_malloc_explicitly_typed(size_t lb, GC_descr d)
+{
+    ptr_t op;
+    ptr_t * opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    lb += TYPD_EXTRA_BYTES;
+    if(SMALL_OBJ(lb)) {
+        lg = GC_size_map[lb];
+        opp = &(GC_eobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) == 0 ) {
+            UNLOCK();
+            op = (ptr_t)GENERAL_MALLOC((word)lb, GC_explicit_kind);
+            if (0 == op) return 0;
+            lg = GC_size_map[lb];       /* May have been uninitialized. */
+        } else {
+            *opp = obj_link(op);
+            obj_link(op) = 0;
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+            UNLOCK();
+        }
+        ((word *)op)[GRANULES_TO_WORDS(lg) - 1] = d;
+   } else {
+       op = (ptr_t)GENERAL_MALLOC((word)lb, GC_explicit_kind);
+       if (op != NULL) {
+            lg = BYTES_TO_GRANULES(GC_size(op));
+            ((word *)op)[GRANULES_TO_WORDS(lg) - 1] = d;
+       }
+   }
+   return((void *) op);
+}
+
+GC_API void * GC_CALL GC_malloc_explicitly_typed_ignore_off_page(size_t lb,
+                                                                 GC_descr d)
+{
+    ptr_t op;
+    ptr_t * opp;
+    size_t lg;
+    DCL_LOCK_STATE;
+
+    lb += TYPD_EXTRA_BYTES;
+    if( SMALL_OBJ(lb) ) {
+        lg = GC_size_map[lb];
+        opp = &(GC_eobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) == 0 ) {
+            UNLOCK();
+            op = (ptr_t)GENERAL_MALLOC_IOP(lb, GC_explicit_kind);
+            if (0 == op) return 0;
+            lg = GC_size_map[lb];       /* May have been uninitialized. */
+        } else {
+            *opp = obj_link(op);
+            obj_link(op) = 0;
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+            UNLOCK();
+        }
+        ((word *)op)[GRANULES_TO_WORDS(lg) - 1] = d;
+   } else {
+       op = (ptr_t)GENERAL_MALLOC_IOP(lb, GC_explicit_kind);
+       if (op != NULL) {
+         lg = BYTES_TO_GRANULES(GC_size(op));
+         ((word *)op)[GRANULES_TO_WORDS(lg) - 1] = d;
+       }
+   }
+   return((void *) op);
+}
+
+GC_API void * GC_CALL GC_calloc_explicitly_typed(size_t n, size_t lb,
+                                                 GC_descr d)
+{
+    ptr_t op;
+    ptr_t * opp;
+    size_t lg;
+    GC_descr simple_descr;
+    complex_descriptor *complex_descr;
+    register int descr_type;
+    struct LeafDescriptor leaf;
+    DCL_LOCK_STATE;
+
+    descr_type = GC_make_array_descriptor((word)n, (word)lb, d,
+                                          &simple_descr, &complex_descr, &leaf);
+    switch(descr_type) {
+        case NO_MEM: return(0);
+        case SIMPLE: return(GC_malloc_explicitly_typed(n*lb, simple_descr));
+        case LEAF:
+            lb *= n;
+            lb += sizeof(struct LeafDescriptor) + TYPD_EXTRA_BYTES;
+            break;
+        case COMPLEX:
+            lb *= n;
+            lb += TYPD_EXTRA_BYTES;
+            break;
+    }
+    if( SMALL_OBJ(lb) ) {
+        lg = GC_size_map[lb];
+        opp = &(GC_arobjfreelist[lg]);
+        LOCK();
+        if( (op = *opp) == 0 ) {
+            UNLOCK();
+            op = (ptr_t)GENERAL_MALLOC((word)lb, GC_array_kind);
+            if (0 == op) return(0);
+            lg = GC_size_map[lb];       /* May have been uninitialized. */
+        } else {
+            *opp = obj_link(op);
+            obj_link(op) = 0;
+            GC_bytes_allocd += GRANULES_TO_BYTES(lg);
+            UNLOCK();
+        }
+   } else {
+       op = (ptr_t)GENERAL_MALLOC((word)lb, GC_array_kind);
+       if (0 == op) return(0);
+       lg = BYTES_TO_GRANULES(GC_size(op));
+   }
+   if (descr_type == LEAF) {
+       /* Set up the descriptor inside the object itself. */
+       volatile struct LeafDescriptor * lp =
+           (struct LeafDescriptor *)
+               ((word *)op
+                + GRANULES_TO_WORDS(lg)
+                - (BYTES_TO_WORDS(sizeof(struct LeafDescriptor)) + 1));
+
+       lp -> ld_tag = LEAF_TAG;
+       lp -> ld_size = leaf.ld_size;
+       lp -> ld_nelements = leaf.ld_nelements;
+       lp -> ld_descriptor = leaf.ld_descriptor;
+       ((volatile word *)op)[GRANULES_TO_WORDS(lg) - 1] = (word)lp;
+   } else {
+       size_t lw = GRANULES_TO_WORDS(lg);
+
+       ((word *)op)[lw - 1] = (word)complex_descr;
+       /* Make sure the descriptor is cleared once there is any danger  */
+       /* it may have been collected.                                   */
+       if (GC_general_register_disappearing_link((void * *)((word *)op+lw-1),
+                                                 op) == GC_NO_MEMORY) {
+           /* Couldn't register it due to lack of memory.  Punt.        */
+           /* This will probably fail too, but gives the recovery code  */
+           /* a chance.                                                 */
+           return(GC_malloc(n*lb));
+       }
+   }
+   return((void *) op);
+}
diff --git a/src/gc/bdwgc/unused/README.QUICK b/src/gc/bdwgc/unused/README.QUICK
new file mode 100644
index 0000000..6d7471f
--- /dev/null
+++ b/src/gc/bdwgc/unused/README.QUICK
@@ -0,0 +1,88 @@
+Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+Copyright (c) 1991-1995 by Xerox Corporation.  All rights reserved.
+Copyright (c) 1996-1999 by Silicon Graphics.  All rights reserved.
+Copyright (c) 1999-2001 by Hewlett-Packard. All rights reserved.
+
+THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+
+Permission is hereby granted to use or copy this program
+for any purpose,  provided the above notices are retained on all copies.
+Permission to modify the code and to distribute modified code is granted,
+provided the above notices are retained, and a notice that the code was
+modified is included with the above copyright notice.
+
+A few files have other copyright holders. A few of the files needed
+to use the GNU-style build procedure come with a modified GPL license
+that appears not to significantly restrict use of the collector, though
+use of those files for a purpose other than building the collector may
+require the resulting code to be covered by the GPL.
+
+For more details and the names of other contributors, see the
+doc/README* files and include/gc.h.  This file describes typical use of
+the collector on a machine that is already supported.
+
+For the version number, see doc/README or version.h.
+
+INSTALLATION:
+Under UN*X, Linux:
+Alternative 1 (the old way): type "make test" in this directory.
+        Link against gc.a.  With the most recent GC distributions
+        you may have to type "make -f Makefile.direct test" or
+        copy Makefile.direct to Makefile first.
+
+Alternative 2 (the new way): type
+        "./configure --prefix=<dir>; make; make check; make install".
+        Link against <dir>/lib/libgc.a or <dir>/lib/libgc.so.
+        See README.autoconf for details
+
+Under Windows 95, 98, Me, NT, or 2000:
+copy the appropriate makefile to MAKEFILE, read it, and type "nmake test".
+(Under Windows, this assumes you have Microsoft command-line tools
+installed, and suitably configured.)
+Read the machine specific README in the doc directory if one exists.
+
+If you need thread support, you will need to follow the special
+platform-dependent instructions (win32), or define GC_THREADS
+as described in doc/README.macros, or possibly use
+--enable-threads=posix when running the configure script.
+
+If you wish to use the cord (structured string) library with the stand-alone
+Makefile.direct, type "make cords", after copying to "Makefile".
+(This requires an ANSI C compiler.  You may
+need to redefine CC in the Makefile. The CORD_printf implementation in
+cordprnt.c is known to be less than perfectly portable.  The rest of the
+package should still work.)
+
+If you wish to use the collector from C++, type "make c++", or use
+--enable-cplusplus with the configure script.   With Makefile.direct,
+these ones add further files to gc.a and to the include subdirectory.
+With the alternate build process,this generates libgccpp.
+See cord/cord.h and include/gc_cpp.h.
+
+TYPICAL USE:
+Include "gc.h" from the include subdirectory.  Link against the
+appropriate library ("gc.a" under UN*X).  Replace calls to malloc
+by calls to GC_MALLOC, and calls to realloc by calls to GC_REALLOC.
+If the object is known to never contain pointers, use GC_MALLOC_ATOMIC
+instead of GC_MALLOC.
+
+Define GC_DEBUG before including gc.h for additional checking.
+
+More documentation on the collector interface can be found at
+doc/gcinterface.html,
+in doc/README and other files in the doc directory, and in include/gc.h .
+
+WARNINGS:
+
+Do not store the only pointer to an object in memory allocated
+with system malloc, since the collector usually does not scan
+memory allocated in this way.
+
+Use with threads may be supported on your system, but requires the
+collector to be built with thread support.  See Makefile.  The collector
+does not guarantee to scan thread-local storage (e.g. of the kind
+accessed with pthread_getspecific()).  The collector does scan
+thread stacks though, so generally the best solution is to ensure that
+any pointers stored in thread-local storage are also stored on the
+thread's stack for the duration of their lifetime.
diff --git a/src/gc/bdwgc/unused/aclocal.m4 b/src/gc/bdwgc/unused/aclocal.m4
new file mode 100644
index 0000000..1118830
--- /dev/null
+++ b/src/gc/bdwgc/unused/aclocal.m4
@@ -0,0 +1,1400 @@
+# generated automatically by aclocal 1.14.1 -*- Autoconf -*-
+
+# Copyright (C) 1996-2013 Free Software Foundation, Inc.
+
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY, to the extent permitted by law; without
+# even the implied warranty of MERCHANTABILITY or FITNESS FOR A
+# PARTICULAR PURPOSE.
+
+m4_ifndef([AC_CONFIG_MACRO_DIRS], [m4_defun([_AM_CONFIG_MACRO_DIRS], [])m4_defun([AC_CONFIG_MACRO_DIRS], [_AM_CONFIG_MACRO_DIRS($@)])])
+m4_ifndef([AC_AUTOCONF_VERSION],
+  [m4_copy([m4_PACKAGE_VERSION], [AC_AUTOCONF_VERSION])])dnl
+m4_if(m4_defn([AC_AUTOCONF_VERSION]), [2.69],,
+[m4_warning([this file was generated for autoconf 2.69.
+You have another version of autoconf.  It may work, but is not guaranteed to.
+If you have problems, you may need to regenerate the build system entirely.
+To do so, use the procedure documented by the package, typically 'autoreconf'.])])
+
+# pkg.m4 - Macros to locate and utilise pkg-config.            -*- Autoconf -*-
+# serial 1 (pkg-config-0.24)
+# 
+# Copyright  2004 Scott James Remnant <scott@netsplit.com>.
+#
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 2 of the License, or
+# (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+# General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+#
+# As a special exception to the GNU General Public License, if you
+# distribute this file as part of a program that contains a
+# configuration script generated by Autoconf, you may include it under
+# the same distribution terms that you use for the rest of that program.
+
+# PKG_PROG_PKG_CONFIG([MIN-VERSION])
+# ----------------------------------
+AC_DEFUN([PKG_PROG_PKG_CONFIG],
+[m4_pattern_forbid([^_?PKG_[A-Z_]+$])
+m4_pattern_allow([^PKG_CONFIG(_(PATH|LIBDIR|SYSROOT_DIR|ALLOW_SYSTEM_(CFLAGS|LIBS)))?$])
+m4_pattern_allow([^PKG_CONFIG_(DISABLE_UNINSTALLED|TOP_BUILD_DIR|DEBUG_SPEW)$])
+AC_ARG_VAR([PKG_CONFIG], [path to pkg-config utility])
+AC_ARG_VAR([PKG_CONFIG_PATH], [directories to add to pkg-config's search path])
+AC_ARG_VAR([PKG_CONFIG_LIBDIR], [path overriding pkg-config's built-in search path])
+
+if test "x$ac_cv_env_PKG_CONFIG_set" != "xset"; then
+	AC_PATH_TOOL([PKG_CONFIG], [pkg-config])
+fi
+if test -n "$PKG_CONFIG"; then
+	_pkg_min_version=m4_default([$1], [0.9.0])
+	AC_MSG_CHECKING([pkg-config is at least version $_pkg_min_version])
+	if $PKG_CONFIG --atleast-pkgconfig-version $_pkg_min_version; then
+		AC_MSG_RESULT([yes])
+	else
+		AC_MSG_RESULT([no])
+		PKG_CONFIG=""
+	fi
+fi[]dnl
+])# PKG_PROG_PKG_CONFIG
+
+# PKG_CHECK_EXISTS(MODULES, [ACTION-IF-FOUND], [ACTION-IF-NOT-FOUND])
+#
+# Check to see whether a particular set of modules exists.  Similar
+# to PKG_CHECK_MODULES(), but does not set variables or print errors.
+#
+# Please remember that m4 expands AC_REQUIRE([PKG_PROG_PKG_CONFIG])
+# only at the first occurence in configure.ac, so if the first place
+# it's called might be skipped (such as if it is within an "if", you
+# have to call PKG_CHECK_EXISTS manually
+# --------------------------------------------------------------
+AC_DEFUN([PKG_CHECK_EXISTS],
+[AC_REQUIRE([PKG_PROG_PKG_CONFIG])dnl
+if test -n "$PKG_CONFIG" && \
+    AC_RUN_LOG([$PKG_CONFIG --exists --print-errors "$1"]); then
+  m4_default([$2], [:])
+m4_ifvaln([$3], [else
+  $3])dnl
+fi])
+
+# _PKG_CONFIG([VARIABLE], [COMMAND], [MODULES])
+# ---------------------------------------------
+m4_define([_PKG_CONFIG],
+[if test -n "$$1"; then
+    pkg_cv_[]$1="$$1"
+ elif test -n "$PKG_CONFIG"; then
+    PKG_CHECK_EXISTS([$3],
+                     [pkg_cv_[]$1=`$PKG_CONFIG --[]$2 "$3" 2>/dev/null`
+		      test "x$?" != "x0" && pkg_failed=yes ],
+		     [pkg_failed=yes])
+ else
+    pkg_failed=untried
+fi[]dnl
+])# _PKG_CONFIG
+
+# _PKG_SHORT_ERRORS_SUPPORTED
+# -----------------------------
+AC_DEFUN([_PKG_SHORT_ERRORS_SUPPORTED],
+[AC_REQUIRE([PKG_PROG_PKG_CONFIG])
+if $PKG_CONFIG --atleast-pkgconfig-version 0.20; then
+        _pkg_short_errors_supported=yes
+else
+        _pkg_short_errors_supported=no
+fi[]dnl
+])# _PKG_SHORT_ERRORS_SUPPORTED
+
+
+# PKG_CHECK_MODULES(VARIABLE-PREFIX, MODULES, [ACTION-IF-FOUND],
+# [ACTION-IF-NOT-FOUND])
+#
+#
+# Note that if there is a possibility the first call to
+# PKG_CHECK_MODULES might not happen, you should be sure to include an
+# explicit call to PKG_PROG_PKG_CONFIG in your configure.ac
+#
+#
+# --------------------------------------------------------------
+AC_DEFUN([PKG_CHECK_MODULES],
+[AC_REQUIRE([PKG_PROG_PKG_CONFIG])dnl
+AC_ARG_VAR([$1][_CFLAGS], [C compiler flags for $1, overriding pkg-config])dnl
+AC_ARG_VAR([$1][_LIBS], [linker flags for $1, overriding pkg-config])dnl
+
+pkg_failed=no
+AC_MSG_CHECKING([for $1])
+
+_PKG_CONFIG([$1][_CFLAGS], [cflags], [$2])
+_PKG_CONFIG([$1][_LIBS], [libs], [$2])
+
+m4_define([_PKG_TEXT], [Alternatively, you may set the environment variables $1[]_CFLAGS
+and $1[]_LIBS to avoid the need to call pkg-config.
+See the pkg-config man page for more details.])
+
+if test $pkg_failed = yes; then
+   	AC_MSG_RESULT([no])
+        _PKG_SHORT_ERRORS_SUPPORTED
+        if test $_pkg_short_errors_supported = yes; then
+	        $1[]_PKG_ERRORS=`$PKG_CONFIG --short-errors --print-errors --cflags --libs "$2" 2>&1`
+        else 
+	        $1[]_PKG_ERRORS=`$PKG_CONFIG --print-errors --cflags --libs "$2" 2>&1`
+        fi
+	# Put the nasty error message in config.log where it belongs
+	echo "$$1[]_PKG_ERRORS" >&AS_MESSAGE_LOG_FD
+
+	m4_default([$4], [AC_MSG_ERROR(
+[Package requirements ($2) were not met:
+
+$$1_PKG_ERRORS
+
+Consider adjusting the PKG_CONFIG_PATH environment variable if you
+installed software in a non-standard prefix.
+
+_PKG_TEXT])[]dnl
+        ])
+elif test $pkg_failed = untried; then
+     	AC_MSG_RESULT([no])
+	m4_default([$4], [AC_MSG_FAILURE(
+[The pkg-config script could not be found or is too old.  Make sure it
+is in your PATH or set the PKG_CONFIG environment variable to the full
+path to pkg-config.
+
+_PKG_TEXT
+
+To get pkg-config, see <http://pkg-config.freedesktop.org/>.])[]dnl
+        ])
+else
+	$1[]_CFLAGS=$pkg_cv_[]$1[]_CFLAGS
+	$1[]_LIBS=$pkg_cv_[]$1[]_LIBS
+        AC_MSG_RESULT([yes])
+	$3
+fi[]dnl
+])# PKG_CHECK_MODULES
+
+# Copyright (C) 2002-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_AUTOMAKE_VERSION(VERSION)
+# ----------------------------
+# Automake X.Y traces this macro to ensure aclocal.m4 has been
+# generated from the m4 files accompanying Automake X.Y.
+# (This private macro should not be called outside this file.)
+AC_DEFUN([AM_AUTOMAKE_VERSION],
+[am__api_version='1.14'
+dnl Some users find AM_AUTOMAKE_VERSION and mistake it for a way to
+dnl require some minimum version.  Point them to the right macro.
+m4_if([$1], [1.14.1], [],
+      [AC_FATAL([Do not call $0, use AM_INIT_AUTOMAKE([$1]).])])dnl
+])
+
+# _AM_AUTOCONF_VERSION(VERSION)
+# -----------------------------
+# aclocal traces this macro to find the Autoconf version.
+# This is a private macro too.  Using m4_define simplifies
+# the logic in aclocal, which can simply ignore this definition.
+m4_define([_AM_AUTOCONF_VERSION], [])
+
+# AM_SET_CURRENT_AUTOMAKE_VERSION
+# -------------------------------
+# Call AM_AUTOMAKE_VERSION and AM_AUTOMAKE_VERSION so they can be traced.
+# This function is AC_REQUIREd by AM_INIT_AUTOMAKE.
+AC_DEFUN([AM_SET_CURRENT_AUTOMAKE_VERSION],
+[AM_AUTOMAKE_VERSION([1.14.1])dnl
+m4_ifndef([AC_AUTOCONF_VERSION],
+  [m4_copy([m4_PACKAGE_VERSION], [AC_AUTOCONF_VERSION])])dnl
+_AM_AUTOCONF_VERSION(m4_defn([AC_AUTOCONF_VERSION]))])
+
+# Figure out how to run the assembler.                      -*- Autoconf -*-
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_PROG_AS
+# ----------
+AC_DEFUN([AM_PROG_AS],
+[# By default we simply use the C compiler to build assembly code.
+AC_REQUIRE([AC_PROG_CC])
+test "${CCAS+set}" = set || CCAS=$CC
+test "${CCASFLAGS+set}" = set || CCASFLAGS=$CFLAGS
+AC_ARG_VAR([CCAS],      [assembler compiler command (defaults to CC)])
+AC_ARG_VAR([CCASFLAGS], [assembler compiler flags (defaults to CFLAGS)])
+_AM_IF_OPTION([no-dependencies],, [_AM_DEPENDENCIES([CCAS])])dnl
+])
+
+# AM_AUX_DIR_EXPAND                                         -*- Autoconf -*-
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# For projects using AC_CONFIG_AUX_DIR([foo]), Autoconf sets
+# $ac_aux_dir to '$srcdir/foo'.  In other projects, it is set to
+# '$srcdir', '$srcdir/..', or '$srcdir/../..'.
+#
+# Of course, Automake must honor this variable whenever it calls a
+# tool from the auxiliary directory.  The problem is that $srcdir (and
+# therefore $ac_aux_dir as well) can be either absolute or relative,
+# depending on how configure is run.  This is pretty annoying, since
+# it makes $ac_aux_dir quite unusable in subdirectories: in the top
+# source directory, any form will work fine, but in subdirectories a
+# relative path needs to be adjusted first.
+#
+# $ac_aux_dir/missing
+#    fails when called from a subdirectory if $ac_aux_dir is relative
+# $top_srcdir/$ac_aux_dir/missing
+#    fails if $ac_aux_dir is absolute,
+#    fails when called from a subdirectory in a VPATH build with
+#          a relative $ac_aux_dir
+#
+# The reason of the latter failure is that $top_srcdir and $ac_aux_dir
+# are both prefixed by $srcdir.  In an in-source build this is usually
+# harmless because $srcdir is '.', but things will broke when you
+# start a VPATH build or use an absolute $srcdir.
+#
+# So we could use something similar to $top_srcdir/$ac_aux_dir/missing,
+# iff we strip the leading $srcdir from $ac_aux_dir.  That would be:
+#   am_aux_dir='\$(top_srcdir)/'`expr "$ac_aux_dir" : "$srcdir//*\(.*\)"`
+# and then we would define $MISSING as
+#   MISSING="\${SHELL} $am_aux_dir/missing"
+# This will work as long as MISSING is not called from configure, because
+# unfortunately $(top_srcdir) has no meaning in configure.
+# However there are other variables, like CC, which are often used in
+# configure, and could therefore not use this "fixed" $ac_aux_dir.
+#
+# Another solution, used here, is to always expand $ac_aux_dir to an
+# absolute PATH.  The drawback is that using absolute paths prevent a
+# configured tree to be moved without reconfiguration.
+
+AC_DEFUN([AM_AUX_DIR_EXPAND],
+[dnl Rely on autoconf to set up CDPATH properly.
+AC_PREREQ([2.50])dnl
+# expand $ac_aux_dir to an absolute path
+am_aux_dir=`cd $ac_aux_dir && pwd`
+])
+
+# AM_CONDITIONAL                                            -*- Autoconf -*-
+
+# Copyright (C) 1997-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_CONDITIONAL(NAME, SHELL-CONDITION)
+# -------------------------------------
+# Define a conditional.
+AC_DEFUN([AM_CONDITIONAL],
+[AC_PREREQ([2.52])dnl
+ m4_if([$1], [TRUE],  [AC_FATAL([$0: invalid condition: $1])],
+       [$1], [FALSE], [AC_FATAL([$0: invalid condition: $1])])dnl
+AC_SUBST([$1_TRUE])dnl
+AC_SUBST([$1_FALSE])dnl
+_AM_SUBST_NOTMAKE([$1_TRUE])dnl
+_AM_SUBST_NOTMAKE([$1_FALSE])dnl
+m4_define([_AM_COND_VALUE_$1], [$2])dnl
+if $2; then
+  $1_TRUE=
+  $1_FALSE='#'
+else
+  $1_TRUE='#'
+  $1_FALSE=
+fi
+AC_CONFIG_COMMANDS_PRE(
+[if test -z "${$1_TRUE}" && test -z "${$1_FALSE}"; then
+  AC_MSG_ERROR([[conditional "$1" was never defined.
+Usually this means the macro was only invoked conditionally.]])
+fi])])
+
+# Copyright (C) 1999-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+
+# There are a few dirty hacks below to avoid letting 'AC_PROG_CC' be
+# written in clear, in which case automake, when reading aclocal.m4,
+# will think it sees a *use*, and therefore will trigger all it's
+# C support machinery.  Also note that it means that autoscan, seeing
+# CC etc. in the Makefile, will ask for an AC_PROG_CC use...
+
+
+# _AM_DEPENDENCIES(NAME)
+# ----------------------
+# See how the compiler implements dependency checking.
+# NAME is "CC", "CXX", "OBJC", "OBJCXX", "UPC", or "GJC".
+# We try a few techniques and use that to set a single cache variable.
+#
+# We don't AC_REQUIRE the corresponding AC_PROG_CC since the latter was
+# modified to invoke _AM_DEPENDENCIES(CC); we would have a circular
+# dependency, and given that the user is not expected to run this macro,
+# just rely on AC_PROG_CC.
+AC_DEFUN([_AM_DEPENDENCIES],
+[AC_REQUIRE([AM_SET_DEPDIR])dnl
+AC_REQUIRE([AM_OUTPUT_DEPENDENCY_COMMANDS])dnl
+AC_REQUIRE([AM_MAKE_INCLUDE])dnl
+AC_REQUIRE([AM_DEP_TRACK])dnl
+
+m4_if([$1], [CC],   [depcc="$CC"   am_compiler_list=],
+      [$1], [CXX],  [depcc="$CXX"  am_compiler_list=],
+      [$1], [OBJC], [depcc="$OBJC" am_compiler_list='gcc3 gcc'],
+      [$1], [OBJCXX], [depcc="$OBJCXX" am_compiler_list='gcc3 gcc'],
+      [$1], [UPC],  [depcc="$UPC"  am_compiler_list=],
+      [$1], [GCJ],  [depcc="$GCJ"  am_compiler_list='gcc3 gcc'],
+                    [depcc="$$1"   am_compiler_list=])
+
+AC_CACHE_CHECK([dependency style of $depcc],
+               [am_cv_$1_dependencies_compiler_type],
+[if test -z "$AMDEP_TRUE" && test -f "$am_depcomp"; then
+  # We make a subdir and do the tests there.  Otherwise we can end up
+  # making bogus files that we don't know about and never remove.  For
+  # instance it was reported that on HP-UX the gcc test will end up
+  # making a dummy file named 'D' -- because '-MD' means "put the output
+  # in D".
+  rm -rf conftest.dir
+  mkdir conftest.dir
+  # Copy depcomp to subdir because otherwise we won't find it if we're
+  # using a relative directory.
+  cp "$am_depcomp" conftest.dir
+  cd conftest.dir
+  # We will build objects and dependencies in a subdirectory because
+  # it helps to detect inapplicable dependency modes.  For instance
+  # both Tru64's cc and ICC support -MD to output dependencies as a
+  # side effect of compilation, but ICC will put the dependencies in
+  # the current directory while Tru64 will put them in the object
+  # directory.
+  mkdir sub
+
+  am_cv_$1_dependencies_compiler_type=none
+  if test "$am_compiler_list" = ""; then
+     am_compiler_list=`sed -n ['s/^#*\([a-zA-Z0-9]*\))$/\1/p'] < ./depcomp`
+  fi
+  am__universal=false
+  m4_case([$1], [CC],
+    [case " $depcc " in #(
+     *\ -arch\ *\ -arch\ *) am__universal=true ;;
+     esac],
+    [CXX],
+    [case " $depcc " in #(
+     *\ -arch\ *\ -arch\ *) am__universal=true ;;
+     esac])
+
+  for depmode in $am_compiler_list; do
+    # Setup a source with many dependencies, because some compilers
+    # like to wrap large dependency lists on column 80 (with \), and
+    # we should not choose a depcomp mode which is confused by this.
+    #
+    # We need to recreate these files for each test, as the compiler may
+    # overwrite some of them when testing with obscure command lines.
+    # This happens at least with the AIX C compiler.
+    : > sub/conftest.c
+    for i in 1 2 3 4 5 6; do
+      echo '#include "conftst'$i'.h"' >> sub/conftest.c
+      # Using ": > sub/conftst$i.h" creates only sub/conftst1.h with
+      # Solaris 10 /bin/sh.
+      echo '/* dummy */' > sub/conftst$i.h
+    done
+    echo "${am__include} ${am__quote}sub/conftest.Po${am__quote}" > confmf
+
+    # We check with '-c' and '-o' for the sake of the "dashmstdout"
+    # mode.  It turns out that the SunPro C++ compiler does not properly
+    # handle '-M -o', and we need to detect this.  Also, some Intel
+    # versions had trouble with output in subdirs.
+    am__obj=sub/conftest.${OBJEXT-o}
+    am__minus_obj="-o $am__obj"
+    case $depmode in
+    gcc)
+      # This depmode causes a compiler race in universal mode.
+      test "$am__universal" = false || continue
+      ;;
+    nosideeffect)
+      # After this tag, mechanisms are not by side-effect, so they'll
+      # only be used when explicitly requested.
+      if test "x$enable_dependency_tracking" = xyes; then
+	continue
+      else
+	break
+      fi
+      ;;
+    msvc7 | msvc7msys | msvisualcpp | msvcmsys)
+      # This compiler won't grok '-c -o', but also, the minuso test has
+      # not run yet.  These depmodes are late enough in the game, and
+      # so weak that their functioning should not be impacted.
+      am__obj=conftest.${OBJEXT-o}
+      am__minus_obj=
+      ;;
+    none) break ;;
+    esac
+    if depmode=$depmode \
+       source=sub/conftest.c object=$am__obj \
+       depfile=sub/conftest.Po tmpdepfile=sub/conftest.TPo \
+       $SHELL ./depcomp $depcc -c $am__minus_obj sub/conftest.c \
+         >/dev/null 2>conftest.err &&
+       grep sub/conftst1.h sub/conftest.Po > /dev/null 2>&1 &&
+       grep sub/conftst6.h sub/conftest.Po > /dev/null 2>&1 &&
+       grep $am__obj sub/conftest.Po > /dev/null 2>&1 &&
+       ${MAKE-make} -s -f confmf > /dev/null 2>&1; then
+      # icc doesn't choke on unknown options, it will just issue warnings
+      # or remarks (even with -Werror).  So we grep stderr for any message
+      # that says an option was ignored or not supported.
+      # When given -MP, icc 7.0 and 7.1 complain thusly:
+      #   icc: Command line warning: ignoring option '-M'; no argument required
+      # The diagnosis changed in icc 8.0:
+      #   icc: Command line remark: option '-MP' not supported
+      if (grep 'ignoring option' conftest.err ||
+          grep 'not supported' conftest.err) >/dev/null 2>&1; then :; else
+        am_cv_$1_dependencies_compiler_type=$depmode
+        break
+      fi
+    fi
+  done
+
+  cd ..
+  rm -rf conftest.dir
+else
+  am_cv_$1_dependencies_compiler_type=none
+fi
+])
+AC_SUBST([$1DEPMODE], [depmode=$am_cv_$1_dependencies_compiler_type])
+AM_CONDITIONAL([am__fastdep$1], [
+  test "x$enable_dependency_tracking" != xno \
+  && test "$am_cv_$1_dependencies_compiler_type" = gcc3])
+])
+
+
+# AM_SET_DEPDIR
+# -------------
+# Choose a directory name for dependency files.
+# This macro is AC_REQUIREd in _AM_DEPENDENCIES.
+AC_DEFUN([AM_SET_DEPDIR],
+[AC_REQUIRE([AM_SET_LEADING_DOT])dnl
+AC_SUBST([DEPDIR], ["${am__leading_dot}deps"])dnl
+])
+
+
+# AM_DEP_TRACK
+# ------------
+AC_DEFUN([AM_DEP_TRACK],
+[AC_ARG_ENABLE([dependency-tracking], [dnl
+AS_HELP_STRING(
+  [--enable-dependency-tracking],
+  [do not reject slow dependency extractors])
+AS_HELP_STRING(
+  [--disable-dependency-tracking],
+  [speeds up one-time build])])
+if test "x$enable_dependency_tracking" != xno; then
+  am_depcomp="$ac_aux_dir/depcomp"
+  AMDEPBACKSLASH='\'
+  am__nodep='_no'
+fi
+AM_CONDITIONAL([AMDEP], [test "x$enable_dependency_tracking" != xno])
+AC_SUBST([AMDEPBACKSLASH])dnl
+_AM_SUBST_NOTMAKE([AMDEPBACKSLASH])dnl
+AC_SUBST([am__nodep])dnl
+_AM_SUBST_NOTMAKE([am__nodep])dnl
+])
+
+# Generate code to set up dependency tracking.              -*- Autoconf -*-
+
+# Copyright (C) 1999-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+
+# _AM_OUTPUT_DEPENDENCY_COMMANDS
+# ------------------------------
+AC_DEFUN([_AM_OUTPUT_DEPENDENCY_COMMANDS],
+[{
+  # Older Autoconf quotes --file arguments for eval, but not when files
+  # are listed without --file.  Let's play safe and only enable the eval
+  # if we detect the quoting.
+  case $CONFIG_FILES in
+  *\'*) eval set x "$CONFIG_FILES" ;;
+  *)   set x $CONFIG_FILES ;;
+  esac
+  shift
+  for mf
+  do
+    # Strip MF so we end up with the name of the file.
+    mf=`echo "$mf" | sed -e 's/:.*$//'`
+    # Check whether this is an Automake generated Makefile or not.
+    # We used to match only the files named 'Makefile.in', but
+    # some people rename them; so instead we look at the file content.
+    # Grep'ing the first line is not enough: some people post-process
+    # each Makefile.in and add a new line on top of each file to say so.
+    # Grep'ing the whole file is not good either: AIX grep has a line
+    # limit of 2048, but all sed's we know have understand at least 4000.
+    if sed -n 's,^#.*generated by automake.*,X,p' "$mf" | grep X >/dev/null 2>&1; then
+      dirpart=`AS_DIRNAME("$mf")`
+    else
+      continue
+    fi
+    # Extract the definition of DEPDIR, am__include, and am__quote
+    # from the Makefile without running 'make'.
+    DEPDIR=`sed -n 's/^DEPDIR = //p' < "$mf"`
+    test -z "$DEPDIR" && continue
+    am__include=`sed -n 's/^am__include = //p' < "$mf"`
+    test -z "$am__include" && continue
+    am__quote=`sed -n 's/^am__quote = //p' < "$mf"`
+    # Find all dependency output files, they are included files with
+    # $(DEPDIR) in their names.  We invoke sed twice because it is the
+    # simplest approach to changing $(DEPDIR) to its actual value in the
+    # expansion.
+    for file in `sed -n "
+      s/^$am__include $am__quote\(.*(DEPDIR).*\)$am__quote"'$/\1/p' <"$mf" | \
+	 sed -e 's/\$(DEPDIR)/'"$DEPDIR"'/g'`; do
+      # Make sure the directory exists.
+      test -f "$dirpart/$file" && continue
+      fdir=`AS_DIRNAME(["$file"])`
+      AS_MKDIR_P([$dirpart/$fdir])
+      # echo "creating $dirpart/$file"
+      echo '# dummy' > "$dirpart/$file"
+    done
+  done
+}
+])# _AM_OUTPUT_DEPENDENCY_COMMANDS
+
+
+# AM_OUTPUT_DEPENDENCY_COMMANDS
+# -----------------------------
+# This macro should only be invoked once -- use via AC_REQUIRE.
+#
+# This code is only required when automatic dependency tracking
+# is enabled.  FIXME.  This creates each '.P' file that we will
+# need in order to bootstrap the dependency handling code.
+AC_DEFUN([AM_OUTPUT_DEPENDENCY_COMMANDS],
+[AC_CONFIG_COMMANDS([depfiles],
+     [test x"$AMDEP_TRUE" != x"" || _AM_OUTPUT_DEPENDENCY_COMMANDS],
+     [AMDEP_TRUE="$AMDEP_TRUE" ac_aux_dir="$ac_aux_dir"])
+])
+
+# Do all the work for Automake.                             -*- Autoconf -*-
+
+# Copyright (C) 1996-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# This macro actually does too much.  Some checks are only needed if
+# your package does certain things.  But this isn't really a big deal.
+
+dnl Redefine AC_PROG_CC to automatically invoke _AM_PROG_CC_C_O.
+m4_define([AC_PROG_CC],
+m4_defn([AC_PROG_CC])
+[_AM_PROG_CC_C_O
+])
+
+# AM_INIT_AUTOMAKE(PACKAGE, VERSION, [NO-DEFINE])
+# AM_INIT_AUTOMAKE([OPTIONS])
+# -----------------------------------------------
+# The call with PACKAGE and VERSION arguments is the old style
+# call (pre autoconf-2.50), which is being phased out.  PACKAGE
+# and VERSION should now be passed to AC_INIT and removed from
+# the call to AM_INIT_AUTOMAKE.
+# We support both call styles for the transition.  After
+# the next Automake release, Autoconf can make the AC_INIT
+# arguments mandatory, and then we can depend on a new Autoconf
+# release and drop the old call support.
+AC_DEFUN([AM_INIT_AUTOMAKE],
+[AC_PREREQ([2.65])dnl
+dnl Autoconf wants to disallow AM_ names.  We explicitly allow
+dnl the ones we care about.
+m4_pattern_allow([^AM_[A-Z]+FLAGS$])dnl
+AC_REQUIRE([AM_SET_CURRENT_AUTOMAKE_VERSION])dnl
+AC_REQUIRE([AC_PROG_INSTALL])dnl
+if test "`cd $srcdir && pwd`" != "`pwd`"; then
+  # Use -I$(srcdir) only when $(srcdir) != ., so that make's output
+  # is not polluted with repeated "-I."
+  AC_SUBST([am__isrc], [' -I$(srcdir)'])_AM_SUBST_NOTMAKE([am__isrc])dnl
+  # test to see if srcdir already configured
+  if test -f $srcdir/config.status; then
+    AC_MSG_ERROR([source directory already configured; run "make distclean" there first])
+  fi
+fi
+
+# test whether we have cygpath
+if test -z "$CYGPATH_W"; then
+  if (cygpath --version) >/dev/null 2>/dev/null; then
+    CYGPATH_W='cygpath -w'
+  else
+    CYGPATH_W=echo
+  fi
+fi
+AC_SUBST([CYGPATH_W])
+
+# Define the identity of the package.
+dnl Distinguish between old-style and new-style calls.
+m4_ifval([$2],
+[AC_DIAGNOSE([obsolete],
+             [$0: two- and three-arguments forms are deprecated.])
+m4_ifval([$3], [_AM_SET_OPTION([no-define])])dnl
+ AC_SUBST([PACKAGE], [$1])dnl
+ AC_SUBST([VERSION], [$2])],
+[_AM_SET_OPTIONS([$1])dnl
+dnl Diagnose old-style AC_INIT with new-style AM_AUTOMAKE_INIT.
+m4_if(
+  m4_ifdef([AC_PACKAGE_NAME], [ok]):m4_ifdef([AC_PACKAGE_VERSION], [ok]),
+  [ok:ok],,
+  [m4_fatal([AC_INIT should be called with package and version arguments])])dnl
+ AC_SUBST([PACKAGE], ['AC_PACKAGE_TARNAME'])dnl
+ AC_SUBST([VERSION], ['AC_PACKAGE_VERSION'])])dnl
+
+_AM_IF_OPTION([no-define],,
+[AC_DEFINE_UNQUOTED([PACKAGE], ["$PACKAGE"], [Name of package])
+ AC_DEFINE_UNQUOTED([VERSION], ["$VERSION"], [Version number of package])])dnl
+
+# Some tools Automake needs.
+AC_REQUIRE([AM_SANITY_CHECK])dnl
+AC_REQUIRE([AC_ARG_PROGRAM])dnl
+AM_MISSING_PROG([ACLOCAL], [aclocal-${am__api_version}])
+AM_MISSING_PROG([AUTOCONF], [autoconf])
+AM_MISSING_PROG([AUTOMAKE], [automake-${am__api_version}])
+AM_MISSING_PROG([AUTOHEADER], [autoheader])
+AM_MISSING_PROG([MAKEINFO], [makeinfo])
+AC_REQUIRE([AM_PROG_INSTALL_SH])dnl
+AC_REQUIRE([AM_PROG_INSTALL_STRIP])dnl
+AC_REQUIRE([AC_PROG_MKDIR_P])dnl
+# For better backward compatibility.  To be removed once Automake 1.9.x
+# dies out for good.  For more background, see:
+# <http://lists.gnu.org/archive/html/automake/2012-07/msg00001.html>
+# <http://lists.gnu.org/archive/html/automake/2012-07/msg00014.html>
+AC_SUBST([mkdir_p], ['$(MKDIR_P)'])
+# We need awk for the "check" target.  The system "awk" is bad on
+# some platforms.
+AC_REQUIRE([AC_PROG_AWK])dnl
+AC_REQUIRE([AC_PROG_MAKE_SET])dnl
+AC_REQUIRE([AM_SET_LEADING_DOT])dnl
+_AM_IF_OPTION([tar-ustar], [_AM_PROG_TAR([ustar])],
+	      [_AM_IF_OPTION([tar-pax], [_AM_PROG_TAR([pax])],
+			     [_AM_PROG_TAR([v7])])])
+_AM_IF_OPTION([no-dependencies],,
+[AC_PROVIDE_IFELSE([AC_PROG_CC],
+		  [_AM_DEPENDENCIES([CC])],
+		  [m4_define([AC_PROG_CC],
+			     m4_defn([AC_PROG_CC])[_AM_DEPENDENCIES([CC])])])dnl
+AC_PROVIDE_IFELSE([AC_PROG_CXX],
+		  [_AM_DEPENDENCIES([CXX])],
+		  [m4_define([AC_PROG_CXX],
+			     m4_defn([AC_PROG_CXX])[_AM_DEPENDENCIES([CXX])])])dnl
+AC_PROVIDE_IFELSE([AC_PROG_OBJC],
+		  [_AM_DEPENDENCIES([OBJC])],
+		  [m4_define([AC_PROG_OBJC],
+			     m4_defn([AC_PROG_OBJC])[_AM_DEPENDENCIES([OBJC])])])dnl
+AC_PROVIDE_IFELSE([AC_PROG_OBJCXX],
+		  [_AM_DEPENDENCIES([OBJCXX])],
+		  [m4_define([AC_PROG_OBJCXX],
+			     m4_defn([AC_PROG_OBJCXX])[_AM_DEPENDENCIES([OBJCXX])])])dnl
+])
+AC_REQUIRE([AM_SILENT_RULES])dnl
+dnl The testsuite driver may need to know about EXEEXT, so add the
+dnl 'am__EXEEXT' conditional if _AM_COMPILER_EXEEXT was seen.  This
+dnl macro is hooked onto _AC_COMPILER_EXEEXT early, see below.
+AC_CONFIG_COMMANDS_PRE(dnl
+[m4_provide_if([_AM_COMPILER_EXEEXT],
+  [AM_CONDITIONAL([am__EXEEXT], [test -n "$EXEEXT"])])])dnl
+
+# POSIX will say in a future version that running "rm -f" with no argument
+# is OK; and we want to be able to make that assumption in our Makefile
+# recipes.  So use an aggressive probe to check that the usage we want is
+# actually supported "in the wild" to an acceptable degree.
+# See automake bug#10828.
+# To make any issue more visible, cause the running configure to be aborted
+# by default if the 'rm' program in use doesn't match our expectations; the
+# user can still override this though.
+if rm -f && rm -fr && rm -rf; then : OK; else
+  cat >&2 <<'END'
+Oops!
+
+Your 'rm' program seems unable to run without file operands specified
+on the command line, even when the '-f' option is present.  This is contrary
+to the behaviour of most rm programs out there, and not conforming with
+the upcoming POSIX standard: <http://austingroupbugs.net/view.php?id=542>
+
+Please tell bug-automake@gnu.org about your system, including the value
+of your $PATH and any error possibly output before this message.  This
+can help us improve future automake versions.
+
+END
+  if test x"$ACCEPT_INFERIOR_RM_PROGRAM" = x"yes"; then
+    echo 'Configuration will proceed anyway, since you have set the' >&2
+    echo 'ACCEPT_INFERIOR_RM_PROGRAM variable to "yes"' >&2
+    echo >&2
+  else
+    cat >&2 <<'END'
+Aborting the configuration process, to ensure you take notice of the issue.
+
+You can download and install GNU coreutils to get an 'rm' implementation
+that behaves properly: <http://www.gnu.org/software/coreutils/>.
+
+If you want to complete the configuration process using your problematic
+'rm' anyway, export the environment variable ACCEPT_INFERIOR_RM_PROGRAM
+to "yes", and re-run configure.
+
+END
+    AC_MSG_ERROR([Your 'rm' program is bad, sorry.])
+  fi
+fi])
+
+dnl Hook into '_AC_COMPILER_EXEEXT' early to learn its expansion.  Do not
+dnl add the conditional right here, as _AC_COMPILER_EXEEXT may be further
+dnl mangled by Autoconf and run in a shell conditional statement.
+m4_define([_AC_COMPILER_EXEEXT],
+m4_defn([_AC_COMPILER_EXEEXT])[m4_provide([_AM_COMPILER_EXEEXT])])
+
+# When config.status generates a header, we must update the stamp-h file.
+# This file resides in the same directory as the config header
+# that is generated.  The stamp files are numbered to have different names.
+
+# Autoconf calls _AC_AM_CONFIG_HEADER_HOOK (when defined) in the
+# loop where config.status creates the headers, so we can generate
+# our stamp files there.
+AC_DEFUN([_AC_AM_CONFIG_HEADER_HOOK],
+[# Compute $1's index in $config_headers.
+_am_arg=$1
+_am_stamp_count=1
+for _am_header in $config_headers :; do
+  case $_am_header in
+    $_am_arg | $_am_arg:* )
+      break ;;
+    * )
+      _am_stamp_count=`expr $_am_stamp_count + 1` ;;
+  esac
+done
+echo "timestamp for $_am_arg" >`AS_DIRNAME(["$_am_arg"])`/stamp-h[]$_am_stamp_count])
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_PROG_INSTALL_SH
+# ------------------
+# Define $install_sh.
+AC_DEFUN([AM_PROG_INSTALL_SH],
+[AC_REQUIRE([AM_AUX_DIR_EXPAND])dnl
+if test x"${install_sh}" != xset; then
+  case $am_aux_dir in
+  *\ * | *\	*)
+    install_sh="\${SHELL} '$am_aux_dir/install-sh'" ;;
+  *)
+    install_sh="\${SHELL} $am_aux_dir/install-sh"
+  esac
+fi
+AC_SUBST([install_sh])])
+
+# Copyright (C) 2003-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# Check whether the underlying file-system supports filenames
+# with a leading dot.  For instance MS-DOS doesn't.
+AC_DEFUN([AM_SET_LEADING_DOT],
+[rm -rf .tst 2>/dev/null
+mkdir .tst 2>/dev/null
+if test -d .tst; then
+  am__leading_dot=.
+else
+  am__leading_dot=_
+fi
+rmdir .tst 2>/dev/null
+AC_SUBST([am__leading_dot])])
+
+# Add --enable-maintainer-mode option to configure.         -*- Autoconf -*-
+# From Jim Meyering
+
+# Copyright (C) 1996-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_MAINTAINER_MODE([DEFAULT-MODE])
+# ----------------------------------
+# Control maintainer-specific portions of Makefiles.
+# Default is to disable them, unless 'enable' is passed literally.
+# For symmetry, 'disable' may be passed as well.  Anyway, the user
+# can override the default with the --enable/--disable switch.
+AC_DEFUN([AM_MAINTAINER_MODE],
+[m4_case(m4_default([$1], [disable]),
+       [enable], [m4_define([am_maintainer_other], [disable])],
+       [disable], [m4_define([am_maintainer_other], [enable])],
+       [m4_define([am_maintainer_other], [enable])
+        m4_warn([syntax], [unexpected argument to AM@&t@_MAINTAINER_MODE: $1])])
+AC_MSG_CHECKING([whether to enable maintainer-specific portions of Makefiles])
+  dnl maintainer-mode's default is 'disable' unless 'enable' is passed
+  AC_ARG_ENABLE([maintainer-mode],
+    [AS_HELP_STRING([--]am_maintainer_other[-maintainer-mode],
+      am_maintainer_other[ make rules and dependencies not useful
+      (and sometimes confusing) to the casual installer])],
+    [USE_MAINTAINER_MODE=$enableval],
+    [USE_MAINTAINER_MODE=]m4_if(am_maintainer_other, [enable], [no], [yes]))
+  AC_MSG_RESULT([$USE_MAINTAINER_MODE])
+  AM_CONDITIONAL([MAINTAINER_MODE], [test $USE_MAINTAINER_MODE = yes])
+  MAINT=$MAINTAINER_MODE_TRUE
+  AC_SUBST([MAINT])dnl
+]
+)
+
+# Check to see how 'make' treats includes.	            -*- Autoconf -*-
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_MAKE_INCLUDE()
+# -----------------
+# Check to see how make treats includes.
+AC_DEFUN([AM_MAKE_INCLUDE],
+[am_make=${MAKE-make}
+cat > confinc << 'END'
+am__doit:
+	@echo this is the am__doit target
+.PHONY: am__doit
+END
+# If we don't find an include directive, just comment out the code.
+AC_MSG_CHECKING([for style of include used by $am_make])
+am__include="#"
+am__quote=
+_am_result=none
+# First try GNU make style include.
+echo "include confinc" > confmf
+# Ignore all kinds of additional output from 'make'.
+case `$am_make -s -f confmf 2> /dev/null` in #(
+*the\ am__doit\ target*)
+  am__include=include
+  am__quote=
+  _am_result=GNU
+  ;;
+esac
+# Now try BSD make style include.
+if test "$am__include" = "#"; then
+   echo '.include "confinc"' > confmf
+   case `$am_make -s -f confmf 2> /dev/null` in #(
+   *the\ am__doit\ target*)
+     am__include=.include
+     am__quote="\""
+     _am_result=BSD
+     ;;
+   esac
+fi
+AC_SUBST([am__include])
+AC_SUBST([am__quote])
+AC_MSG_RESULT([$_am_result])
+rm -f confinc confmf
+])
+
+# Fake the existence of programs that GNU maintainers use.  -*- Autoconf -*-
+
+# Copyright (C) 1997-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_MISSING_PROG(NAME, PROGRAM)
+# ------------------------------
+AC_DEFUN([AM_MISSING_PROG],
+[AC_REQUIRE([AM_MISSING_HAS_RUN])
+$1=${$1-"${am_missing_run}$2"}
+AC_SUBST($1)])
+
+# AM_MISSING_HAS_RUN
+# ------------------
+# Define MISSING if not defined so far and test if it is modern enough.
+# If it is, set am_missing_run to use it, otherwise, to nothing.
+AC_DEFUN([AM_MISSING_HAS_RUN],
+[AC_REQUIRE([AM_AUX_DIR_EXPAND])dnl
+AC_REQUIRE_AUX_FILE([missing])dnl
+if test x"${MISSING+set}" != xset; then
+  case $am_aux_dir in
+  *\ * | *\	*)
+    MISSING="\${SHELL} \"$am_aux_dir/missing\"" ;;
+  *)
+    MISSING="\${SHELL} $am_aux_dir/missing" ;;
+  esac
+fi
+# Use eval to expand $SHELL
+if eval "$MISSING --is-lightweight"; then
+  am_missing_run="$MISSING "
+else
+  am_missing_run=
+  AC_MSG_WARN(['missing' script is too old or missing])
+fi
+])
+
+#  -*- Autoconf -*-
+# Obsolete and "removed" macros, that must however still report explicit
+# error messages when used, to smooth transition.
+#
+# Copyright (C) 1996-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+AC_DEFUN([AM_CONFIG_HEADER],
+[AC_DIAGNOSE([obsolete],
+['$0': this macro is obsolete.
+You should use the 'AC][_CONFIG_HEADERS' macro instead.])dnl
+AC_CONFIG_HEADERS($@)])
+
+AC_DEFUN([AM_PROG_CC_STDC],
+[AC_PROG_CC
+am_cv_prog_cc_stdc=$ac_cv_prog_cc_stdc
+AC_DIAGNOSE([obsolete],
+['$0': this macro is obsolete.
+You should simply use the 'AC][_PROG_CC' macro instead.
+Also, your code should no longer depend upon 'am_cv_prog_cc_stdc',
+but upon 'ac_cv_prog_cc_stdc'.])])
+
+AC_DEFUN([AM_C_PROTOTYPES],
+         [AC_FATAL([automatic de-ANSI-fication support has been removed])])
+AU_DEFUN([fp_C_PROTOTYPES], [AM_C_PROTOTYPES])
+
+# Helper functions for option handling.                     -*- Autoconf -*-
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# _AM_MANGLE_OPTION(NAME)
+# -----------------------
+AC_DEFUN([_AM_MANGLE_OPTION],
+[[_AM_OPTION_]m4_bpatsubst($1, [[^a-zA-Z0-9_]], [_])])
+
+# _AM_SET_OPTION(NAME)
+# --------------------
+# Set option NAME.  Presently that only means defining a flag for this option.
+AC_DEFUN([_AM_SET_OPTION],
+[m4_define(_AM_MANGLE_OPTION([$1]), [1])])
+
+# _AM_SET_OPTIONS(OPTIONS)
+# ------------------------
+# OPTIONS is a space-separated list of Automake options.
+AC_DEFUN([_AM_SET_OPTIONS],
+[m4_foreach_w([_AM_Option], [$1], [_AM_SET_OPTION(_AM_Option)])])
+
+# _AM_IF_OPTION(OPTION, IF-SET, [IF-NOT-SET])
+# -------------------------------------------
+# Execute IF-SET if OPTION is set, IF-NOT-SET otherwise.
+AC_DEFUN([_AM_IF_OPTION],
+[m4_ifset(_AM_MANGLE_OPTION([$1]), [$2], [$3])])
+
+# Copyright (C) 1999-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# _AM_PROG_CC_C_O
+# ---------------
+# Like AC_PROG_CC_C_O, but changed for automake.  We rewrite AC_PROG_CC
+# to automatically call this.
+AC_DEFUN([_AM_PROG_CC_C_O],
+[AC_REQUIRE([AM_AUX_DIR_EXPAND])dnl
+AC_REQUIRE_AUX_FILE([compile])dnl
+AC_LANG_PUSH([C])dnl
+AC_CACHE_CHECK(
+  [whether $CC understands -c and -o together],
+  [am_cv_prog_cc_c_o],
+  [AC_LANG_CONFTEST([AC_LANG_PROGRAM([])])
+  # Make sure it works both with $CC and with simple cc.
+  # Following AC_PROG_CC_C_O, we do the test twice because some
+  # compilers refuse to overwrite an existing .o file with -o,
+  # though they will create one.
+  am_cv_prog_cc_c_o=yes
+  for am_i in 1 2; do
+    if AM_RUN_LOG([$CC -c conftest.$ac_ext -o conftest2.$ac_objext]) \
+         && test -f conftest2.$ac_objext; then
+      : OK
+    else
+      am_cv_prog_cc_c_o=no
+      break
+    fi
+  done
+  rm -f core conftest*
+  unset am_i])
+if test "$am_cv_prog_cc_c_o" != yes; then
+   # Losing compiler, so override with the script.
+   # FIXME: It is wrong to rewrite CC.
+   # But if we don't then we get into trouble of one sort or another.
+   # A longer-term fix would be to have automake use am__CC in this case,
+   # and then we could set am__CC="\$(top_srcdir)/compile \$(CC)"
+   CC="$am_aux_dir/compile $CC"
+fi
+AC_LANG_POP([C])])
+
+# For backward compatibility.
+AC_DEFUN_ONCE([AM_PROG_CC_C_O], [AC_REQUIRE([AC_PROG_CC])])
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_RUN_LOG(COMMAND)
+# -------------------
+# Run COMMAND, save the exit status in ac_status, and log it.
+# (This has been adapted from Autoconf's _AC_RUN_LOG macro.)
+AC_DEFUN([AM_RUN_LOG],
+[{ echo "$as_me:$LINENO: $1" >&AS_MESSAGE_LOG_FD
+   ($1) >&AS_MESSAGE_LOG_FD 2>&AS_MESSAGE_LOG_FD
+   ac_status=$?
+   echo "$as_me:$LINENO: \$? = $ac_status" >&AS_MESSAGE_LOG_FD
+   (exit $ac_status); }])
+
+# Check to make sure that the build environment is sane.    -*- Autoconf -*-
+
+# Copyright (C) 1996-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_SANITY_CHECK
+# ---------------
+AC_DEFUN([AM_SANITY_CHECK],
+[AC_MSG_CHECKING([whether build environment is sane])
+# Reject unsafe characters in $srcdir or the absolute working directory
+# name.  Accept space and tab only in the latter.
+am_lf='
+'
+case `pwd` in
+  *[[\\\"\#\$\&\'\`$am_lf]]*)
+    AC_MSG_ERROR([unsafe absolute working directory name]);;
+esac
+case $srcdir in
+  *[[\\\"\#\$\&\'\`$am_lf\ \	]]*)
+    AC_MSG_ERROR([unsafe srcdir value: '$srcdir']);;
+esac
+
+# Do 'set' in a subshell so we don't clobber the current shell's
+# arguments.  Must try -L first in case configure is actually a
+# symlink; some systems play weird games with the mod time of symlinks
+# (eg FreeBSD returns the mod time of the symlink's containing
+# directory).
+if (
+   am_has_slept=no
+   for am_try in 1 2; do
+     echo "timestamp, slept: $am_has_slept" > conftest.file
+     set X `ls -Lt "$srcdir/configure" conftest.file 2> /dev/null`
+     if test "$[*]" = "X"; then
+	# -L didn't work.
+	set X `ls -t "$srcdir/configure" conftest.file`
+     fi
+     if test "$[*]" != "X $srcdir/configure conftest.file" \
+	&& test "$[*]" != "X conftest.file $srcdir/configure"; then
+
+	# If neither matched, then we have a broken ls.  This can happen
+	# if, for instance, CONFIG_SHELL is bash and it inherits a
+	# broken ls alias from the environment.  This has actually
+	# happened.  Such a system could not be considered "sane".
+	AC_MSG_ERROR([ls -t appears to fail.  Make sure there is not a broken
+  alias in your environment])
+     fi
+     if test "$[2]" = conftest.file || test $am_try -eq 2; then
+       break
+     fi
+     # Just in case.
+     sleep 1
+     am_has_slept=yes
+   done
+   test "$[2]" = conftest.file
+   )
+then
+   # Ok.
+   :
+else
+   AC_MSG_ERROR([newly created file is older than distributed files!
+Check your system clock])
+fi
+AC_MSG_RESULT([yes])
+# If we didn't sleep, we still need to ensure time stamps of config.status and
+# generated files are strictly newer.
+am_sleep_pid=
+if grep 'slept: no' conftest.file >/dev/null 2>&1; then
+  ( sleep 1 ) &
+  am_sleep_pid=$!
+fi
+AC_CONFIG_COMMANDS_PRE(
+  [AC_MSG_CHECKING([that generated files are newer than configure])
+   if test -n "$am_sleep_pid"; then
+     # Hide warnings about reused PIDs.
+     wait $am_sleep_pid 2>/dev/null
+   fi
+   AC_MSG_RESULT([done])])
+rm -f conftest.file
+])
+
+# Copyright (C) 2009-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_SILENT_RULES([DEFAULT])
+# --------------------------
+# Enable less verbose build rules; with the default set to DEFAULT
+# ("yes" being less verbose, "no" or empty being verbose).
+AC_DEFUN([AM_SILENT_RULES],
+[AC_ARG_ENABLE([silent-rules], [dnl
+AS_HELP_STRING(
+  [--enable-silent-rules],
+  [less verbose build output (undo: "make V=1")])
+AS_HELP_STRING(
+  [--disable-silent-rules],
+  [verbose build output (undo: "make V=0")])dnl
+])
+case $enable_silent_rules in @%:@ (((
+  yes) AM_DEFAULT_VERBOSITY=0;;
+   no) AM_DEFAULT_VERBOSITY=1;;
+    *) AM_DEFAULT_VERBOSITY=m4_if([$1], [yes], [0], [1]);;
+esac
+dnl
+dnl A few 'make' implementations (e.g., NonStop OS and NextStep)
+dnl do not support nested variable expansions.
+dnl See automake bug#9928 and bug#10237.
+am_make=${MAKE-make}
+AC_CACHE_CHECK([whether $am_make supports nested variables],
+   [am_cv_make_support_nested_variables],
+   [if AS_ECHO([['TRUE=$(BAR$(V))
+BAR0=false
+BAR1=true
+V=1
+am__doit:
+	@$(TRUE)
+.PHONY: am__doit']]) | $am_make -f - >/dev/null 2>&1; then
+  am_cv_make_support_nested_variables=yes
+else
+  am_cv_make_support_nested_variables=no
+fi])
+if test $am_cv_make_support_nested_variables = yes; then
+  dnl Using '$V' instead of '$(V)' breaks IRIX make.
+  AM_V='$(V)'
+  AM_DEFAULT_V='$(AM_DEFAULT_VERBOSITY)'
+else
+  AM_V=$AM_DEFAULT_VERBOSITY
+  AM_DEFAULT_V=$AM_DEFAULT_VERBOSITY
+fi
+AC_SUBST([AM_V])dnl
+AM_SUBST_NOTMAKE([AM_V])dnl
+AC_SUBST([AM_DEFAULT_V])dnl
+AM_SUBST_NOTMAKE([AM_DEFAULT_V])dnl
+AC_SUBST([AM_DEFAULT_VERBOSITY])dnl
+AM_BACKSLASH='\'
+AC_SUBST([AM_BACKSLASH])dnl
+_AM_SUBST_NOTMAKE([AM_BACKSLASH])dnl
+])
+
+# Copyright (C) 2001-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# AM_PROG_INSTALL_STRIP
+# ---------------------
+# One issue with vendor 'install' (even GNU) is that you can't
+# specify the program used to strip binaries.  This is especially
+# annoying in cross-compiling environments, where the build's strip
+# is unlikely to handle the host's binaries.
+# Fortunately install-sh will honor a STRIPPROG variable, so we
+# always use install-sh in "make install-strip", and initialize
+# STRIPPROG with the value of the STRIP variable (set by the user).
+AC_DEFUN([AM_PROG_INSTALL_STRIP],
+[AC_REQUIRE([AM_PROG_INSTALL_SH])dnl
+# Installed binaries are usually stripped using 'strip' when the user
+# run "make install-strip".  However 'strip' might not be the right
+# tool to use in cross-compilation environments, therefore Automake
+# will honor the 'STRIP' environment variable to overrule this program.
+dnl Don't test for $cross_compiling = yes, because it might be 'maybe'.
+if test "$cross_compiling" != no; then
+  AC_CHECK_TOOL([STRIP], [strip], :)
+fi
+INSTALL_STRIP_PROGRAM="\$(install_sh) -c -s"
+AC_SUBST([INSTALL_STRIP_PROGRAM])])
+
+# Copyright (C) 2006-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# _AM_SUBST_NOTMAKE(VARIABLE)
+# ---------------------------
+# Prevent Automake from outputting VARIABLE = @VARIABLE@ in Makefile.in.
+# This macro is traced by Automake.
+AC_DEFUN([_AM_SUBST_NOTMAKE])
+
+# AM_SUBST_NOTMAKE(VARIABLE)
+# --------------------------
+# Public sister of _AM_SUBST_NOTMAKE.
+AC_DEFUN([AM_SUBST_NOTMAKE], [_AM_SUBST_NOTMAKE($@)])
+
+# Check how to create a tarball.                            -*- Autoconf -*-
+
+# Copyright (C) 2004-2013 Free Software Foundation, Inc.
+#
+# This file is free software; the Free Software Foundation
+# gives unlimited permission to copy and/or distribute it,
+# with or without modifications, as long as this notice is preserved.
+
+# _AM_PROG_TAR(FORMAT)
+# --------------------
+# Check how to create a tarball in format FORMAT.
+# FORMAT should be one of 'v7', 'ustar', or 'pax'.
+#
+# Substitute a variable $(am__tar) that is a command
+# writing to stdout a FORMAT-tarball containing the directory
+# $tardir.
+#     tardir=directory && $(am__tar) > result.tar
+#
+# Substitute a variable $(am__untar) that extract such
+# a tarball read from stdin.
+#     $(am__untar) < result.tar
+#
+AC_DEFUN([_AM_PROG_TAR],
+[# Always define AMTAR for backward compatibility.  Yes, it's still used
+# in the wild :-(  We should find a proper way to deprecate it ...
+AC_SUBST([AMTAR], ['$${TAR-tar}'])
+
+# We'll loop over all known methods to create a tar archive until one works.
+_am_tools='gnutar m4_if([$1], [ustar], [plaintar]) pax cpio none'
+
+m4_if([$1], [v7],
+  [am__tar='$${TAR-tar} chof - "$$tardir"' am__untar='$${TAR-tar} xf -'],
+
+  [m4_case([$1],
+    [ustar],
+     [# The POSIX 1988 'ustar' format is defined with fixed-size fields.
+      # There is notably a 21 bits limit for the UID and the GID.  In fact,
+      # the 'pax' utility can hang on bigger UID/GID (see automake bug#8343
+      # and bug#13588).
+      am_max_uid=2097151 # 2^21 - 1
+      am_max_gid=$am_max_uid
+      # The $UID and $GID variables are not portable, so we need to resort
+      # to the POSIX-mandated id(1) utility.  Errors in the 'id' calls
+      # below are definitely unexpected, so allow the users to see them
+      # (that is, avoid stderr redirection).
+      am_uid=`id -u || echo unknown`
+      am_gid=`id -g || echo unknown`
+      AC_MSG_CHECKING([whether UID '$am_uid' is supported by ustar format])
+      if test $am_uid -le $am_max_uid; then
+         AC_MSG_RESULT([yes])
+      else
+         AC_MSG_RESULT([no])
+         _am_tools=none
+      fi
+      AC_MSG_CHECKING([whether GID '$am_gid' is supported by ustar format])
+      if test $am_gid -le $am_max_gid; then
+         AC_MSG_RESULT([yes])
+      else
+        AC_MSG_RESULT([no])
+        _am_tools=none
+      fi],
+
+  [pax],
+    [],
+
+  [m4_fatal([Unknown tar format])])
+
+  AC_MSG_CHECKING([how to create a $1 tar archive])
+
+  # Go ahead even if we have the value already cached.  We do so because we
+  # need to set the values for the 'am__tar' and 'am__untar' variables.
+  _am_tools=${am_cv_prog_tar_$1-$_am_tools}
+
+  for _am_tool in $_am_tools; do
+    case $_am_tool in
+    gnutar)
+      for _am_tar in tar gnutar gtar; do
+        AM_RUN_LOG([$_am_tar --version]) && break
+      done
+      am__tar="$_am_tar --format=m4_if([$1], [pax], [posix], [$1]) -chf - "'"$$tardir"'
+      am__tar_="$_am_tar --format=m4_if([$1], [pax], [posix], [$1]) -chf - "'"$tardir"'
+      am__untar="$_am_tar -xf -"
+      ;;
+    plaintar)
+      # Must skip GNU tar: if it does not support --format= it doesn't create
+      # ustar tarball either.
+      (tar --version) >/dev/null 2>&1 && continue
+      am__tar='tar chf - "$$tardir"'
+      am__tar_='tar chf - "$tardir"'
+      am__untar='tar xf -'
+      ;;
+    pax)
+      am__tar='pax -L -x $1 -w "$$tardir"'
+      am__tar_='pax -L -x $1 -w "$tardir"'
+      am__untar='pax -r'
+      ;;
+    cpio)
+      am__tar='find "$$tardir" -print | cpio -o -H $1 -L'
+      am__tar_='find "$tardir" -print | cpio -o -H $1 -L'
+      am__untar='cpio -i -H $1 -d'
+      ;;
+    none)
+      am__tar=false
+      am__tar_=false
+      am__untar=false
+      ;;
+    esac
+
+    # If the value was cached, stop now.  We just wanted to have am__tar
+    # and am__untar set.
+    test -n "${am_cv_prog_tar_$1}" && break
+
+    # tar/untar a dummy directory, and stop if the command works.
+    rm -rf conftest.dir
+    mkdir conftest.dir
+    echo GrepMe > conftest.dir/file
+    AM_RUN_LOG([tardir=conftest.dir && eval $am__tar_ >conftest.tar])
+    rm -rf conftest.dir
+    if test -s conftest.tar; then
+      AM_RUN_LOG([$am__untar <conftest.tar])
+      AM_RUN_LOG([cat conftest.dir/file])
+      grep GrepMe conftest.dir/file >/dev/null 2>&1 && break
+    fi
+  done
+  rm -rf conftest.dir
+
+  AC_CACHE_VAL([am_cv_prog_tar_$1], [am_cv_prog_tar_$1=$_am_tool])
+  AC_MSG_RESULT([$am_cv_prog_tar_$1])])
+
+AC_SUBST([am__tar])
+AC_SUBST([am__untar])
+]) # _AM_PROG_TAR
+
+m4_include([m4/gc_set_version.m4])
+m4_include([m4/libtool.m4])
+m4_include([m4/ltoptions.m4])
+m4_include([m4/ltsugar.m4])
+m4_include([m4/ltversion.m4])
+m4_include([m4/lt~obsolete.m4])
diff --git a/src/gc/bdwgc/unused/bdw-gc.pc.in b/src/gc/bdwgc/unused/bdw-gc.pc.in
new file mode 100644
index 0000000..ef4c234
--- /dev/null
+++ b/src/gc/bdwgc/unused/bdw-gc.pc.in
@@ -0,0 +1,10 @@
+prefix=@prefix@
+exec_prefix=@exec_prefix@
+libdir=@libdir@
+includedir=@includedir@
+
+Name: Boehm-Demers-Weiser Conservative Garbage Collector
+Description: A garbage collector for C and C++
+Version: @PACKAGE_VERSION@
+Libs: -L${libdir} -lgc
+Cflags: -I${includedir}
diff --git a/src/gc/bdwgc/unused/configure.ac b/src/gc/bdwgc/unused/configure.ac
new file mode 100644
index 0000000..1ba1be7
--- /dev/null
+++ b/src/gc/bdwgc/unused/configure.ac
@@ -0,0 +1,817 @@
+# Copyright (c) 1999-2001 by Red Hat, Inc. All rights reserved.
+#
+# THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+# OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+#
+# Permission is hereby granted to use or copy this program
+# for any purpose,  provided the above notices are retained on all copies.
+# Permission to modify the code and to distribute modified code is granted,
+# provided the above notices are retained, and a notice that the code was
+# modified is included with the above copyright notice.
+#
+# Original author: Tom Tromey
+# Modified by: Grzegorz Jakacki <jakacki at acm dot org>
+
+dnl Process this file with autoconf to produce configure.
+
+# Initialization
+# ==============
+
+AC_INIT(gc,7.2,bdwgc@lists.opendylan.org)
+    ## version must conform to [0-9]+[.][0-9]+(alpha[0-9]+)?
+AC_CONFIG_SRCDIR(gcj_mlc.c)
+AC_CONFIG_MACRO_DIR([m4])
+AC_CANONICAL_TARGET
+AC_PREREQ(2.64)
+AC_REVISION($Revision: 1.69 $)
+GC_SET_VERSION
+AM_INIT_AUTOMAKE([foreign dist-bzip2 nostdinc subdir-objects])
+AM_CONFIG_HEADER([include/private/config.h])
+AM_MAINTAINER_MODE
+
+AC_SUBST(PACKAGE)
+AC_SUBST(GC_VERSION)
+
+AM_PROG_CC_C_O
+AC_PROG_CXX
+
+AM_PROG_AS
+
+AC_PROG_INSTALL
+
+. ${srcdir}/configure.host
+
+GC_CFLAGS=${gc_cflags}
+AC_SUBST(GC_CFLAGS)
+
+AC_ARG_ENABLE(threads,
+  [AC_HELP_STRING([--enable-threads=TYPE], [choose threading package])],
+  THREADS=$enableval,
+  [ AC_MSG_CHECKING([for thread model used by GCC])
+    THREADS=`$CC -v 2>&1 | sed -n 's/^Thread model: //p'`
+    if test -z "$THREADS"; then
+      THREADS=no
+    fi
+    if test "$THREADS" = "posix"; then
+      case "$host" in
+        *-*-mingw*)
+          # Adjust thread model if cross-compiling for MinGW.
+          THREADS=win32
+          ;;
+      esac
+    fi
+    AC_MSG_RESULT([$THREADS]) ])
+
+AC_ARG_ENABLE(parallel-mark,
+   [AC_HELP_STRING([--enable-parallel-mark],
+        [parallelize marking and free list construction])],
+   [case "$THREADS" in
+      no | none | single)
+        AC_MSG_ERROR([Parallel mark requires --enable-threads=x spec])
+        ;;
+    esac ]
+)
+
+AC_ARG_ENABLE(cplusplus,
+    [AC_HELP_STRING([--enable-cplusplus], [install C++ support])])
+
+dnl Features which may be selected in the following thread-detection switch.
+AH_TEMPLATE([PARALLEL_MARK], [Define to enable parallel marking.])
+AH_TEMPLATE([THREAD_LOCAL_ALLOC],
+            [Define to enable thread-local allocation optimization.])
+AH_TEMPLATE([USE_COMPILER_TLS],
+            [Define to use of compiler-support for thread-local variables.])
+
+dnl Thread selection macros.
+AH_TEMPLATE([GC_THREADS],           [Define to support platform-specific \
+                                     threads.])
+AH_TEMPLATE([GC_AIX_THREADS],       [Define to support IBM AIX threads.])
+AH_TEMPLATE([GC_DARWIN_THREADS],    [Define to support Darwin pthreads.])
+AH_TEMPLATE([GC_FREEBSD_THREADS],   [Define to support FreeBSD pthreads.])
+AH_TEMPLATE([GC_GNU_THREADS],       [Define to support GNU pthreads.])
+AH_TEMPLATE([GC_HPUX_THREADS],      [Define to support HP/UX 11 pthreads.])
+AH_TEMPLATE([GC_IRIX_THREADS],      [Define to support Irix pthreads.])
+AH_TEMPLATE([GC_LINUX_THREADS],     [Define to support pthreads on Linux.])
+AH_TEMPLATE([GC_NETBSD_THREADS],    [Define to support NetBSD pthreads.])
+AH_TEMPLATE([GC_OPENBSD_THREADS],   [Define to support OpenBSD pthreads.])
+AH_TEMPLATE([GC_OSF1_THREADS],      [Define to support Tru64 pthreads.])
+AH_TEMPLATE([GC_SOLARIS_THREADS],   [Define to support Solaris pthreads.])
+AH_TEMPLATE([GC_WIN32_THREADS],     [Define to support Win32 threads.])
+AH_TEMPLATE([GC_WIN32_PTHREADS],    [Define to support win32-pthreads.])
+AH_TEMPLATE([GC_RTEMS_PTHREADS],    [Define to support rtems-pthreads.])
+
+dnl System header feature requests.
+AH_TEMPLATE([_POSIX_C_SOURCE], [The POSIX feature macro.])
+AH_TEMPLATE([_PTHREADS], [Indicates the use of pthreads (NetBSD).])
+
+dnl GC API symbols export control.
+AH_TEMPLATE([GC_DLL],
+        [Define to build dynamic libraries with only API symbols exposed.])
+
+dnl Check for a flavor of supported inline keyword.
+AC_C_INLINE
+
+THREADDLLIBS=
+need_atomic_ops_asm=false
+## Libraries needed to support dynamic loading and/or threads.
+case "$THREADS" in
+ no | none | single)
+    THREADS=none
+    ;;
+ posix | pthreads)
+    THREADS=posix
+    AC_CHECK_LIB(pthread, pthread_self, THREADDLLIBS="-lpthread",,)
+    case "$host" in
+     x86-*-linux* | ia64-*-linux* | i586-*-linux* | i686-*-linux* \
+     | x86_64-*-linux* | alpha-*-linux* | sparc*-*-linux*)
+        AC_DEFINE(GC_LINUX_THREADS)
+        AC_DEFINE(_REENTRANT)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        AC_MSG_WARN("Explicit GC_INIT() calls may be required.");
+        ;;
+     *-*-linux*)
+        AC_DEFINE(GC_LINUX_THREADS)
+        AC_DEFINE(_REENTRANT)
+        ;;
+     *-*-aix*)
+        AC_DEFINE(GC_AIX_THREADS)
+        AC_DEFINE(_REENTRANT)
+        ;;
+     *-*-hpux11*)
+        AC_MSG_WARN("Only HP/UX 11 POSIX threads are supported.")
+        AC_DEFINE(GC_HPUX_THREADS)
+        AC_DEFINE(_POSIX_C_SOURCE,199506L)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        AC_MSG_WARN("Explicit GC_INIT() calls may be required.");
+        THREADDLLIBS="-lpthread -lrt"
+        # HPUX needs REENTRANT for the _r calls.
+        AC_DEFINE(_REENTRANT, 1, [Required define if using POSIX threads.])
+        ;;
+     *-*-hpux10*)
+        AC_MSG_WARN("Only HP-UX 11 POSIX threads are supported.")
+        ;;
+     *-*-openbsd*)
+        AC_DEFINE(GC_OPENBSD_THREADS)
+        THREADDLLIBS=-pthread
+        AM_CFLAGS="$AM_CFLAGS -pthread"
+        openbsd_threads=true
+        ;;
+     *-*-freebsd*)
+        AC_MSG_WARN("FreeBSD does not yet fully support threads with Boehm GC.")
+        AC_DEFINE(GC_FREEBSD_THREADS)
+        AM_CFLAGS="$AM_CFLAGS -pthread"
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        ;;
+     *-*-kfreebsd*-gnu)
+        AC_DEFINE(GC_FREEBSD_THREADS)
+        AM_CFLAGS="$AM_CFLAGS -pthread"
+        THREADDLLIBS=-pthread
+        AC_DEFINE(_REENTRANT)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        AC_DEFINE(USE_COMPILER_TLS)
+        ;;
+     *-*-gnu*)
+        AC_DEFINE(GC_GNU_THREADS)
+        AC_DEFINE(_REENTRANT)
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        ;;
+     *-*-netbsd*)
+        AC_MSG_WARN("Only on NetBSD 2.0 or later.")
+        AC_DEFINE(GC_NETBSD_THREADS)
+        AC_DEFINE(_REENTRANT)
+        AC_DEFINE(_PTHREADS)
+        THREADDLLIBS="-lpthread -lrt"
+        ;;
+     *-*-solaris*)
+        AC_DEFINE(GC_SOLARIS_THREADS)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        # Need to use alternate thread library, otherwise gctest hangs
+        # on Solaris 8.
+        multi_os_directory=`$CC -print-multi-os-directory`
+        THREADDLLIBS="-L/usr/lib/lwp/$multi_os_directory \
+                      -R/usr/lib/lwp/$multi_os_directory -lpthread -lrt"
+        ;;
+     *-*-irix*)
+        AC_DEFINE(GC_IRIX_THREADS)
+        ;;
+     *-*-cygwin*)
+        AC_DEFINE(GC_WIN32_THREADS)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        # Cygwin doesn't have a real libpthread, so Libtool can't link
+        # against it.
+        THREADDLLIBS=""
+        win32_threads=true
+        ;;
+     *-*-mingw*)
+        AC_DEFINE(GC_WIN32_PTHREADS)
+        # Using win32-pthreads
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        THREADDLLIBS="-lpthread"
+        win32_threads=true
+        ;;
+     *-*-darwin*)
+        AC_DEFINE(GC_DARWIN_THREADS)
+        AC_MSG_WARN("Explicit GC_INIT() calls may be required.");
+        # Parallel-mark is not well-tested on Darwin
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+        fi
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+        darwin_threads=true
+        ;;
+     *-*-osf*)
+        AC_DEFINE(GC_OSF1_THREADS)
+        if test "${enable_parallel_mark}" = yes; then
+          AC_DEFINE(PARALLEL_MARK)
+          AC_DEFINE(THREAD_LOCAL_ALLOC)
+          AC_MSG_WARN("Explicit GC_INIT() calls may be required.");
+          # May want to enable it in other cases, too.
+          # Measurements have not yet been done.
+        fi
+        AM_CFLAGS="$AM_CFLAGS -pthread"
+        THREADDLLIBS="-lpthread -lrt"
+        ;;
+      *)
+        AC_MSG_ERROR("Pthreads not supported by the GC on this platform.")
+        ;;
+    esac
+    case "$host" in
+      sparc*-*-solaris*)
+        if test "$GCC" != yes; then
+          need_atomic_ops_asm=true
+        fi
+        ;;
+    esac
+    ;;
+ win32)
+    AC_DEFINE(GC_WIN32_THREADS)
+    if test "${enable_parallel_mark}" = yes; then
+      AC_DEFINE(PARALLEL_MARK)
+      AC_DEFINE(THREAD_LOCAL_ALLOC)
+    else
+      if test "${enable_shared}" != yes || test "${enable_static}" != no; then
+        # Imply THREAD_LOCAL_ALLOC unless GC_DLL.
+        AC_DEFINE(THREAD_LOCAL_ALLOC)
+      fi
+    fi
+    win32_threads=true
+    AC_DEFINE([EMPTY_GETENV_RESULTS], [1],
+              [Wine getenv may not return NULL for missing entry.])
+    ;;
+ dgux386)
+    THREADS=dgux386
+    AC_MSG_RESULT($THREADDLLIBS)
+    # Use pthread GCC switch
+    THREADDLLIBS=-pthread
+    if test "${enable_parallel_mark}" = yes; then
+        AC_DEFINE(PARALLEL_MARK)
+    fi
+    AC_DEFINE(THREAD_LOCAL_ALLOC)
+    AC_MSG_WARN("Explicit GC_INIT() calls may be required.");
+    AC_DEFINE([GC_DGUX386_THREADS], 1,
+              [Define to enable support for DB/UX threads on i386.])
+    AC_DEFINE([DGUX_THREADS], 1,
+              [Define to enable support for DB/UX threads.])
+    # Enable _POSIX4A_DRAFT10_SOURCE with flag -pthread
+    AM_CFLAGS="-pthread $AM_CFLAGS"
+    ;;
+ aix)
+    THREADS=posix
+    THREADDLLIBS=-lpthread
+    AC_DEFINE(GC_AIX_THREADS)
+    AC_DEFINE(_REENTRANT)
+    ;;
+ rtems)
+    THREADS=posix
+    AC_DEFINE(GC_RTEMS_PTHREADS)
+    AC_DEFINE(THREAD_LOCAL_ALLOC)
+    ;;
+ decosf1 | irix | mach | os2 | solaris | dce | vxworks)
+    AC_MSG_ERROR(thread package $THREADS not yet supported)
+    ;;
+ *)
+    AC_MSG_ERROR($THREADS is an unknown thread package)
+    ;;
+esac
+AC_SUBST(THREADDLLIBS)
+AM_CONDITIONAL(THREADS, test x$THREADS != xnone)
+AM_CONDITIONAL(PTHREADS, test x$THREADS = xposix)
+AM_CONDITIONAL(DARWIN_THREADS, test x$darwin_threads = xtrue)
+AM_CONDITIONAL(WIN32_THREADS, test x$win32_threads = xtrue)
+AM_CONDITIONAL(OPENBSD_THREADS, test x$openbsd_threads = xtrue)
+
+compiler_suncc=no
+case "$host" in
+   powerpc-*-darwin*)
+      powerpc_darwin=true
+      ;;
+   *-*-solaris*)
+      if test "$GCC" != yes; then
+        # Solaris SunCC
+        compiler_suncc=yes
+        CFLAGS="-O $CFLAGS"
+      fi
+      ;;
+esac
+
+AC_MSG_CHECKING(for xlc)
+AC_TRY_COMPILE([],[
+ #ifndef __xlC__
+ # error
+ #endif
+], [compiler_xlc=yes], [compiler_xlc=no])
+AC_MSG_RESULT($compiler_xlc)
+if test $compiler_xlc = yes -a "$powerpc_darwin" = true; then
+  # the darwin stack-frame-walking code is completely broken on xlc
+  AC_DEFINE([DARWIN_DONT_PARSE_STACK], 1, [See doc/README.macros.])
+fi
+
+# XLC neither requires nor tolerates the unnecessary assembler goop.
+# Similar for the Sun C compiler.
+AM_CONDITIONAL([ASM_WITH_CPP_UNSUPPORTED],
+    [test $compiler_xlc = yes -o $compiler_suncc = yes])
+
+if test "$GCC" = yes; then
+  # Disable aliasing optimization unless forced to.
+  AC_MSG_CHECKING([whether gcc supports -fno-strict-aliasing])
+  ac_cv_fno_strict_aliasing=no
+  for cflag in $CFLAGS; do
+    case "$cflag" in
+      -fstrict-aliasing)
+        # Opposite option already present
+        ac_cv_fno_strict_aliasing=skipped
+        break
+        ;;
+    esac
+  done
+  if test "$ac_cv_fno_strict_aliasing" != skipped; then
+    old_CFLAGS="$CFLAGS"
+    CFLAGS="$CFLAGS -fno-strict-aliasing"
+    AC_TRY_COMPILE([],[], [ac_cv_fno_strict_aliasing=yes], [])
+    CFLAGS="$old_CFLAGS"
+    AS_IF([test "$ac_cv_fno_strict_aliasing" = yes],
+          [CFLAGS="$CFLAGS -fno-strict-aliasing"], [])
+  fi
+  AC_MSG_RESULT($ac_cv_fno_strict_aliasing)
+fi
+
+case "$host" in
+# While IRIX 6 has libdl for the O32 and N32 ABIs, it's missing for N64
+# and unnecessary everywhere.
+  mips-sgi-irix6*) ;;
+# We never want libdl on darwin. It is a fake libdl that just ends up making
+# dyld calls anyway.  The same applies to Cygwin.
+  *-*-darwin*) ;;
+  *-*-cygwin*) ;;
+  *)
+    AC_CHECK_LIB(dl, dlopen, THREADDLLIBS="$THREADDLLIBS -ldl")
+    ;;
+esac
+
+case "$host" in
+  *-*-hpux*)
+    avoid_cpp_lib=yes;;
+  *)
+    avoid_cpp_lib=no;
+    ;;
+esac
+AM_CONDITIONAL(AVOID_CPP_LIB,test $avoid_cpp_lib = yes)
+
+# extra LD Flags which are required for targets
+case "${host}" in
+  *-*-darwin*)
+    extra_ldflags_libgc=-Wl,-single_module
+    ;;
+esac
+AC_SUBST(extra_ldflags_libgc)
+
+AC_SUBST(EXTRA_TEST_LIBS)
+
+target_all=libgc.la
+AC_SUBST(target_all)
+
+dnl If the target is an eCos system, use the appropriate eCos
+dnl I/O routines.
+dnl FIXME: this should not be a local option but a global target
+dnl system; at present there is no eCos target.
+TARGET_ECOS="no"
+AC_ARG_WITH(ecos,
+[  --with-ecos             enable runtime eCos target support],
+TARGET_ECOS="$with_ecos"
+)
+
+addobjs=
+addlibs=
+
+case "$TARGET_ECOS" in
+   no)
+      ;;
+   *)
+      AC_DEFINE([ECOS], 1, [Define to enable eCos target support.])
+      AM_CPPFLAGS="-I${TARGET_ECOS}/include $AM_CPPFLAGS"
+      addobjs="$addobjs ecos.lo"
+      ;;
+esac
+
+AM_CONDITIONAL(CPLUSPLUS, test "${enable_cplusplus}" = yes)
+
+AC_SUBST(CXX)
+AC_SUBST(AM_CFLAGS)
+AC_SUBST(AM_CPPFLAGS)
+
+# Configuration of shared libraries
+#
+AC_MSG_CHECKING(whether to build shared libraries)
+AC_ENABLE_SHARED
+
+case "$host" in
+ alpha-*-openbsd*)
+     enable_shared=no
+     ;;
+ *)
+     ;;
+esac
+
+AC_MSG_RESULT($enable_shared)
+
+# Compile with GC_DLL defined unless building static libraries.
+if test "${enable_shared}" = yes; then
+  if test "${enable_static}" = no; then
+    AC_DEFINE(GC_DLL)
+    # FIXME: Also pass -fvisibility=hidden option if GCC v4+ and not Win32.
+  fi
+fi
+
+# Configuration of machine-dependent code
+#
+AC_MSG_CHECKING(which machine-dependent code should be used)
+machdep=
+case "$host" in
+ alpha-*-openbsd*)
+    machdep="mach_dep.lo"
+    if test x"${ac_cv_lib_dl_dlopen}" != xyes ; then
+       AC_MSG_WARN(OpenBSD/Alpha without dlopen(). Shared library support is disabled)
+    fi
+    ;;
+ alpha*-*-linux*)
+    machdep="mach_dep.lo"
+    ;;
+ i?86-*-solaris2.[[89]])
+    # PROC_VDB appears to work in 2.8 and 2.9 but not in 2.10+ (for now).
+    AC_DEFINE([SOLARIS25_PROC_VDB_BUG_FIXED], 1,
+              [See the comment in gcconfig.h.])
+    ;;
+ mipstx39-*-elf*)
+    machdep="mach_dep.lo"
+    ;;
+ mips-dec-ultrix*)
+    machdep="mach-dep.lo"
+    ;;
+ mips-nec-sysv*|mips-unknown-sysv*)
+    ;;
+ mips*-*-linux*)
+    ;;
+ mips-*-*)
+    machdep="mach_dep.lo"
+    dnl AC_DEFINE(NO_EXECUTE_PERMISSION)
+    dnl This is now redundant, but it is also important for incremental GC
+    dnl performance under Irix.
+    ;;
+ sparc*-*-linux*)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    ;;
+ sparc-*-netbsd*)
+    machdep="mach_dep.lo sparc_netbsd_mach_dep.lo"
+    ;;
+ sparc64-*-netbsd*)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    ;;
+ sparc*-*-openbsd*)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    ;;
+ sparc64-*-freebsd*)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    ;;
+ sparc-sun-solaris2.3)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    AC_DEFINE(SUNOS53_SHARED_LIB, 1,
+              [Define to work around a Solaris 5.3 bug (see dyn_load.c).])
+    ;;
+ sparc*-sun-solaris2*)
+    machdep="mach_dep.lo sparc_mach_dep.lo"
+    ;;
+ ia64-*-*)
+    machdep="mach_dep.lo ia64_save_regs_in_stack.lo"
+    ;;
+esac
+if test x"$machdep" = x; then
+AC_MSG_RESULT($machdep)
+   machdep="mach_dep.lo"
+fi
+addobjs="$addobjs $machdep"
+AC_SUBST(addobjs)
+AC_SUBST(addlibs)
+
+AC_PROG_LIBTOOL
+
+dnl We use these options to decide which functions to include.
+AC_ARG_WITH(target-subdir,
+[  --with-target-subdir=SUBDIR
+                          configuring with a cross compiler])
+AC_ARG_WITH(cross-host,
+[  --with-cross-host=HOST  configuring with a cross compiler])
+
+# automake wants to see AC_EXEEXT.  But we don't need it.  And having
+# it is actually a problem, because the compiler we're passed can't
+# necessarily do a full link.  So we fool automake here.
+if false; then
+  # autoconf 2.50 runs AC_EXEEXT by default, and the macro expands
+  # to nothing, so nothing would remain between `then' and `fi' if it
+  # were not for the `:' below.
+  :
+  AC_EXEEXT
+fi
+
+dnl As of 4.13a2, the collector will not properly work on Solaris when
+dnl built with gcc and -O.  So we remove -O in the appropriate case.
+dnl Not needed anymore on Solaris.
+AC_MSG_CHECKING(whether Solaris gcc optimization fix is necessary)
+case "$host" in
+ *aix*)
+    if test "$GCC" = yes; then
+       AC_MSG_RESULT(yes)
+       new_CFLAGS=
+       for i in $CFLAGS; do
+          case "$i" in
+           -O*)
+              ;;
+           *)
+              new_CFLAGS="$new_CFLAGS $i"
+              ;;
+          esac
+       done
+       CFLAGS="$new_CFLAGS"
+    else
+       AC_MSG_RESULT(no)
+    fi
+    ;;
+ *) AC_MSG_RESULT(no) ;;
+esac
+
+dnl Include defines that have become de facto standard.
+dnl ALL_INTERIOR_POINTERS and NO_EXECUTE_PERMISSION can be overridden in the startup code.
+AC_DEFINE([NO_EXECUTE_PERMISSION], [1],
+          [Define to make the collector not allocate executable memory by default.])
+AC_DEFINE([ALL_INTERIOR_POINTERS], [1],
+          [Define to recognise all pointers to the interior of objects.])
+
+
+dnl Interface Selection
+dnl -------------------
+dnl
+dnl By default, make the library as general as possible.
+dnl enable_gcj_support=no
+AC_ARG_ENABLE(gcj-support,
+    [AC_HELP_STRING([--disable-gcj-support],
+        [Disable support for gcj.])])
+AM_CONDITIONAL(ENABLE_GCJ_SUPPORT,
+    [test x"$enable_gcj_support" != xno])
+if test x"$enable_gcj_support" != xno; then
+    AC_DEFINE(GC_GCJ_SUPPORT, 1, [Define to include support for gcj.])
+fi
+
+dnl Debugging
+dnl ---------
+
+AH_TEMPLATE([GC_HAVE_BUILTIN_BACKTRACE],
+            [Define if backtrace information is supported.])
+AH_TEMPLATE([MAKE_BACK_GRAPH], [See doc/README.macros.])
+AH_TEMPLATE([SAVE_CALL_COUNT],
+            [The number of caller frames saved when allocating with the
+             debugging API.])
+UNWINDLIBS=
+AC_ARG_ENABLE(gc-debug,
+[AC_HELP_STRING([--enable-gc-debug],
+    [include full support for pointer backtracing etc.])],
+[ if test "$enable_gc_debug" = "yes"; then
+    AC_MSG_WARN("Should define GC_DEBUG and use debug alloc. in clients.")
+    AC_DEFINE([KEEP_BACK_PTRS], 1,
+              [Define to save back-pointers in debugging headers.])
+    keep_back_ptrs=true
+    AC_DEFINE([DBG_HDRS_ALL], 1,
+              [Define to force debug headers on all objects.])
+    case $host in
+      ia64-*-linux* )
+        AC_DEFINE(MAKE_BACK_GRAPH)
+        AC_DEFINE(SAVE_CALL_COUNT, 8)
+        AC_CHECK_LIB(unwind, backtrace, [
+          AC_DEFINE(GC_HAVE_BUILTIN_BACKTRACE)
+          UNWINDLIBS=-lunwind
+          AC_MSG_WARN("Client code may need to link against libunwind.")
+        ])
+      ;;
+      x86-*-linux* | i586-*-linux* | i686-*-linux* | x86_64-*-linux* )
+        AC_DEFINE(MAKE_BACK_GRAPH)
+        AC_MSG_WARN("Client must not use -fomit-frame-pointer.")
+        AC_DEFINE(SAVE_CALL_COUNT, 8)
+      ;;
+      i[3456]86-*-dgux*)
+        AC_DEFINE(MAKE_BACK_GRAPH)
+      ;;
+    esac ]
+  fi)
+AM_CONDITIONAL([KEEP_BACK_PTRS], [test x"$keep_back_ptrs" = xtrue])
+
+# Check for AViiON Machines running DGUX
+ac_is_dgux=no
+AC_CHECK_HEADER(sys/dg_sys_info.h,
+[ac_is_dgux=yes;])
+
+    ## :GOTCHA: we do not check anything but sys/dg_sys_info.h
+if test $ac_is_dgux = yes; then
+    dgux_spec_opts="-DDGUX -D_DGUX_SOURCE -Di386 -mno-legend -O2"
+    CFLAGS="$dgux_spec_opts $CFLAGS"
+    CXXFLAGS="$dgux_spec_opts $CXXFLAGS"
+    if test "$enable_gc_debug" = "yes"; then
+      CFLAGS="-g -mstandard $CFLAGS"
+      CXXFLAGS="-g -mstandard $CXXFLAGS"
+    fi
+    AC_SUBST(CFLAGS)
+    AC_SUBST(CXXFLAGS)
+fi
+
+AC_ARG_ENABLE(java-finalization,
+    [AC_HELP_STRING([--disable-java-finalization],
+        [Disable support for java finalization.])])
+if test x"$enable_java_finalization" != xno; then
+    AC_DEFINE([JAVA_FINALIZATION], 1, [See doc/README.macros.])
+fi
+
+AC_ARG_ENABLE(atomic-uncollectable,
+    [AC_HELP_STRING([--disable-atomic-uncollectible],
+        [Disable support for atomic uncollectible allocation.])])
+if test x"$enable_atomic_uncollectible" != x"no"; then
+    AC_DEFINE(ATOMIC_UNCOLLECTABLE, 1,
+        [Define to enable atomic uncollectible allocation.])
+fi
+
+AC_ARG_ENABLE(redirect-malloc,
+    [AC_HELP_STRING([--enable-redirect-malloc],
+        [Redirect malloc and friends to GC routines])])
+
+if test "${enable_redirect_malloc}" = yes; then
+    if test "${enable_gc_debug}" = yes; then
+        AC_DEFINE([REDIRECT_MALLOC], GC_debug_malloc_replacement,
+                  [If defined, redirect malloc to this function.])
+        AC_DEFINE([REDIRECT_REALLOC], GC_debug_realloc_replacement,
+                  [If defined, redirect GC_realloc to this function.])
+        AC_DEFINE([REDIRECT_FREE], GC_debug_free,
+                  [If defined, redirect free to this function.])
+    else
+        AC_DEFINE(REDIRECT_MALLOC, GC_malloc)
+    fi
+    AC_DEFINE([GC_USE_DLOPEN_WRAP], 1, [See doc/README.macros.])
+fi
+
+AC_ARG_ENABLE(large-config,
+    [AC_HELP_STRING([--enable-large-config],
+        [Optimize for large (> 100 MB) heap or root set])])
+
+if test "${enable_large_config}" = yes; then
+    AC_DEFINE(LARGE_CONFIG, 1, [Define to optimize for large heaps or root sets.])
+fi
+
+AC_ARG_ENABLE(handle-fork,
+    [AC_HELP_STRING([--enable-handle-fork],
+        [Attempt to ensure a usable collector after fork() in multi-threaded
+         programs.])])
+
+if test "${enable_handle_fork}" = yes; then
+    AC_DEFINE(HANDLE_FORK, 1,
+              [Define to install pthread_atfork() handlers by default.])
+elif test "${enable_handle_fork}" = no; then
+    AC_DEFINE(NO_HANDLE_FORK, 1,
+              [Prohibit installation of pthread_atfork() handlers.])
+fi
+
+dnl This is something of a hack.  When cross-compiling we turn off
+dnl some functionality.  We also enable the "small" configuration.
+dnl These is only correct when targetting an embedded system.  FIXME.
+if test -n "${with_cross_host}"; then
+   AC_DEFINE([NO_CLOCK], 1, [Define to not use system clock (cross compiling).])
+   AC_DEFINE([SMALL_CONFIG], 1,
+             [Define to tune the collector for small heap sizes.])
+fi
+
+if test "$enable_gc_debug" = "no"; then
+   AC_DEFINE([NO_DEBUGGING], 1,
+             [Disable debugging, like GC_dump and its callees.])
+fi
+
+AC_SUBST(UNWINDLIBS)
+
+AC_ARG_ENABLE(gc-assertions,
+    [AC_HELP_STRING([--enable-gc-assertions],
+        [collector-internal assertion checking])])
+if test "${enable_gc_assertions}" = yes; then
+    AC_DEFINE([GC_ASSERTIONS], 1, [Define to enable internal debug assertions.])
+fi
+
+AC_ARG_ENABLE(munmap,
+    [AC_HELP_STRING([--enable-munmap=N],
+        [return page to the os if empty for N collections])],
+  MUNMAP_THRESHOLD=$enableval;
+   [case "$MMAP" in
+      no)
+        AC_MSG_ERROR([--enable-munmap requires --enable-mmap])
+        ;;
+    esac]
+   )
+if test "${enable_munmap}" != ""; then
+    AC_DEFINE([USE_MMAP], 1,
+              [Define to use mmap instead of sbrk to expand the heap.])
+    AC_DEFINE([USE_MUNMAP], 1,
+              [Define to return memory to OS with munmap calls
+               (see doc/README.macros).])
+    if test "${MUNMAP_THRESHOLD}" = "yes"; then
+      MUNMAP_THRESHOLD=6
+    fi
+    AC_DEFINE_UNQUOTED([MUNMAP_THRESHOLD], [${MUNMAP_THRESHOLD}],
+        [Number of GC cycles to wait before unmapping an unused block.])
+fi
+
+AM_CONDITIONAL(USE_LIBDIR, test -z "$with_cross_host")
+
+
+# Atomic Ops
+# ----------
+
+# Do we want to use an external libatomic_ops?  By default use it if it's
+# found.
+AC_ARG_WITH([libatomic-ops],
+    [AS_HELP_STRING([--with-libatomic-ops[=yes|no|check]],
+                    [Use a external libatomic_ops? (default: check)])],
+    [], [with_libatomic_ops=check])
+
+# Check for an external libatomic_ops if the answer was yes or check.  If not
+# found, fail on yes, and convert check to no.
+AS_IF([test x"$with_libatomic_ops" != xno],
+  [ PKG_CHECK_MODULES([ATOMIC_OPS], [atomic_ops], [],
+      [ AS_IF([test x"$with_libatomic_ops" != xcheck],
+              [AC_MSG_ERROR([A external libatomic_ops was not found.])])
+        with_libatomic_ops=no ]) ])
+
+# If we have neither an external or an internal version, offer a useful hint
+# and exit.
+AS_IF([test x"$with_libatomic_ops" = xno -a ! -e "$srcdir/libatomic_ops"],
+  [ AC_MSG_ERROR([libatomic_ops is required.  You can either install it on your system, or fetch and unpack a recent version into the source directory and link or rename it to libatomic_ops.]) ])
+
+# Finally, emit the definitions for bundled or external AO.
+AC_MSG_CHECKING([which libatomic_ops to use])
+AS_IF([test x"$with_libatomic_ops" != xno],
+  [ AC_MSG_RESULT([external]) ],
+  [ AC_MSG_RESULT([internal])
+    ATOMIC_OPS_CFLAGS='-I$(top_builddir)/libatomic_ops/src -I$(top_srcdir)/libatomic_ops/src'
+    ATOMIC_OPS_LIBS=""
+    AC_SUBST([ATOMIC_OPS_CFLAGS])
+    AC_CONFIG_SUBDIRS([libatomic_ops])
+  ])
+AM_CONDITIONAL([USE_INTERNAL_LIBATOMIC_OPS],
+    [test x$with_libatomic_ops = xno -a x"$THREADS" != xnone])
+AM_CONDITIONAL([NEED_ATOMIC_OPS_ASM],
+    [test x$with_libatomic_ops = xno -a x$need_atomic_ops_asm = xtrue])
+
+dnl Produce the Files
+dnl -----------------
+
+AC_CONFIG_FILES([Makefile bdw-gc.pc])
+
+AC_CONFIG_COMMANDS([default],,
+  [ srcdir="${srcdir}"
+    host=${host}
+    CONFIG_SHELL=${CONFIG_SHELL-/bin/sh}
+    CC="${CC}"
+    DEFS="$DEFS" ])
+
+AC_OUTPUT
diff --git a/src/gc/bdwgc/unused/configure.host b/src/gc/bdwgc/unused/configure.host
new file mode 100644
index 0000000..898c923
--- /dev/null
+++ b/src/gc/bdwgc/unused/configure.host
@@ -0,0 +1,61 @@
+# configure.host
+
+# This shell script handles all host based configuration for the garbage
+# collector.
+# It sets various shell variables based on the host and the
+# configuration options.  You can modify this shell script without
+# needing to rerun autoconf.
+
+# This shell script should be invoked as
+#   . configure.host
+# If it encounters an error, it will exit with a message.
+
+# It uses the following shell variables:
+#   host		The configuration host
+#   host_cpu		The configuration host CPU
+#   target_optspace	--enable-target-optspace ("yes", "no", "")
+#   GCC                 should be "yes" if using gcc
+
+# It sets the following shell variables:
+#   gc_cflags	Special CFLAGS to use when building
+
+gc_cflags=""
+
+# We should set -fexceptions if we are using gcc and might be used
+# inside something like gcj.  This is the zeroth approximation:
+if test :"$GCC": = :yes: ; then
+    gc_cflags="${gc_cflags} -fexceptions"
+else
+    case "$host" in
+        hppa*-*-hpux* )
+	if test :$GCC: != :"yes": ; then
+            gc_cflags="${gc_flags} +ESdbgasm"
+	fi
+        # :TODO: actaully we should check using Autoconf if
+        #     the compiler supports this option.
+        ;;
+    esac
+fi
+
+case "${target_optspace}:${host}" in
+  yes:*)
+    gc_cflags="${gc_cflags} -Os"
+    ;;
+  :m32r-* | :d10v-* | :d30v-*)
+    gc_cflags="${gc_cflags} -Os"
+    ;;
+  no:* | :*)
+    # Nothing.
+    ;;
+esac
+
+# Set any host dependent compiler flags.
+# THIS TABLE IS SORTED.  KEEP IT THAT WAY.
+
+case "${host}" in
+  mips-tx39-*|mipstx39-unknown-*)
+	gc_cflags="${gc_cflags} -G 0"
+	;;
+  *)
+	;;
+esac
diff --git a/src/gc/bdwgc/unused/digimars.mak b/src/gc/bdwgc/unused/digimars.mak
new file mode 100644
index 0000000..41178fd
--- /dev/null
+++ b/src/gc/bdwgc/unused/digimars.mak
@@ -0,0 +1,90 @@
+# Makefile to build Hans Boehm garbage collector using the Digital Mars
+# compiler from www.digitalmars.com
+# Written by Walter Bright
+
+
+DEFINES=-DNDEBUG -DGC_BUILD -D_WINDOWS -DGC_DLL -DALL_INTERIOR_POINTERS -D__STDC__ -DWIN32_THREADS
+CFLAGS=-Iinclude $(DEFINES) -wx -g
+LFLAGS=/ma/implib/co
+CC=sc
+
+.c.obj:
+	$(CC) -c $(CFLAGS) $*
+
+.cpp.obj:
+	$(CC) -c $(CFLAGS) -Aa $*
+
+OBJS=	\
+	allchblk.obj\
+	alloc.obj\
+	blacklst.obj\
+	checksums.obj\
+	dbg_mlc.obj\
+	dyn_load.obj\
+	finalize.obj\
+	gc_cpp.obj\
+	headers.obj\
+	mach_dep.obj\
+	malloc.obj\
+	mallocx.obj\
+	mark.obj\
+	mark_rts.obj\
+	misc.obj\
+	new_hblk.obj\
+	obj_map.obj\
+	os_dep.obj\
+	ptr_chck.obj\
+	reclaim.obj\
+	stubborn.obj\
+	typd_mlc.obj\
+	win32_threads.obj
+
+targets: gc.dll gc.lib gctest.exe
+
+gc.dll: $(OBJS) gc.def digimars.mak
+	sc -ogc.dll $(OBJS) -L$(LFLAGS) gc.def 	kernel32.lib user32.lib
+
+gc.def: digimars.mak
+	echo LIBRARY GC >gc.def
+	echo DESCRIPTION "Hans Boehm Garbage Collector" >>gc.def
+	echo EXETYPE NT	>>gc.def
+	echo EXPORTS >>gc.def
+	echo GC_is_visible_print_proc >>gc.def
+	echo GC_is_valid_displacement_print_proc >>gc.def
+
+clean:
+	del gc.def
+	del $(OBJS)
+
+
+gctest.exe : gc.lib tests\test.obj
+	sc -ogctest.exe tests\test.obj gc.lib
+
+tests\test.obj : tests\test.c
+	$(CC) -c -g -DNDEBUG -DGC_BUILD -D_WINDOWS -DGC_DLL \
+	-DALL_INTERIOR_POINTERS -DWIN32_THREADS \
+	-Iinclude tests\test.c -otests\test.obj
+
+allchblk.obj: allchblk.c
+alloc.obj: alloc.c
+blacklst.obj: blacklst.c
+checksums.obj: checksums.c
+dbg_mlc.obj: dbg_mlc.c
+dyn_load.obj: dyn_load.c
+finalize.obj: finalize.c
+gc_cpp.obj: gc_cpp.cpp
+headers.obj: headers.c
+mach_dep.obj: mach_dep.c
+malloc.obj: malloc.c
+mallocx.obj: mallocx.c
+mark.obj: mark.c
+mark_rts.obj: mark_rts.c
+misc.obj: misc.c
+new_hblk.obj: new_hblk.c
+obj_map.obj: obj_map.c
+os_dep.obj: os_dep.c
+ptr_chck.obj: ptr_chck.c
+reclaim.obj: reclaim.c
+stubborn.obj: stubborn.c
+typd_mlc.obj: typd_mlc.c
+win32_threads.obj: win32_threads.c
diff --git a/src/gc/bdwgc/unused/dyn_load.c b/src/gc/bdwgc/unused/dyn_load.c
new file mode 100644
index 0000000..a4d25a6
--- /dev/null
+++ b/src/gc/bdwgc/unused/dyn_load.c
@@ -0,0 +1,1482 @@
+/*
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1997 by Silicon Graphics.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ *
+ * Original author: Bill Janssen
+ * Heavily modified by Hans Boehm and others
+ */
+
+#include "private/gc_priv.h"
+
+/*
+ * This is incredibly OS specific code for tracking down data sections in
+ * dynamic libraries.  There appears to be no way of doing this quickly
+ * without groveling through undocumented data structures.  We would argue
+ * that this is a bug in the design of the dlopen interface.  THIS CODE
+ * MAY BREAK IN FUTURE OS RELEASES.  If this matters to you, don't hesitate
+ * to let your vendor know ...
+ *
+ * None of this is safe with dlclose and incremental collection.
+ * But then not much of anything is safe in the presence of dlclose.
+ */
+
+#if !defined(MACOS) && !defined(_WIN32_WCE) && !defined(__CC_ARM)
+# include <sys/types.h>
+#endif
+
+/* BTL: avoid circular redefinition of dlopen if GC_SOLARIS_THREADS defined */
+#undef GC_MUST_RESTORE_REDEFINED_DLOPEN
+#if defined(GC_PTHREADS) && !defined(GC_NO_DLOPEN) \
+    && !defined(GC_NO_THREAD_REDIRECTS) && !defined(GC_USE_LD_WRAP)
+  /* To support threads in Solaris, gc.h interposes on dlopen by        */
+  /* defining "dlopen" to be "GC_dlopen", which is implemented below.   */
+  /* However, both GC_FirstDLOpenedLinkMap() and GC_dlopen() use the    */
+  /* real system dlopen() in their implementation. We first remove      */
+  /* gc.h's dlopen definition and restore it later, after GC_dlopen().  */
+# undef dlopen
+# define GC_MUST_RESTORE_REDEFINED_DLOPEN
+#endif /* !GC_NO_DLOPEN */
+
+/* A user-supplied routine (custom filter) that might be called to      */
+/* determine whether a DSO really needs to be scanned by the GC.        */
+/* 0 means no filter installed.  May be unused on some platforms.       */
+/* FIXME: Add filter support for more platforms.                        */
+STATIC GC_has_static_roots_func GC_has_static_roots = 0;
+
+#if (defined(DYNAMIC_LOADING) || defined(MSWIN32) || defined(MSWINCE) \
+    || defined(CYGWIN32)) && !defined(PCR)
+
+#if !defined(SOLARISDL) && !defined(IRIX5) && \
+    !defined(MSWIN32) && !defined(MSWINCE) && !defined(CYGWIN32) && \
+    !(defined(ALPHA) && defined(OSF1)) && \
+    !defined(HPUX) && !(defined(LINUX) && defined(__ELF__)) && \
+    !defined(AIX) && !defined(SCO_ELF) && !defined(DGUX) && \
+    !(defined(FREEBSD) && defined(__ELF__)) && \
+    !(defined(OPENBSD) && (defined(__ELF__) || defined(M68K))) && \
+    !(defined(NETBSD) && defined(__ELF__)) && !defined(HURD) && \
+    !defined(DARWIN) && !defined(CYGWIN32)
+ --> We only know how to find data segments of dynamic libraries for the
+ --> above.  Additional SVR4 variants might not be too
+ --> hard to add.
+#endif
+
+#include <stdio.h>
+#ifdef SOLARISDL
+#   include <sys/elf.h>
+#   include <dlfcn.h>
+#   include <link.h>
+#endif
+
+#if defined(NETBSD)
+#   include <sys/param.h>
+#   include <dlfcn.h>
+#   include <machine/elf_machdep.h>
+#   define ELFSIZE ARCH_ELFSIZE
+#endif
+
+#if defined(SCO_ELF) || defined(DGUX) || defined(HURD) \
+    || (defined(__ELF__) && (defined(LINUX) || defined(FREEBSD) \
+                             || defined(NETBSD) || defined(OPENBSD)))
+# include <stddef.h>
+# if !defined(OPENBSD) && !defined(PLATFORM_ANDROID)
+    /* FIXME: Why we exclude it for OpenBSD? */
+    /* Exclude Android because linker.h below includes its own version. */
+#   include <elf.h>
+# endif
+# ifdef PLATFORM_ANDROID
+    /* The header file is in "bionic/linker" folder of Android sources. */
+    /* If you don't need the "dynamic loading" feature, you may build   */
+    /* the collector with -D IGNORE_DYNAMIC_LOADING.                    */
+#   include <linker.h>
+# else
+#   include <link.h>
+# endif
+#endif
+
+/* Newer versions of GNU/Linux define this macro.  We
+ * define it similarly for any ELF systems that don't.  */
+#  ifndef ElfW
+#    if defined(FREEBSD)
+#      if __ELF_WORD_SIZE == 32
+#        define ElfW(type) Elf32_##type
+#      else
+#        define ElfW(type) Elf64_##type
+#      endif
+#    elif defined(NETBSD) || defined(OPENBSD)
+#      if ELFSIZE == 32
+#        define ElfW(type) Elf32_##type
+#      else
+#        define ElfW(type) Elf64_##type
+#      endif
+#    else
+#      if !defined(ELF_CLASS) || ELF_CLASS == ELFCLASS32
+#        define ElfW(type) Elf32_##type
+#      else
+#        define ElfW(type) Elf64_##type
+#      endif
+#    endif
+#  endif
+
+#if defined(SOLARISDL) && !defined(USE_PROC_FOR_LIBRARIES)
+
+#ifdef LINT
+    Elf32_Dyn _DYNAMIC;
+#endif
+
+STATIC struct link_map *
+GC_FirstDLOpenedLinkMap(void)
+{
+    extern ElfW(Dyn) _DYNAMIC;
+    ElfW(Dyn) *dp;
+    static struct link_map * cachedResult = 0;
+    static ElfW(Dyn) *dynStructureAddr = 0;
+                /* BTL: added to avoid Solaris 5.3 ld.so _DYNAMIC bug   */
+
+#   ifdef SUNOS53_SHARED_LIB
+        /* BTL: Avoid the Solaris 5.3 bug that _DYNAMIC isn't being set */
+        /* up properly in dynamically linked .so's. This means we have  */
+        /* to use its value in the set of original object files loaded  */
+        /* at program startup.                                          */
+        if( dynStructureAddr == 0 ) {
+          void* startupSyms = dlopen(0, RTLD_LAZY);
+          dynStructureAddr = (ElfW(Dyn)*)dlsym(startupSyms, "_DYNAMIC");
+        }
+#   else
+        dynStructureAddr = &_DYNAMIC;
+#   endif
+
+    if( dynStructureAddr == 0) {
+        return(0);
+    }
+    if( cachedResult == 0 ) {
+        int tag;
+        for( dp = ((ElfW(Dyn) *)(&_DYNAMIC)); (tag = dp->d_tag) != 0; dp++ ) {
+            if( tag == DT_DEBUG ) {
+                struct link_map *lm
+                        = ((struct r_debug *)(dp->d_un.d_ptr))->r_map;
+                if( lm != 0 ) cachedResult = lm->l_next; /* might be NULL */
+                break;
+            }
+        }
+    }
+    return cachedResult;
+}
+
+#endif /* SOLARISDL ... */
+
+/* BTL: added to fix circular dlopen definition if GC_SOLARIS_THREADS defined */
+# ifdef GC_MUST_RESTORE_REDEFINED_DLOPEN
+#   define dlopen GC_dlopen
+# endif
+
+# if defined(SOLARISDL)
+/* Add dynamic library data sections to the root set.           */
+# if !defined(PCR) && !defined(GC_SOLARIS_THREADS) && defined(THREADS)
+        --> fix mutual exclusion with dlopen
+# endif
+
+# ifndef USE_PROC_FOR_LIBRARIES
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+  struct link_map *lm;
+
+  for (lm = GC_FirstDLOpenedLinkMap(); lm != 0; lm = lm->l_next) {
+        ElfW(Ehdr) * e;
+        ElfW(Phdr) * p;
+        unsigned long offset;
+        char * start;
+        int i;
+
+        e = (ElfW(Ehdr) *) lm->l_addr;
+#       ifdef PLATFORM_ANDROID
+          if (e == NULL)
+            continue;
+#       endif
+        p = ((ElfW(Phdr) *)(((char *)(e)) + e->e_phoff));
+        offset = ((unsigned long)(lm->l_addr));
+        for( i = 0; i < (int)e->e_phnum; i++, p++ ) {
+          switch( p->p_type ) {
+            case PT_LOAD:
+              {
+                if( !(p->p_flags & PF_W) ) break;
+                start = ((char *)(p->p_vaddr)) + offset;
+                GC_add_roots_inner(
+                  start,
+                  start + p->p_memsz,
+                  TRUE
+                );
+              }
+              break;
+            default:
+              break;
+          }
+        }
+    }
+}
+
+# endif /* !USE_PROC ... */
+# endif /* SOLARISDL */
+
+#if defined(SCO_ELF) || defined(DGUX) || defined(HURD) \
+    || (defined(__ELF__) && (defined(LINUX) || defined(FREEBSD) \
+                             || defined(NETBSD) || defined(OPENBSD)))
+
+#ifdef USE_PROC_FOR_LIBRARIES
+
+#include <string.h>
+
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <unistd.h>
+
+#define MAPS_BUF_SIZE (32*1024)
+
+/* Sort an array of HeapSects by start address.                         */
+/* Unfortunately at least some versions of                              */
+/* Linux qsort end up calling malloc by way of sysconf, and hence can't */
+/* be used in the collector.  Hence we roll our own.  Should be         */
+/* reasonably fast if the array is already mostly sorted, as we expect  */
+/* it to be.                                                            */
+static void sort_heap_sects(struct HeapSect *base, size_t number_of_elements)
+{
+    signed_word n = (signed_word)number_of_elements;
+    signed_word nsorted = 1;
+    signed_word i;
+
+    while (nsorted < n) {
+      while (nsorted < n &&
+             base[nsorted-1].hs_start < base[nsorted].hs_start)
+          ++nsorted;
+      if (nsorted == n) break;
+      GC_ASSERT(base[nsorted-1].hs_start > base[nsorted].hs_start);
+      i = nsorted - 1;
+      while (i >= 0 && base[i].hs_start > base[i+1].hs_start) {
+        struct HeapSect tmp = base[i];
+        base[i] = base[i+1];
+        base[i+1] = tmp;
+        --i;
+      }
+      GC_ASSERT(base[nsorted-1].hs_start < base[nsorted].hs_start);
+      ++nsorted;
+    }
+}
+
+STATIC word GC_register_map_entries(char *maps)
+{
+    char *prot;
+    char *buf_ptr = maps;
+    ptr_t start, end;
+    unsigned int maj_dev;
+    ptr_t least_ha, greatest_ha;
+    unsigned i;
+    ptr_t datastart;
+
+#   ifdef DATASTART_IS_FUNC
+      static ptr_t datastart_cached = (ptr_t)(word)-1;
+
+      /* Evaluate DATASTART only once.  */
+      if (datastart_cached == (ptr_t)(word)-1) {
+        datastart_cached = (ptr_t)(DATASTART);
+      }
+      datastart = datastart_cached;
+#   else
+      datastart = (ptr_t)(DATASTART);
+#   endif
+
+    GC_ASSERT(I_HOLD_LOCK());
+    sort_heap_sects(GC_our_memory, GC_n_memory);
+    least_ha = GC_our_memory[0].hs_start;
+    greatest_ha = GC_our_memory[GC_n_memory-1].hs_start
+                  + GC_our_memory[GC_n_memory-1].hs_bytes;
+
+    for (;;) {
+        buf_ptr = GC_parse_map_entry(buf_ptr, &start, &end, &prot,
+                                     &maj_dev, 0);
+        if (buf_ptr == NULL) return 1;
+        if (prot[1] == 'w') {
+            /* This is a writable mapping.  Add it to           */
+            /* the root set unless it is already otherwise      */
+            /* accounted for.                                   */
+            if (start <= GC_stackbottom && end >= GC_stackbottom) {
+                /* Stack mapping; discard       */
+                continue;
+            }
+#           ifdef THREADS
+              /* This may fail, since a thread may already be           */
+              /* unregistered, but its thread stack may still be there. */
+              /* That can fail because the stack may disappear while    */
+              /* we're marking.  Thus the marker is, and has to be      */
+              /* prepared to recover from segmentation faults.          */
+
+              if (GC_segment_is_thread_stack(start, end)) continue;
+
+              /* FIXME: NPTL squirrels                                  */
+              /* away pointers in pieces of the stack segment that we   */
+              /* don't scan.  We work around this                       */
+              /* by treating anything allocated by libpthread as        */
+              /* uncollectible, as we do in some other cases.           */
+              /* A specifically identified problem is that              */
+              /* thread stacks contain pointers to dynamic thread       */
+              /* vectors, which may be reused due to thread caching.    */
+              /* They may not be marked if the thread is still live.    */
+              /* This specific instance should be addressed by          */
+              /* INCLUDE_LINUX_THREAD_DESCR, but that doesn't quite     */
+              /* seem to suffice.                                       */
+              /* We currently trace entire thread stacks, if they are   */
+              /* are currently cached but unused.  This is              */
+              /* very suboptimal for performance reasons.               */
+#           endif
+            /* We no longer exclude the main data segment.              */
+            if (end <= least_ha || start >= greatest_ha) {
+              /* The easy case; just trace entire segment */
+              GC_add_roots_inner((char *)start, (char *)end, TRUE);
+              continue;
+            }
+            /* Add sections that don't belong to us. */
+              i = 0;
+              while (GC_our_memory[i].hs_start + GC_our_memory[i].hs_bytes
+                     < start)
+                  ++i;
+              GC_ASSERT(i < GC_n_memory);
+              if (GC_our_memory[i].hs_start <= start) {
+                  start = GC_our_memory[i].hs_start
+                          + GC_our_memory[i].hs_bytes;
+                  ++i;
+              }
+              while (i < GC_n_memory && GC_our_memory[i].hs_start < end
+                     && start < end) {
+                  if ((char *)start < GC_our_memory[i].hs_start)
+                    GC_add_roots_inner((char *)start,
+                                       GC_our_memory[i].hs_start, TRUE);
+                  start = GC_our_memory[i].hs_start
+                          + GC_our_memory[i].hs_bytes;
+                  ++i;
+              }
+              if (start < end)
+                  GC_add_roots_inner((char *)start, (char *)end, TRUE);
+        }
+    }
+    return 1;
+}
+
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+    if (!GC_register_map_entries(GC_get_maps()))
+        ABORT("Failed to read /proc for library registration");
+}
+
+/* We now take care of the main data segment ourselves: */
+GC_INNER GC_bool GC_register_main_static_data(void)
+{
+    return FALSE;
+}
+
+# define HAVE_REGISTER_MAIN_STATIC_DATA
+
+#else /* !USE_PROC_FOR_LIBRARIES */
+
+/* The following is the preferred way to walk dynamic libraries */
+/* For glibc 2.2.4+.  Unfortunately, it doesn't work for older  */
+/* versions.  Thanks to Jakub Jelinek for most of the code.     */
+
+#if (defined(LINUX) || defined (__GLIBC__)) /* Are others OK here, too? */ \
+     && (__GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ > 2) \
+         || (__GLIBC__ == 2 && __GLIBC_MINOR__ == 2 && defined(DT_CONFIG)))
+/* We have the header files for a glibc that includes dl_iterate_phdr.  */
+/* It may still not be available in the library on the target system.   */
+/* Thus we also treat it as a weak symbol.                              */
+# define HAVE_DL_ITERATE_PHDR
+# pragma weak dl_iterate_phdr
+#endif
+
+#if (defined(FREEBSD) && __FreeBSD__ >= 7)
+  /* On the FreeBSD system, any target system at major version 7 shall   */
+  /* have dl_iterate_phdr; therefore, we need not make it weak as above. */
+# define HAVE_DL_ITERATE_PHDR
+# define DL_ITERATE_PHDR_STRONG
+#endif
+
+#if defined(HAVE_DL_ITERATE_PHDR)
+
+# ifdef PT_GNU_RELRO
+/* Instead of registering PT_LOAD sections directly, we keep them       */
+/* in a temporary list, and filter them by excluding PT_GNU_RELRO       */
+/* segments.  Processing PT_GNU_RELRO sections with                     */
+/* GC_exclude_static_roots instead would be superficially cleaner.  But */
+/* it runs into trouble if a client registers an overlapping segment,   */
+/* which unfortunately seems quite possible.                            */
+
+#   define MAX_LOAD_SEGS MAX_ROOT_SETS
+
+    static struct load_segment {
+      ptr_t start;
+      ptr_t end;
+      /* Room for a second segment if we remove a RELRO segment */
+      /* from the middle.                                       */
+      ptr_t start2;
+      ptr_t end2;
+    } load_segs[MAX_LOAD_SEGS];
+
+    static int n_load_segs;
+# endif /* PT_GNU_RELRO */
+
+STATIC int GC_register_dynlib_callback(struct dl_phdr_info * info,
+                                       size_t size, void * ptr)
+{
+  const ElfW(Phdr) * p;
+  ptr_t start, end;
+  int i;
+
+  /* Make sure struct dl_phdr_info is at least as big as we need.  */
+  if (size < offsetof (struct dl_phdr_info, dlpi_phnum)
+      + sizeof (info->dlpi_phnum))
+    return -1;
+
+  p = info->dlpi_phdr;
+  for( i = 0; i < (int)info->dlpi_phnum; i++, p++ ) {
+    switch( p->p_type ) {
+#     ifdef PT_GNU_RELRO
+        case PT_GNU_RELRO:
+        /* This entry is known to be constant and will eventually be remapped
+           read-only.  However, the address range covered by this entry is
+           typically a subset of a previously encountered `LOAD' segment, so
+           we need to exclude it.  */
+        {
+            int j;
+
+            start = ((ptr_t)(p->p_vaddr)) + info->dlpi_addr;
+            end = start + p->p_memsz;
+            for (j = n_load_segs; --j >= 0; ) {
+              if (start >= load_segs[j].start && start < load_segs[j].end) {
+                if (load_segs[j].start2 != 0) {
+                  WARN("More than one GNU_RELRO segment per load seg\n",0);
+                } else {
+                  GC_ASSERT(end <= load_segs[j].end);
+                  /* Remove from the existing load segment */
+                  load_segs[j].end2 = load_segs[j].end;
+                  load_segs[j].end = start;
+                  load_segs[j].start2 = end;
+                }
+                break;
+              }
+              if (j == 0) WARN("Failed to find PT_GNU_RELRO segment"
+                               " inside PT_LOAD region", 0);
+            }
+        }
+
+        break;
+#     endif
+
+      case PT_LOAD:
+        {
+          GC_has_static_roots_func callback = GC_has_static_roots;
+          if( !(p->p_flags & PF_W) ) break;
+          start = ((char *)(p->p_vaddr)) + info->dlpi_addr;
+          end = start + p->p_memsz;
+
+          if (callback != 0 && !callback(info->dlpi_name, start, p->p_memsz))
+            break;
+#         ifdef PT_GNU_RELRO
+            if (n_load_segs >= MAX_LOAD_SEGS) ABORT("Too many PT_LOAD segs");
+#           if CPP_WORDSZ == 64
+              /* FIXME: GC_push_all eventually does the correct         */
+              /* rounding to the next multiple of ALIGNMENT, so, most   */
+              /* probably, we should remove the corresponding assertion */
+              /* check in GC_add_roots_inner along with this code line. */
+              /* start pointer value may require aligning */
+              start = (ptr_t)((word)start & ~(sizeof(word) - 1));
+#           endif
+            load_segs[n_load_segs].start = start;
+            load_segs[n_load_segs].end = end;
+            load_segs[n_load_segs].start2 = 0;
+            load_segs[n_load_segs].end2 = 0;
+            ++n_load_segs;
+#         else
+            GC_add_roots_inner(start, end, TRUE);
+#         endif /* PT_GNU_RELRO */
+        }
+      break;
+      default:
+        break;
+    }
+  }
+
+  *(int *)ptr = 1;     /* Signal that we were called */
+  return 0;
+}
+
+/* Do we need to separately register the main static data segment? */
+GC_INNER GC_bool GC_register_main_static_data(void)
+{
+# ifdef DL_ITERATE_PHDR_STRONG
+    /* If dl_iterate_phdr is not a weak symbol then don't test against  */
+    /* zero (otherwise a compiler might issue a warning).               */
+    return FALSE;
+# else
+    return (dl_iterate_phdr == 0); /* implicit conversion to function ptr */
+# endif
+}
+
+/* Return TRUE if we succeed, FALSE if dl_iterate_phdr wasn't there. */
+STATIC GC_bool GC_register_dynamic_libraries_dl_iterate_phdr(void)
+{
+  int did_something;
+  if (GC_register_main_static_data())
+    return FALSE;
+
+# ifdef PT_GNU_RELRO
+    {
+      static GC_bool excluded_segs = FALSE;
+      n_load_segs = 0;
+      if (!excluded_segs) {
+        GC_exclude_static_roots_inner((ptr_t)load_segs,
+                                      (ptr_t)load_segs + sizeof(load_segs));
+        excluded_segs = TRUE;
+      }
+    }
+# endif
+
+  did_something = 0;
+  dl_iterate_phdr(GC_register_dynlib_callback, &did_something);
+  if (did_something) {
+#   ifdef PT_GNU_RELRO
+      int i;
+
+      for (i = 0; i < n_load_segs; ++i) {
+        if (load_segs[i].end > load_segs[i].start) {
+          GC_add_roots_inner(load_segs[i].start, load_segs[i].end, TRUE);
+        }
+        if (load_segs[i].end2 > load_segs[i].start2) {
+          GC_add_roots_inner(load_segs[i].start2, load_segs[i].end2, TRUE);
+        }
+      }
+#   endif
+  } else {
+      char *datastart;
+      char *dataend;
+#     ifdef DATASTART_IS_FUNC
+        static ptr_t datastart_cached = (ptr_t)(word)-1;
+
+        /* Evaluate DATASTART only once.  */
+        if (datastart_cached == (ptr_t)(word)-1) {
+          datastart_cached = (ptr_t)(DATASTART);
+        }
+        datastart = (char *)datastart_cached;
+#     else
+        datastart = DATASTART;
+#     endif
+#     ifdef DATAEND_IS_FUNC
+        {
+          static ptr_t dataend_cached = 0;
+          /* Evaluate DATAEND only once. */
+          if (dataend_cached == 0) {
+            dataend_cached = (ptr_t)(DATAEND);
+          }
+          dataend = (char *)dataend_cached;
+        }
+#     else
+        dataend = DATAEND;
+#     endif
+
+      /* dl_iterate_phdr may forget the static data segment in  */
+      /* statically linked executables.                         */
+      GC_add_roots_inner(datastart, dataend, TRUE);
+#     if defined(DATASTART2)
+        GC_add_roots_inner(DATASTART2, (char *)(DATAEND2), TRUE);
+#     endif
+  }
+  return TRUE;
+}
+
+# define HAVE_REGISTER_MAIN_STATIC_DATA
+
+#else /* !HAVE_DL_ITERATE_PHDR */
+
+/* Dynamic loading code for Linux running ELF. Somewhat tested on
+ * Linux/x86, untested but hopefully should work on Linux/Alpha.
+ * This code was derived from the Solaris/ELF support. Thanks to
+ * whatever kind soul wrote that.  - Patrick Bridges */
+
+/* This doesn't necessarily work in all cases, e.g. with preloaded
+ * dynamic libraries.                                           */
+
+# if defined(NETBSD) || defined(OPENBSD)
+#   include <sys/exec_elf.h>
+   /* for compatibility with 1.4.x */
+#   ifndef DT_DEBUG
+#     define DT_DEBUG   21
+#   endif
+#   ifndef PT_LOAD
+#     define PT_LOAD    1
+#   endif
+#   ifndef PF_W
+#     define PF_W       2
+#   endif
+# elif !defined(PLATFORM_ANDROID)
+#  include <elf.h>
+# endif
+
+# ifndef PLATFORM_ANDROID
+#   include <link.h>
+# endif
+
+#endif /* !HAVE_DL_ITERATE_PHDR */
+
+#ifdef __GNUC__
+# pragma weak _DYNAMIC
+#endif
+extern ElfW(Dyn) _DYNAMIC[];
+
+STATIC struct link_map *
+GC_FirstDLOpenedLinkMap(void)
+{
+    ElfW(Dyn) *dp;
+    static struct link_map *cachedResult = 0;
+
+    if( _DYNAMIC == 0) {
+        return(0);
+    }
+    if( cachedResult == 0 ) {
+#     if defined(NETBSD) && defined(RTLD_DI_LINKMAP)
+        struct link_map *lm = NULL;
+        if (!dlinfo(RTLD_SELF, RTLD_DI_LINKMAP, &lm) && lm != NULL) {
+            /* Now lm points link_map object of libgc.  Since it    */
+            /* might not be the first dynamically linked object,    */
+            /* try to find it (object next to the main object).     */
+            while (lm->l_prev != NULL) {
+                lm = lm->l_prev;
+            }
+            cachedResult = lm->l_next;
+        }
+#     else
+        int tag;
+        for( dp = _DYNAMIC; (tag = dp->d_tag) != 0; dp++ ) {
+            if( tag == DT_DEBUG ) {
+                struct link_map *lm
+                        = ((struct r_debug *)(dp->d_un.d_ptr))->r_map;
+                if( lm != 0 ) cachedResult = lm->l_next; /* might be NULL */
+                break;
+            }
+        }
+#     endif /* !NETBSD || !RTLD_DI_LINKMAP */
+    }
+    return cachedResult;
+}
+
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+  struct link_map *lm;
+
+# ifdef HAVE_DL_ITERATE_PHDR
+    if (GC_register_dynamic_libraries_dl_iterate_phdr()) {
+        return;
+    }
+# endif
+  for (lm = GC_FirstDLOpenedLinkMap(); lm != 0; lm = lm->l_next)
+    {
+        ElfW(Ehdr) * e;
+        ElfW(Phdr) * p;
+        unsigned long offset;
+        char * start;
+        int i;
+
+        e = (ElfW(Ehdr) *) lm->l_addr;
+#       ifdef PLATFORM_ANDROID
+          if (e == NULL)
+            continue;
+#       endif
+        p = ((ElfW(Phdr) *)(((char *)(e)) + e->e_phoff));
+        offset = ((unsigned long)(lm->l_addr));
+        for( i = 0; i < (int)e->e_phnum; i++, p++ ) {
+          switch( p->p_type ) {
+            case PT_LOAD:
+              {
+                if( !(p->p_flags & PF_W) ) break;
+                start = ((char *)(p->p_vaddr)) + offset;
+                GC_add_roots_inner(start, start + p->p_memsz, TRUE);
+              }
+              break;
+            default:
+              break;
+          }
+        }
+    }
+}
+
+#endif /* !USE_PROC_FOR_LIBRARIES */
+
+#endif /* LINUX */
+
+#if defined(IRIX5) || (defined(USE_PROC_FOR_LIBRARIES) && !defined(LINUX))
+
+#include <sys/procfs.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <elf.h>
+#include <errno.h>
+#include <signal.h>  /* Only for the following test. */
+#ifndef _sigargs
+# define IRIX6
+#endif
+
+/* We use /proc to track down all parts of the address space that are   */
+/* mapped by the process, and throw out regions we know we shouldn't    */
+/* worry about.  This may also work under other SVR4 variants.          */
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+    static int fd = -1;
+    char buf[30];
+    static prmap_t * addr_map = 0;
+    static int current_sz = 0;  /* Number of records currently in addr_map */
+    static int needed_sz;       /* Required size of addr_map            */
+    int i;
+    long flags;
+    ptr_t start;
+    ptr_t limit;
+    ptr_t heap_start = HEAP_START;
+    ptr_t heap_end = heap_start;
+
+#   ifdef SOLARISDL
+#     define MA_PHYS 0
+#   endif /* SOLARISDL */
+
+    if (fd < 0) {
+      sprintf(buf, "/proc/%ld", (long)getpid());
+        /* The above generates a lint complaint, since pid_t varies.    */
+        /* It's unclear how to improve this.                            */
+      fd = open(buf, O_RDONLY);
+      if (fd < 0) {
+        ABORT("/proc open failed");
+      }
+    }
+    if (ioctl(fd, PIOCNMAP, &needed_sz) < 0) {
+        GC_err_printf("fd = %d, errno = %d\n", fd, errno);
+        ABORT("/proc PIOCNMAP ioctl failed");
+    }
+    if (needed_sz >= current_sz) {
+        current_sz = needed_sz * 2 + 1;
+                        /* Expansion, plus room for 0 record */
+        addr_map = (prmap_t *)GC_scratch_alloc(
+                                (word)current_sz * sizeof(prmap_t));
+        if (addr_map == NULL)
+          ABORT("Insufficient memory for address map");
+    }
+    if (ioctl(fd, PIOCMAP, addr_map) < 0) {
+        GC_err_printf("fd = %d, errno = %d, needed_sz = %d, addr_map = %p\n",
+                        fd, errno, needed_sz, addr_map);
+        ABORT("/proc PIOCMAP ioctl failed");
+    };
+    if (GC_n_heap_sects > 0) {
+        heap_end = GC_heap_sects[GC_n_heap_sects-1].hs_start
+                        + GC_heap_sects[GC_n_heap_sects-1].hs_bytes;
+        if (heap_end < GC_scratch_last_end_ptr) heap_end = GC_scratch_last_end_ptr;
+    }
+    for (i = 0; i < needed_sz; i++) {
+        flags = addr_map[i].pr_mflags;
+        if ((flags & (MA_BREAK | MA_STACK | MA_PHYS
+                      | MA_FETCHOP | MA_NOTCACHED)) != 0) goto irrelevant;
+        if ((flags & (MA_READ | MA_WRITE)) != (MA_READ | MA_WRITE))
+            goto irrelevant;
+          /* The latter test is empirically useless in very old Irix    */
+          /* versions.  Other than the                                  */
+          /* main data and stack segments, everything appears to be     */
+          /* mapped readable, writable, executable, and shared(!!).     */
+          /* This makes no sense to me. - HB                            */
+        start = (ptr_t)(addr_map[i].pr_vaddr);
+        if (GC_roots_present(start)) goto irrelevant;
+        if (start < heap_end && start >= heap_start)
+                goto irrelevant;
+#       ifdef MMAP_STACKS
+          if (GC_is_thread_stack(start)) goto irrelevant;
+#       endif /* MMAP_STACKS */
+
+        limit = start + addr_map[i].pr_size;
+        /* The following seemed to be necessary for very old versions   */
+        /* of Irix, but it has been reported to discard relevant        */
+        /* segments under Irix 6.5.                                     */
+#       ifndef IRIX6
+          if (addr_map[i].pr_off == 0 && strncmp(start, ELFMAG, 4) == 0) {
+            /* Discard text segments, i.e. 0-offset mappings against    */
+            /* executable files which appear to have ELF headers.       */
+            caddr_t arg;
+            int obj;
+#           define MAP_IRR_SZ 10
+            static ptr_t map_irr[MAP_IRR_SZ];
+                                        /* Known irrelevant map entries */
+            static int n_irr = 0;
+            struct stat buf;
+            register int j;
+
+            for (j = 0; j < n_irr; j++) {
+                if (map_irr[j] == start) goto irrelevant;
+            }
+            arg = (caddr_t)start;
+            obj = ioctl(fd, PIOCOPENM, &arg);
+            if (obj >= 0) {
+                fstat(obj, &buf);
+                close(obj);
+                if ((buf.st_mode & 0111) != 0) {
+                    if (n_irr < MAP_IRR_SZ) {
+                        map_irr[n_irr++] = start;
+                    }
+                    goto irrelevant;
+                }
+            }
+          }
+#       endif /* !IRIX6 */
+        GC_add_roots_inner(start, limit, TRUE);
+      irrelevant: ;
+    }
+    /* Don't keep cached descriptor, for now.  Some kernels don't like us */
+    /* to keep a /proc file descriptor around during kill -9.             */
+        if (close(fd) < 0) ABORT("Couldn't close /proc file");
+        fd = -1;
+}
+
+# endif /* USE_PROC || IRIX5 */
+
+# if defined(MSWIN32) || defined(MSWINCE) || defined(CYGWIN32)
+
+# ifndef WIN32_LEAN_AND_MEAN
+#   define WIN32_LEAN_AND_MEAN 1
+# endif
+# define NOSERVICE
+# include <windows.h>
+# include <stdlib.h>
+
+  /* We traverse the entire address space and register all segments     */
+  /* that could possibly have been written to.                          */
+  STATIC void GC_cond_add_roots(char *base, char * limit)
+  {
+#   ifdef GC_WIN32_THREADS
+      char * curr_base = base;
+      char * next_stack_lo;
+      char * next_stack_hi;
+
+      if (base == limit) return;
+      for(;;) {
+          GC_get_next_stack(curr_base, limit, &next_stack_lo, &next_stack_hi);
+          if (next_stack_lo >= limit) break;
+          if (next_stack_lo > curr_base)
+            GC_add_roots_inner(curr_base, next_stack_lo, TRUE);
+          curr_base = next_stack_hi;
+      }
+      if (curr_base < limit) GC_add_roots_inner(curr_base, limit, TRUE);
+#   else
+      char * stack_top
+         = (char *)((word)GC_approx_sp() &
+                        ~(GC_sysinfo.dwAllocationGranularity - 1));
+      if (base == limit) return;
+      if (limit > stack_top && base < GC_stackbottom) {
+          /* Part of the stack; ignore it. */
+          return;
+      }
+      GC_add_roots_inner(base, limit, TRUE);
+#   endif
+  }
+
+#ifdef DYNAMIC_LOADING
+  /* GC_register_main_static_data is not needed unless DYNAMIC_LOADING. */
+  GC_INNER GC_bool GC_register_main_static_data(void)
+  {
+#   if defined(MSWINCE) || defined(CYGWIN32)
+      /* Do we need to separately register the main static data segment? */
+      return FALSE;
+#   else
+      return GC_no_win32_dlls;
+#   endif
+  }
+# define HAVE_REGISTER_MAIN_STATIC_DATA
+#endif /* DYNAMIC_LOADING */
+
+# ifdef DEBUG_VIRTUALQUERY
+  void GC_dump_meminfo(MEMORY_BASIC_INFORMATION *buf)
+  {
+    GC_printf("BaseAddress = 0x%lx, AllocationBase = 0x%lx,"
+              " RegionSize = 0x%lx(%lu)\n", buf -> BaseAddress,
+              buf -> AllocationBase, buf -> RegionSize, buf -> RegionSize);
+    GC_printf("\tAllocationProtect = 0x%lx, State = 0x%lx, Protect = 0x%lx, "
+              "Type = 0x%lx\n", buf -> AllocationProtect, buf -> State,
+              buf -> Protect, buf -> Type);
+  }
+# endif /* DEBUG_VIRTUALQUERY */
+
+# if defined(MSWINCE) || defined(CYGWIN32)
+    /* FIXME: Should we really need to scan MEM_PRIVATE sections?       */
+    /* For now, we don't add MEM_PRIVATE sections to the data roots for */
+    /* WinCE because otherwise SEGV fault sometimes happens to occur in */
+    /* GC_mark_from() (and, even if we use WRAP_MARK_SOME, WinCE prints */
+    /* a "Data Abort" message to the debugging console).                */
+    /* To workaround that, use -DGC_REGISTER_MEM_PRIVATE.               */
+#   define GC_wnt TRUE
+# endif
+
+  GC_INNER void GC_register_dynamic_libraries(void)
+  {
+    MEMORY_BASIC_INFORMATION buf;
+    size_t result;
+    DWORD protect;
+    LPVOID p;
+    char * base;
+    char * limit, * new_limit;
+
+#   ifdef MSWIN32
+      if (GC_no_win32_dlls) return;
+#   endif
+    base = limit = p = GC_sysinfo.lpMinimumApplicationAddress;
+    while (p < GC_sysinfo.lpMaximumApplicationAddress) {
+        result = VirtualQuery(p, &buf, sizeof(buf));
+#       ifdef MSWINCE
+          if (result == 0) {
+            /* Page is free; advance to the next possible allocation base */
+            new_limit = (char *)
+                (((DWORD) p + GC_sysinfo.dwAllocationGranularity)
+                 & ~(GC_sysinfo.dwAllocationGranularity-1));
+          } else
+#       endif
+        /* else */ {
+            if (result != sizeof(buf)) {
+                ABORT("Weird VirtualQuery result");
+            }
+            new_limit = (char *)p + buf.RegionSize;
+            protect = buf.Protect;
+            if (buf.State == MEM_COMMIT
+                && (protect == PAGE_EXECUTE_READWRITE
+                    || protect == PAGE_READWRITE)
+                && (buf.Type == MEM_IMAGE
+#                   ifdef GC_REGISTER_MEM_PRIVATE
+                      || (protect == PAGE_READWRITE && buf.Type == MEM_PRIVATE)
+#                   else
+                      /* There is some evidence that we cannot always   */
+                      /* ignore MEM_PRIVATE sections under Windows ME   */
+                      /* and predecessors.  Hence we now also check for */
+                      /* that case.                                     */
+                      || (!GC_wnt && buf.Type == MEM_PRIVATE)
+#                   endif
+                   )
+                && !GC_is_heap_base(buf.AllocationBase)) {
+#               ifdef DEBUG_VIRTUALQUERY
+                  GC_dump_meminfo(&buf);
+#               endif
+                if ((char *)p != limit) {
+                    GC_cond_add_roots(base, limit);
+                    base = p;
+                }
+                limit = new_limit;
+            }
+        }
+        if (p > (LPVOID)new_limit /* overflow */) break;
+        p = (LPVOID)new_limit;
+    }
+    GC_cond_add_roots(base, limit);
+  }
+
+#endif /* MSWIN32 || MSWINCE || CYGWIN32 */
+
+#if defined(ALPHA) && defined(OSF1)
+
+#include <loader.h>
+
+extern char *sys_errlist[];
+extern int sys_nerr;
+extern int errno;
+
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+  int status;
+  ldr_process_t mypid;
+
+  /* module */
+    ldr_module_t moduleid = LDR_NULL_MODULE;
+    ldr_module_info_t moduleinfo;
+    size_t moduleinfosize = sizeof(moduleinfo);
+    size_t modulereturnsize;
+
+  /* region */
+    ldr_region_t region;
+    ldr_region_info_t regioninfo;
+    size_t regioninfosize = sizeof(regioninfo);
+    size_t regionreturnsize;
+
+  /* Obtain id of this process */
+    mypid = ldr_my_process();
+
+  /* For each module */
+    while (TRUE) {
+
+      /* Get the next (first) module */
+        status = ldr_next_module(mypid, &moduleid);
+
+      /* Any more modules? */
+        if (moduleid == LDR_NULL_MODULE)
+            break;    /* No more modules */
+
+      /* Check status AFTER checking moduleid because */
+      /* of a bug in the non-shared ldr_next_module stub */
+        if (status != 0) {
+          if (GC_print_stats) {
+            GC_log_printf("dynamic_load: status = %d\n", status);
+            if (errno < sys_nerr) {
+              GC_log_printf("dynamic_load: %s\n", sys_errlist[errno]);
+            } else {
+              GC_log_printf("dynamic_load: err_code = %d\n", errno);
+            }
+          }
+          ABORT("ldr_next_module failed");
+        }
+
+      /* Get the module information */
+        status = ldr_inq_module(mypid, moduleid, &moduleinfo,
+                                moduleinfosize, &modulereturnsize);
+        if (status != 0 )
+            ABORT("ldr_inq_module failed");
+
+      /* is module for the main program (i.e. nonshared portion)? */
+          if (moduleinfo.lmi_flags & LDR_MAIN)
+              continue;    /* skip the main module */
+
+#     ifdef DL_VERBOSE
+        GC_log_printf("---Module---\n");
+        GC_log_printf("Module ID\t = %16ld\n", moduleinfo.lmi_modid);
+        GC_log_printf("Count of regions = %16d\n", moduleinfo.lmi_nregion);
+        GC_log_printf("flags for module = %16lx\n", moduleinfo.lmi_flags);
+        GC_log_printf("module pathname\t = \"%s\"\n", moduleinfo.lmi_name);
+#     endif
+
+      /* For each region in this module */
+        for (region = 0; region < moduleinfo.lmi_nregion; region++) {
+          /* Get the region information */
+            status = ldr_inq_region(mypid, moduleid, region, &regioninfo,
+                                    regioninfosize, &regionreturnsize);
+            if (status != 0 )
+                ABORT("ldr_inq_region failed");
+
+          /* only process writable (data) regions */
+            if (! (regioninfo.lri_prot & LDR_W))
+                continue;
+
+#         ifdef DL_VERBOSE
+            GC_log_printf("--- Region ---\n");
+            GC_log_printf("Region number\t = %16ld\n",
+                          regioninfo.lri_region_no);
+            GC_log_printf("Protection flags = %016x\n", regioninfo.lri_prot);
+            GC_log_printf("Virtual address\t = %16p\n", regioninfo.lri_vaddr);
+            GC_log_printf("Mapped address\t = %16p\n",
+                          regioninfo.lri_mapaddr);
+            GC_log_printf("Region size\t = %16ld\n", regioninfo.lri_size);
+            GC_log_printf("Region name\t = \"%s\"\n", regioninfo.lri_name);
+#         endif
+
+          /* register region as a garbage collection root */
+          GC_add_roots_inner((char *)regioninfo.lri_mapaddr,
+                        (char *)regioninfo.lri_mapaddr + regioninfo.lri_size,
+                        TRUE);
+
+        }
+    }
+}
+#endif
+
+#if defined(HPUX)
+
+#include <errno.h>
+#include <dl.h>
+
+extern char *sys_errlist[];
+extern int sys_nerr;
+
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+  int status;
+  int index = 1; /* Ordinal position in shared library search list */
+  struct shl_descriptor *shl_desc; /* Shared library info, see dl.h */
+
+  /* For each dynamic library loaded */
+    while (TRUE) {
+
+      /* Get info about next shared library */
+        status = shl_get(index, &shl_desc);
+
+      /* Check if this is the end of the list or if some error occurred */
+        if (status != 0) {
+#        ifdef GC_HPUX_THREADS
+           /* I've seen errno values of 0.  The man page is not clear   */
+           /* as to whether errno should get set on a -1 return.        */
+           break;
+#        else
+          if (errno == EINVAL) {
+            break; /* Moved past end of shared library list --> finished */
+          } else {
+            if (GC_print_stats) {
+              if (errno < sys_nerr) {
+                GC_log_printf("dynamic_load: %s\n", sys_errlist[errno]);
+              } else {
+                GC_log_printf("dynamic_load: err_code = %d\n", errno);
+              }
+            }
+            ABORT("shl_get failed");
+          }
+#        endif
+        }
+
+#     ifdef DL_VERBOSE
+        GC_log_printf("---Shared library---\n");
+        GC_log_printf("\tfilename\t= \"%s\"\n", shl_desc->filename);
+        GC_log_printf("\tindex\t\t= %d\n", index);
+        GC_log_printf("\thandle\t\t= %08x\n",
+                      (unsigned long) shl_desc->handle);
+        GC_log_printf("\ttext seg.start\t= %08x\n", shl_desc->tstart);
+        GC_log_printf("\ttext seg.end\t= %08x\n", shl_desc->tend);
+        GC_log_printf("\tdata seg.start\t= %08x\n", shl_desc->dstart);
+        GC_log_printf("\tdata seg.end\t= %08x\n", shl_desc->dend);
+        GC_log_printf("\tref.count\t= %lu\n", shl_desc->ref_count);
+#     endif
+
+      /* register shared library's data segment as a garbage collection root */
+        GC_add_roots_inner((char *) shl_desc->dstart,
+                           (char *) shl_desc->dend, TRUE);
+
+        index++;
+    }
+}
+#endif /* HPUX */
+
+#ifdef AIX
+# pragma alloca
+# include <sys/ldr.h>
+# include <sys/errno.h>
+  GC_INNER void GC_register_dynamic_libraries(void)
+  {
+        int len;
+        char *ldibuf;
+        int ldibuflen;
+        struct ld_info *ldi;
+
+        ldibuf = alloca(ldibuflen = 8192);
+
+        while ( (len = loadquery(L_GETINFO,ldibuf,ldibuflen)) < 0) {
+                if (errno != ENOMEM) {
+                        ABORT("loadquery failed");
+                }
+                ldibuf = alloca(ldibuflen *= 2);
+        }
+
+        ldi = (struct ld_info *)ldibuf;
+        while (ldi) {
+                len = ldi->ldinfo_next;
+                GC_add_roots_inner(
+                                ldi->ldinfo_dataorg,
+                                (ptr_t)(unsigned long)ldi->ldinfo_dataorg
+                                + ldi->ldinfo_datasize,
+                                TRUE);
+                ldi = len ? (struct ld_info *)((char *)ldi + len) : 0;
+        }
+  }
+#endif /* AIX */
+
+#ifdef DARWIN
+
+/* __private_extern__ hack required for pre-3.4 gcc versions.   */
+#ifndef __private_extern__
+# define __private_extern__ extern
+# include <mach-o/dyld.h>
+# undef __private_extern__
+#else
+# include <mach-o/dyld.h>
+#endif
+#include <mach-o/getsect.h>
+
+/*#define DARWIN_DEBUG*/
+
+/* Writable sections generally available on Darwin.     */
+STATIC const struct {
+    const char *seg;
+    const char *sect;
+} GC_dyld_sections[] = {
+    { SEG_DATA, SECT_DATA },
+    /* Used by FSF GCC, but not by OS X system tools, so far.   */
+    { SEG_DATA, "__static_data" },
+    { SEG_DATA, SECT_BSS },
+    { SEG_DATA, SECT_COMMON },
+    /* FSF GCC - zero-sized object sections for targets         */
+    /*supporting section anchors.                               */
+    { SEG_DATA, "__zobj_data" },
+    { SEG_DATA, "__zobj_bss" }
+};
+
+/* Additional writable sections:                                */
+/* GCC on Darwin constructs aligned sections "on demand", where */
+/* the alignment size is embedded in the section name.          */
+/* Furthermore, there are distinctions between sections         */
+/* containing private vs. public symbols.  It also constructs   */
+/* sections specifically for zero-sized objects, when the       */
+/* target supports section anchors.                             */
+STATIC const char * GC_dyld_add_sect_fmts[] =
+{
+  "__bss%u",
+  "__pu_bss%u",
+  "__zo_bss%u",
+  "__zo_pu_bss%u",
+  NULL
+};
+
+/* Currently, mach-o will allow up to the max of 2^15 alignment */
+/* in an object file.                                           */
+#ifndef L2_MAX_OFILE_ALIGNMENT
+# define L2_MAX_OFILE_ALIGNMENT 15
+#endif
+
+STATIC const char *GC_dyld_name_for_hdr(const struct GC_MACH_HEADER *hdr)
+{
+    unsigned long i, c;
+    c = _dyld_image_count();
+    for (i = 0; i < c; i++)
+      if ((const struct GC_MACH_HEADER *)_dyld_get_image_header(i) == hdr)
+        return _dyld_get_image_name(i);
+    return NULL;
+}
+
+/* This should never be called by a thread holding the lock.    */
+STATIC void GC_dyld_image_add(const struct GC_MACH_HEADER *hdr,
+                              intptr_t slide)
+{
+  unsigned long start, end;
+  unsigned i, j;
+  const struct GC_MACH_SECTION *sec;
+  const char *name;
+  GC_has_static_roots_func callback = GC_has_static_roots;
+  char secnam[16];
+  const char *fmt;
+  DCL_LOCK_STATE;
+
+  if (GC_no_dls) return;
+# ifdef DARWIN_DEBUG
+    name = GC_dyld_name_for_hdr(hdr);
+# else
+    name = callback != 0 ? GC_dyld_name_for_hdr(hdr) : NULL;
+# endif
+  for (i = 0; i < sizeof(GC_dyld_sections)/sizeof(GC_dyld_sections[0]); i++) {
+    sec = GC_GETSECTBYNAME(hdr, GC_dyld_sections[i].seg,
+                           GC_dyld_sections[i].sect);
+    if (sec == NULL || sec->size < sizeof(word))
+      continue;
+    start = slide + sec->addr;
+    end = start + sec->size;
+    LOCK();
+    /* The user callback is called holding the lock.    */
+    if (callback == 0 || callback(name, (void*)start, (size_t)sec->size)) {
+#     ifdef DARWIN_DEBUG
+        GC_log_printf(
+              "Adding section __DATA,%s at %p-%p (%lu bytes) from image %s\n",
+               GC_dyld_sections[i].sect, (void*)start, (void*)end,
+               (unsigned long)sec->size, name);
+#     endif
+      GC_add_roots_inner((ptr_t)start, (ptr_t)end, FALSE);
+    }
+    UNLOCK();
+  }
+
+  /* Sections constructed on demand.    */
+  for (j = 0; (fmt = GC_dyld_add_sect_fmts[j]) != NULL; j++) {
+    /* Add our manufactured aligned BSS sections.       */
+    for (i = 0; i <= L2_MAX_OFILE_ALIGNMENT; i++) {
+      snprintf(secnam, sizeof(secnam), fmt, (unsigned)i);
+      sec = GC_GETSECTBYNAME(hdr, SEG_DATA, secnam);
+      if (sec == NULL || sec->size == 0)
+        continue;
+      start = slide + sec->addr;
+      end = start + sec->size;
+#     ifdef DARWIN_DEBUG
+        GC_log_printf("Adding on-demand section __DATA,%s at"
+                      " %p-%p (%lu bytes) from image %s\n",
+                      secnam, (void*)start, (void*)end,
+                      (unsigned long)sec->size, name);
+#     endif
+      GC_add_roots((char*)start, (char*)end);
+    }
+  }
+
+# ifdef DARWIN_DEBUG
+    GC_print_static_roots();
+# endif
+}
+
+/* This should never be called by a thread holding the lock.    */
+STATIC void GC_dyld_image_remove(const struct GC_MACH_HEADER *hdr,
+                                 intptr_t slide)
+{
+  unsigned long start, end;
+  unsigned i, j;
+  const struct GC_MACH_SECTION *sec;
+  char secnam[16];
+  const char *fmt;
+
+  for (i = 0; i < sizeof(GC_dyld_sections)/sizeof(GC_dyld_sections[0]); i++) {
+    sec = GC_GETSECTBYNAME(hdr, GC_dyld_sections[i].seg,
+                           GC_dyld_sections[i].sect);
+    if (sec == NULL || sec->size == 0)
+      continue;
+    start = slide + sec->addr;
+    end = start + sec->size;
+#   ifdef DARWIN_DEBUG
+      GC_log_printf(
+            "Removing section __DATA,%s at %p-%p (%lu bytes) from image %s\n",
+            GC_dyld_sections[i].sect, (void*)start, (void*)end,
+            (unsigned long)sec->size, GC_dyld_name_for_hdr(hdr));
+#   endif
+    GC_remove_roots((char*)start, (char*)end);
+  }
+
+  /* Remove our on-demand sections.     */
+  for (j = 0; (fmt = GC_dyld_add_sect_fmts[j]) != NULL; j++) {
+    for (i = 0; i <= L2_MAX_OFILE_ALIGNMENT; i++) {
+      snprintf(secnam, sizeof(secnam), fmt, (unsigned)i);
+      sec = GC_GETSECTBYNAME(hdr, SEG_DATA, secnam);
+      if (sec == NULL || sec->size == 0)
+        continue;
+      start = slide + sec->addr;
+      end = start + sec->size;
+#     ifdef DARWIN_DEBUG
+        GC_log_printf("Removing on-demand section __DATA,%s at"
+                      " %p-%p (%lu bytes) from image %s\n", secnam,
+                      (void*)start, (void*)end, (unsigned long)sec->size,
+                      GC_dyld_name_for_hdr(hdr));
+#     endif
+      GC_remove_roots((char*)start, (char*)end);
+    }
+  }
+
+# ifdef DARWIN_DEBUG
+    GC_print_static_roots();
+# endif
+}
+
+GC_INNER void GC_register_dynamic_libraries(void)
+{
+    /* Currently does nothing. The callbacks are setup by GC_init_dyld()
+    The dyld library takes it from there. */
+}
+
+/* The _dyld_* functions have an internal lock so no _dyld functions
+   can be called while the world is stopped without the risk of a deadlock.
+   Because of this we MUST setup callbacks BEFORE we ever stop the world.
+   This should be called BEFORE any thread in created and WITHOUT the
+   allocation lock held. */
+
+GC_INNER void GC_init_dyld(void)
+{
+  static GC_bool initialized = FALSE;
+
+  if (initialized) return;
+
+# ifdef DARWIN_DEBUG
+    GC_log_printf("Registering dyld callbacks...\n");
+# endif
+
+  /* Apple's Documentation:
+     When you call _dyld_register_func_for_add_image, the dynamic linker
+     runtime calls the specified callback (func) once for each of the images
+     that is currently loaded into the program. When a new image is added to
+     the program, your callback is called again with the mach_header for the
+     new image, and the virtual memory slide amount of the new image.
+
+     This WILL properly register already linked libraries and libraries
+     linked in the future.
+  */
+
+  _dyld_register_func_for_add_image(GC_dyld_image_add);
+  _dyld_register_func_for_remove_image(GC_dyld_image_remove);
+      /* Ignore 2 compiler warnings here: passing argument 1 of       */
+      /* '_dyld_register_func_for_add/remove_image' from incompatible */
+      /* pointer type.                                                */
+
+  /* Set this early to avoid reentrancy issues. */
+  initialized = TRUE;
+
+# ifdef NO_DYLD_BIND_FULLY_IMAGE
+    /* FIXME: What should we do in this case?   */
+# else
+    if (GC_no_dls) return; /* skip main data segment registration */
+
+    /* When the environment variable is set, the dynamic linker binds   */
+    /* all undefined symbols the application needs at launch time.      */
+    /* This includes function symbols that are normally bound lazily at */
+    /* the time of their first invocation.                              */
+    if (GETENV("DYLD_BIND_AT_LAUNCH") == 0) {
+      /* The environment variable is unset, so we should bind manually. */
+#     ifdef DARWIN_DEBUG
+        GC_log_printf("Forcing full bind of GC code...\n");
+#     endif
+      /* FIXME: '_dyld_bind_fully_image_containing_address' is deprecated. */
+      if (!_dyld_bind_fully_image_containing_address(
+                                                  (unsigned long *)GC_malloc))
+        ABORT("_dyld_bind_fully_image_containing_address failed");
+    }
+# endif
+}
+
+#define HAVE_REGISTER_MAIN_STATIC_DATA
+GC_INNER GC_bool GC_register_main_static_data(void)
+{
+  /* Already done through dyld callbacks */
+  return FALSE;
+}
+
+#endif /* DARWIN */
+
+#elif defined(PCR)
+
+# include "il/PCR_IL.h"
+# include "th/PCR_ThCtl.h"
+# include "mm/PCR_MM.h"
+
+  GC_INNER void GC_register_dynamic_libraries(void)
+  {
+    /* Add new static data areas of dynamically loaded modules. */
+    PCR_IL_LoadedFile * p = PCR_IL_GetLastLoadedFile();
+    PCR_IL_LoadedSegment * q;
+
+    /* Skip uncommitted files */
+    while (p != NIL && !(p -> lf_commitPoint)) {
+        /* The loading of this file has not yet been committed    */
+        /* Hence its description could be inconsistent.           */
+        /* Furthermore, it hasn't yet been run.  Hence its data   */
+        /* segments can't possibly reference heap allocated       */
+        /* objects.                                               */
+        p = p -> lf_prev;
+    }
+    for (; p != NIL; p = p -> lf_prev) {
+      for (q = p -> lf_ls; q != NIL; q = q -> ls_next) {
+        if ((q -> ls_flags & PCR_IL_SegFlags_Traced_MASK)
+            == PCR_IL_SegFlags_Traced_on) {
+          GC_add_roots_inner((char *)(q -> ls_addr),
+                             (char *)(q -> ls_addr) + q -> ls_bytes, TRUE);
+        }
+      }
+    }
+  }
+#endif /* PCR && !DYNAMIC_LOADING && !MSWIN32 */
+
+#if !defined(HAVE_REGISTER_MAIN_STATIC_DATA) && defined(DYNAMIC_LOADING)
+  /* Do we need to separately register the main static data segment? */
+  GC_INNER GC_bool GC_register_main_static_data(void)
+  {
+    return TRUE;
+  }
+#endif /* HAVE_REGISTER_MAIN_STATIC_DATA */
+
+/* Register a routine to filter dynamic library registration.  */
+GC_API void GC_CALL GC_register_has_static_roots_callback(
+                                        GC_has_static_roots_func callback)
+{
+    GC_has_static_roots = callback;
+}
diff --git a/src/gc/bdwgc/unused/gc.mak b/src/gc/bdwgc/unused/gc.mak
new file mode 100644
index 0000000..22ee909
--- /dev/null
+++ b/src/gc/bdwgc/unused/gc.mak
@@ -0,0 +1,2219 @@
+# Microsoft Developer Studio Generated NMAKE File, Format Version 4.10
+# This has been hand-edited way too many times.
+# A clean, manually generated makefile would be an improvement.
+
+# TARGTYPE "Win32 (x86) Application" 0x0101
+# TARGTYPE "Win32 (x86) Dynamic-Link Library" 0x0102
+
+!IF "$(CFG)" == ""
+CFG=gctest - Win32 Release
+!MESSAGE No configuration specified.  Defaulting to cord - Win32 Debug.
+!ENDIF 
+
+!IF "$(CFG)" != "gc - Win32 Release" && "$(CFG)" != "gc - Win32 Debug" &&\
+ "$(CFG)" != "gctest - Win32 Release" && "$(CFG)" != "gctest - Win32 Debug" &&\
+ "$(CFG)" != "cord - Win32 Release" && "$(CFG)" != "cord - Win32 Debug"
+!MESSAGE Invalid configuration "$(CFG)" specified.
+!MESSAGE You can specify a configuration when running NMAKE on this makefile
+!MESSAGE by defining the macro CFG on the command line.  For example:
+!MESSAGE 
+!MESSAGE NMAKE /f "gc.mak" CFG="cord - Win32 Debug"
+!MESSAGE 
+!MESSAGE Possible choices for configuration are:
+!MESSAGE 
+!MESSAGE "gc - Win32 Release" (based on "Win32 (x86) Dynamic-Link Library")
+!MESSAGE "gc - Win32 Debug" (based on "Win32 (x86) Dynamic-Link Library")
+!MESSAGE "gctest - Win32 Release" (based on "Win32 (x86) Application")
+!MESSAGE "gctest - Win32 Debug" (based on "Win32 (x86) Application")
+!MESSAGE "cord - Win32 Release" (based on "Win32 (x86) Application")
+!MESSAGE "cord - Win32 Debug" (based on "Win32 (x86) Application")
+!MESSAGE 
+!ERROR An invalid configuration is specified.
+!ENDIF 
+
+!IF "$(OS)" == "Windows_NT"
+NULL=
+!ELSE 
+NULL=nul
+!ENDIF 
+################################################################################
+# Begin Project
+# PROP Target_Last_Scanned "gctest - Win32 Debug"
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 0
+# PROP BASE Output_Dir "Release"
+# PROP BASE Intermediate_Dir "Release"
+# PROP BASE Target_Dir ""
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 0
+# PROP Output_Dir "Release"
+# PROP Intermediate_Dir "Release"
+# PROP Target_Dir ""
+OUTDIR=.\Release
+INTDIR=.\Release
+
+ALL : ".\Release\gc.dll" ".\Release\gc.bsc"
+
+CLEAN : 
+	-@erase ".\Release\allchblk.obj"
+	-@erase ".\Release\allchblk.sbr"
+	-@erase ".\Release\alloc.obj"
+	-@erase ".\Release\alloc.sbr"
+	-@erase ".\Release\blacklst.obj"
+	-@erase ".\Release\blacklst.sbr"
+	-@erase ".\Release\checksums.obj"
+	-@erase ".\Release\checksums.sbr"
+	-@erase ".\Release\dbg_mlc.obj"
+	-@erase ".\Release\dbg_mlc.sbr"
+	-@erase ".\Release\dyn_load.obj"
+	-@erase ".\Release\dyn_load.sbr"
+	-@erase ".\Release\finalize.obj"
+	-@erase ".\Release\finalize.sbr"
+	-@erase ".\Release\gc.bsc"
+	-@erase ".\Release\gc_cpp.obj"
+	-@erase ".\Release\gc_cpp.sbr"
+	-@erase ".\Release\gc.dll"
+	-@erase ".\Release\gc.exp"
+	-@erase ".\Release\gc.lib"
+	-@erase ".\Release\headers.obj"
+	-@erase ".\Release\headers.sbr"
+	-@erase ".\Release\mach_dep.obj"
+	-@erase ".\Release\mach_dep.sbr"
+	-@erase ".\Release\malloc.obj"
+	-@erase ".\Release\malloc.sbr"
+	-@erase ".\Release\mallocx.obj"
+	-@erase ".\Release\mallocx.sbr"
+	-@erase ".\Release\mark.obj"
+	-@erase ".\Release\mark.sbr"
+	-@erase ".\Release\mark_rts.obj"
+	-@erase ".\Release\mark_rts.sbr"
+	-@erase ".\Release\misc.obj"
+	-@erase ".\Release\misc.sbr"
+	-@erase ".\Release\new_hblk.obj"
+	-@erase ".\Release\new_hblk.sbr"
+	-@erase ".\Release\obj_map.obj"
+	-@erase ".\Release\obj_map.sbr"
+	-@erase ".\Release\os_dep.obj"
+	-@erase ".\Release\os_dep.sbr"
+	-@erase ".\Release\ptr_chck.obj"
+	-@erase ".\Release\ptr_chck.sbr"
+	-@erase ".\Release\reclaim.obj"
+	-@erase ".\Release\reclaim.sbr"
+	-@erase ".\Release\stubborn.obj"
+	-@erase ".\Release\stubborn.sbr"
+	-@erase ".\Release\typd_mlc.obj"
+	-@erase ".\Release\typd_mlc.sbr"
+	-@erase ".\Release\win32_threads.obj"
+	-@erase ".\Release\win32_threads.sbr"
+	-@erase ".\Release\msvc_dbg.obj"
+	-@erase ".\Release\msvc_dbg.sbr"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /MT /W3 /GX /O2 /D "WIN32" /D "NDEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MD /W3 /GX /O2 /I include /D "NDEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" /FR /YX /c
+CPP_PROJ=/nologo /MD /W3 /GX /O2 /I include /D "NDEBUG" /D\
+ "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" \
+ /FR"$(INTDIR)/" /Fp"$(INTDIR)/gc.pch" \
+ /Ilibatomic_ops/src /YX /Fo"$(INTDIR)/" /c 
+CPP_OBJS=.\Release/
+CPP_SBRS=.\Release/
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "NDEBUG" /win32
+# ADD MTL /nologo /D "NDEBUG" /win32
+MTL_PROJ=/nologo /D "NDEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "NDEBUG"
+# ADD RSC /l 0x809 /d "NDEBUG"
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/gc.bsc" 
+BSC32_SBRS= \
+	".\Release\allchblk.sbr" \
+	".\Release\alloc.sbr" \
+	".\Release\blacklst.sbr" \
+	".\Release\checksums.sbr" \
+	".\Release\dbg_mlc.sbr" \
+	".\Release\dyn_load.sbr" \
+	".\Release\finalize.sbr" \
+	".\Release\gc_cpp.sbr" \
+	".\Release\headers.sbr" \
+	".\Release\mach_dep.sbr" \
+	".\Release\malloc.sbr" \
+	".\Release\mallocx.sbr" \
+	".\Release\mark.sbr" \
+	".\Release\mark_rts.sbr" \
+	".\Release\misc.sbr" \
+	".\Release\new_hblk.sbr" \
+	".\Release\obj_map.sbr" \
+	".\Release\os_dep.sbr" \
+	".\Release\ptr_chck.sbr" \
+	".\Release\reclaim.sbr" \
+	".\Release\stubborn.sbr" \
+	".\Release\typd_mlc.sbr" \
+	".\Release\msvc_dbg.sbr" \
+	".\Release\win32_threads.sbr"
+
+".\Release\gc.bsc" : "$(OUTDIR)" $(BSC32_SBRS)
+    $(BSC32) @<<
+  $(BSC32_FLAGS) $(BSC32_SBRS)
+<<
+
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /dll /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /dll /machine:I386
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /dll /incremental:no\
+ /pdb:"$(OUTDIR)/gc.pdb" /machine:I386 /out:"$(OUTDIR)/gc.dll"\
+ /implib:"$(OUTDIR)/gc.lib" 
+LINK32_OBJS= \
+	".\Release\allchblk.obj" \
+	".\Release\alloc.obj" \
+	".\Release\blacklst.obj" \
+	".\Release\checksums.obj" \
+	".\Release\dbg_mlc.obj" \
+	".\Release\dyn_load.obj" \
+	".\Release\finalize.obj" \
+	".\Release\gc_cpp.obj" \
+	".\Release\headers.obj" \
+	".\Release\mach_dep.obj" \
+	".\Release\malloc.obj" \
+	".\Release\mallocx.obj" \
+	".\Release\mark.obj" \
+	".\Release\mark_rts.obj" \
+	".\Release\misc.obj" \
+	".\Release\new_hblk.obj" \
+	".\Release\obj_map.obj" \
+	".\Release\os_dep.obj" \
+	".\Release\ptr_chck.obj" \
+	".\Release\reclaim.obj" \
+	".\Release\stubborn.obj" \
+	".\Release\typd_mlc.obj" \
+	".\Release\msvc_dbg.obj" \
+	".\Release\win32_threads.obj"
+
+".\Release\gc.dll" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 1
+# PROP BASE Output_Dir "Debug"
+# PROP BASE Intermediate_Dir "Debug"
+# PROP BASE Target_Dir ""
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 1
+# PROP Output_Dir "Debug"
+# PROP Intermediate_Dir "Debug"
+# PROP Target_Dir ""
+OUTDIR=.\Debug
+INTDIR=.\Debug
+
+ALL : ".\Debug\gc.dll" ".\Debug\gc.bsc"
+
+CLEAN : 
+	-@erase ".\Debug\allchblk.obj"
+	-@erase ".\Debug\allchblk.sbr"
+	-@erase ".\Debug\alloc.obj"
+	-@erase ".\Debug\alloc.sbr"
+	-@erase ".\Debug\blacklst.obj"
+	-@erase ".\Debug\blacklst.sbr"
+	-@erase ".\Debug\checksums.obj"
+	-@erase ".\Debug\checksums.sbr"
+	-@erase ".\Debug\dbg_mlc.obj"
+	-@erase ".\Debug\dbg_mlc.sbr"
+	-@erase ".\Debug\dyn_load.obj"
+	-@erase ".\Debug\dyn_load.sbr"
+	-@erase ".\Debug\finalize.obj"
+	-@erase ".\Debug\finalize.sbr"
+	-@erase ".\Debug\gc_cpp.obj"
+	-@erase ".\Debug\gc_cpp.sbr"
+	-@erase ".\Debug\gc.bsc"
+	-@erase ".\Debug\gc.dll"
+	-@erase ".\Debug\gc.exp"
+	-@erase ".\Debug\gc.lib"
+	-@erase ".\Debug\gc.map"
+	-@erase ".\Debug\gc.pdb"
+	-@erase ".\Debug\headers.obj"
+	-@erase ".\Debug\headers.sbr"
+	-@erase ".\Debug\mach_dep.obj"
+	-@erase ".\Debug\mach_dep.sbr"
+	-@erase ".\Debug\malloc.obj"
+	-@erase ".\Debug\malloc.sbr"
+	-@erase ".\Debug\mallocx.obj"
+	-@erase ".\Debug\mallocx.sbr"
+	-@erase ".\Debug\mark.obj"
+	-@erase ".\Debug\mark.sbr"
+	-@erase ".\Debug\mark_rts.obj"
+	-@erase ".\Debug\mark_rts.sbr"
+	-@erase ".\Debug\misc.obj"
+	-@erase ".\Debug\misc.sbr"
+	-@erase ".\Debug\new_hblk.obj"
+	-@erase ".\Debug\new_hblk.sbr"
+	-@erase ".\Debug\obj_map.obj"
+	-@erase ".\Debug\obj_map.sbr"
+	-@erase ".\Debug\os_dep.obj"
+	-@erase ".\Debug\os_dep.sbr"
+	-@erase ".\Debug\ptr_chck.obj"
+	-@erase ".\Debug\ptr_chck.sbr"
+	-@erase ".\Debug\reclaim.obj"
+	-@erase ".\Debug\reclaim.sbr"
+	-@erase ".\Debug\stubborn.obj"
+	-@erase ".\Debug\stubborn.sbr"
+	-@erase ".\Debug\typd_mlc.obj"
+	-@erase ".\Debug\typd_mlc.sbr"
+	-@erase ".\Debug\vc40.idb"
+	-@erase ".\Debug\vc40.pdb"
+	-@erase ".\Debug\win32_threads.obj"
+	-@erase ".\Debug\win32_threads.sbr"
+	-@erase ".\Debug\msvc_dbg.obj"
+	-@erase ".\Debug\msvc_dbg.sbr"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /MTd /W3 /Gm /GX /Zi /Od /D "WIN32" /D "_DEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MDd /W3 /Gm /GX /Zi /Od /I include /D "_DEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" /FR /YX /c
+CPP_PROJ=/nologo /MDd /W3 /Gm /GX /Zi /Od /I include /D "_DEBUG"\
+ /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" \
+ /D "GC_ASSERTIONS" /D "GC_THREADS" \
+ /FR"$(INTDIR)/" /Fp"$(INTDIR)/gc.pch" /YX /Fo"$(INTDIR)/"\
+ /Ilibatomic_ops/src /Fd"$(INTDIR)/" /c 
+CPP_OBJS=.\Debug/
+CPP_SBRS=.\Debug/
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "_DEBUG" /win32
+# ADD MTL /nologo /D "_DEBUG" /win32
+MTL_PROJ=/nologo /D "_DEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "_DEBUG"
+# ADD RSC /l 0x809 /d "_DEBUG"
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/gc.bsc" 
+BSC32_SBRS= \
+	".\Debug\allchblk.sbr" \
+	".\Debug\alloc.sbr" \
+	".\Debug\blacklst.sbr" \
+	".\Debug\checksums.sbr" \
+	".\Debug\dbg_mlc.sbr" \
+	".\Debug\dyn_load.sbr" \
+	".\Debug\finalize.sbr" \
+	".\Debug\gc_cpp.sbr" \
+	".\Debug\headers.sbr" \
+	".\Debug\mach_dep.sbr" \
+	".\Debug\malloc.sbr" \
+	".\Debug\mallocx.sbr" \
+	".\Debug\mark.sbr" \
+	".\Debug\mark_rts.sbr" \
+	".\Debug\misc.sbr" \
+	".\Debug\new_hblk.sbr" \
+	".\Debug\obj_map.sbr" \
+	".\Debug\os_dep.sbr" \
+	".\Debug\ptr_chck.sbr" \
+	".\Debug\reclaim.sbr" \
+	".\Debug\stubborn.sbr" \
+	".\Debug\typd_mlc.sbr" \
+	".\Debug\msvc_dbg.sbr" \
+	".\Debug\win32_threads.sbr"
+
+".\Debug\gc.bsc" : "$(OUTDIR)" $(BSC32_SBRS)
+    $(BSC32) @<<
+  $(BSC32_FLAGS) $(BSC32_SBRS)
+<<
+
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /dll /debug /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /dll /incremental:no /map /debug /machine:I386
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /dll /incremental:no\
+ /pdb:"$(OUTDIR)/gc.pdb" /map:"$(INTDIR)/gc.map" /debug /machine:I386\
+ /out:"$(OUTDIR)/gc.dll" /implib:"$(OUTDIR)/gc.lib" 
+LINK32_OBJS= \
+	".\Debug\allchblk.obj" \
+	".\Debug\alloc.obj" \
+	".\Debug\blacklst.obj" \
+	".\Debug\checksums.obj" \
+	".\Debug\dbg_mlc.obj" \
+	".\Debug\dyn_load.obj" \
+	".\Debug\finalize.obj" \
+	".\Debug\gc_cpp.obj" \
+	".\Debug\headers.obj" \
+	".\Debug\mach_dep.obj" \
+	".\Debug\malloc.obj" \
+	".\Debug\mallocx.obj" \
+	".\Debug\mark.obj" \
+	".\Debug\mark_rts.obj" \
+	".\Debug\misc.obj" \
+	".\Debug\new_hblk.obj" \
+	".\Debug\obj_map.obj" \
+	".\Debug\os_dep.obj" \
+	".\Debug\ptr_chck.obj" \
+	".\Debug\reclaim.obj" \
+	".\Debug\stubborn.obj" \
+	".\Debug\typd_mlc.obj" \
+	".\Debug\msvc_dbg.obj" \
+	".\Debug\win32_threads.obj"
+
+".\Debug\gc.dll" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ELSEIF  "$(CFG)" == "gctest - Win32 Release"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 0
+# PROP BASE Output_Dir "gctest\Release"
+# PROP BASE Intermediate_Dir "gctest\Release"
+# PROP BASE Target_Dir "gctest"
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 0
+# PROP Output_Dir "gctest\Release"
+# PROP Intermediate_Dir "gctest\Release"
+# PROP Target_Dir "gctest"
+OUTDIR=.\gctest\Release
+INTDIR=.\gctest\Release
+
+ALL : "gc - Win32 Release" ".\Release\gctest.exe"
+
+CLEAN : 
+	-@erase ".\gctest\Release\test.obj"
+	-@erase ".\Release\gctest.exe"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+test.c : tests\test.c
+	copy tests\test.c test.c
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /W3 /GX /O2 /D "WIN32" /D "NDEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MD /W3 /GX /O2 /I include /D "NDEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" /YX /c
+CPP_PROJ=/nologo /MD /W3 /GX /O2 /I include /D "NDEBUG" /D "WIN32" /D "_WINDOWS" /D\
+ "ALL_INTERIOR_POINTERS" /D "GC_THREADS" \
+ /Ilibatomic_ops/src /Fp"$(INTDIR)/gctest.pch" \
+ /YX /Fo"$(INTDIR)/" /c 
+CPP_OBJS=.\gctest\Release/
+CPP_SBRS=.\.
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "NDEBUG" /win32
+# ADD MTL /nologo /D "NDEBUG" /win32
+MTL_PROJ=/nologo /D "NDEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "NDEBUG"
+# ADD RSC /l 0x809 /d "NDEBUG"
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/gctest.bsc" 
+BSC32_SBRS= \
+	
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /machine:I386 /out:"Release/gctest.exe"
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /incremental:no\
+ /pdb:"$(OUTDIR)/gctest.pdb" /machine:I386 /out:"Release/gctest.exe" 
+LINK32_OBJS= \
+	".\gctest\Release\test.obj" \
+	".\Release\gc.lib"
+
+".\Release\gctest.exe" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ELSEIF  "$(CFG)" == "gctest - Win32 Debug"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 1
+# PROP BASE Output_Dir "gctest\Debug"
+# PROP BASE Intermediate_Dir "gctest\Debug"
+# PROP BASE Target_Dir "gctest"
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 1
+# PROP Output_Dir "gctest\Debug"
+# PROP Intermediate_Dir "gctest\Debug"
+# PROP Target_Dir "gctest"
+OUTDIR=.\gctest\Debug
+INTDIR=.\gctest\Debug
+
+ALL : "gc - Win32 Debug" ".\Debug\gctest.exe" ".\gctest\Debug\gctest.bsc"
+
+CLEAN : 
+	-@erase ".\Debug\gctest.exe"
+	-@erase ".\gctest\Debug\gctest.bsc"
+	-@erase ".\gctest\Debug\gctest.map"
+	-@erase ".\gctest\Debug\gctest.pdb"
+	-@erase ".\gctest\Debug\test.obj"
+	-@erase ".\gctest\Debug\test.sbr"
+	-@erase ".\gctest\Debug\vc40.idb"
+	-@erase ".\gctest\Debug\vc40.pdb"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /W3 /Gm /GX /Zi /Od /D "WIN32" /D "_DEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MDd /W3 /Gm /GX /Zi /Od /D "_DEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" /FR /YX /c
+CPP_PROJ=/nologo /MDd /W3 /Gm /GX /Zi /Od /I include /D "_DEBUG" /D "WIN32" /D "_WINDOWS"\
+ /D "ALL_INTERIOR_POINTERS" /D "GC_THREADS" /FR"$(INTDIR)/"\
+ /Ilibatomic_ops/src /Fp"$(INTDIR)/gctest.pch" /YX /Fo"$(INTDIR)/" /Fd"$(INTDIR)/" /c 
+CPP_OBJS=.\gctest\Debug/
+CPP_SBRS=.\gctest\Debug/
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "_DEBUG" /win32
+# ADD MTL /nologo /D "_DEBUG" /win32
+MTL_PROJ=/nologo /D "_DEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "_DEBUG"
+# ADD RSC /l 0x809 /d "_DEBUG"
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/gctest.bsc" 
+BSC32_SBRS= \
+	".\gctest\Debug\test.sbr"
+
+".\gctest\Debug\gctest.bsc" : "$(OUTDIR)" $(BSC32_SBRS)
+    $(BSC32) @<<
+  $(BSC32_FLAGS) $(BSC32_SBRS)
+<<
+
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /debug /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /incremental:no /map /debug /machine:I386 /out:"Debug/gctest.exe"
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /incremental:no\
+ /pdb:"$(OUTDIR)/gctest.pdb" /map:"$(INTDIR)/gctest.map" /debug /machine:I386\
+ /out:"Debug/gctest.exe" 
+LINK32_OBJS= \
+	".\Debug\gc.lib" \
+	".\gctest\Debug\test.obj"
+
+".\Debug\gctest.exe" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Release"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 0
+# PROP BASE Output_Dir "cord\Release"
+# PROP BASE Intermediate_Dir "cord\Release"
+# PROP BASE Target_Dir "cord"
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 0
+# PROP Output_Dir "cord\Release"
+# PROP Intermediate_Dir "cord\Release"
+# PROP Target_Dir "cord"
+OUTDIR=.\cord\Release
+INTDIR=.\cord\Release
+
+ALL : "gc - Win32 Release" ".\Release\de.exe"
+
+CLEAN : 
+	-@erase ".\cord\Release\cordbscs.obj"
+	-@erase ".\cord\Release\cordxtra.obj"
+	-@erase ".\cord\Release\de.obj"
+	-@erase ".\cord\Release\de_win.obj"
+	-@erase ".\cord\Release\de_win.res"
+	-@erase ".\Release\de.exe"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /W3 /GX /O2 /D "WIN32" /D "NDEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MD /W3 /GX /O2 /I "." /D "NDEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /YX /c
+CPP_PROJ=/nologo /MD /W3 /GX /O2 /I "." /I include /D "NDEBUG" /D "WIN32" /D "_WINDOWS" /D\
+ /Ilibatomic_ops/src "ALL_INTERIOR_POINTERS" /Fp"$(INTDIR)/cord.pch" /YX /Fo"$(INTDIR)/" /c 
+CPP_OBJS=.\cord\Release/
+CPP_SBRS=.\.
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "NDEBUG" /win32
+# ADD MTL /nologo /D "NDEBUG" /win32
+MTL_PROJ=/nologo /D "NDEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "NDEBUG"
+# ADD RSC /l 0x809 /d "NDEBUG"
+RSC_PROJ=/l 0x809 /fo"$(INTDIR)/de_win.res" /d "NDEBUG" 
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/cord.bsc" 
+BSC32_SBRS= \
+	
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /machine:I386 /out:"Release/de.exe"
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /incremental:no /pdb:"$(OUTDIR)/de.pdb"\
+ /machine:I386 /out:"Release/de.exe" 
+LINK32_OBJS= \
+	".\cord\Release\cordbscs.obj" \
+	".\cord\Release\cordxtra.obj" \
+	".\cord\Release\de.obj" \
+	".\cord\Release\de_win.obj" \
+	".\cord\Release\de_win.res" \
+	".\Release\gc.lib"
+
+".\Release\de.exe" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+# PROP BASE Use_MFC 0
+# PROP BASE Use_Debug_Libraries 1
+# PROP BASE Output_Dir "cord\Debug"
+# PROP BASE Intermediate_Dir "cord\Debug"
+# PROP BASE Target_Dir "cord"
+# PROP Use_MFC 0
+# PROP Use_Debug_Libraries 1
+# PROP Output_Dir "cord\Debug"
+# PROP Intermediate_Dir "cord\Debug"
+# PROP Target_Dir "cord"
+OUTDIR=.\cord\Debug
+INTDIR=.\cord\Debug
+
+ALL : "gc - Win32 Debug" ".\Debug\de.exe"
+
+CLEAN : 
+	-@erase ".\cord\Debug\cordbscs.obj"
+	-@erase ".\cord\Debug\cordxtra.obj"
+	-@erase ".\cord\Debug\de.obj"
+	-@erase ".\cord\Debug\de.pdb"
+	-@erase ".\cord\Debug\de_win.obj"
+	-@erase ".\cord\Debug\de_win.res"
+	-@erase ".\cord\Debug\vc40.idb"
+	-@erase ".\cord\Debug\vc40.pdb"
+	-@erase ".\Debug\de.exe"
+	-@erase ".\Debug\de.ilk"
+
+"$(OUTDIR)" :
+    if not exist "$(OUTDIR)/$(NULL)" mkdir "$(OUTDIR)"
+
+CPP=cl.exe
+# ADD BASE CPP /nologo /W3 /Gm /GX /Zi /Od /D "WIN32" /D "_DEBUG" /D "_WINDOWS" /YX /c
+# ADD CPP /nologo /MDd /W3 /Gm /GX /Zi /Od /I "." /D "_DEBUG" /D "WIN32" /D "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /YX /c
+CPP_PROJ=/nologo /MDd /W3 /Gm /GX /Zi /Od /I "." /I include /D "_DEBUG" /D "WIN32" /D\
+ "_WINDOWS" /D "ALL_INTERIOR_POINTERS" /Fp"$(INTDIR)/cord.pch" /YX\
+ /Ilibatomic_ops/src /Fo"$(INTDIR)/" /Fd"$(INTDIR)/" /c 
+CPP_OBJS=.\cord\Debug/
+CPP_SBRS=.\.
+
+.c{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_OBJS)}.obj:
+   $(CPP) $(CPP_PROJ) $<  
+
+.c{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cpp{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+.cxx{$(CPP_SBRS)}.sbr:
+   $(CPP) $(CPP_PROJ) $<  
+
+MTL=mktyplib.exe
+# ADD BASE MTL /nologo /D "_DEBUG" /win32
+# ADD MTL /nologo /D "_DEBUG" /win32
+MTL_PROJ=/nologo /D "_DEBUG" /win32 
+RSC=rc.exe
+# ADD BASE RSC /l 0x809 /d "_DEBUG"
+# ADD RSC /l 0x809 /d "_DEBUG"
+RSC_PROJ=/l 0x809 /fo"$(INTDIR)/de_win.res" /d "_DEBUG" 
+BSC32=bscmake.exe
+# ADD BASE BSC32 /nologo
+# ADD BSC32 /nologo
+BSC32_FLAGS=/nologo /o"$(OUTDIR)/cord.bsc" 
+BSC32_SBRS= \
+	
+LINK32=link.exe
+# ADD BASE LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /debug /machine:I386
+# ADD LINK32 kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /nologo /subsystem:windows /debug /machine:I386 /out:"Debug/de.exe"
+LINK32_FLAGS=kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib\
+ advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib\
+ odbccp32.lib /nologo /subsystem:windows /incremental:yes\
+ /pdb:"$(OUTDIR)/de.pdb" /debug /machine:I386 /out:"Debug/de.exe" 
+LINK32_OBJS= \
+	".\cord\Debug\cordbscs.obj" \
+	".\cord\Debug\cordxtra.obj" \
+	".\cord\Debug\de.obj" \
+	".\cord\Debug\de_win.obj" \
+	".\cord\Debug\de_win.res" \
+	".\Debug\gc.lib"
+
+".\Debug\de.exe" : "$(OUTDIR)" $(DEF_FILE) $(LINK32_OBJS)
+    $(LINK32) @<<
+  $(LINK32_FLAGS) $(LINK32_OBJS)
+<<
+
+!ENDIF 
+
+################################################################################
+# Begin Target
+
+# Name "gc - Win32 Release"
+# Name "gc - Win32 Debug"
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+!ENDIF 
+
+################################################################################
+# Begin Source File
+
+SOURCE=.\gc_cpp.cpp
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_RECLA=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	".\include\gc_cpp.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_RECLA=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\gc_cpp.obj" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+".\Release\gc_cpp.sbr" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_RECLA=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	".\include\gc_cpp.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_RECLA=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\gc_cpp.obj" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+".\Debug\gc_cpp.sbr" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\reclaim.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_RECLA=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_RECLA=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\reclaim.obj" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+".\Release\reclaim.sbr" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_RECLA=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_RECLA=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\reclaim.obj" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+".\Debug\reclaim.sbr" : $(SOURCE) $(DEP_CPP_RECLA) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+
+################################################################################
+# Begin Source File
+
+SOURCE=.\os_dep.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_OS_DE=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\STAT.H"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_OS_DE=\
+	".\il\PCR_IL.h"\
+	".\mm\PCR_MM.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	".\vd\PCR_VD.h"\
+	
+
+".\Release\os_dep.obj" : $(SOURCE) $(DEP_CPP_OS_DE) "$(INTDIR)"
+
+".\Release\os_dep.sbr" : $(SOURCE) $(DEP_CPP_OS_DE) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_OS_DE=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\STAT.H"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_OS_DE=\
+	".\il\PCR_IL.h"\
+	".\mm\PCR_MM.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	".\vd\PCR_VD.h"\
+	
+
+".\Debug\os_dep.obj" : $(SOURCE) $(DEP_CPP_OS_DE) "$(INTDIR)"
+
+".\Debug\os_dep.sbr" : $(SOURCE) $(DEP_CPP_OS_DE) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\misc.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MISC_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MISC_=\
+	".\il\PCR_IL.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\misc.obj" : $(SOURCE) $(DEP_CPP_MISC_) "$(INTDIR)"
+
+".\Release\misc.sbr" : $(SOURCE) $(DEP_CPP_MISC_) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MISC_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MISC_=\
+	".\il\PCR_IL.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\misc.obj" : $(SOURCE) $(DEP_CPP_MISC_) "$(INTDIR)"
+
+".\Debug\misc.sbr" : $(SOURCE) $(DEP_CPP_MISC_) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\mark_rts.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MARK_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MARK_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\mark_rts.obj" : $(SOURCE) $(DEP_CPP_MARK_) "$(INTDIR)"
+
+".\Release\mark_rts.sbr" : $(SOURCE) $(DEP_CPP_MARK_) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MARK_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MARK_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\mark_rts.obj" : $(SOURCE) $(DEP_CPP_MARK_) "$(INTDIR)"
+
+".\Debug\mark_rts.sbr" : $(SOURCE) $(DEP_CPP_MARK_) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\mach_dep.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MACH_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MACH_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\mach_dep.obj" : $(SOURCE) $(DEP_CPP_MACH_) "$(INTDIR)"
+
+".\Release\mach_dep.sbr" : $(SOURCE) $(DEP_CPP_MACH_) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MACH_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MACH_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\mach_dep.obj" : $(SOURCE) $(DEP_CPP_MACH_) "$(INTDIR)"
+
+".\Debug\mach_dep.sbr" : $(SOURCE) $(DEP_CPP_MACH_) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\headers.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_HEADE=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_HEADE=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\headers.obj" : $(SOURCE) $(DEP_CPP_HEADE) "$(INTDIR)"
+
+".\Release\headers.sbr" : $(SOURCE) $(DEP_CPP_HEADE) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_HEADE=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_HEADE=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\headers.obj" : $(SOURCE) $(DEP_CPP_HEADE) "$(INTDIR)"
+
+".\Debug\headers.sbr" : $(SOURCE) $(DEP_CPP_HEADE) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\alloc.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_ALLOC=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_ALLOC=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\alloc.obj" : $(SOURCE) $(DEP_CPP_ALLOC) "$(INTDIR)"
+
+".\Release\alloc.sbr" : $(SOURCE) $(DEP_CPP_ALLOC) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_ALLOC=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_ALLOC=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\alloc.obj" : $(SOURCE) $(DEP_CPP_ALLOC) "$(INTDIR)"
+
+".\Debug\alloc.sbr" : $(SOURCE) $(DEP_CPP_ALLOC) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\allchblk.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_ALLCH=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_ALLCH=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\allchblk.obj" : $(SOURCE) $(DEP_CPP_ALLCH) "$(INTDIR)"
+
+".\Release\allchblk.sbr" : $(SOURCE) $(DEP_CPP_ALLCH) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_ALLCH=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_ALLCH=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\allchblk.obj" : $(SOURCE) $(DEP_CPP_ALLCH) "$(INTDIR)"
+
+".\Debug\allchblk.sbr" : $(SOURCE) $(DEP_CPP_ALLCH) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\stubborn.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_STUBB=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_STUBB=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\stubborn.obj" : $(SOURCE) $(DEP_CPP_STUBB) "$(INTDIR)"
+
+".\Release\stubborn.sbr" : $(SOURCE) $(DEP_CPP_STUBB) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_STUBB=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_STUBB=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\stubborn.obj" : $(SOURCE) $(DEP_CPP_STUBB) "$(INTDIR)"
+
+".\Debug\stubborn.sbr" : $(SOURCE) $(DEP_CPP_STUBB) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\obj_map.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_OBJ_M=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_OBJ_M=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\obj_map.obj" : $(SOURCE) $(DEP_CPP_OBJ_M) "$(INTDIR)"
+
+".\Release\obj_map.sbr" : $(SOURCE) $(DEP_CPP_OBJ_M) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_OBJ_M=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_OBJ_M=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\obj_map.obj" : $(SOURCE) $(DEP_CPP_OBJ_M) "$(INTDIR)"
+
+".\Debug\obj_map.sbr" : $(SOURCE) $(DEP_CPP_OBJ_M) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\new_hblk.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_NEW_H=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_NEW_H=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\new_hblk.obj" : $(SOURCE) $(DEP_CPP_NEW_H) "$(INTDIR)"
+
+".\Release\new_hblk.sbr" : $(SOURCE) $(DEP_CPP_NEW_H) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_NEW_H=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_NEW_H=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\new_hblk.obj" : $(SOURCE) $(DEP_CPP_NEW_H) "$(INTDIR)"
+
+".\Debug\new_hblk.sbr" : $(SOURCE) $(DEP_CPP_NEW_H) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\mark.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MARK_C=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MARK_C=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\mark.obj" : $(SOURCE) $(DEP_CPP_MARK_C) "$(INTDIR)"
+
+".\Release\mark.sbr" : $(SOURCE) $(DEP_CPP_MARK_C) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MARK_C=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MARK_C=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\mark.obj" : $(SOURCE) $(DEP_CPP_MARK_C) "$(INTDIR)"
+
+".\Debug\mark.sbr" : $(SOURCE) $(DEP_CPP_MARK_C) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\malloc.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MALLO=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MALLO=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\malloc.obj" : $(SOURCE) $(DEP_CPP_MALLO) "$(INTDIR)"
+
+".\Release\malloc.sbr" : $(SOURCE) $(DEP_CPP_MALLO) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MALLO=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MALLO=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\malloc.obj" : $(SOURCE) $(DEP_CPP_MALLO) "$(INTDIR)"
+
+".\Debug\malloc.sbr" : $(SOURCE) $(DEP_CPP_MALLO) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\mallocx.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_MALLX=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MALLX=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\mallocx.obj" : $(SOURCE) $(DEP_CPP_MALLX) "$(INTDIR)"
+
+".\Release\mallocx.sbr" : $(SOURCE) $(DEP_CPP_MALLX) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_MALLX=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_MALLX=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\mallocx.obj" : $(SOURCE) $(DEP_CPP_MALLX) "$(INTDIR)"
+
+".\Debug\mallocx.sbr" : $(SOURCE) $(DEP_CPP_MALLX) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\finalize.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_FINAL=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_FINAL=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\finalize.obj" : $(SOURCE) $(DEP_CPP_FINAL) "$(INTDIR)"
+
+".\Release\finalize.sbr" : $(SOURCE) $(DEP_CPP_FINAL) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_FINAL=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_FINAL=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\finalize.obj" : $(SOURCE) $(DEP_CPP_FINAL) "$(INTDIR)"
+
+".\Debug\finalize.sbr" : $(SOURCE) $(DEP_CPP_FINAL) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\dbg_mlc.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_DBG_M=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_DBG_M=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\dbg_mlc.obj" : $(SOURCE) $(DEP_CPP_DBG_M) "$(INTDIR)"
+
+".\Release\dbg_mlc.sbr" : $(SOURCE) $(DEP_CPP_DBG_M) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_DBG_M=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_DBG_M=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\dbg_mlc.obj" : $(SOURCE) $(DEP_CPP_DBG_M) "$(INTDIR)"
+
+".\Debug\dbg_mlc.sbr" : $(SOURCE) $(DEP_CPP_DBG_M) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\blacklst.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_BLACK=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_BLACK=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\blacklst.obj" : $(SOURCE) $(DEP_CPP_BLACK) "$(INTDIR)"
+
+".\Release\blacklst.sbr" : $(SOURCE) $(DEP_CPP_BLACK) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_BLACK=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_BLACK=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\blacklst.obj" : $(SOURCE) $(DEP_CPP_BLACK) "$(INTDIR)"
+
+".\Debug\blacklst.sbr" : $(SOURCE) $(DEP_CPP_BLACK) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\typd_mlc.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_TYPD_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	".\include\gc_typed.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_TYPD_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\typd_mlc.obj" : $(SOURCE) $(DEP_CPP_TYPD_) "$(INTDIR)"
+
+".\Release\typd_mlc.sbr" : $(SOURCE) $(DEP_CPP_TYPD_) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_TYPD_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	".\include\gc_typed.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_TYPD_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\typd_mlc.obj" : $(SOURCE) $(DEP_CPP_TYPD_) "$(INTDIR)"
+
+".\Debug\typd_mlc.sbr" : $(SOURCE) $(DEP_CPP_TYPD_) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\ptr_chck.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_PTR_C=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_PTR_C=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\ptr_chck.obj" : $(SOURCE) $(DEP_CPP_PTR_C) "$(INTDIR)"
+
+".\Release\ptr_chck.sbr" : $(SOURCE) $(DEP_CPP_PTR_C) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_PTR_C=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_pmark.h"\
+	".\include\gc_mark.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_PTR_C=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\ptr_chck.obj" : $(SOURCE) $(DEP_CPP_PTR_C) "$(INTDIR)"
+
+".\Debug\ptr_chck.sbr" : $(SOURCE) $(DEP_CPP_PTR_C) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\dyn_load.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_DYN_L=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\STAT.H"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_DYN_L=\
+	".\il\PCR_IL.h"\
+	".\mm\PCR_MM.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\dyn_load.obj" : $(SOURCE) $(DEP_CPP_DYN_L) "$(INTDIR)"
+
+".\Release\dyn_load.sbr" : $(SOURCE) $(DEP_CPP_DYN_L) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_DYN_L=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\STAT.H"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_DYN_L=\
+	".\il\PCR_IL.h"\
+	".\mm\PCR_MM.h"\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\dyn_load.obj" : $(SOURCE) $(DEP_CPP_DYN_L) "$(INTDIR)"
+
+".\Debug\dyn_load.sbr" : $(SOURCE) $(DEP_CPP_DYN_L) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\win32_threads.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_WIN32=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_WIN32=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\win32_threads.obj" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+".\Release\win32_threads.sbr" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_WIN32=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_WIN32=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\win32_threads.obj" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+".\Debug\win32_threads.sbr" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\extra\msvc_dbg.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_WIN32=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	".\include\private\msvc_dbg.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_WIN32=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\msvc_dbg.obj" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+".\Release\msvc_dbg.sbr" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_WIN32=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	".\include\private\msvc_dbg.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_WIN32=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\msvc_dbg.obj" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+".\Debug\msvc_dbg.sbr" : $(SOURCE) $(DEP_CPP_WIN32) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\checksums.c
+
+!IF  "$(CFG)" == "gc - Win32 Release"
+
+DEP_CPP_CHECK=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_CHECK=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Release\checksums.obj" : $(SOURCE) $(DEP_CPP_CHECK) "$(INTDIR)"
+
+".\Release\checksums.sbr" : $(SOURCE) $(DEP_CPP_CHECK) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gc - Win32 Debug"
+
+DEP_CPP_CHECK=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_CHECK=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+".\Debug\checksums.obj" : $(SOURCE) $(DEP_CPP_CHECK) "$(INTDIR)"
+
+".\Debug\checksums.sbr" : $(SOURCE) $(DEP_CPP_CHECK) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+# End Target
+################################################################################
+# Begin Target
+
+# Name "gctest - Win32 Release"
+# Name "gctest - Win32 Debug"
+
+!IF  "$(CFG)" == "gctest - Win32 Release"
+
+!ELSEIF  "$(CFG)" == "gctest - Win32 Debug"
+
+!ENDIF 
+
+################################################################################
+# Begin Project Dependency
+
+# Project_Dep_Name "gc"
+
+!IF  "$(CFG)" == "gctest - Win32 Release"
+
+"gc - Win32 Release" : 
+   $(MAKE) /$(MAKEFLAGS) /F ".\gc.mak" CFG="gc - Win32 Release" 
+
+!ELSEIF  "$(CFG)" == "gctest - Win32 Debug"
+
+"gc - Win32 Debug" : 
+   $(MAKE) /$(MAKEFLAGS) /F ".\gc.mak" CFG="gc - Win32 Debug" 
+
+!ENDIF 
+
+# End Project Dependency
+################################################################################
+# Begin Source File
+
+SOURCE=.\tests\test.c
+DEP_CPP_TEST_=\
+	".\include\private\gcconfig.h"\
+	".\include\gc.h"\
+	".\include\private\gc_hdrs.h"\
+	".\include\private\gc_priv.h"\
+	".\include\gc_typed.h"\
+	{$(INCLUDE)}"\sys\TYPES.H"\
+	
+NODEP_CPP_TEST_=\
+	".\th\PCR_Th.h"\
+	".\th\PCR_ThCrSec.h"\
+	".\th\PCR_ThCtl.h"\
+	
+
+!IF  "$(CFG)" == "gctest - Win32 Release"
+
+
+".\gctest\Release\test.obj" : $(SOURCE) $(DEP_CPP_TEST_) "$(INTDIR)"
+
+
+!ELSEIF  "$(CFG)" == "gctest - Win32 Debug"
+
+
+".\gctest\Debug\test.obj" : $(SOURCE) $(DEP_CPP_TEST_) "$(INTDIR)"
+
+".\gctest\Debug\test.sbr" : $(SOURCE) $(DEP_CPP_TEST_) "$(INTDIR)"
+
+
+!ENDIF 
+
+# End Source File
+# End Target
+################################################################################
+# Begin Target
+
+# Name "cord - Win32 Release"
+# Name "cord - Win32 Debug"
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+!ENDIF 
+
+################################################################################
+# Begin Project Dependency
+
+# Project_Dep_Name "gc"
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+"gc - Win32 Release" : 
+   $(MAKE) /$(MAKEFLAGS) /F ".\gc.mak" CFG="gc - Win32 Release" 
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+"gc - Win32 Debug" : 
+   $(MAKE) /$(MAKEFLAGS) /F ".\gc.mak" CFG="gc - Win32 Debug" 
+
+!ENDIF 
+
+# End Project Dependency
+################################################################################
+# Begin Source File
+
+SOURCE=.\cord\de_win.c
+DEP_CPP_DE_WI=\
+	".\include\cord.h"\
+	".\cord\de_cmds.h"\
+	".\cord\de_win.h"\
+	".\include\private\cord_pos.h"\
+	
+NODEP_CPP_DE_WI=\
+	".\include\gc.h"\
+	
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+
+".\cord\Release\de_win.obj" : $(SOURCE) $(DEP_CPP_DE_WI) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+
+".\cord\Debug\de_win.obj" : $(SOURCE) $(DEP_CPP_DE_WI) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\cord\de.c
+DEP_CPP_DE_C2e=\
+	".\include\cord.h"\
+	".\cord\de_cmds.h"\
+	".\cord\de_win.h"\
+	".\include\private\cord_pos.h"\
+	
+NODEP_CPP_DE_C2e=\
+	".\include\gc.h"\
+	
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+
+".\cord\Release\de.obj" : $(SOURCE) $(DEP_CPP_DE_C2e) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+
+".\cord\Debug\de.obj" : $(SOURCE) $(DEP_CPP_DE_C2e) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\cord\cordxtra.c
+DEP_CPP_CORDX=\
+	".\include\cord.h"\
+	".\include\ec.h"\
+	".\include\private\cord_pos.h"\
+	
+NODEP_CPP_CORDX=\
+	".\include\gc.h"\
+	
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+
+".\cord\Release\cordxtra.obj" : $(SOURCE) $(DEP_CPP_CORDX) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+
+".\cord\Debug\cordxtra.obj" : $(SOURCE) $(DEP_CPP_CORDX) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\cord\cordbscs.c
+DEP_CPP_CORDB=\
+	".\include\cord.h"\
+	".\include\private\cord_pos.h"\
+	
+NODEP_CPP_CORDB=\
+	".\include\gc.h"\
+	
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+
+".\cord\Release\cordbscs.obj" : $(SOURCE) $(DEP_CPP_CORDB) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+
+".\cord\Debug\cordbscs.obj" : $(SOURCE) $(DEP_CPP_CORDB) "$(INTDIR)"
+   $(CPP) $(CPP_PROJ) $(SOURCE)
+
+
+!ENDIF 
+
+# End Source File
+################################################################################
+# Begin Source File
+
+SOURCE=.\cord\de_win.RC
+
+!IF  "$(CFG)" == "cord - Win32 Release"
+
+
+".\cord\Release\de_win.res" : $(SOURCE) "$(INTDIR)"
+   $(RSC) /l 0x809 /fo"$(INTDIR)/de_win.res" /i "cord" /d "NDEBUG" $(SOURCE)
+
+
+!ELSEIF  "$(CFG)" == "cord - Win32 Debug"
+
+
+".\cord\Debug\de_win.res" : $(SOURCE) "$(INTDIR)"
+   $(RSC) /l 0x809 /fo"$(INTDIR)/de_win.res" /i "cord" /d "_DEBUG" $(SOURCE)
+
+
+!ENDIF 
+
+# End Source File
+# End Target
+# End Project
+################################################################################
diff --git a/src/gc/bdwgc/unused/gc_cpp.cc b/src/gc/bdwgc/unused/gc_cpp.cc
new file mode 100644
index 0000000..a1da827
--- /dev/null
+++ b/src/gc/bdwgc/unused/gc_cpp.cc
@@ -0,0 +1,79 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ *     Last modified on Sat Nov 19 19:31:14 PST 1994 by ellis
+ *
+ * Permission is hereby granted to copy this code for any purpose,
+ * provided the above notices are retained on all copies.
+ */
+
+/*************************************************************************
+This implementation module for gc_c++.h provides an implementation of
+the global operators "new" and "delete" that calls the Boehm
+allocator.  All objects allocated by this implementation will be
+uncollectible but part of the root set of the collector.
+
+You should ensure (using implementation-dependent techniques) that the
+linker finds this module before the library that defines the default
+built-in "new" and "delete".
+
+Authors: John R. Ellis and Jesse Hull
+
+**************************************************************************/
+
+# ifdef HAVE_CONFIG_H
+#   include "private/config.h"
+# endif
+
+# ifndef GC_BUILD
+#   define GC_BUILD
+# endif
+
+#include "gc_cpp.h"
+
+void* operator new( size_t size ) {
+    return GC_MALLOC_UNCOLLECTABLE( size );}
+
+#if !defined(__CYGWIN__)
+  void operator delete( void* obj ) {
+    GC_FREE( obj );
+  }
+#endif /* !__CYGWIN__ */
+
+#ifdef GC_OPERATOR_NEW_ARRAY
+
+void* operator new[]( size_t size ) {
+    return GC_MALLOC_UNCOLLECTABLE( size );}
+
+void operator delete[]( void* obj ) {
+    GC_FREE( obj );}
+
+#endif /* GC_OPERATOR_NEW_ARRAY */
+
+#ifdef _MSC_VER
+
+// This new operator is used by VC++ in case of Debug builds !
+void* operator new( size_t size,
+                          int ,//nBlockUse,
+                          const char * szFileName,
+                          int nLine )
+{
+#ifndef GC_DEBUG
+        return GC_malloc_uncollectable( size );
+#else
+        return GC_debug_malloc_uncollectable(size, szFileName, nLine);
+#endif
+}
+
+#if _MSC_VER > 1020
+// This new operator is used by VC++ 7.0 and later in Debug builds.
+void* operator new[](size_t size, int nBlockUse, const char* szFileName, int nLine)
+{
+    return operator new(size, nBlockUse, szFileName, nLine);
+}
+#endif
+
+#endif /* _MSC_VER */
diff --git a/src/gc/bdwgc/unused/gc_cpp.cpp b/src/gc/bdwgc/unused/gc_cpp.cpp
new file mode 100644
index 0000000..f6bd95e
--- /dev/null
+++ b/src/gc/bdwgc/unused/gc_cpp.cpp
@@ -0,0 +1,2 @@
+// Visual C++ seems to prefer a .cpp extension to .cc
+#include "gc_cpp.cc"
diff --git a/src/gc/bdwgc/unused/real_malloc.c b/src/gc/bdwgc/unused/real_malloc.c
new file mode 100644
index 0000000..39dab86
--- /dev/null
+++ b/src/gc/bdwgc/unused/real_malloc.c
@@ -0,0 +1,44 @@
+/*
+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers
+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+# ifdef HAVE_CONFIG_H
+#   include "private/config.h"
+# endif
+
+# ifdef PCR
+/*
+ * This definition should go in its own file that includes no other
+ * header files.  Otherwise, we risk not getting the underlying system
+ * malloc.
+ */
+# define PCR_NO_RENAME
+
+#ifdef NAUT
+# include <nautilus/mm.h>
+#else 
+# include <stdlib.h>
+#endif
+
+void * real_malloc(size_t size)
+{
+    return(malloc(size));
+}
+
+# else
+
+extern int GC_quiet;
+        /* ANSI C doesn't allow translation units to be empty.  */
+        /* So we guarantee this one is nonempty.                */
+
+#endif /* PCR */
diff --git a/src/gc/bdwgc/unused/win32_threads.c b/src/gc/bdwgc/unused/win32_threads.c
new file mode 100644
index 0000000..df487d0
--- /dev/null
+++ b/src/gc/bdwgc/unused/win32_threads.c
@@ -0,0 +1,2774 @@
+/*
+ * Copyright (c) 1994 by Xerox Corporation.  All rights reserved.
+ * Copyright (c) 1996 by Silicon Graphics.  All rights reserved.
+ * Copyright (c) 1998 by Fergus Henderson.  All rights reserved.
+ * Copyright (c) 2000-2008 by Hewlett-Packard Development Company.
+ * All rights reserved.
+ *
+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED
+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
+ *
+ * Permission is hereby granted to use or copy this program
+ * for any purpose,  provided the above notices are retained on all copies.
+ * Permission to modify the code and to distribute modified code is granted,
+ * provided the above notices are retained, and a notice that the code was
+ * modified is included with the above copyright notice.
+ */
+
+#include "private/gc_priv.h"
+
+#if defined(GC_WIN32_THREADS)
+
+#ifndef WIN32_LEAN_AND_MEAN
+# define WIN32_LEAN_AND_MEAN 1
+#endif
+#define NOSERVICE
+#include <windows.h>
+
+#ifdef THREAD_LOCAL_ALLOC
+# include "private/thread_local_alloc.h"
+#endif /* THREAD_LOCAL_ALLOC */
+
+/* Allocation lock declarations.        */
+#if !defined(USE_PTHREAD_LOCKS)
+  GC_INNER CRITICAL_SECTION GC_allocate_ml;
+  GC_INNER DWORD GC_lock_holder = NO_THREAD;
+        /* Thread id for current holder of allocation lock */
+#else
+  GC_INNER pthread_mutex_t GC_allocate_ml = PTHREAD_MUTEX_INITIALIZER;
+  GC_INNER unsigned long GC_lock_holder = NO_THREAD;
+#endif
+
+#ifdef GC_PTHREADS
+# include <errno.h> /* for EAGAIN */
+
+ /* Cygwin-specific forward decls */
+# undef pthread_create
+# undef pthread_join
+# undef pthread_detach
+
+# ifndef GC_NO_PTHREAD_SIGMASK
+#   undef pthread_sigmask
+# endif
+
+  STATIC void * GC_pthread_start(void * arg);
+  STATIC void GC_thread_exit_proc(void *arg);
+
+# include <pthread.h>
+# ifdef CAN_HANDLE_FORK
+#   include <unistd.h>
+# endif
+
+#else
+
+# undef CreateThread
+# undef ExitThread
+# undef _beginthreadex
+# undef _endthreadex
+
+# ifndef MSWINCE
+#   include <process.h>  /* For _beginthreadex, _endthreadex */
+#   include <errno.h> /* for errno, EAGAIN */
+# endif
+
+#endif
+
+/* DllMain-based thread registration is currently incompatible  */
+/* with thread-local allocation, pthreads and WinCE.            */
+#if defined(GC_DLL) && !defined(GC_NO_THREADS_DISCOVERY) && !defined(MSWINCE) \
+        && !defined(THREAD_LOCAL_ALLOC) && !defined(GC_PTHREADS)
+# include "atomic_ops.h"
+
+  /* This code operates in two distinct modes, depending on     */
+  /* the setting of GC_win32_dll_threads.                       */
+  /* If GC_win32_dll_threads is set, all threads in the process */
+  /* are implicitly registered with the GC by DllMain.          */
+  /* No explicit registration is required, and attempts at      */
+  /* explicit registration are ignored.  This mode is           */
+  /* very different from the Posix operation of the collector.  */
+  /* In this mode access to the thread table is lock-free.      */
+  /* Hence there is a static limit on the number of threads.    */
+
+# ifdef GC_DISCOVER_TASK_THREADS
+    /* GC_DISCOVER_TASK_THREADS should be used if DllMain-based */
+    /* thread registration is required but it is impossible to  */
+    /* call GC_use_threads_discovery before other GC routines.  */
+#   define GC_win32_dll_threads TRUE
+# else
+    STATIC GC_bool GC_win32_dll_threads = FALSE;
+    /* GC_win32_dll_threads must be set (if needed) at the      */
+    /* application initialization time, i.e. before any         */
+    /* collector or thread calls.  We make it a "dynamic"       */
+    /* option only to avoid multiple library versions.          */
+# endif
+
+#else
+  /* If GC_win32_dll_threads is FALSE (or the collector is      */
+  /* built without GC_DLL defined), things operate in a way     */
+  /* that is very similar to Posix platforms, and new threads   */
+  /* must be registered with the collector, e.g. by using       */
+  /* preprocessor-based interception of the thread primitives.  */
+  /* In this case, we use a real data structure for the thread  */
+  /* table.  Note that there is no equivalent of linker-based   */
+  /* call interception, since we don't have ELF-like            */
+  /* facilities.  The Windows analog appears to be "API         */
+  /* hooking", which really seems to be a standard way to       */
+  /* do minor binary rewriting (?).  I'd prefer not to have     */
+  /* the basic collector rely on such facilities, but an        */
+  /* optional package that intercepts thread calls this way     */
+  /* would probably be nice.                                    */
+# ifndef GC_NO_THREADS_DISCOVERY
+#   define GC_NO_THREADS_DISCOVERY
+# endif
+# define GC_win32_dll_threads FALSE
+# undef MAX_THREADS
+# define MAX_THREADS 1 /* dll_thread_table[] is always empty.   */
+#endif /* GC_NO_THREADS_DISCOVERY */
+
+/* We have two versions of the thread table.  Which one */
+/* we us depends on whether or not GC_win32_dll_threads */
+/* is set.  Note that before initialization, we don't   */
+/* add any entries to either table, even if DllMain is  */
+/* called.  The main thread will be added on            */
+/* initialization.                                      */
+
+/* The type of the first argument to InterlockedExchange.       */
+/* Documented to be LONG volatile *, but at least gcc likes     */
+/* this better.                                                 */
+typedef LONG * IE_t;
+
+STATIC GC_bool GC_thr_initialized = FALSE;
+
+GC_INNER GC_bool GC_need_to_lock = FALSE;
+
+static GC_bool parallel_initialized = FALSE;
+
+/* GC_use_threads_discovery() is currently incompatible with pthreads   */
+/* and WinCE.  It might be possible to get DllMain-based thread         */
+/* registration to work with Cygwin, but if you try it then you are on  */
+/* your own.                                                            */
+GC_API void GC_CALL GC_use_threads_discovery(void)
+{
+# ifdef GC_NO_THREADS_DISCOVERY
+    ABORT("GC DllMain-based thread registration unsupported");
+# else
+    /* Turn on GC_win32_dll_threads. */
+    GC_ASSERT(!parallel_initialized);
+#   ifndef GC_DISCOVER_TASK_THREADS
+      GC_win32_dll_threads = TRUE;
+#   endif
+    GC_init_parallel();
+# endif
+}
+
+STATIC DWORD GC_main_thread = 0;
+
+#define ADDR_LIMIT ((ptr_t)(word)-1)
+
+struct GC_Thread_Rep {
+  union {
+#   ifndef GC_NO_THREADS_DISCOVERY
+      AO_t in_use;      /* Updated without lock.                */
+                        /* We assert that unused                */
+                        /* entries have invalid ids of          */
+                        /* zero and zero stack fields.          */
+                        /* Used only with GC_win32_dll_threads. */
+#   endif
+    struct GC_Thread_Rep * next;
+                        /* Hash table link without              */
+                        /* GC_win32_dll_threads.                */
+                        /* More recently allocated threads      */
+                        /* with a given pthread id come         */
+                        /* first.  (All but the first are       */
+                        /* guaranteed to be dead, but we may    */
+                        /* not yet have registered the join.)   */
+  } tm; /* table_management */
+  DWORD id;
+
+# ifdef MSWINCE
+    /* According to MSDN specs for WinCE targets:                       */
+    /* - DuplicateHandle() is not applicable to thread handles; and     */
+    /* - the value returned by GetCurrentThreadId() could be used as    */
+    /* a "real" thread handle (for SuspendThread(), ResumeThread() and  */
+    /* GetThreadContext()).                                             */
+#   define THREAD_HANDLE(t) (HANDLE)(word)(t)->id
+# else
+    HANDLE handle;
+#   define THREAD_HANDLE(t) (t)->handle
+# endif
+
+  ptr_t stack_base;     /* The cold end of the stack.   */
+                        /* 0 ==> entry not valid.       */
+                        /* !in_use ==> stack_base == 0  */
+  ptr_t last_stack_min; /* Last known minimum (hottest) address */
+                        /* in stack or ADDR_LIMIT if unset      */
+# ifdef IA64
+    ptr_t backing_store_end;
+    ptr_t backing_store_ptr;
+# endif
+
+  ptr_t thread_blocked_sp;      /* Protected by GC lock.                */
+                                /* NULL value means thread unblocked.   */
+                                /* If set to non-NULL, thread will      */
+                                /* acquire GC lock before doing any     */
+                                /* pointer manipulations.  Thus it does */
+                                /* not need to stop this thread.        */
+
+  struct GC_traced_stack_sect_s *traced_stack_sect;
+                                /* Points to the "stack section" data   */
+                                /* held in stack by the innermost       */
+                                /* GC_call_with_gc_active() of this     */
+                                /* thread.  May be NULL.                */
+
+  unsigned short finalizer_skipped;
+  unsigned char finalizer_nested;
+                                /* Used by GC_check_finalizer_nested()  */
+                                /* to minimize the level of recursion   */
+                                /* when a client finalizer allocates    */
+                                /* memory (initially both are 0).       */
+
+  unsigned char suspended; /* really of GC_bool type */
+
+# ifdef GC_PTHREADS
+    unsigned char flags;        /* Protected by GC lock.                */
+#   define FINISHED 1           /* Thread has exited.                   */
+#   define DETACHED 2           /* Thread is intended to be detached.   */
+#   define KNOWN_FINISHED(t) (((t) -> flags) & FINISHED)
+    pthread_t pthread_id;
+    void *status;  /* hold exit value until join in case it's a pointer */
+# else
+#   define KNOWN_FINISHED(t) 0
+# endif
+
+# ifdef THREAD_LOCAL_ALLOC
+    struct thread_local_freelists tlfs;
+# endif
+};
+
+typedef struct GC_Thread_Rep * GC_thread;
+typedef volatile struct GC_Thread_Rep * GC_vthread;
+
+#ifndef GC_NO_THREADS_DISCOVERY
+  /* We assumed that volatile ==> memory ordering, at least among       */
+  /* volatiles.  This code should consistently use atomic_ops.          */
+  STATIC volatile GC_bool GC_please_stop = FALSE;
+#elif defined(GC_ASSERTIONS)
+  STATIC GC_bool GC_please_stop = FALSE;
+#endif
+
+/*
+ * We track thread attachments while the world is supposed to be stopped.
+ * Unfortunately, we can't stop them from starting, since blocking in
+ * DllMain seems to cause the world to deadlock.  Thus we have to recover
+ * If we notice this in the middle of marking.
+ */
+
+#ifndef GC_NO_THREADS_DISCOVERY
+  STATIC AO_t GC_attached_thread = FALSE;
+#endif
+
+#if !defined(__GNUC__)
+  /* Return TRUE if an thread was attached since we last asked or */
+  /* since GC_attached_thread was explicitly reset.               */
+  GC_bool GC_started_thread_while_stopped(void)
+  {
+#   ifndef GC_NO_THREADS_DISCOVERY
+      AO_t result;
+
+      if (GC_win32_dll_threads) {
+        AO_nop_full();  /* Prior heap reads need to complete earlier. */
+        result = AO_load(&GC_attached_thread);
+        if (result) {
+          AO_store(&GC_attached_thread, FALSE);
+          return TRUE;
+        }
+      }
+#   endif
+    return FALSE;
+  }
+#endif /* !__GNUC__ */
+
+/* Thread table used if GC_win32_dll_threads is set.    */
+/* This is a fixed size array.                          */
+/* Since we use runtime conditionals, both versions     */
+/* are always defined.                                  */
+# ifndef MAX_THREADS
+#   define MAX_THREADS 512
+# endif
+
+/* Things may get quite slow for large numbers of threads,      */
+/* since we look them up with sequential search.                */
+volatile struct GC_Thread_Rep dll_thread_table[MAX_THREADS];
+
+STATIC volatile LONG GC_max_thread_index = 0;
+                        /* Largest index in dll_thread_table    */
+                        /* that was ever used.                  */
+
+/* And now the version used if GC_win32_dll_threads is not set. */
+/* This is a chained hash table, with much of the code borrowed */
+/* From the Posix implementation.                               */
+#ifndef THREAD_TABLE_SZ
+# define THREAD_TABLE_SZ 256    /* Power of 2 (for speed). */
+#endif
+#define THREAD_TABLE_INDEX(id) (((word)(id) >> 2) % THREAD_TABLE_SZ)
+STATIC GC_thread GC_threads[THREAD_TABLE_SZ];
+
+/* It may not be safe to allocate when we register the first thread.    */
+/* Thus we allocated one statically.                                    */
+static struct GC_Thread_Rep first_thread;
+static GC_bool first_thread_used = FALSE;
+
+/* Add a thread to GC_threads.  We assume it wasn't already there.      */
+/* Caller holds allocation lock.                                        */
+/* Unlike the pthreads version, the id field is set by the caller.      */
+STATIC GC_thread GC_new_thread(DWORD id)
+{
+  word hv = THREAD_TABLE_INDEX(id);
+  GC_thread result;
+
+  GC_ASSERT(I_HOLD_LOCK());
+  if (!first_thread_used) {
+    result = &first_thread;
+    first_thread_used = TRUE;
+  } else {
+    GC_ASSERT(!GC_win32_dll_threads);
+    result = (struct GC_Thread_Rep *)
+                GC_INTERNAL_MALLOC(sizeof(struct GC_Thread_Rep), NORMAL);
+    /* result can be NULL */
+    if (result == 0) return(0);
+  }
+  /* result -> id = id; Done by caller.       */
+  result -> tm.next = GC_threads[hv];
+  GC_threads[hv] = result;
+# ifdef GC_PTHREADS
+    GC_ASSERT(result -> flags == 0);
+# endif
+  GC_ASSERT(result -> thread_blocked_sp == NULL);
+  return(result);
+}
+
+STATIC GC_bool GC_in_thread_creation = FALSE;
+                                /* Protected by allocation lock. */
+
+GC_INLINE void GC_record_stack_base(GC_vthread me,
+                                    const struct GC_stack_base *sb)
+{
+  me -> stack_base = sb -> mem_base;
+# ifdef IA64
+    me -> backing_store_end = sb -> reg_base;
+# endif
+  if (me -> stack_base == NULL)
+    ABORT("Bad stack base in GC_register_my_thread");
+}
+
+/* This may be called from DllMain, and hence operates under unusual    */
+/* constraints.  In particular, it must be lock-free if                 */
+/* GC_win32_dll_threads is set.  Always called from the thread being    */
+/* added.  If GC_win32_dll_threads is not set, we already hold the      */
+/* allocation lock except possibly during single-threaded startup code. */
+STATIC GC_thread GC_register_my_thread_inner(const struct GC_stack_base *sb,
+                                             DWORD thread_id)
+{
+  GC_vthread me;
+
+  /* The following should be a no-op according to the win32     */
+  /* documentation.  There is empirical evidence that it        */
+  /* isn't.             - HB                                    */
+# if defined(MPROTECT_VDB)
+    if (GC_incremental
+#       ifdef GWW_VDB
+          && !GC_gww_dirty_init()
+#       endif
+        )
+      GC_set_write_fault_handler();
+# endif
+
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      int i;
+      /* It appears to be unsafe to acquire a lock here, since this     */
+      /* code is apparently not preemptible on some systems.            */
+      /* (This is based on complaints, not on Microsoft's official      */
+      /* documentation, which says this should perform "only simple     */
+      /* initialization tasks".)                                        */
+      /* Hence we make do with nonblocking synchronization.             */
+      /* It has been claimed that DllMain is really only executed with  */
+      /* a particular system lock held, and thus careful use of locking */
+      /* around code that doesn't call back into the system libraries   */
+      /* might be OK.  But this hasn't been tested across all win32     */
+      /* variants.                                                      */
+                  /* cast away volatile qualifier */
+      for (i = 0;
+           InterlockedExchange((void*)&dll_thread_table[i].tm.in_use, 1) != 0;
+           i++) {
+        /* Compare-and-swap would make this cleaner, but that's not     */
+        /* supported before Windows 98 and NT 4.0.  In Windows 2000,    */
+        /* InterlockedExchange is supposed to be replaced by            */
+        /* InterlockedExchangePointer, but that's not really what I     */
+        /* want here.                                                   */
+        /* FIXME: We should eventually declare Win95 dead and use AO_   */
+        /* primitives here.                                             */
+        if (i == MAX_THREADS - 1)
+          ABORT("Too many threads");
+      }
+      /* Update GC_max_thread_index if necessary.  The following is     */
+      /* safe, and unlike CompareExchange-based solutions seems to work */
+      /* on all Windows95 and later platforms.                          */
+      /* Unfortunately, GC_max_thread_index may be temporarily out of   */
+      /* bounds, so readers have to compensate.                         */
+      while (i > GC_max_thread_index) {
+        InterlockedIncrement((IE_t)&GC_max_thread_index);
+      }
+      if (GC_max_thread_index >= MAX_THREADS) {
+        /* We overshot due to simultaneous increments.  */
+        /* Setting it to MAX_THREADS-1 is always safe.  */
+        GC_max_thread_index = MAX_THREADS - 1;
+      }
+      me = dll_thread_table + i;
+    } else
+# endif
+  /* else */ /* Not using DllMain */ {
+    GC_ASSERT(I_HOLD_LOCK());
+    GC_in_thread_creation = TRUE; /* OK to collect from unknown thread. */
+    me = GC_new_thread(thread_id);
+    GC_in_thread_creation = FALSE;
+    if (me == 0)
+      ABORT("Failed to allocate memory for thread registering");
+  }
+# ifdef GC_PTHREADS
+    /* me can be NULL -> segfault */
+    me -> pthread_id = pthread_self();
+# endif
+# ifndef MSWINCE
+    /* GetCurrentThread() returns a pseudohandle (a const value).       */
+    if (!DuplicateHandle(GetCurrentProcess(), GetCurrentThread(),
+                        GetCurrentProcess(),
+                        (HANDLE*)&(me -> handle),
+                        0 /* dwDesiredAccess */, FALSE /* bInheritHandle */,
+                        DUPLICATE_SAME_ACCESS)) {
+        if (GC_print_stats)
+          GC_log_printf("DuplicateHandle failed with error code: %d\n",
+                        (int)GetLastError());
+        ABORT("DuplicateHandle failed");
+    }
+# endif
+  me -> last_stack_min = ADDR_LIMIT;
+  GC_record_stack_base(me, sb);
+  /* Up until this point, GC_push_all_stacks considers this thread      */
+  /* invalid.                                                           */
+  /* Up until this point, this entry is viewed as reserved but invalid  */
+  /* by GC_delete_thread.                                               */
+  me -> id = thread_id;
+# if defined(THREAD_LOCAL_ALLOC)
+    GC_init_thread_local((GC_tlfs)(&(me->tlfs)));
+# endif
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      if (GC_please_stop) {
+        AO_store(&GC_attached_thread, TRUE);
+        AO_nop_full(); /* Later updates must become visible after this. */
+      }
+      /* We'd like to wait here, but can't, since waiting in DllMain    */
+      /* provokes deadlocks.                                            */
+      /* Thus we force marking to be restarted instead.                 */
+    } else
+# endif
+  /* else */ {
+    GC_ASSERT(!GC_please_stop);
+        /* Otherwise both we and the thread stopping code would be      */
+        /* holding the allocation lock.                                 */
+  }
+  return (GC_thread)(me);
+}
+
+/*
+ * GC_max_thread_index may temporarily be larger than MAX_THREADS.
+ * To avoid subscript errors, we check on access.
+ */
+GC_INLINE LONG GC_get_max_thread_index(void)
+{
+  LONG my_max = GC_max_thread_index;
+  if (my_max >= MAX_THREADS) return MAX_THREADS - 1;
+  return my_max;
+}
+
+/* Return the GC_thread corresponding to a thread id.  May be called    */
+/* without a lock, but should be called in contexts in which the        */
+/* requested thread cannot be asynchronously deleted, e.g. from the     */
+/* thread itself.                                                       */
+/* This version assumes that either GC_win32_dll_threads is set, or     */
+/* we hold the allocator lock.                                          */
+/* Also used (for assertion checking only) from thread_local_alloc.c.   */
+STATIC GC_thread GC_lookup_thread_inner(DWORD thread_id)
+{
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      int i;
+      LONG my_max = GC_get_max_thread_index();
+      for (i = 0; i <= my_max &&
+                  (!AO_load_acquire(&dll_thread_table[i].tm.in_use)
+                  || dll_thread_table[i].id != thread_id);
+           /* Must still be in_use, since nobody else can store our     */
+           /* thread_id.                                                */
+           i++) {
+        /* empty */
+      }
+      return i <= my_max ? (GC_thread)(dll_thread_table + i) : NULL;
+    } else
+# endif
+  /* else */ {
+    word hv = THREAD_TABLE_INDEX(thread_id);
+    register GC_thread p = GC_threads[hv];
+
+    GC_ASSERT(I_HOLD_LOCK());
+    while (p != 0 && p -> id != thread_id) p = p -> tm.next;
+    return(p);
+  }
+}
+
+#ifdef LINT2
+# define CHECK_LOOKUP_MY_THREAD(me) \
+        if (!(me)) ABORT("GC_lookup_thread_inner(GetCurrentThreadId) failed")
+#else
+# define CHECK_LOOKUP_MY_THREAD(me) /* empty */
+#endif
+
+/* Called by GC_finalize() (in case of an allocation failure observed). */
+/* GC_reset_finalizer_nested() is the same as in pthread_support.c.     */
+GC_INNER void GC_reset_finalizer_nested(void)
+{
+  GC_thread me = GC_lookup_thread_inner(GetCurrentThreadId());
+  CHECK_LOOKUP_MY_THREAD(me);
+  me->finalizer_nested = 0;
+}
+
+/* Checks and updates the thread-local level of finalizers recursion.   */
+/* Returns NULL if GC_invoke_finalizers() should not be called by the   */
+/* collector (to minimize the risk of a deep finalizers recursion),     */
+/* otherwise returns a pointer to the thread-local finalizer_nested.    */
+/* Called by GC_notify_or_invoke_finalizers() only (the lock is held).  */
+/* GC_check_finalizer_nested() is the same as in pthread_support.c.     */
+GC_INNER unsigned char *GC_check_finalizer_nested(void)
+{
+  GC_thread me = GC_lookup_thread_inner(GetCurrentThreadId());
+  unsigned nesting_level;
+  CHECK_LOOKUP_MY_THREAD(me);
+  nesting_level = me->finalizer_nested;
+  if (nesting_level) {
+    /* We are inside another GC_invoke_finalizers().            */
+    /* Skip some implicitly-called GC_invoke_finalizers()       */
+    /* depending on the nesting (recursion) level.              */
+    if (++me->finalizer_skipped < (1U << nesting_level)) return NULL;
+    me->finalizer_skipped = 0;
+  }
+  me->finalizer_nested = (unsigned char)(nesting_level + 1);
+  return &me->finalizer_nested;
+}
+
+#if defined(GC_ASSERTIONS) && defined(THREAD_LOCAL_ALLOC)
+  /* This is called from thread-local GC_malloc(). */
+  GC_bool GC_is_thread_tsd_valid(void *tsd)
+  {
+    GC_thread me;
+    DCL_LOCK_STATE;
+
+    LOCK();
+    me = GC_lookup_thread_inner(GetCurrentThreadId());
+    UNLOCK();
+    return (char *)tsd >= (char *)&me->tlfs
+            && (char *)tsd < (char *)&me->tlfs + sizeof(me->tlfs);
+  }
+#endif /* GC_ASSERTIONS && THREAD_LOCAL_ALLOC */
+
+/* Make sure thread descriptor t is not protected by the VDB            */
+/* implementation.                                                      */
+/* Used to prevent write faults when the world is (partially) stopped,  */
+/* since it may have been stopped with a system lock held, and that     */
+/* lock may be required for fault handling.                             */
+#if defined(MPROTECT_VDB)
+# define UNPROTECT_THREAD(t) \
+    if (!GC_win32_dll_threads && GC_dirty_maintained \
+        && t != &first_thread) { \
+      GC_ASSERT(SMALL_OBJ(GC_size(t))); \
+      GC_remove_protection(HBLKPTR(t), 1, FALSE); \
+    }
+#else
+# define UNPROTECT_THREAD(t)
+#endif
+
+#ifdef CYGWIN32
+# define GC_PTHREAD_PTRVAL(pthread_id) pthread_id
+#elif defined(GC_WIN32_PTHREADS) || defined(GC_PTHREADS_PARAMARK)
+# define GC_PTHREAD_PTRVAL(pthread_id) pthread_id.p
+#endif
+
+/* If a thread has been joined, but we have not yet             */
+/* been notified, then there may be more than one thread        */
+/* in the table with the same win32 id.                         */
+/* This is OK, but we need a way to delete a specific one.      */
+/* Assumes we hold the allocation lock unless                   */
+/* GC_win32_dll_threads is set.                                 */
+/* If GC_win32_dll_threads is set it should be called from the  */
+/* thread being deleted.                                        */
+STATIC void GC_delete_gc_thread(GC_vthread t)
+{
+# ifndef MSWINCE
+    CloseHandle(t->handle);
+# endif
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      /* This is intended to be lock-free.                              */
+      /* It is either called synchronously from the thread being        */
+      /* deleted, or by the joining thread.                             */
+      /* In this branch asynchronous changes to (*t) are possible.      */
+      /* It's not allowed to call GC_printf (and the friends) here,     */
+      /* see GC_stop_world() for the information.                       */
+      t -> stack_base = 0;
+      t -> id = 0;
+#     ifdef GC_PTHREADS
+        GC_PTHREAD_PTRVAL(t->pthread_id) = 0;
+#     endif
+      AO_store_release(&t->tm.in_use, FALSE);
+    } else
+# endif
+  /* else */ {
+    DWORD id = ((GC_thread)t) -> id;
+                /* Cast away volatile qualifier, since we have lock.    */
+    word hv = THREAD_TABLE_INDEX(id);
+    register GC_thread p = GC_threads[hv];
+    register GC_thread prev = 0;
+
+    GC_ASSERT(I_HOLD_LOCK());
+    while (p != (GC_thread)t) {
+      prev = p;
+      p = p -> tm.next;
+    }
+    if (prev == 0) {
+      GC_threads[hv] = p -> tm.next;
+    } else {
+      prev -> tm.next = p -> tm.next;
+    }
+    GC_INTERNAL_FREE(p);
+  }
+}
+
+/* Delete a thread from GC_threads.  We assume it is there.     */
+/* (The code intentionally traps if it wasn't.)  Assumes we     */
+/* hold the allocation lock unless GC_win32_dll_threads is set. */
+/* If GC_win32_dll_threads is set then it should be called from */
+/* the thread being deleted.  It is also safe to delete the     */
+/* main thread (unless GC_win32_dll_threads).                   */
+STATIC void GC_delete_thread(DWORD id)
+{
+  if (GC_win32_dll_threads) {
+    GC_thread t = GC_lookup_thread_inner(id);
+
+    if (0 == t) {
+      WARN("Removing nonexistent thread, id = %" GC_PRIdPTR "\n", id);
+    } else {
+      GC_delete_gc_thread(t);
+    }
+  } else {
+    word hv = THREAD_TABLE_INDEX(id);
+    register GC_thread p = GC_threads[hv];
+    register GC_thread prev = 0;
+
+    GC_ASSERT(I_HOLD_LOCK());
+    while (p -> id != id) {
+      prev = p;
+      p = p -> tm.next;
+    }
+#   ifndef MSWINCE
+      CloseHandle(p->handle);
+#   endif
+    if (prev == 0) {
+      GC_threads[hv] = p -> tm.next;
+    } else {
+      prev -> tm.next = p -> tm.next;
+    }
+    if (p != &first_thread) {
+      GC_INTERNAL_FREE(p);
+    }
+  }
+}
+
+GC_API void GC_CALL GC_allow_register_threads(void)
+{
+  /* Check GC is initialized and the current thread is registered. */
+  GC_ASSERT(GC_lookup_thread_inner(GetCurrentThreadId()) != 0);
+
+# if !defined(GC_NO_THREADS_DISCOVERY) && !defined(PARALLEL_MARK)
+    /* GC_init() doesn't call GC_init_parallel() in this case.  */
+    parallel_initialized = TRUE;
+# endif
+  GC_need_to_lock = TRUE; /* We are multi-threaded now. */
+}
+
+GC_API int GC_CALL GC_register_my_thread(const struct GC_stack_base *sb)
+{
+  GC_thread me;
+  DWORD thread_id = GetCurrentThreadId();
+  DCL_LOCK_STATE;
+
+  if (GC_need_to_lock == FALSE)
+    ABORT("Threads explicit registering is not previously enabled");
+
+  /* We lock here, since we want to wait for an ongoing GC.     */
+  LOCK();
+  me = GC_lookup_thread_inner(thread_id);
+  if (me == 0) {
+#   ifdef GC_PTHREADS
+      me = GC_register_my_thread_inner(sb, thread_id);
+      me -> flags |= DETACHED;
+          /* Treat as detached, since we do not need to worry about     */
+          /* pointer results.                                           */
+#   else
+      GC_register_my_thread_inner(sb, thread_id);
+#   endif
+    UNLOCK();
+    return GC_SUCCESS;
+  } else
+#   ifdef GC_PTHREADS
+      /* else */ if ((me -> flags & FINISHED) != 0) {
+        GC_record_stack_base(me, sb);
+        me -> flags &= ~FINISHED; /* but not DETACHED */
+#       ifdef THREAD_LOCAL_ALLOC
+          GC_init_thread_local((GC_tlfs)(&me->tlfs));
+#       endif
+        UNLOCK();
+        return GC_SUCCESS;
+      } else
+#   endif
+  /* else */ {
+    UNLOCK();
+    return GC_DUPLICATE;
+  }
+}
+
+/* Similar to that in pthread_support.c.        */
+STATIC void GC_wait_for_gc_completion(GC_bool wait_for_all)
+{
+  GC_ASSERT(I_HOLD_LOCK());
+  if (GC_incremental && GC_collection_in_progress()) {
+    word old_gc_no = GC_gc_no;
+
+    /* Make sure that no part of our stack is still on the mark stack,  */
+    /* since it's about to be unmapped.                                 */
+    do {
+      ENTER_GC();
+      GC_in_thread_creation = TRUE;
+      GC_collect_a_little_inner(1);
+      GC_in_thread_creation = FALSE;
+      EXIT_GC();
+
+      UNLOCK();
+      Sleep(0); /* yield */
+      LOCK();
+    } while (GC_incremental && GC_collection_in_progress()
+             && (wait_for_all || old_gc_no == GC_gc_no));
+  }
+}
+
+GC_API int GC_CALL GC_unregister_my_thread(void)
+{
+  DCL_LOCK_STATE;
+
+# ifdef DEBUG_THREADS
+    GC_log_printf("Unregistering thread 0x%lx\n", (long)GetCurrentThreadId());
+# endif
+
+  if (GC_win32_dll_threads) {
+#   if defined(THREAD_LOCAL_ALLOC)
+      /* Can't happen: see GC_use_threads_discovery(). */
+      GC_ASSERT(FALSE);
+#   else
+#     ifdef GC_PTHREADS
+        /* FIXME: If not DETACHED then just set FINISHED. */
+#     endif
+      /* FIXME: Should we just ignore this? */
+      GC_delete_thread(GetCurrentThreadId());
+#   endif
+  } else {
+#   if defined(THREAD_LOCAL_ALLOC) || defined(GC_PTHREADS)
+      GC_thread me;
+#   endif
+    DWORD thread_id = GetCurrentThreadId();
+
+    LOCK();
+    GC_wait_for_gc_completion(FALSE);
+#   if defined(THREAD_LOCAL_ALLOC) || defined(GC_PTHREADS)
+      me = GC_lookup_thread_inner(thread_id);
+      CHECK_LOOKUP_MY_THREAD(me);
+      GC_ASSERT(!KNOWN_FINISHED(me));
+#   endif
+#   if defined(THREAD_LOCAL_ALLOC)
+      GC_destroy_thread_local(&(me->tlfs));
+#   endif
+#   ifdef GC_PTHREADS
+      if ((me -> flags & DETACHED) == 0) {
+        me -> flags |= FINISHED;
+      } else
+#   endif
+    /* else */ {
+      GC_delete_thread(thread_id);
+    }
+    UNLOCK();
+  }
+  return GC_SUCCESS;
+}
+
+/* Wrapper for functions that are likely to block for an appreciable    */
+/* length of time.                                                      */
+
+/* GC_do_blocking_inner() is nearly the same as in pthread_support.c    */
+/*ARGSUSED*/
+GC_INNER void GC_do_blocking_inner(ptr_t data, void * context)
+{
+  struct blocking_data * d = (struct blocking_data *) data;
+  DWORD thread_id = GetCurrentThreadId();
+  GC_thread me;
+# ifdef IA64
+    ptr_t stack_ptr = GC_save_regs_in_stack();
+# endif
+  DCL_LOCK_STATE;
+
+  LOCK();
+  me = GC_lookup_thread_inner(thread_id);
+  CHECK_LOOKUP_MY_THREAD(me);
+  GC_ASSERT(me -> thread_blocked_sp == NULL);
+# ifdef IA64
+    me -> backing_store_ptr = stack_ptr;
+# endif
+  me -> thread_blocked_sp = (ptr_t) &d; /* save approx. sp */
+  /* Save context here if we want to support precise stack marking */
+  UNLOCK();
+  d -> client_data = (d -> fn)(d -> client_data);
+  LOCK();   /* This will block if the world is stopped. */
+  me -> thread_blocked_sp = NULL;
+  UNLOCK();
+}
+
+/* GC_call_with_gc_active() has the opposite to GC_do_blocking()        */
+/* functionality.  It might be called from a user function invoked by   */
+/* GC_do_blocking() to temporarily back allow calling any GC function   */
+/* and/or manipulating pointers to the garbage collected heap.          */
+GC_API void * GC_CALL GC_call_with_gc_active(GC_fn_type fn,
+                                             void * client_data)
+{
+  struct GC_traced_stack_sect_s stacksect;
+  GC_thread me;
+  DCL_LOCK_STATE;
+
+  LOCK();   /* This will block if the world is stopped.         */
+  me = GC_lookup_thread_inner(GetCurrentThreadId());
+  CHECK_LOOKUP_MY_THREAD(me);
+  /* Adjust our stack base value (this could happen unless      */
+  /* GC_get_stack_base() was used which returned GC_SUCCESS).   */
+  GC_ASSERT(me -> stack_base != NULL);
+  if (me -> stack_base < (ptr_t)(&stacksect))
+    me -> stack_base = (ptr_t)(&stacksect);
+
+  if (me -> thread_blocked_sp == NULL) {
+    /* We are not inside GC_do_blocking() - do nothing more.    */
+    UNLOCK();
+    client_data = fn(client_data);
+    /* Prevent treating the above as a tail call.       */
+    GC_noop1((word)(&stacksect));
+    return client_data; /* result */
+  }
+
+  /* Setup new "stack section". */
+  stacksect.saved_stack_ptr = me -> thread_blocked_sp;
+# ifdef IA64
+    /* This is the same as in GC_call_with_stack_base().        */
+    stacksect.backing_store_end = GC_save_regs_in_stack();
+    /* Unnecessarily flushes register stack,    */
+    /* but that probably doesn't hurt.          */
+    stacksect.saved_backing_store_ptr = me -> backing_store_ptr;
+# endif
+  stacksect.prev = me -> traced_stack_sect;
+  me -> thread_blocked_sp = NULL;
+  me -> traced_stack_sect = &stacksect;
+
+  UNLOCK();
+  client_data = fn(client_data);
+  GC_ASSERT(me -> thread_blocked_sp == NULL);
+  GC_ASSERT(me -> traced_stack_sect == &stacksect);
+
+  /* Restore original "stack section".  */
+  LOCK();
+  me -> traced_stack_sect = stacksect.prev;
+# ifdef IA64
+    me -> backing_store_ptr = stacksect.saved_backing_store_ptr;
+# endif
+  me -> thread_blocked_sp = stacksect.saved_stack_ptr;
+  UNLOCK();
+
+  return client_data; /* result */
+}
+
+#ifdef GC_PTHREADS
+
+  /* A quick-and-dirty cache of the mapping between pthread_t   */
+  /* and win32 thread id.                                       */
+# define PTHREAD_MAP_SIZE 512
+  DWORD GC_pthread_map_cache[PTHREAD_MAP_SIZE] = {0};
+# define PTHREAD_MAP_INDEX(pthread_id) \
+                ((NUMERIC_THREAD_ID(pthread_id) >> 5) % PTHREAD_MAP_SIZE)
+        /* It appears pthread_t is really a pointer type ... */
+# define SET_PTHREAD_MAP_CACHE(pthread_id, win32_id) \
+          (GC_pthread_map_cache[PTHREAD_MAP_INDEX(pthread_id)] = (win32_id))
+# define GET_PTHREAD_MAP_CACHE(pthread_id) \
+          GC_pthread_map_cache[PTHREAD_MAP_INDEX(pthread_id)]
+
+  /* Return a GC_thread corresponding to a given pthread_t.     */
+  /* Returns 0 if it's not there.                               */
+  /* We assume that this is only called for pthread ids that    */
+  /* have not yet terminated or are still joinable, and         */
+  /* cannot be concurrently terminated.                         */
+  /* Assumes we do NOT hold the allocation lock.                */
+  STATIC GC_thread GC_lookup_pthread(pthread_t id)
+  {
+#   ifndef GC_NO_THREADS_DISCOVERY
+      if (GC_win32_dll_threads) {
+        int i;
+        LONG my_max = GC_get_max_thread_index();
+
+        for (i = 0; i <= my_max &&
+                    (!AO_load_acquire(&dll_thread_table[i].tm.in_use)
+                    || THREAD_EQUAL(dll_thread_table[i].pthread_id, id));
+                    /* Must still be in_use, since nobody else can      */
+                    /* store our thread_id.                             */
+             i++) {
+          /* empty */
+        }
+        return i <= my_max ? (GC_thread)(dll_thread_table + i) : NULL;
+      } else
+#   endif
+    /* else */ {
+      /* We first try the cache.  If that fails, we use a very slow     */
+      /* approach.                                                      */
+      word hv_guess = THREAD_TABLE_INDEX(GET_PTHREAD_MAP_CACHE(id));
+      int hv;
+      GC_thread p;
+      DCL_LOCK_STATE;
+
+      LOCK();
+      for (p = GC_threads[hv_guess]; 0 != p; p = p -> tm.next) {
+        if (THREAD_EQUAL(p -> pthread_id, id))
+          goto foundit;
+      }
+      for (hv = 0; hv < THREAD_TABLE_SZ; ++hv) {
+        for (p = GC_threads[hv]; 0 != p; p = p -> tm.next) {
+          if (THREAD_EQUAL(p -> pthread_id, id))
+            goto foundit;
+        }
+      }
+      p = 0;
+     foundit:
+      UNLOCK();
+      return p;
+    }
+  }
+
+#endif /* GC_PTHREADS */
+
+#ifdef CAN_HANDLE_FORK
+    /* Similar to that in pthread_support.c but also rehashes the table */
+    /* since hash map key (thread_id) differs from that in the parent.  */
+    STATIC void GC_remove_all_threads_but_me(void)
+    {
+      int hv;
+      GC_thread p, next, me = NULL;
+      DWORD thread_id;
+      pthread_t pthread_id = pthread_self(); /* same as in parent */
+
+      GC_ASSERT(!GC_win32_dll_threads);
+      for (hv = 0; hv < THREAD_TABLE_SZ; ++hv) {
+        for (p = GC_threads[hv]; 0 != p; p = next) {
+          next = p -> tm.next;
+          if (THREAD_EQUAL(p -> pthread_id, pthread_id)) {
+            GC_ASSERT(me == NULL);
+            me = p;
+            p -> tm.next = 0;
+          } else {
+#           ifdef THREAD_LOCAL_ALLOC
+              if ((p -> flags & FINISHED) == 0) {
+                GC_destroy_thread_local(&p->tlfs);
+              }
+#           endif
+            if (&first_thread != p)
+              GC_INTERNAL_FREE(p);
+          }
+        }
+        GC_threads[hv] = NULL;
+      }
+
+      /* Put "me" back to GC_threads.   */
+      GC_ASSERT(me != NULL);
+      thread_id = GetCurrentThreadId(); /* differs from that in parent */
+      GC_threads[THREAD_TABLE_INDEX(thread_id)] = me;
+
+      /* Update Win32 thread Id and handle.     */
+      me -> id = thread_id;
+#     ifndef MSWINCE
+        if (!DuplicateHandle(GetCurrentProcess(), GetCurrentThread(),
+                        GetCurrentProcess(), (HANDLE *)&me->handle,
+                        0 /* dwDesiredAccess */, FALSE /* bInheritHandle */,
+                        DUPLICATE_SAME_ACCESS))
+          ABORT("DuplicateHandle failed");
+#     endif
+
+#     if defined(THREAD_LOCAL_ALLOC) && !defined(USE_CUSTOM_SPECIFIC)
+        /* For Cygwin, we need to re-assign thread-local pointer to     */
+        /* 'tlfs' (it is ok to call GC_destroy_thread_local and         */
+        /* GC_free_internal before this action).                        */
+        if (GC_setspecific(GC_thread_key, &me->tlfs) != 0)
+          ABORT("GC_setspecific failed (in child)");
+#     endif
+    }
+
+    STATIC void GC_fork_prepare_proc(void)
+    {
+      LOCK();
+#     ifdef PARALLEL_MARK
+        if (GC_parallel)
+          GC_wait_for_reclaim();
+#     endif
+      GC_wait_for_gc_completion(TRUE);
+#     ifdef PARALLEL_MARK
+        if (GC_parallel)
+          GC_acquire_mark_lock();
+#     endif
+    }
+
+    STATIC void GC_fork_parent_proc(void)
+    {
+#     ifdef PARALLEL_MARK
+        if (GC_parallel)
+          GC_release_mark_lock();
+#     endif
+      UNLOCK();
+    }
+
+    STATIC void GC_fork_child_proc(void)
+    {
+#     ifdef PARALLEL_MARK
+        if (GC_parallel) {
+          GC_release_mark_lock();
+          GC_markers = 1;
+          GC_parallel = FALSE;
+                /* Turn off parallel marking in the child, since we are */
+                /* probably just going to exec, and we would have to    */
+                /* restart mark threads.                                */
+        }
+#     endif
+      GC_remove_all_threads_but_me();
+      UNLOCK();
+    }
+#endif /* CAN_HANDLE_FORK */
+
+void GC_push_thread_structures(void)
+{
+  GC_ASSERT(I_HOLD_LOCK());
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      /* Unlike the other threads implementations, the thread table here */
+      /* contains no pointers to the collectable heap.  Thus we have     */
+      /* no private structures we need to preserve.                      */
+#     ifdef GC_PTHREADS
+        int i; /* pthreads may keep a pointer in the thread exit value */
+        LONG my_max = GC_get_max_thread_index();
+
+        for (i = 0; i <= my_max; i++)
+          if (dll_thread_table[i].tm.in_use)
+            GC_push_all((ptr_t)&(dll_thread_table[i].status),
+                        (ptr_t)(&(dll_thread_table[i].status)+1));
+#     endif
+    } else
+# endif
+  /* else */ {
+    GC_push_all((ptr_t)(GC_threads), (ptr_t)(GC_threads)+sizeof(GC_threads));
+  }
+# if defined(THREAD_LOCAL_ALLOC)
+    GC_push_all((ptr_t)(&GC_thread_key),
+                (ptr_t)(&GC_thread_key) + sizeof(GC_thread_key));
+    /* Just in case we ever use our own TLS implementation.     */
+# endif
+}
+
+/* Suspend the given thread, if it's still active.      */
+STATIC void GC_suspend(GC_thread t)
+{
+# ifndef MSWINCE
+    /* Apparently the Windows 95 GetOpenFileName call creates           */
+    /* a thread that does not properly get cleaned up, and              */
+    /* SuspendThread on its descriptor may provoke a crash.             */
+    /* This reduces the probability of that event, though it still      */
+    /* appears there's a race here.                                     */
+    DWORD exitCode;
+# endif
+  UNPROTECT_THREAD(t);
+# ifndef MSWINCE
+    if (GetExitCodeThread(t -> handle, &exitCode) &&
+        exitCode != STILL_ACTIVE) {
+#     ifdef GC_PTHREADS
+        t -> stack_base = 0; /* prevent stack from being pushed */
+#     else
+        /* this breaks pthread_join on Cygwin, which is guaranteed to  */
+        /* only see user pthreads                                      */
+        GC_ASSERT(GC_win32_dll_threads);
+        GC_delete_gc_thread(t);
+#     endif
+      return;
+    }
+# endif
+# if defined(MPROTECT_VDB)
+    /* Acquire the spin lock we use to update dirty bits.       */
+    /* Threads shouldn't get stopped holding it.  But we may    */
+    /* acquire and release it in the UNPROTECT_THREAD call.     */
+    while (AO_test_and_set_acquire(&GC_fault_handler_lock) == AO_TS_SET) {
+      /* empty */
+    }
+# endif
+
+# ifdef MSWINCE
+    /* SuspendThread() will fail if thread is running kernel code.      */
+    while (SuspendThread(THREAD_HANDLE(t)) == (DWORD)-1)
+      Sleep(10); /* in millis */
+# else
+    if (SuspendThread(t -> handle) == (DWORD)-1)
+      ABORT("SuspendThread failed");
+# endif /* !MSWINCE */
+  t -> suspended = (unsigned char)TRUE;
+# if defined(MPROTECT_VDB)
+    AO_CLEAR(&GC_fault_handler_lock);
+# endif
+}
+
+#if defined(GC_ASSERTIONS) && !defined(CYGWIN32)
+  GC_INNER GC_bool GC_write_disabled = FALSE;
+                /* TRUE only if GC_stop_world() acquired GC_write_cs.   */
+#endif
+
+GC_INNER void GC_stop_world(void)
+{
+  DWORD thread_id = GetCurrentThreadId();
+
+  if (!GC_thr_initialized)
+    ABORT("GC_stop_world() called before GC_thr_init()");
+  GC_ASSERT(I_HOLD_LOCK());
+
+  /* This code is the same as in pthread_stop_world.c */
+# ifdef PARALLEL_MARK
+    if (GC_parallel) {
+      GC_acquire_mark_lock();
+      GC_ASSERT(GC_fl_builder_count == 0);
+      /* We should have previously waited for it to become zero. */
+    }
+# endif /* PARALLEL_MARK */
+
+# if !defined(GC_NO_THREADS_DISCOVERY) || defined(GC_ASSERTIONS)
+    GC_please_stop = TRUE;
+# endif
+# ifndef CYGWIN32
+    GC_ASSERT(!GC_write_disabled);
+    EnterCriticalSection(&GC_write_cs);
+    /* It's not allowed to call GC_printf() (and friends) here down to  */
+    /* LeaveCriticalSection (same applies recursively to                */
+    /* GC_get_max_thread_index(), GC_suspend(), GC_delete_gc_thread()   */
+    /* (only if GC_win32_dll_threads), GC_size() and                    */
+    /* GC_remove_protection()).                                         */
+#   ifdef GC_ASSERTIONS
+      GC_write_disabled = TRUE;
+#   endif
+# endif
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      int i;
+      int my_max;
+      /* Any threads being created during this loop will end up setting */
+      /* GC_attached_thread when they start.  This will force marking   */
+      /* to restart.  This is not ideal, but hopefully correct.         */
+      GC_attached_thread = FALSE;
+      my_max = (int)GC_get_max_thread_index();
+      for (i = 0; i <= my_max; i++) {
+        GC_vthread t = dll_thread_table + i;
+        if (t -> stack_base != 0 && t -> thread_blocked_sp == NULL
+            && t -> id != thread_id) {
+          GC_suspend((GC_thread)t);
+        }
+      }
+    } else
+# endif
+  /* else */ {
+    GC_thread t;
+    int i;
+
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      for (t = GC_threads[i]; t != 0; t = t -> tm.next) {
+        if (t -> stack_base != 0 && t -> thread_blocked_sp == NULL
+            && !KNOWN_FINISHED(t) && t -> id != thread_id) {
+          GC_suspend(t);
+        }
+      }
+    }
+  }
+# ifndef CYGWIN32
+#   ifdef GC_ASSERTIONS
+      GC_write_disabled = FALSE;
+#   endif
+    LeaveCriticalSection(&GC_write_cs);
+# endif
+# ifdef PARALLEL_MARK
+    if (GC_parallel)
+      GC_release_mark_lock();
+# endif
+}
+
+GC_INNER void GC_start_world(void)
+{
+# ifdef GC_ASSERTIONS
+    DWORD thread_id = GetCurrentThreadId();
+# endif
+  int i;
+
+  GC_ASSERT(I_HOLD_LOCK());
+  if (GC_win32_dll_threads) {
+    LONG my_max = GC_get_max_thread_index();
+    for (i = 0; i <= my_max; i++) {
+      GC_thread t = (GC_thread)(dll_thread_table + i);
+      if (t -> suspended) {
+        GC_ASSERT(t -> stack_base != 0 && t -> id != thread_id);
+        if (ResumeThread(THREAD_HANDLE(t)) == (DWORD)-1)
+          ABORT("ResumeThread failed");
+        t -> suspended = FALSE;
+      }
+    }
+  } else {
+    GC_thread t;
+    int i;
+
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      for (t = GC_threads[i]; t != 0; t = t -> tm.next) {
+        if (t -> suspended) {
+          GC_ASSERT(t -> stack_base != 0 && t -> id != thread_id);
+          if (ResumeThread(THREAD_HANDLE(t)) == (DWORD)-1)
+            ABORT("ResumeThread failed");
+          UNPROTECT_THREAD(t);
+          t -> suspended = FALSE;
+        }
+      }
+    }
+  }
+# if !defined(GC_NO_THREADS_DISCOVERY) || defined(GC_ASSERTIONS)
+    GC_please_stop = FALSE;
+# endif
+}
+
+#ifdef MSWINCE
+  /* The VirtualQuery calls below won't work properly on some old WinCE */
+  /* versions, but since each stack is restricted to an aligned 64 KiB  */
+  /* region of virtual memory we can just take the next lowest multiple */
+  /* of 64 KiB.  The result of this macro must not be used as its       */
+  /* argument later and must not be used as the lower bound for sp      */
+  /* check (since the stack may be bigger than 64 KiB).                 */
+# define GC_wince_evaluate_stack_min(s) \
+                        (ptr_t)(((word)(s) - 1) & ~(word)0xFFFF)
+#elif defined(GC_ASSERTIONS)
+# define GC_dont_query_stack_min FALSE
+#endif
+
+/* A cache holding the results of the recent VirtualQuery call. */
+/* Protected by the allocation lock.                            */
+static ptr_t last_address = 0;
+static MEMORY_BASIC_INFORMATION last_info;
+
+/* Probe stack memory region (starting at "s") to find out its  */
+/* lowest address (i.e. stack top).                             */
+/* S must be a mapped address inside the region, NOT the first  */
+/* unmapped address.                                            */
+STATIC ptr_t GC_get_stack_min(ptr_t s)
+{
+  ptr_t bottom;
+
+  GC_ASSERT(I_HOLD_LOCK());
+  if (s != last_address) {
+    VirtualQuery(s, &last_info, sizeof(last_info));
+    last_address = s;
+  }
+  do {
+    bottom = last_info.BaseAddress;
+    VirtualQuery(bottom - 1, &last_info, sizeof(last_info));
+    last_address = bottom - 1;
+  } while ((last_info.Protect & PAGE_READWRITE)
+           && !(last_info.Protect & PAGE_GUARD));
+  return(bottom);
+}
+
+/* Return true if the page at s has protections appropriate     */
+/* for a stack page.                                            */
+static GC_bool may_be_in_stack(ptr_t s)
+{
+  GC_ASSERT(I_HOLD_LOCK());
+  if (s != last_address) {
+    VirtualQuery(s, &last_info, sizeof(last_info));
+    last_address = s;
+  }
+  return (last_info.Protect & PAGE_READWRITE)
+          && !(last_info.Protect & PAGE_GUARD);
+}
+
+STATIC word GC_push_stack_for(GC_thread thread, DWORD me)
+{
+  ptr_t sp, stack_min;
+
+  struct GC_traced_stack_sect_s *traced_stack_sect =
+                                      thread -> traced_stack_sect;
+  if (thread -> id == me) {
+    GC_ASSERT(thread -> thread_blocked_sp == NULL);
+    sp = GC_approx_sp();
+  } else if ((sp = thread -> thread_blocked_sp) == NULL) {
+              /* Use saved sp value for blocked threads. */
+    /* For unblocked threads call GetThreadContext().   */
+    CONTEXT context;
+    context.ContextFlags = CONTEXT_INTEGER|CONTEXT_CONTROL;
+    if (!GetThreadContext(THREAD_HANDLE(thread), &context))
+      ABORT("GetThreadContext failed");
+
+    /* Push all registers that might point into the heap.  Frame        */
+    /* pointer registers are included in case client code was           */
+    /* compiled with the 'omit frame pointer' optimization.             */
+#   define PUSH1(reg) GC_push_one((word)context.reg)
+#   define PUSH2(r1,r2) PUSH1(r1), PUSH1(r2)
+#   define PUSH4(r1,r2,r3,r4) PUSH2(r1,r2), PUSH2(r3,r4)
+#   if defined(I386)
+      PUSH4(Edi,Esi,Ebx,Edx), PUSH2(Ecx,Eax), PUSH1(Ebp);
+      sp = (ptr_t)context.Esp;
+#   elif defined(X86_64)
+      PUSH4(Rax,Rcx,Rdx,Rbx); PUSH2(Rbp, Rsi); PUSH1(Rdi);
+      PUSH4(R8, R9, R10, R11); PUSH4(R12, R13, R14, R15);
+      sp = (ptr_t)context.Rsp;
+#   elif defined(ARM32)
+      PUSH4(R0,R1,R2,R3),PUSH4(R4,R5,R6,R7),PUSH4(R8,R9,R10,R11);
+      PUSH1(R12);
+      sp = (ptr_t)context.Sp;
+#   elif defined(SHx)
+      PUSH4(R0,R1,R2,R3), PUSH4(R4,R5,R6,R7), PUSH4(R8,R9,R10,R11);
+      PUSH2(R12,R13), PUSH1(R14);
+      sp = (ptr_t)context.R15;
+#   elif defined(MIPS)
+      PUSH4(IntAt,IntV0,IntV1,IntA0), PUSH4(IntA1,IntA2,IntA3,IntT0);
+      PUSH4(IntT1,IntT2,IntT3,IntT4), PUSH4(IntT5,IntT6,IntT7,IntS0);
+      PUSH4(IntS1,IntS2,IntS3,IntS4), PUSH4(IntS5,IntS6,IntS7,IntT8);
+      PUSH4(IntT9,IntK0,IntK1,IntS8);
+      sp = (ptr_t)context.IntSp;
+#   elif defined(PPC)
+      PUSH4(Gpr0, Gpr3, Gpr4, Gpr5),  PUSH4(Gpr6, Gpr7, Gpr8, Gpr9);
+      PUSH4(Gpr10,Gpr11,Gpr12,Gpr14), PUSH4(Gpr15,Gpr16,Gpr17,Gpr18);
+      PUSH4(Gpr19,Gpr20,Gpr21,Gpr22), PUSH4(Gpr23,Gpr24,Gpr25,Gpr26);
+      PUSH4(Gpr27,Gpr28,Gpr29,Gpr30), PUSH1(Gpr31);
+      sp = (ptr_t)context.Gpr1;
+#   elif defined(ALPHA)
+      PUSH4(IntV0,IntT0,IntT1,IntT2), PUSH4(IntT3,IntT4,IntT5,IntT6);
+      PUSH4(IntT7,IntS0,IntS1,IntS2), PUSH4(IntS3,IntS4,IntS5,IntFp);
+      PUSH4(IntA0,IntA1,IntA2,IntA3), PUSH4(IntA4,IntA5,IntT8,IntT9);
+      PUSH4(IntT10,IntT11,IntT12,IntAt);
+      sp = (ptr_t)context.IntSp;
+#   else
+#     error "architecture is not supported"
+#   endif
+  } /* ! current thread */
+
+  /* Set stack_min to the lowest address in the thread stack,   */
+  /* or to an address in the thread stack no larger than sp,    */
+  /* taking advantage of the old value to avoid slow traversals */
+  /* of large stacks.                                           */
+  if (thread -> last_stack_min == ADDR_LIMIT) {
+#   ifdef MSWINCE
+      if (GC_dont_query_stack_min) {
+        stack_min = GC_wince_evaluate_stack_min(traced_stack_sect != NULL ?
+                      (ptr_t)traced_stack_sect : thread -> stack_base);
+        /* Keep last_stack_min value unmodified. */
+      } else
+#   endif
+    /* else */ {
+      stack_min = GC_get_stack_min(traced_stack_sect != NULL ?
+                      (ptr_t)traced_stack_sect : thread -> stack_base);
+      UNPROTECT_THREAD(thread);
+      thread -> last_stack_min = stack_min;
+    }
+  } else {
+    /* First, adjust the latest known minimum stack address if we       */
+    /* are inside GC_call_with_gc_active().                             */
+    if (traced_stack_sect != NULL &&
+        thread -> last_stack_min > (ptr_t)traced_stack_sect) {
+      UNPROTECT_THREAD(thread);
+      thread -> last_stack_min = (ptr_t)traced_stack_sect;
+    }
+
+    if (sp < thread -> stack_base && sp >= thread -> last_stack_min) {
+      stack_min = sp;
+    } else {
+      /* In the current thread it is always safe to use sp value.       */
+      if (may_be_in_stack(thread -> id == me &&
+                          sp < thread -> last_stack_min ?
+                          sp : thread -> last_stack_min)) {
+        stack_min = last_info.BaseAddress;
+        /* Do not probe rest of the stack if sp is correct. */
+        if (sp < stack_min || sp >= thread->stack_base)
+          stack_min = GC_get_stack_min(thread -> last_stack_min);
+      } else {
+        /* Stack shrunk?  Is this possible? */
+        stack_min = GC_get_stack_min(thread -> stack_base);
+      }
+      UNPROTECT_THREAD(thread);
+      thread -> last_stack_min = stack_min;
+    }
+  }
+
+  GC_ASSERT(GC_dont_query_stack_min
+            || stack_min == GC_get_stack_min(thread -> stack_base)
+            || (sp >= stack_min && stack_min < thread -> stack_base
+                && stack_min > GC_get_stack_min(thread -> stack_base)));
+
+  if (sp >= stack_min && sp < thread->stack_base) {
+#   ifdef DEBUG_THREADS
+      GC_log_printf("Pushing stack for 0x%x from sp %p to %p from 0x%x\n",
+                    (int)thread -> id, sp, thread -> stack_base, (int)me);
+#   endif
+    GC_push_all_stack_sections(sp, thread->stack_base, traced_stack_sect);
+  } else {
+    /* If not current thread then it is possible for sp to point to     */
+    /* the guarded (untouched yet) page just below the current          */
+    /* stack_min of the thread.                                         */
+    if (thread -> id == me || sp >= thread->stack_base
+        || sp + GC_page_size < stack_min)
+      WARN("Thread stack pointer %p out of range, pushing everything\n",
+           sp);
+#   ifdef DEBUG_THREADS
+      GC_log_printf("Pushing stack for 0x%x from (min) %p to %p from 0x%x\n",
+                    (int)thread->id, stack_min, thread->stack_base, (int)me);
+#   endif
+    /* Push everything - ignore "traced stack section" data.            */
+    GC_push_all_stack(stack_min, thread->stack_base);
+  }
+  return thread->stack_base - sp; /* stack grows down */
+}
+
+GC_INNER void GC_push_all_stacks(void)
+{
+  DWORD thread_id = GetCurrentThreadId();
+  GC_bool found_me = FALSE;
+# ifndef SMALL_CONFIG
+    unsigned nthreads = 0;
+# endif
+  word total_size = 0;
+# ifndef GC_NO_THREADS_DISCOVERY
+    if (GC_win32_dll_threads) {
+      int i;
+      LONG my_max = GC_get_max_thread_index();
+
+      for (i = 0; i <= my_max; i++) {
+        GC_thread t = (GC_thread)(dll_thread_table + i);
+        if (t -> tm.in_use && t -> stack_base) {
+#         ifndef SMALL_CONFIG
+            ++nthreads;
+#         endif
+          total_size += GC_push_stack_for(t, thread_id);
+          if (t -> id == thread_id) found_me = TRUE;
+        }
+      }
+    } else
+# endif
+  /* else */ {
+    int i;
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      GC_thread t;
+      for (t = GC_threads[i]; t != 0; t = t -> tm.next) {
+        if (!KNOWN_FINISHED(t) && t -> stack_base) {
+#         ifndef SMALL_CONFIG
+            ++nthreads;
+#         endif
+          total_size += GC_push_stack_for(t, thread_id);
+          if (t -> id == thread_id) found_me = TRUE;
+        }
+      }
+    }
+  }
+# ifndef SMALL_CONFIG
+    if (GC_print_stats == VERBOSE) {
+      GC_log_printf("Pushed %d thread stacks%s\n", nthreads,
+            GC_win32_dll_threads ? " based on DllMain thread tracking" : "");
+    }
+# endif
+  if (!found_me && !GC_in_thread_creation)
+    ABORT("Collecting from unknown thread");
+  GC_total_stacksize = total_size;
+}
+
+#ifdef PARALLEL_MARK
+
+# ifndef MAX_MARKERS
+#   define MAX_MARKERS 16
+# endif
+
+  static ptr_t marker_sp[MAX_MARKERS - 1]; /* The cold end of the stack */
+                                           /* for markers.              */
+# ifdef IA64
+    static ptr_t marker_bsp[MAX_MARKERS - 1];
+# endif
+
+  static ptr_t marker_last_stack_min[MAX_MARKERS - 1];
+                                /* Last known minimum (hottest) address */
+                                /* in stack (or ADDR_LIMIT if unset)    */
+                                /* for markers.                         */
+
+#endif /* PARALLEL_MARK */
+
+/* Find stack with the lowest address which overlaps the        */
+/* interval [start, limit).                                     */
+/* Return stack bounds in *lo and *hi.  If no such stack        */
+/* is found, both *hi and *lo will be set to an address         */
+/* higher than limit.                                           */
+GC_INNER void GC_get_next_stack(char *start, char *limit,
+                                char **lo, char **hi)
+{
+  int i;
+  char * current_min = ADDR_LIMIT;  /* Least in-range stack base      */
+  ptr_t *plast_stack_min = NULL;    /* Address of last_stack_min      */
+                                    /* field for thread corresponding */
+                                    /* to current_min.                */
+  GC_thread thread = NULL;          /* Either NULL or points to the   */
+                                    /* thread's hash table entry      */
+                                    /* containing *plast_stack_min.   */
+
+  /* First set current_min, ignoring limit. */
+  if (GC_win32_dll_threads) {
+    LONG my_max = GC_get_max_thread_index();
+
+    for (i = 0; i <= my_max; i++) {
+      ptr_t s = (ptr_t)(dll_thread_table[i].stack_base);
+
+      if (s > start && s < current_min) {
+        /* Update address of last_stack_min. */
+        plast_stack_min = (ptr_t * /* no volatile */)
+                            &dll_thread_table[i].last_stack_min;
+        current_min = s;
+      }
+    }
+  } else {
+    for (i = 0; i < THREAD_TABLE_SZ; i++) {
+      GC_thread t;
+
+      for (t = GC_threads[i]; t != 0; t = t -> tm.next) {
+        ptr_t s = t -> stack_base;
+
+        if (s > start && s < current_min) {
+          /* Update address of last_stack_min. */
+          plast_stack_min = &t -> last_stack_min;
+          thread = t; /* Remember current thread to unprotect. */
+          current_min = s;
+        }
+      }
+    }
+#   ifdef PARALLEL_MARK
+      for (i = 0; i < GC_markers - 1; ++i) {
+        ptr_t s = marker_sp[i];
+#       ifdef IA64
+          /* FIXME: not implemented */
+#       endif
+        if (s > start && s < current_min) {
+          GC_ASSERT(marker_last_stack_min[i] != NULL);
+          plast_stack_min = &marker_last_stack_min[i];
+          current_min = s;
+          thread = NULL; /* Not a thread's hash table entry. */
+        }
+      }
+#   endif
+  }
+
+  *hi = current_min;
+  if (current_min == ADDR_LIMIT) {
+      *lo = ADDR_LIMIT;
+      return;
+  }
+
+  GC_ASSERT(current_min > start && plast_stack_min != NULL);
+# ifdef MSWINCE
+    if (GC_dont_query_stack_min) {
+      *lo = GC_wince_evaluate_stack_min(current_min);
+      /* Keep last_stack_min value unmodified. */
+      return;
+    }
+# endif
+
+  if (current_min > limit && !may_be_in_stack(limit)) {
+    /* Skip the rest since the memory region at limit address is        */
+    /* not a stack (so the lowest address of the found stack would      */
+    /* be above the limit value anyway).                                */
+    *lo = ADDR_LIMIT;
+    return;
+  }
+
+  /* Get the minimum address of the found stack by probing its memory   */
+  /* region starting from the recent known minimum (if set).            */
+  if (*plast_stack_min == ADDR_LIMIT
+      || !may_be_in_stack(*plast_stack_min)) {
+    /* Unsafe to start from last_stack_min value. */
+    *lo = GC_get_stack_min(current_min);
+  } else {
+    /* Use the recent value to optimize search for min address. */
+    *lo = GC_get_stack_min(*plast_stack_min);
+  }
+
+  /* Remember current stack_min value. */
+  if (thread != NULL) {
+    UNPROTECT_THREAD(thread);
+  }
+  *plast_stack_min = *lo;
+}
+
+#ifdef PARALLEL_MARK
+
+# if defined(GC_PTHREADS) && !defined(GC_PTHREADS_PARAMARK)
+    /* Use pthread-based parallel mark implementation.    */
+#   define GC_PTHREADS_PARAMARK
+# endif
+
+# if !defined(GC_PTHREADS_PARAMARK)
+    STATIC HANDLE GC_marker_cv[MAX_MARKERS - 1] = {0};
+                        /* Events with manual reset (one for each       */
+                        /* mark helper).                                */
+
+    STATIC DWORD GC_marker_Id[MAX_MARKERS - 1] = {0};
+                        /* This table is used for mapping helper        */
+                        /* threads ID to mark helper index (linear      */
+                        /* search is used since the mapping contains    */
+                        /* only a few entries).                         */
+# endif
+
+  /* GC_mark_thread() is the same as in pthread_support.c */
+# ifdef GC_PTHREADS_PARAMARK
+    STATIC void * GC_mark_thread(void * id)
+# elif defined(MSWINCE)
+    STATIC DWORD WINAPI GC_mark_thread(LPVOID id)
+# else
+    STATIC unsigned __stdcall GC_mark_thread(void * id)
+# endif
+  {
+    word my_mark_no = 0;
+
+    if ((word)id == (word)-1) return 0; /* to make compiler happy */
+    marker_sp[(word)id] = GC_approx_sp();
+#   ifdef IA64
+      marker_bsp[(word)id] = GC_save_regs_in_stack();
+#   endif
+#   if !defined(GC_PTHREADS_PARAMARK)
+      GC_marker_Id[(word)id] = GetCurrentThreadId();
+#   endif
+
+    /* Inform start_mark_threads() about completion of marker data init. */
+    GC_acquire_mark_lock();
+    if (0 == --GC_fl_builder_count)
+      GC_notify_all_builder();
+    GC_release_mark_lock();
+
+    for (;; ++my_mark_no) {
+      if (my_mark_no - GC_mark_no > (word)2) {
+        /* resynchronize if we get far off, e.g. because GC_mark_no     */
+        /* wrapped.                                                     */
+        my_mark_no = GC_mark_no;
+      }
+#     ifdef DEBUG_THREADS
+        GC_log_printf("Starting mark helper for mark number %lu\n",
+                      (unsigned long)my_mark_no);
+#     endif
+      GC_help_marker(my_mark_no);
+    }
+  }
+
+# ifdef GC_ASSERTIONS
+    GC_INNER unsigned long GC_mark_lock_holder = NO_THREAD;
+# endif
+
+  /* GC_mark_threads[] is unused here unlike that in pthread_support.c  */
+
+# ifdef GC_PTHREADS_PARAMARK
+#   include <pthread.h>
+
+#   ifndef NUMERIC_THREAD_ID
+#     define NUMERIC_THREAD_ID(id) (unsigned long)GC_PTHREAD_PTRVAL(id)
+#   endif
+
+    /* start_mark_threads() is the same as in pthread_support.c except for: */
+    /* - GC_markers value is adjusted already;                              */
+    /* - thread stack is assumed to be large enough; and                    */
+    /* - statistics about the number of marker threads is printed outside.  */
+    static void start_mark_threads(void)
+    {
+      int i;
+      pthread_attr_t attr;
+      pthread_t new_thread;
+
+      GC_ASSERT(GC_fl_builder_count == 0);
+      if (0 != pthread_attr_init(&attr)) ABORT("pthread_attr_init failed");
+
+      if (0 != pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED))
+        ABORT("pthread_attr_setdetachstate failed");
+
+      for (i = 0; i < GC_markers - 1; ++i) {
+        marker_last_stack_min[i] = ADDR_LIMIT;
+        if (0 != pthread_create(&new_thread, &attr,
+                                GC_mark_thread, (void *)(word)i)) {
+          WARN("Marker thread creation failed.\n", 0);
+          /* Don't try to create other marker threads.    */
+          GC_markers = i + 1;
+          if (i == 0) GC_parallel = FALSE;
+          break;
+        }
+      }
+      pthread_attr_destroy(&attr);
+      GC_wait_for_markers_init();
+    }
+
+    static pthread_mutex_t mark_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+    static pthread_cond_t builder_cv = PTHREAD_COND_INITIALIZER;
+
+    /* GC_acquire/release_mark_lock(), GC_wait_builder/marker(),          */
+    /* GC_wait_for_reclaim(), GC_notify_all_builder/marker() are the same */
+    /* as in pthread_support.c except that GC_generic_lock() is not used. */
+
+#   ifdef LOCK_STATS
+      AO_t GC_block_count = 0;
+#   endif
+
+    GC_INNER void GC_acquire_mark_lock(void)
+    {
+      if (pthread_mutex_lock(&mark_mutex) != 0) {
+        ABORT("pthread_mutex_lock failed");
+      }
+#     ifdef LOCK_STATS
+        (void)AO_fetch_and_add1(&GC_block_count);
+#     endif
+      /* GC_generic_lock(&mark_mutex); */
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NUMERIC_THREAD_ID(pthread_self());
+#     endif
+    }
+
+    GC_INNER void GC_release_mark_lock(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == NUMERIC_THREAD_ID(pthread_self()));
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NO_THREAD;
+#     endif
+      if (pthread_mutex_unlock(&mark_mutex) != 0) {
+        ABORT("pthread_mutex_unlock failed");
+      }
+    }
+
+    /* Collector must wait for a freelist builders for 2 reasons:       */
+    /* 1) Mark bits may still be getting examined without lock.         */
+    /* 2) Partial free lists referenced only by locals may not be       */
+    /* scanned correctly, e.g. if they contain "pointer-free" objects,  */
+    /* since the free-list link may be ignored.                         */
+    STATIC void GC_wait_builder(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == NUMERIC_THREAD_ID(pthread_self()));
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NO_THREAD;
+#     endif
+      if (pthread_cond_wait(&builder_cv, &mark_mutex) != 0) {
+        ABORT("pthread_cond_wait failed");
+      }
+      GC_ASSERT(GC_mark_lock_holder == NO_THREAD);
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NUMERIC_THREAD_ID(pthread_self());
+#     endif
+    }
+
+    GC_INNER void GC_wait_for_reclaim(void)
+    {
+      GC_acquire_mark_lock();
+      while (GC_fl_builder_count > 0) {
+        GC_wait_builder();
+      }
+      GC_release_mark_lock();
+    }
+
+    GC_INNER void GC_notify_all_builder(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == NUMERIC_THREAD_ID(pthread_self()));
+      if (pthread_cond_broadcast(&builder_cv) != 0) {
+        ABORT("pthread_cond_broadcast failed");
+      }
+    }
+
+    static pthread_cond_t mark_cv = PTHREAD_COND_INITIALIZER;
+
+    GC_INNER void GC_wait_marker(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == NUMERIC_THREAD_ID(pthread_self()));
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NO_THREAD;
+#     endif
+      if (pthread_cond_wait(&mark_cv, &mark_mutex) != 0) {
+        ABORT("pthread_cond_wait failed");
+      }
+      GC_ASSERT(GC_mark_lock_holder == NO_THREAD);
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NUMERIC_THREAD_ID(pthread_self());
+#     endif
+    }
+
+    GC_INNER void GC_notify_all_marker(void)
+    {
+      if (pthread_cond_broadcast(&mark_cv) != 0) {
+        ABORT("pthread_cond_broadcast failed");
+      }
+    }
+
+# else /* ! GC_PTHREADS_PARAMARK */
+
+#   ifndef MARK_THREAD_STACK_SIZE
+#     define MARK_THREAD_STACK_SIZE 0   /* default value */
+#   endif
+
+    /* mark_mutex_event, builder_cv, mark_cv are initialized in GC_thr_init */
+    static HANDLE mark_mutex_event = (HANDLE)0; /* Event with auto-reset.   */
+    static HANDLE builder_cv = (HANDLE)0; /* Event with manual reset.       */
+    static HANDLE mark_cv = (HANDLE)0; /* Event with manual reset.          */
+
+    static void start_mark_threads(void)
+    {
+      int i;
+#     ifdef MSWINCE
+        HANDLE handle;
+        DWORD thread_id;
+#     else
+        GC_uintptr_t handle;
+        unsigned thread_id;
+#     endif
+
+      GC_ASSERT(GC_fl_builder_count == 0);
+        /* Initialize GC_marker_cv[] fully before starting the  */
+        /* first helper thread.                                 */
+        for (i = 0; i < GC_markers - 1; ++i) {
+          if ((GC_marker_cv[i] = CreateEvent(NULL /* attrs */,
+                                        TRUE /* isManualReset */,
+                                        FALSE /* initialState */,
+                                        NULL /* name (A/W) */)) == (HANDLE)0)
+            ABORT("CreateEvent() failed");
+        }
+
+      for (i = 0; i < GC_markers - 1; ++i) {
+        marker_last_stack_min[i] = ADDR_LIMIT;
+#       ifdef MSWINCE
+          /* There is no _beginthreadex() in WinCE. */
+          handle = CreateThread(NULL /* lpsa */,
+                                MARK_THREAD_STACK_SIZE /* ignored */,
+                                GC_mark_thread, (LPVOID)(word)i,
+                                0 /* fdwCreate */, &thread_id);
+          if (handle == NULL) {
+            WARN("Marker thread creation failed\n", 0);
+            /* The most probable failure reason is "not enough memory". */
+            /* Don't try to create other marker threads.                */
+            break;
+          } else {
+            /* It's safe to detach the thread.  */
+            CloseHandle(handle);
+          }
+#       else
+          handle = _beginthreadex(NULL /* security_attr */,
+                                MARK_THREAD_STACK_SIZE, GC_mark_thread,
+                                (void *)(word)i, 0 /* flags */, &thread_id);
+          if (!handle || handle == (GC_uintptr_t)-1L) {
+            WARN("Marker thread creation failed\n", 0);
+            /* Don't try to create other marker threads.                */
+            break;
+          } else {/* We may detach the thread (if handle is of HANDLE type) */
+            /* CloseHandle((HANDLE)handle); */
+          }
+#       endif
+      }
+
+      /* Adjust GC_markers (and free unused resources) in case of failure. */
+        while ((int)GC_markers > i + 1) {
+          GC_markers--;
+          CloseHandle(GC_marker_cv[(int)GC_markers - 1]);
+        }
+      GC_wait_for_markers_init();
+      if (i == 0) {
+        GC_parallel = FALSE;
+        CloseHandle(mark_cv);
+        CloseHandle(builder_cv);
+        CloseHandle(mark_mutex_event);
+      }
+    }
+
+    STATIC /* volatile */ LONG GC_mark_mutex_state = 0;
+                                /* Mutex state: 0 - unlocked,           */
+                                /* 1 - locked and no other waiters,     */
+                                /* -1 - locked and waiters may exist.   */
+                                /* Accessed by InterlockedExchange().   */
+
+    /* #define LOCK_STATS */
+#   ifdef LOCK_STATS
+      AO_t GC_block_count = 0;
+      AO_t GC_unlocked_count = 0;
+#   endif
+
+    GC_INNER void GC_acquire_mark_lock(void)
+    {
+      if (InterlockedExchange(&GC_mark_mutex_state, 1 /* locked */) != 0) {
+#       ifdef LOCK_STATS
+          (void)AO_fetch_and_add1(&GC_block_count);
+#       endif
+        /* Repeatedly reset the state and wait until acquire the lock.  */
+        while (InterlockedExchange(&GC_mark_mutex_state,
+                                   -1 /* locked_and_has_waiters */) != 0) {
+          if (WaitForSingleObject(mark_mutex_event, INFINITE) == WAIT_FAILED)
+            ABORT("WaitForSingleObject() failed");
+        }
+      }
+#     ifdef LOCK_STATS
+        else {
+          (void)AO_fetch_and_add1(&GC_unlocked_count);
+        }
+#     endif
+
+      GC_ASSERT(GC_mark_lock_holder == NO_THREAD);
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = (unsigned long)GetCurrentThreadId();
+#     endif
+    }
+
+    GC_INNER void GC_release_mark_lock(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == (unsigned long)GetCurrentThreadId());
+#     ifdef GC_ASSERTIONS
+        GC_mark_lock_holder = NO_THREAD;
+#     endif
+        if (InterlockedExchange(&GC_mark_mutex_state, 0 /* unlocked */) < 0)
+        {
+          /* wake a waiter */
+          if (SetEvent(mark_mutex_event) == FALSE)
+            ABORT("SetEvent() failed");
+        }
+    }
+
+    /* In GC_wait_for_reclaim/GC_notify_all_builder() we emulate POSIX    */
+    /* cond_wait/cond_broadcast() primitives with WinAPI Event object     */
+    /* (working in "manual reset" mode).  This works here because         */
+    /* GC_notify_all_builder() is always called holding lock on           */
+    /* mark_mutex and the checked condition (GC_fl_builder_count == 0)    */
+    /* is the only one for which broadcasting on builder_cv is performed. */
+
+    GC_INNER void GC_wait_for_reclaim(void)
+    {
+      GC_ASSERT(builder_cv != 0);
+      for (;;) {
+        GC_acquire_mark_lock();
+        if (GC_fl_builder_count == 0)
+          break;
+        if (ResetEvent(builder_cv) == FALSE)
+          ABORT("ResetEvent() failed");
+        GC_release_mark_lock();
+        if (WaitForSingleObject(builder_cv, INFINITE) == WAIT_FAILED)
+          ABORT("WaitForSingleObject() failed");
+      }
+      GC_release_mark_lock();
+    }
+
+    GC_INNER void GC_notify_all_builder(void)
+    {
+      GC_ASSERT(GC_mark_lock_holder == (unsigned long)GetCurrentThreadId());
+      GC_ASSERT(builder_cv != 0);
+      GC_ASSERT(GC_fl_builder_count == 0);
+      if (SetEvent(builder_cv) == FALSE)
+        ABORT("SetEvent() failed");
+    }
+
+      /* mark_cv is used (for waiting) by a non-helper thread.  */
+
+      GC_INNER void GC_wait_marker(void)
+      {
+        HANDLE event = mark_cv;
+        DWORD thread_id = GetCurrentThreadId();
+        int i = (int)GC_markers - 1;
+        while (i-- > 0) {
+          if (GC_marker_Id[i] == thread_id) {
+            event = GC_marker_cv[i];
+            break;
+          }
+        }
+
+        if (ResetEvent(event) == FALSE)
+          ABORT("ResetEvent() failed");
+        GC_release_mark_lock();
+        if (WaitForSingleObject(event, INFINITE) == WAIT_FAILED)
+          ABORT("WaitForSingleObject() failed");
+        GC_acquire_mark_lock();
+      }
+
+      GC_INNER void GC_notify_all_marker(void)
+      {
+        DWORD thread_id = GetCurrentThreadId();
+        int i = (int)GC_markers - 1;
+        while (i-- > 0) {
+          /* Notify every marker ignoring self (for efficiency).  */
+          if (SetEvent(GC_marker_Id[i] != thread_id ? GC_marker_cv[i] :
+                       mark_cv) == FALSE)
+            ABORT("SetEvent() failed");
+        }
+      }
+
+# endif /* ! GC_PTHREADS_PARAMARK */
+
+#endif /* PARALLEL_MARK */
+
+#ifndef GC_PTHREADS
+
+  /* We have no DllMain to take care of new threads.  Thus we   */
+  /* must properly intercept thread creation.                   */
+
+  typedef struct {
+    LPTHREAD_START_ROUTINE start;
+    LPVOID param;
+  } thread_args;
+
+  STATIC void * GC_CALLBACK GC_win32_start_inner(struct GC_stack_base *sb,
+                                                 void *arg)
+  {
+    void * ret;
+    LPTHREAD_START_ROUTINE start = ((thread_args *)arg)->start;
+    LPVOID param = ((thread_args *)arg)->param;
+
+    GC_register_my_thread(sb); /* This waits for an in-progress GC.     */
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread 0x%lx starting...\n", (long)GetCurrentThreadId());
+#   endif
+
+    GC_free(arg);
+
+    /* Clear the thread entry even if we exit with an exception.        */
+    /* This is probably pointless, since an uncaught exception is       */
+    /* supposed to result in the process being killed.                  */
+#   ifndef __GNUC__
+      __try
+#   endif
+    {
+      ret = (void *)(word)(*start)(param);
+    }
+#   ifndef __GNUC__
+      __finally
+#   endif
+    {
+      GC_unregister_my_thread();
+    }
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread 0x%lx returned from start routine\n",
+                    (long)GetCurrentThreadId());
+#   endif
+    return ret;
+  }
+
+  STATIC DWORD WINAPI GC_win32_start(LPVOID arg)
+  {
+    return (DWORD)(word)GC_call_with_stack_base(GC_win32_start_inner, arg);
+  }
+
+  GC_API HANDLE WINAPI GC_CreateThread(
+                        LPSECURITY_ATTRIBUTES lpThreadAttributes,
+                        GC_WIN32_SIZE_T dwStackSize,
+                        LPTHREAD_START_ROUTINE lpStartAddress,
+                        LPVOID lpParameter, DWORD dwCreationFlags,
+                        LPDWORD lpThreadId)
+  {
+    HANDLE thread_h;
+    thread_args *args;
+
+    if (!parallel_initialized) GC_init_parallel();
+                /* make sure GC is initialized (i.e. main thread is     */
+                /* attached, tls initialized).                          */
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("About to create a thread from 0x%lx\n",
+                    (long)GetCurrentThreadId());
+#   endif
+    if (GC_win32_dll_threads) {
+      return CreateThread(lpThreadAttributes, dwStackSize, lpStartAddress,
+                          lpParameter, dwCreationFlags, lpThreadId);
+    } else {
+      args = GC_malloc_uncollectable(sizeof(thread_args));
+                /* Handed off to and deallocated by child thread.       */
+      if (0 == args) {
+        SetLastError(ERROR_NOT_ENOUGH_MEMORY);
+        return NULL;
+      }
+
+      /* set up thread arguments */
+      args -> start = lpStartAddress;
+      args -> param = lpParameter;
+
+      GC_need_to_lock = TRUE;
+      thread_h = CreateThread(lpThreadAttributes, dwStackSize, GC_win32_start,
+                              args, dwCreationFlags, lpThreadId);
+      if (thread_h == 0) GC_free(args);
+      return thread_h;
+    }
+  }
+
+  GC_API DECLSPEC_NORETURN void WINAPI GC_ExitThread(DWORD dwExitCode)
+  {
+    GC_unregister_my_thread();
+    ExitThread(dwExitCode);
+  }
+
+# ifndef MSWINCE
+
+    GC_API GC_uintptr_t GC_CALL GC_beginthreadex(
+                                  void *security, unsigned stack_size,
+                                  unsigned (__stdcall *start_address)(void *),
+                                  void *arglist, unsigned initflag,
+                                  unsigned *thrdaddr)
+    {
+      GC_uintptr_t thread_h;
+      thread_args *args;
+
+      if (!parallel_initialized) GC_init_parallel();
+                /* make sure GC is initialized (i.e. main thread is     */
+                /* attached, tls initialized).                          */
+#     ifdef DEBUG_THREADS
+        GC_log_printf("About to create a thread from 0x%lx\n",
+                      (long)GetCurrentThreadId());
+#     endif
+
+      if (GC_win32_dll_threads) {
+        return _beginthreadex(security, stack_size, start_address,
+                              arglist, initflag, thrdaddr);
+      } else {
+        args = GC_malloc_uncollectable(sizeof(thread_args));
+                /* Handed off to and deallocated by child thread.       */
+        if (0 == args) {
+          /* MSDN docs say _beginthreadex() returns 0 on error and sets */
+          /* errno to either EAGAIN (too many threads) or EINVAL (the   */
+          /* argument is invalid or the stack size is incorrect), so we */
+          /* set errno to EAGAIN on "not enough memory".                */
+          errno = EAGAIN;
+          return 0;
+        }
+
+        /* set up thread arguments */
+        args -> start = (LPTHREAD_START_ROUTINE)start_address;
+        args -> param = arglist;
+
+        GC_need_to_lock = TRUE;
+        thread_h = _beginthreadex(security, stack_size,
+                        (unsigned (__stdcall *)(void *))GC_win32_start,
+                        args, initflag, thrdaddr);
+        if (thread_h == 0) GC_free(args);
+        return thread_h;
+      }
+    }
+
+    GC_API void GC_CALL GC_endthreadex(unsigned retval)
+    {
+      GC_unregister_my_thread();
+      _endthreadex(retval);
+    }
+
+# endif /* !MSWINCE */
+
+#endif /* !GC_PTHREADS */
+
+#ifdef GC_WINMAIN_REDIRECT
+  /* This might be useful on WinCE.  Shouldn't be used with GC_DLL.     */
+
+# if defined(MSWINCE) && defined(UNDER_CE)
+#   define WINMAIN_LPTSTR LPWSTR
+# else
+#   define WINMAIN_LPTSTR LPSTR
+# endif
+
+  /* This is defined in gc.h.   */
+# undef WinMain
+
+  /* Defined outside GC by an application.      */
+  int WINAPI GC_WinMain(HINSTANCE, HINSTANCE, WINMAIN_LPTSTR, int);
+
+  typedef struct {
+    HINSTANCE hInstance;
+    HINSTANCE hPrevInstance;
+    WINMAIN_LPTSTR lpCmdLine;
+    int nShowCmd;
+  } main_thread_args;
+
+  static DWORD WINAPI main_thread_start(LPVOID arg)
+  {
+    main_thread_args * args = (main_thread_args *) arg;
+    return (DWORD)GC_WinMain(args->hInstance, args->hPrevInstance,
+                             args->lpCmdLine, args->nShowCmd);
+  }
+
+  STATIC void * GC_waitForSingleObjectInfinite(void * handle)
+  {
+    return (void *)(word)WaitForSingleObject((HANDLE)handle, INFINITE);
+  }
+
+# ifndef WINMAIN_THREAD_STACK_SIZE
+#   define WINMAIN_THREAD_STACK_SIZE 0  /* default value */
+# endif
+
+  int WINAPI WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance,
+                     WINMAIN_LPTSTR lpCmdLine, int nShowCmd)
+  {
+    DWORD exit_code = 1;
+
+    main_thread_args args = {
+                hInstance, hPrevInstance, lpCmdLine, nShowCmd
+    };
+    HANDLE thread_h;
+    DWORD thread_id;
+
+    /* initialize everything */
+    GC_INIT();
+
+    /* start the main thread */
+    thread_h = GC_CreateThread(NULL /* lpsa */,
+                        WINMAIN_THREAD_STACK_SIZE /* ignored on WinCE */,
+                        main_thread_start, &args, 0 /* fdwCreate */,
+                        &thread_id);
+
+    if (thread_h != NULL) {
+      if ((DWORD)(word)GC_do_blocking(GC_waitForSingleObjectInfinite,
+                                      (void *)thread_h) == WAIT_FAILED)
+        ABORT("WaitForSingleObject(main_thread) failed");
+      GetExitCodeThread (thread_h, &exit_code);
+      CloseHandle (thread_h);
+    } else {
+      ABORT("GC_CreateThread(main_thread) failed");
+    }
+
+#   ifdef MSWINCE
+      GC_deinit();
+      DeleteCriticalSection(&GC_allocate_ml);
+#   endif
+    return (int) exit_code;
+  }
+
+#endif /* GC_WINMAIN_REDIRECT */
+
+/* Called by GC_init() - we hold the allocation lock.   */
+GC_INNER void GC_thr_init(void)
+{
+  struct GC_stack_base sb;
+# ifdef GC_ASSERTIONS
+    int sb_result;
+# endif
+
+  GC_ASSERT(I_HOLD_LOCK());
+  if (GC_thr_initialized) return;
+  GC_main_thread = GetCurrentThreadId();
+  GC_thr_initialized = TRUE;
+
+# ifdef CAN_HANDLE_FORK
+    /* Prepare for forks if requested.  */
+    if (GC_handle_fork
+        && pthread_atfork(GC_fork_prepare_proc, GC_fork_parent_proc,
+                          GC_fork_child_proc) != 0)
+      ABORT("pthread_atfork failed");
+# endif
+
+  /* Add the initial thread, so we can stop it. */
+# ifdef GC_ASSERTIONS
+    sb_result =
+# endif
+        GC_get_stack_base(&sb);
+  GC_ASSERT(sb_result == GC_SUCCESS);
+
+# if defined(PARALLEL_MARK)
+    /* Set GC_markers. */
+    {
+      char * markers_string = GETENV("GC_MARKERS");
+      if (markers_string != NULL) {
+        GC_markers = atoi(markers_string);
+        if (GC_markers > MAX_MARKERS) {
+          WARN("Limiting number of mark threads\n", 0);
+          GC_markers = MAX_MARKERS;
+        }
+      } else {
+#       ifdef MSWINCE
+          /* There is no GetProcessAffinityMask() in WinCE.     */
+          /* GC_sysinfo is already initialized.                 */
+          GC_markers = GC_sysinfo.dwNumberOfProcessors;
+#       else
+#         ifdef _WIN64
+            DWORD_PTR procMask = 0;
+            DWORD_PTR sysMask;
+#         else
+            DWORD procMask = 0;
+            DWORD sysMask;
+#         endif
+          int ncpu = 0;
+          if (GetProcessAffinityMask(GetCurrentProcess(),
+                                     (void *)&procMask, (void *)&sysMask)
+              && procMask) {
+            do {
+              ncpu++;
+            } while ((procMask &= procMask - 1) != 0);
+          }
+          GC_markers = ncpu;
+#       endif
+#       ifdef GC_MIN_MARKERS
+          /* This is primarily for testing on systems without getenv(). */
+          if (GC_markers < GC_MIN_MARKERS)
+            GC_markers = GC_MIN_MARKERS;
+#       endif
+        if (GC_markers >= MAX_MARKERS)
+          GC_markers = MAX_MARKERS; /* silently limit GC_markers value  */
+      }
+    }
+
+    /* Set GC_parallel. */
+    {
+      if (GC_win32_dll_threads || GC_markers <= 1) {
+        /* Disable parallel marking. */
+        GC_parallel = FALSE;
+        GC_markers = 1;
+      } else {
+#       ifndef GC_PTHREADS_PARAMARK
+          /* Initialize Win32 event objects for parallel marking.       */
+          mark_mutex_event = CreateEvent(NULL /* attrs */,
+                                FALSE /* isManualReset */,
+                                FALSE /* initialState */, NULL /* name */);
+          builder_cv = CreateEvent(NULL /* attrs */,
+                                TRUE /* isManualReset */,
+                                FALSE /* initialState */, NULL /* name */);
+          mark_cv = CreateEvent(NULL /* attrs */, TRUE /* isManualReset */,
+                                FALSE /* initialState */, NULL /* name */);
+          if (mark_mutex_event == (HANDLE)0 || builder_cv == (HANDLE)0
+              || mark_cv == (HANDLE)0)
+            ABORT("CreateEvent() failed");
+#       endif
+        GC_parallel = TRUE;
+        /* Disable true incremental collection, but generational is OK. */
+        GC_time_limit = GC_TIME_UNLIMITED;
+      }
+    }
+# endif /* PARALLEL_MARK */
+
+  GC_ASSERT(0 == GC_lookup_thread_inner(GC_main_thread));
+  GC_register_my_thread_inner(&sb, GC_main_thread);
+
+# ifdef PARALLEL_MARK
+    /* If we are using a parallel marker, actually start helper threads. */
+    if (GC_parallel) start_mark_threads();
+    if (GC_print_stats) {
+      GC_log_printf("Started %ld mark helper threads\n", GC_markers - 1);
+    }
+# endif
+}
+
+#ifdef GC_PTHREADS
+
+  struct start_info {
+    void *(*start_routine)(void *);
+    void *arg;
+    GC_bool detached;
+  };
+
+  GC_API int GC_pthread_join(pthread_t pthread_id, void **retval)
+  {
+    int result;
+    GC_thread t;
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread %p(0x%lx) is joining thread %p\n",
+                    GC_PTHREAD_PTRVAL(pthread_self()),
+                    (long)GetCurrentThreadId(), GC_PTHREAD_PTRVAL(pthread_id));
+#   endif
+
+    if (!parallel_initialized) GC_init_parallel();
+
+    /* Thread being joined might not have registered itself yet. */
+    /* After the join,thread id may have been recycled.          */
+    /* FIXME: It would be better if this worked more like        */
+    /* pthread_support.c.                                        */
+#   ifndef GC_WIN32_PTHREADS
+      while ((t = GC_lookup_pthread(pthread_id)) == 0)
+        Sleep(10);
+#   endif
+
+    result = pthread_join(pthread_id, retval);
+
+#   ifdef GC_WIN32_PTHREADS
+      /* win32_pthreads id are unique */
+      t = GC_lookup_pthread(pthread_id);
+#   endif
+
+    if (!GC_win32_dll_threads) {
+      DCL_LOCK_STATE;
+
+      LOCK();
+      GC_delete_gc_thread(t);
+      UNLOCK();
+    } /* otherwise DllMain handles it.  */
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread %p(0x%lx) completed join with thread %p\n",
+                    GC_PTHREAD_PTRVAL(pthread_self()),
+                    (long)GetCurrentThreadId(), GC_PTHREAD_PTRVAL(pthread_id));
+#   endif
+    return result;
+  }
+
+  /* Cygwin-pthreads calls CreateThread internally, but it's not easily */
+  /* interceptable by us..., so intercept pthread_create instead.       */
+  GC_API int GC_pthread_create(pthread_t *new_thread,
+                               GC_PTHREAD_CREATE_CONST pthread_attr_t *attr,
+                               void *(*start_routine)(void *), void *arg)
+  {
+    if (!parallel_initialized) GC_init_parallel();
+             /* make sure GC is initialized (i.e. main thread is attached) */
+    if (GC_win32_dll_threads) {
+      return pthread_create(new_thread, attr, start_routine, arg);
+    } else {
+      int result;
+      struct start_info * si;
+
+      /* This is otherwise saved only in an area mmapped by the thread  */
+      /* library, which isn't visible to the collector.                 */
+      si = GC_malloc_uncollectable(sizeof(struct start_info));
+      if (0 == si) return(EAGAIN);
+
+      si -> start_routine = start_routine;
+      si -> arg = arg;
+      if (attr != 0 &&
+          pthread_attr_getdetachstate(attr, &si->detached)
+          == PTHREAD_CREATE_DETACHED) {
+        si->detached = TRUE;
+      }
+
+#     ifdef DEBUG_THREADS
+        GC_log_printf("About to create a thread from %p(0x%lx)\n",
+                      GC_PTHREAD_PTRVAL(pthread_self()),
+                      (long)GetCurrentThreadId());
+#     endif
+      GC_need_to_lock = TRUE;
+      result = pthread_create(new_thread, attr, GC_pthread_start, si);
+
+      if (result) { /* failure */
+          GC_free(si);
+      }
+      return(result);
+    }
+  }
+
+  STATIC void * GC_CALLBACK GC_pthread_start_inner(struct GC_stack_base *sb,
+                                                   void * arg)
+  {
+    struct start_info * si = arg;
+    void * result;
+    void *(*start)(void *);
+    void *start_arg;
+    DWORD thread_id = GetCurrentThreadId();
+    pthread_t pthread_id = pthread_self();
+    GC_thread me;
+    DCL_LOCK_STATE;
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread %p(0x%x) starting...\n",
+                    GC_PTHREAD_PTRVAL(pthread_id), (int)thread_id);
+#   endif
+
+    GC_ASSERT(!GC_win32_dll_threads);
+    /* If a GC occurs before the thread is registered, that GC will     */
+    /* ignore this thread.  That's fine, since it will block trying to  */
+    /* acquire the allocation lock, and won't yet hold interesting      */
+    /* pointers.                                                        */
+    LOCK();
+    /* We register the thread here instead of in the parent, so that    */
+    /* we don't need to hold the allocation lock during pthread_create. */
+    me = GC_register_my_thread_inner(sb, thread_id);
+    SET_PTHREAD_MAP_CACHE(pthread_id, thread_id);
+    me -> pthread_id = pthread_id;
+    if (si->detached) me -> flags |= DETACHED;
+    UNLOCK();
+
+    start = si -> start_routine;
+    start_arg = si -> arg;
+
+    GC_free(si); /* was allocated uncollectible */
+
+    pthread_cleanup_push(GC_thread_exit_proc, (void *)me);
+    result = (*start)(start_arg);
+    me -> status = result;
+    pthread_cleanup_pop(1);
+
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread %p(0x%x) returned from start routine\n",
+                    GC_PTHREAD_PTRVAL(pthread_id), (int)thread_id);
+#   endif
+    return(result);
+  }
+
+  STATIC void * GC_pthread_start(void * arg)
+  {
+    return GC_call_with_stack_base(GC_pthread_start_inner, arg);
+  }
+
+  STATIC void GC_thread_exit_proc(void *arg)
+  {
+    GC_thread me = (GC_thread)arg;
+    DCL_LOCK_STATE;
+
+    GC_ASSERT(!GC_win32_dll_threads);
+#   ifdef DEBUG_THREADS
+      GC_log_printf("thread %p(0x%lx) called pthread_exit()\n",
+                    GC_PTHREAD_PTRVAL(pthread_self()),
+                    (long)GetCurrentThreadId());
+#   endif
+
+    LOCK();
+    GC_wait_for_gc_completion(FALSE);
+#   if defined(THREAD_LOCAL_ALLOC)
+      GC_destroy_thread_local(&(me->tlfs));
+#   endif
+    if (me -> flags & DETACHED) {
+      GC_delete_thread(GetCurrentThreadId());
+    } else {
+      /* deallocate it as part of join */
+      me -> flags |= FINISHED;
+    }
+    UNLOCK();
+  }
+
+# ifndef GC_NO_PTHREAD_SIGMASK
+    /* Win32 pthread does not support sigmask.  */
+    /* So, nothing required here...             */
+    GC_API int GC_pthread_sigmask(int how, const sigset_t *set,
+                                  sigset_t *oset)
+    {
+      if (!parallel_initialized) GC_init_parallel();
+      return pthread_sigmask(how, set, oset);
+    }
+# endif /* !GC_NO_PTHREAD_SIGMASK */
+
+  GC_API int GC_pthread_detach(pthread_t thread)
+  {
+    int result;
+    GC_thread t;
+    DCL_LOCK_STATE;
+
+    if (!parallel_initialized) GC_init_parallel();
+    LOCK();
+    t = GC_lookup_pthread(thread);
+    UNLOCK();
+    result = pthread_detach(thread);
+    if (result == 0) {
+      LOCK();
+      t -> flags |= DETACHED;
+      /* Here the pthread thread id may have been recycled. */
+      if ((t -> flags & FINISHED) != 0) {
+        GC_delete_gc_thread(t);
+      }
+      UNLOCK();
+    }
+    return result;
+  }
+
+#else /* !GC_PTHREADS */
+
+# ifndef GC_NO_THREADS_DISCOVERY
+    /* We avoid acquiring locks here, since this doesn't seem to be     */
+    /* preemptible.  This may run with an uninitialized collector, in   */
+    /* which case we don't do much.  This implies that no threads other */
+    /* than the main one should be created with an uninitialized        */
+    /* collector.  (The alternative of initializing the collector here  */
+    /* seems dangerous, since DllMain is limited in what it can do.)    */
+
+    /*ARGSUSED*/
+    BOOL WINAPI DllMain(HINSTANCE inst, ULONG reason, LPVOID reserved)
+    {
+      struct GC_stack_base sb;
+      DWORD thread_id;
+#     ifdef GC_ASSERTIONS
+        int sb_result;
+#     endif
+      static int entry_count = 0;
+
+      if (!GC_win32_dll_threads && parallel_initialized) return TRUE;
+
+      switch (reason) {
+       case DLL_THREAD_ATTACH:
+#       ifdef PARALLEL_MARK
+          /* Don't register marker threads. */
+          if (GC_parallel) {
+            /* We could reach here only if parallel_initialized == FALSE. */
+            break;
+          }
+#       endif
+        GC_ASSERT(entry_count == 0 || parallel_initialized);
+        ++entry_count; /* and fall through: */
+       case DLL_PROCESS_ATTACH:
+        /* This may run with the collector uninitialized. */
+        thread_id = GetCurrentThreadId();
+        if (parallel_initialized && GC_main_thread != thread_id) {
+          /* Don't lock here.   */
+#         ifdef GC_ASSERTIONS
+            sb_result =
+#         endif
+              GC_get_stack_base(&sb);
+          GC_ASSERT(sb_result == GC_SUCCESS);
+#         if defined(THREAD_LOCAL_ALLOC) || defined(PARALLEL_MARK)
+            ABORT("Cannot initialize thread local cache from DllMain");
+#         endif
+          GC_register_my_thread_inner(&sb, thread_id);
+        } /* o.w. we already did it during GC_thr_init, called by GC_init */
+        break;
+
+       case DLL_THREAD_DETACH:
+        /* We are hopefully running in the context of the exiting thread. */
+        GC_ASSERT(parallel_initialized);
+        if (GC_win32_dll_threads) {
+          GC_delete_thread(GetCurrentThreadId());
+        }
+        break;
+
+       case DLL_PROCESS_DETACH:
+        if (GC_win32_dll_threads) {
+          int i;
+          int my_max = (int)GC_get_max_thread_index();
+
+          for (i = 0; i <= my_max; ++i) {
+           if (AO_load(&(dll_thread_table[i].tm.in_use)))
+             GC_delete_gc_thread(dll_thread_table + i);
+          }
+          GC_deinit();
+          DeleteCriticalSection(&GC_allocate_ml);
+        }
+        break;
+      }
+      return TRUE;
+    }
+# endif /* !GC_NO_THREADS_DISCOVERY */
+
+#endif /* !GC_PTHREADS */
+
+/* Perform all initializations, including those that    */
+/* may require allocation.                              */
+/* Called without allocation lock.                      */
+/* Must be called before a second thread is created.    */
+GC_INNER void GC_init_parallel(void)
+{
+# if defined(THREAD_LOCAL_ALLOC)
+    GC_thread me;
+    DCL_LOCK_STATE;
+# endif
+
+  if (parallel_initialized) return;
+  parallel_initialized = TRUE;
+  /* GC_init() calls us back, so set flag first.      */
+
+  if (!GC_is_initialized) GC_init();
+  if (GC_win32_dll_threads) {
+    GC_need_to_lock = TRUE;
+        /* Cannot intercept thread creation.  Hence we don't know if    */
+        /* other threads exist.  However, client is not allowed to      */
+        /* create other threads before collector initialization.        */
+        /* Thus it's OK not to lock before this.                        */
+  }
+  /* Initialize thread local free lists if used.        */
+# if defined(THREAD_LOCAL_ALLOC)
+    LOCK();
+    me = GC_lookup_thread_inner(GetCurrentThreadId());
+    CHECK_LOOKUP_MY_THREAD(me);
+    GC_init_thread_local(&me->tlfs);
+    UNLOCK();
+# endif
+}
+
+#if defined(USE_PTHREAD_LOCKS)
+  /* Support for pthread locking code.          */
+  /* Pthread_mutex_try_lock may not win here,   */
+  /* due to builtin support for spinning first? */
+
+  GC_INNER volatile GC_bool GC_collecting = 0;
+                        /* A hint that we're in the collector and       */
+                        /* holding the allocation lock for an           */
+                        /* extended period.                             */
+
+  GC_INNER void GC_lock(void)
+  {
+    pthread_mutex_lock(&GC_allocate_ml);
+  }
+#endif /* USE_PTHREAD_LOCKS */
+
+#if defined(THREAD_LOCAL_ALLOC)
+
+  /* Add thread-local allocation support.  VC++ uses __declspec(thread).  */
+
+  /* We must explicitly mark ptrfree and gcj free lists, since the free   */
+  /* list links wouldn't otherwise be found.  We also set them in the     */
+  /* normal free lists, since that involves touching less memory than if  */
+  /* we scanned them normally.                                            */
+  GC_INNER void GC_mark_thread_local_free_lists(void)
+  {
+    int i;
+    GC_thread p;
+
+    for (i = 0; i < THREAD_TABLE_SZ; ++i) {
+      for (p = GC_threads[i]; 0 != p; p = p -> tm.next) {
+        if (!KNOWN_FINISHED(p)) {
+#         ifdef DEBUG_THREADS
+            GC_log_printf("Marking thread locals for 0x%x\n", (int)p -> id);
+#         endif
+          GC_mark_thread_local_fls_for(&(p->tlfs));
+        }
+      }
+    }
+  }
+
+# if defined(GC_ASSERTIONS)
+    void GC_check_tls_for(GC_tlfs p);
+#   if defined(USE_CUSTOM_SPECIFIC)
+      void GC_check_tsd_marks(tsd *key);
+#   endif
+    /* Check that all thread-local free-lists are completely marked.    */
+    /* also check that thread-specific-data structures are marked.      */
+    void GC_check_tls(void)
+    {
+        int i;
+        GC_thread p;
+
+        for (i = 0; i < THREAD_TABLE_SZ; ++i) {
+          for (p = GC_threads[i]; 0 != p; p = p -> tm.next) {
+            if (!KNOWN_FINISHED(p))
+              GC_check_tls_for(&(p->tlfs));
+          }
+        }
+#       if defined(USE_CUSTOM_SPECIFIC)
+          if (GC_thread_key != 0)
+            GC_check_tsd_marks(GC_thread_key);
+#       endif
+    }
+# endif /* GC_ASSERTIONS */
+
+#endif /* THREAD_LOCAL_ALLOC ... */
+
+# ifndef GC_NO_THREAD_REDIRECTS
+    /* Restore thread calls redirection.        */
+#   define CreateThread GC_CreateThread
+#   define ExitThread GC_ExitThread
+#   undef _beginthreadex
+#   define _beginthreadex GC_beginthreadex
+#   undef _endthreadex
+#   define _endthreadex GC_endthreadex
+# endif /* !GC_NO_THREAD_REDIRECTS */
+
+#endif /* GC_WIN32_THREADS */
diff --git a/src/nautilus/naut_string.c b/src/nautilus/naut_string.c
index d6395b0..e7615eb 100644
--- a/src/nautilus/naut_string.c
+++ b/src/nautilus/naut_string.c
@@ -8,7 +8,7 @@
  * led by Sandia National Laboratories that includes several national 
  * laboratories and universities. You can find out more at:
  * http://www.v3vee.org  and
- * http://xtack.sandia.gov/hobbes
+ * http://xstack.sandia.gov/hobbes
  *
  * Copyright (c) 2015, Kyle C. Hale <kh@u.northwestern.edu>
  * Copyright (c) 2015, The V3VEE Project  <http://www.v3vee.org> 
@@ -426,6 +426,13 @@ strtoi (const char * nptr, char ** endptr)
     return ret;
 }
 
+long int
+atol (const char *nptr)
+{
+  return strtol (nptr, (char **) NULL, 10); 
+}
+
+
 extern long simple_strtol(const char*, char**, unsigned int);
 long 
 strtol (const char * str, char ** endptr, int base)
diff --git a/src/nautilus/scheduler.c b/src/nautilus/scheduler.c
index 9dd6ca4..1a3f59d 100644
--- a/src/nautilus/scheduler.c
+++ b/src/nautilus/scheduler.c
@@ -644,6 +644,24 @@ void nk_sched_dump_threads(int cpu)
     GLOBAL_UNLOCK();
 }
 
+void nk_sched_map_threads(int cpu, void (func)(struct nk_thread *t, void *state), void *state)
+{
+    GLOBAL_LOCK_CONF;
+   
+    GLOBAL_LOCK();
+
+    rt_node *n = global_sched_state.thread_list->head;
+    while (n != NULL) {
+	if (cpu==-1 || n->thread->thread->current_cpu==cpu) { 
+	    func(n->thread->thread,state);
+	}
+        n = n->next;
+    }
+
+    GLOBAL_UNLOCK();
+}
+
+
 struct thread_query {
     uint64_t     tid;
     nk_thread_t *thread;
@@ -709,8 +727,11 @@ void nk_sched_reap(int uncond)
     GLOBAL_UNLOCK();
 
     // Now reap
-    for (i=0;i<reap_count;i++) { 
-	nk_thread_destroy(reap_pool[i]->thread);
+    if (reap_count>0) { 
+	// reverse order to potentially improve frees
+	for (i=reap_count;i>0;i--) { 
+	    nk_thread_destroy(reap_pool[i-1]->thread);
+	}
     }
 
     // done with reaping - another core can now go
diff --git a/src/nautilus/shell.c b/src/nautilus/shell.c
index dced056..aed30da 100644
--- a/src/nautilus/shell.c
+++ b/src/nautilus/shell.c
@@ -48,6 +48,10 @@
 #include <nautilus/isocore.h>
 #endif
 
+#ifdef NAUT_CONFIG_TEST_BDWGC
+#include <gc/bdwgc/bdwgc.h>
+#endif
+
 #define MAX_CMD 80
 
 struct burner_args {
@@ -467,6 +471,13 @@ static int handle_test(char *buf)
 	return test_threads();
     }
 
+#ifdef NAUT_CONFIG_TEST_BDWGC
+    if (!strncasecmp(what,"bdwgc",5)) { 
+	nk_vc_printf("Testing BDWGC garbage collector\n");
+	return nk_gc_bdwgc_test();
+    }
+#endif
+
  dunno:
     nk_vc_printf("Unknown test request\n");
     return -1;
@@ -668,7 +679,7 @@ static int handle_cmd(char *buf, int n)
     nk_vc_printf("blktest dev r|w start count\n");
     nk_vc_printf("blktest dev r|w start count\n");
     nk_vc_printf("isotest\n");
-    nk_vc_printf("test threads|...\n");
+    nk_vc_printf("test threads|bdwgc|...\n");
     nk_vc_printf("vm name [embedded image]\n");
     nk_vc_printf("run path\n");
     return 0;
diff --git a/src/nautilus/thread.c b/src/nautilus/thread.c
index 767b394..4b10a18 100644
--- a/src/nautilus/thread.c
+++ b/src/nautilus/thread.c
@@ -34,6 +34,9 @@
 #include <nautilus/errno.h>
 #include <nautilus/mm.h>
 
+#ifdef NAUT_CONFIG_ENABLE_BDWGC
+#include <gc/bdwgc/bdwgc.h>
+#endif
 
 extern uint8_t malloc_cpus_ready;
 
@@ -194,6 +197,13 @@ _nk_thread_init (nk_thread_t * t,
 	return -EINVAL;
     }
 
+#ifdef NAUT_CONFIG_ENABLE_BDWGC
+    if (!(t->gc_state = nk_gc_bdwgc_thread_state_init(t))) {
+	THREAD_ERROR("Failed to initialize GC state for thread\n");
+	return -1;
+    }
+#endif
+
     t->waitq = nk_thread_queue_create();
 
     if (!t->waitq) {
@@ -619,6 +629,11 @@ nk_thread_destroy (nk_thread_id_t t)
     nk_thread_queue_destroy(thethread->waitq);
 
     nk_sched_thread_state_deinit(thethread);
+
+#ifdef NAUT_CONFIG_ENABLE_BDWGC
+    nk_gc_bdwgc_thread_state_deinit(thethread);
+#endif
+
     free(thethread->stack);
     free(thethread);
     
-- 
1.9.1

